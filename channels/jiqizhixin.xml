<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>特斯拉FSD首次横穿美国，Model3实现1万英里零干预，马斯克预言兑现了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 02 Jan 2026 00:59:47 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-02-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-02-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/682d6fe4-8a4f-4aff-9920-7a4f0436d522/1767286609265.png" style="width: 700%;" class="fr-fic fr-dib"&gt;在 2025 年最后一天，一个名为 David Moss 的小哥完成了一项壮举：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;成功实现世界上首次美国西海岸到东海岸的全自动驾驶之旅，同时也成为世界上第一个连续驾驶特斯拉 FSD 行驶 10000 英里的人。&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuKQ8wKshRI3rhsBkpp8NRLaUiaFUWNFA0nbCvmBsNVZSHVsZIDe7W5Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.272045028142589" data-type="png" data-w="1066" data-width="1066" data-height="1356" data-backw="578" data-backh="735" data-imgfileid="503526482" data-aistatus="1" data-original-style="width:100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/45e45b75-e858-431a-8ddd-3cb9a86685ff/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;他开着一辆搭载 FSD V14.2 的 2025 款 Model 3，从洛杉矶的 Tesla Diner 出发，历时 2 天 20 小时，走了 2732.4 英里（约 4400 公里），最终抵达南卡罗来纳州的 Myrtle Beach。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu9pRjbkzBqXyBtict3cLNBmNYUxY3wCKoaexQBnG4RiajUGWmRYdUuAKA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.75" data-type="png" data-w="680" data-width="680" data-height="510" data-backw="578" data-backh="434" data-imgfileid="503526483" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e0b1116a-d59f-406e-9ba7-4f484381dc06/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 一起完成壮举的搭档&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;整个过程零干预，甚至包括所有停车和在 Tesla 超级充电站充电的环节都没有人为接管。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu78r6Ic6vqgrT8ryjzOqsRqKtOqtFib31fXlKz7LiantcJc2RdEadWuEA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.562962962962963" data-type="png" data-w="1080" data-width="1200" data-height="675" data-backw="578" data-backh="325" data-imgfileid="503526485" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9dbf3409-c9aa-4457-ba58-98c6476eb92c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;他特别强调这是完全靠 FSD 完成的，并且数据可以通过 Whole Mars 的 FSD 数据库公开验证。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuE5aenf2Zkia2KEPvHrXWhFVVr6qLhAt1pWFDHGUHKsZwf1r9dRls5yw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.6537037037037037" data-type="png" data-w="1080" data-width="1725" data-height="1128" data-backw="578" data-backh="378" data-imgfileid="503526486" data-aistatus="1" data-original-style="width: 100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5dd57762-0dcf-4743-995e-9570291e9e69/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;你可在这里追踪他的 FSD 里程 https://fsddb.com/profile/DavidMoss，数据显示，他使用 FSD 在 Model 3 上行驶了 1 万多英里，从未亲自驾驶过车辆。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;Moss 在一张地图上详细标注了横跨美国的路线图：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-backh="254" data-backw="578" data-height="899" data-imgfileid="503526488" data-ratio="0.4388888888888889" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuaVXZ8VOSasrCRhk1RpqWwRUvYhFVosEqG8mtia1FmD8se2T7L0gXyWA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" data-width="2048" data-original-style="width:100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/7b99369e-979b-4d7b-9bad-5f00ceea90ba/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;并列出了大约 30 个超级充电站的几乎完整的停留信息：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu6uDN5ngLW1ybMfjBluNzNoYY5XaE428n5UnjKWalyb1c283Op8RZOw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="2.030456852791878" data-type="png" data-w="591" data-width="591" data-height="1200" data-backw="578" data-backh="1174" data-imgfileid="503526489" data-aistatus="1" data-original-style="width: 100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/1abaf619-2578-4be4-b025-7c0a7c82578f/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;有网友在评论区询问：过程中是否出现过险情，Moss 表示完全没有。即使对于人类驾驶员来说，这也是不容易实现的。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuNF6YhSv7wBNqBmVuksLRotdIwQdXUWPaO63CIqcA9Myx5Ga1mMNRHQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.3133208255159475" data-type="png" data-w="1066" data-width="1066" data-height="334" data-backw="578" data-backh="181" data-imgfileid="503526490" data-aistatus="1" data-original-style="width: 100%;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/7000fae3-5886-4209-a43d-56e0ca70138b/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;马斯克看后直接转发并回复「Cool。」&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIunkrzLIGiaktUCYiczjA5lLsX2JSTwCNTcWJwQiaweBL7vQSGh4sFibzxmg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.223463687150838" data-type="png" data-w="1074" data-width="1074" data-height="1314" data-backw="578" data-backh="707" data-imgfileid="503526491" data-aistatus="1" data-original-style="width:100%;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/4ac9fd7d-9399-4434-b0d3-daa05c4c4712/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;特斯拉 AI 主管 Ashok Elluswamy 称赞道：「这是世界上首次全自动海岸到海岸驾驶，感谢 David Moss！」&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu83KGFH2tfSIpOUv9wiadZIxUBonbeZaJEWEZYRDR6qjrF0icxtKokjAw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.20610687022900764" data-type="png" data-w="1048" data-width="1048" data-height="216" data-backw="578" data-backh="119" data-imgfileid="503526492" data-aistatus="1" data-original-style="width: 100%;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/0709dd22-a7f5-42ea-9481-1604fc595282/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;马斯克预言终兑现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Andrej Karpathy （特斯拉自动驾驶团队前领导、OpenAI 创始成员）也在第一时间发来贺电。他说，横跨美国的 coast-to-coast 自动驾驶，一直是 Autopilot 团队从立项之初就设定的目标。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;事实上，马斯克最初预计，这一里程碑可以在 2017 年底实现。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;「为了完成它，团队投入了大量时间。无数个深夜，大家围坐在一起进行马拉松式的片段复盘，一条条查看自动接管的录像：先分流、再归类、再拆解问题，逐项规划项目，把每一个缺口补上，目标只有一个 &amp;mdash;&amp;mdash; 把干预次数降到零。」&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuKctwdRCRtVvstcWeTsEr3zs4fnw3LFf9EmZloc4HiaXrFxBsVVQJKYg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.592436974789916" data-type="png" data-w="714" data-width="714" data-height="423" data-backw="578" data-backh="342" data-imgfileid="503526493" data-aistatus="1" data-original-style="width: 100%;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/1f97343a-6e65-4f99-ae03-5d8c5072748f/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;FSD v14.2（Full Self-Driving v14.2）是特斯拉在 2025 年底推出的一次关键自动驾驶软件更新，属于其 FSD 路线的最新进化版本，也允许用户跟踪他们在自动驾驶模式下行驶了多少英里。&lt;/p&gt;&lt;p&gt;相较此前的 v14.1.x，这一版本在驾驶表现、感知能力和决策逻辑上都有明显强化。&lt;/p&gt;&lt;p&gt;从特斯拉官方和社区的普遍反馈来看，FSD v14.2 明显朝着「更像人开车」演进。它在感知和决策上更稳定，对复杂路口、无保护左转、车道博弈的处理更果断，整体驾驶节奏更连贯。&lt;/p&gt;&lt;p&gt;虽然从定义上看，它依然是需要驾驶员监督的 L2 级系统，但在真实道路中的完成度已经显著提升。&lt;/p&gt;&lt;p&gt;小鹏汽车 CEO 何小鹏曾这样评价特斯拉 FSD v14.2：「我最近在美国花了四个小时试驾了特斯拉的 FSD V14.2。一句话总结：如果 FSD 在 2024 年仅仅是『不错的 L2 级驾驶辅助』，那么这个最新版本让我相信 L4 级自动驾驶指日可待。」&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu2zn4jOSoI5w2yD2wwXtSgJjtJaPRyolybstuyhUbR0mZ4UGH1POBXw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.33195020746887965" data-type="png" data-w="723" data-width="723" data-height="240" data-backw="578" data-backh="192" data-imgfileid="503526494" data-aistatus="1" data-original-style="width: 100%;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/8d22b21b-a8aa-4140-bb58-e59bf92b19e5/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;特斯拉 vs Waymo：两条路线的较量&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;消息刷屏后，有 X 友 Yuchen Jin 向 Andrej Karpathy 抛出了一个尖锐的问题：你现在还认为 Waymo 的软件更好吗，还是特斯拉已经领先了？&lt;/p&gt;&lt;p&gt;要知道，Karpathy 过去的判断是 &amp;mdash;&amp;mdash;Waymo 有硬件问题，特斯拉有软件问题。&lt;/p&gt;&lt;p&gt;他回复说，如今两者都堪称「完美驾驶」，虽存在差异，但要么需要时间才能显现，要么只能在大规模车辆数据中被统计出来。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuJwN93J78IPAwbj14F0ZqU1HdoFnIALibx1LvWD43s4wK7lutumTBNuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.975925925925926" data-type="png" data-w="1080" data-width="1310" data-height="1278" data-backw="578" data-backh="564" data-imgfileid="503526495" data-aistatus="1" data-original-style="width: 100%;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/91197589-2210-44bd-aa76-2e23d81619ad/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;不久前发生在旧金山的一次停电，或许提供了一个现实注脚。停电导致 Waymo 的服务大面积中断，而特斯拉 FSD 基本未受影响。&lt;/p&gt;&lt;p&gt;网友分析指出，原因在于两条技术路线的根本差异。&lt;/p&gt;&lt;p&gt;Waymo 采用高度模块化的系统，依赖高清地图、激光雷达、多传感器融合、5G 网络以及多套神经网络协同工作。在一切条件正常时，它表现非常出色。但只要其中一个关键模块失效，系统就会迅速退化。&lt;/p&gt;&lt;p&gt;当交通信号灯断电后，现实世界已经发生变化，而 HD 地图无法即时反映，车辆无法确认状态，只能回退到最保守的策略 &amp;mdash;&amp;mdash; 直接停车（brick 模式），车辆还失去了与远程人工接管员的连接，进一步放大了系统的脆弱性。&lt;/p&gt;&lt;p&gt;而特斯拉 FSD 走的是端到端：一个巨大的神经网络，直接把摄像头的像素输入，转换为转向和制动控制。这正是 Andrej 所提出的 Software 2.0 思想 &amp;mdash;&amp;mdash; 不再为每一种场景手写 C++ 逻辑，而是用数十亿英里的人类驾驶数据去训练模型，代码本身就是模型权重。因此，它的驾驶方式更像人类。&lt;/p&gt;&lt;p&gt;一些观察者认为，如今真正面临软件瓶颈的，反而是 Waymo。模块化架构在规模化与依赖关系上，可能是一种长期的陷阱。长期来看，赢家会是特斯拉 FSD。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuZsbfqnDeuUq5HL6M1ReypCUSI39ZqaDdgY8N7QJRXZfD5PUQhLZhEw/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.875" data-type="png" data-w="720" data-width="720" data-height="630" data-backw="578" data-backh="506" data-imgfileid="503526496" data-aistatus="1" data-original-style="width: 100%;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/77eea60a-2550-4499-bd6b-c012c50893f5/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;迈向真正的自动驾驶&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;122 年前，汽车先驱 Horatio Jackson 和 Sewall Crocker （以及他们在途中收养的斗牛犬）曾从旧金山驾车前往纽约，用整整 63 天 横穿美国，只为证明汽车并非昙花一现的新奇玩意。&lt;/p&gt;&lt;p&gt;他们因此成为历史上第一批驾车横跨美国的人类 &amp;mdash;&amp;mdash; 以及那只狗。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuTrNbgBJzOTnvvpyQEnbmd9pqtB10jJjibO7Xkc3lBjDr9EuumTexh7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.9925690021231423" data-type="png" data-w="942" data-width="942" data-height="935" data-backw="578" data-backh="574" data-imgfileid="503526498" data-aistatus="1" data-original-style="width:100%;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/ddd85ba9-9741-4498-955e-359e7873406a/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;今天的这场壮举，或许只是自动驾驶迈出的一小步，但对「自动驾驶即机器人」而言，却可能是一记关键跳跃。&lt;/p&gt;&lt;p&gt;马斯克正在持续加码无人驾驶。6 月，特斯拉已在德克萨斯州奥斯汀推出一项有限规模的机器人出租车服务，使用的是搭载 FSD 的改装版 Model Y。更值得注意的是，马斯克近期透露，这批车辆已进入前排不再配备安全监控员的测试阶段。&lt;/p&gt;&lt;p&gt;从「有人盯着的自动驾驶」，到「系统自己负责」，这一变化幅度貌似不大，却触及了自动驾驶从辅助工具走向真正自主体的临界点。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/DavidMoss/status/2006255297212358686&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/karpathy/status/2006436622909452501?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/Yuchenj_UW/status/2003173409665212629?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/SawyerMerritt/status/2006042728983908626?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/SawyerMerritt/status/2006042725385195766?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/elonmusk/status/2006290674761736346?s=20&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>谷歌三年逆袭：草蛇灰线，伏脉千里</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 02 Jan 2026 00:54:23 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-02-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-02-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ef7460a0-ddc5-49fa-8638-fb56431f1e5e/1767286245200.png" style="width: 700%;" class="fr-fic fr-dib"&gt;2025 年 12 月 1 日，硅谷再次拉响了「红色警报」。&lt;/p&gt;&lt;p&gt;不过这一次，发出警报的不是谷歌，而是 OpenAI。&lt;/p&gt;&lt;p&gt;当 OpenAI CEO 萨姆・奥特曼在内部备忘录中宣布进入最高级别的「红色警报」状态，暂停广告、医疗 AI 智能体等所有非核心项目，将全部资源集中于改进 ChatGPT 时，整个科技圈都意识到风向变了。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LFY6B5ibCGjeB6f4pImWPM4iakYxcXDRhSoCTD6IgGjCMYcCo3bEa3pnw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5736568457538995" data-type="png" data-w="577" data-backw="562" data-backh="322" data-imgfileid="503526291" data-aistatus="1" data-original-style="width:100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e448e29d-f4ff-4189-8a9d-425c866fbed3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;三年前的同一幕还历历在目。&lt;/p&gt;&lt;p&gt;2022 年 11 月 30 日，ChatGPT 横空出世，短短五天用户突破百万，两个月突破一亿。谷歌内部迅速拉响「红色警报」，CEO 桑达尔・皮查伊甚至召回了已「隐退」多年的两位创始人拉里・佩奇和谢尔盖・布林参与高层会议。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LSo6fDoKgVEI78RylKP0vibICsVC281cVkGpGGVC1NtzPvQuJgDgDmiag/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" alt="Sergey Brin and Larry Page" data-ratio="0.75" data-type="png" data-w="700" data-backw="562" data-backh="422" data-imgfileid="503526293" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/a8d4839c-88c4-4ab5-843e-098c9d4389ce/640.png" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;彼时的谷歌，在自己最擅长的 AI 领域，被一家成立仅七年的创业公司杀了个措手不及。&lt;/p&gt;&lt;p&gt;在一段低谷时期，谷歌员工们聚集在走廊里，公开表达对谷歌可能沦为下一个雅虎的担忧。&lt;/p&gt;&lt;p&gt;而今，剧情反转。&lt;/p&gt;&lt;p&gt;谷歌推出 Gemini 3 大语言模型、Nano Banana 图像生成模型、Veo3 视频生成模型以及 TPU 芯片，在各个战线全面开花，重夺技术制高点。&lt;/p&gt;&lt;p&gt;短短三年时间，从被动挨打到主动进攻，谷歌的逆袭绝非偶然。&lt;/p&gt;&lt;p&gt;攻守易形，谷歌究竟做对了什么？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;内部反思：从慢公司到快公司&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2022 年 12 月，ChatGPT 的用户数在 5 天内突破百万，谷歌召开了一场不寻常的全体员工大会。&lt;/p&gt;&lt;p&gt;会议气氛紧张而激烈。&lt;/p&gt;&lt;p&gt;一位员工提出了最受关注的问题：「这对谷歌来说是不是一个错失的机会？考虑到我们已经拥有 LaMDA 很长时间了。」&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个问题获得了大量员工的支持，直指核心痛点：谷歌明明手握先进技术，却眼睁睁看着竞争对手率先占领市场。&lt;/p&gt;&lt;p&gt;谷歌 AI 负责人杰夫・迪恩坦承，谷歌面临着比小型创业公司大得多的「声誉风险」，因此行动「比小型创业公司更加保守」。&lt;/p&gt;&lt;p&gt;作为全球搜索引擎的霸主，谷歌不能容忍错误信息损害其品牌，但这种过度的风险厌恶，恰恰导致了早期的被动局面。&lt;/p&gt;&lt;p&gt;这场会议之后，谷歌的行动也麻利起来，要求「一百天内打造一个能与 ChatGPT 抗衡的产品。」&lt;/p&gt;&lt;p&gt;一份内部备忘录写道：「由于 ChatGPT 的出现，LaMDA 团队被要求优先开发对 ChatGPT 的回应。在短期内，这优先于其他项目。」&lt;/p&gt;&lt;p&gt;谷歌内部开始密集测试 Bard 和其他聊天机器人。&lt;/p&gt;&lt;p&gt;Bard 可以在 LaMDA 的基础上进行开发，但必须更新其知识库并引入新的安全措施。谷歌的基础设施团队将最优秀的员工调去释放更多服务器，以完成所有这些调整。他们几乎耗尽了公司一些数据中心的电力，冒着设备烧毁的风险，同时迅速设计新工具，以更安全地应对不断增长的电力需求。&lt;/p&gt;&lt;p&gt;尽管新的计算能力陆续上线，但 Bard 仍会产生「幻觉」，并以不恰当或冒犯性的方式回应。&lt;/p&gt;&lt;p&gt;面对百日期限，谷歌能做的最好的事情就是尽可能多地发现和修复错误。一些通常专注于处理虐待儿童图像等问题的承包商，转而测试 Bard。&lt;/p&gt;&lt;p&gt;以往推出 AI 项目前，谷歌的大约十几个人的负责任创新团队会花几个月独立测试系统，检查是否存在不良偏见和其他缺陷。但对于 Bard，这个审查过程被压缩。&lt;/p&gt;&lt;p&gt;新模型和功能发布速度太快，审查人员即使周末和晚上都加班也跟不上。当时有人提出推迟 Bard 发布，意见被否决了。&lt;/p&gt;&lt;p&gt;2023 年 2 月 8 日，谷歌举行 Bard 人工智能演示直播。在演示视频中，Bard 回答詹姆斯・韦伯太空望远镜时出现事实性错误，导致 Alphabet 股价下跌近 9%， 市值蒸发了约 1000 亿美元。&lt;/p&gt;&lt;p&gt;谁也没想到如此微不足道的事情会导致股价暴跌，毕竟 ChatGPT 也会犯下各种愚蠢的错误。&lt;/p&gt;&lt;p&gt;领导层向团队保证，没人会因此丢掉工作，但快速吸取教训。「我们是谷歌，不是初创公司，我们不能轻易地说，『哦，这只是技术缺陷』。我们会被点名批评，我们必须以谷歌的方式做出回应。」&lt;/p&gt;&lt;p&gt;谷歌内部留言板 Memegen 上的一篇帖子写道：「Bard 的发布和裁员都太仓促、草率和短视了，请恢复长远眼光。」&lt;/p&gt;&lt;p&gt;望远镜事件后，皮查伊安排了 8 万名员工花费两到四个小时对 Bard 进行内部测试，并为 Bard 项目增派了数百名员工。在团队的 Google Docs 中，皮查伊的头像开始每天出现，频率超过以往任何产品。&lt;/p&gt;&lt;p&gt;由此可见，谷歌一改以往「追求完美才发布」的传统，转变为「先发布再迭代」的敏捷策略。&lt;/p&gt;&lt;p&gt;到了 2024、2025 年，谷歌的节奏进一步加快。&lt;/p&gt;&lt;p&gt;皮查伊在内部会议上直言：「我需要大家内化紧迫感，加快公司运转速度。竞争正在激烈变化，我们的主要业务也面临着前所未有的挑战。」&lt;/p&gt;&lt;p&gt;这并不是空喊口号。&lt;/p&gt;&lt;p&gt;为了打破那种长期的「慢」，谷歌在 2024 年至 2025 年间启动了历史上最大规模的组织扁平化行动。&lt;/p&gt;&lt;p&gt;据内部统计，谷歌裁撤了约 35% 的负责小团队的经理岗位，特别是那些直接下属少于三人的管理层，消除「经理的经理」这一冗余层级，确保指令能够从决策层直接触达一线的算法工程师，减少沟通损耗和决策摩擦 。 &amp;nbsp;&lt;/p&gt;&lt;p&gt;在产品研发模式上，谷歌实验室的联合负责人乔什・伍德沃德在负责 Gemini 应用期间，打破了谷歌传统的长周期路线图，引入类似创业公司的快速迭代机制 。&lt;/p&gt;&lt;p&gt;伍德沃德会在 X 或 Reddit 等社交媒体平台直接回应用户的反馈，并将这些反馈实时转化为工程师的修复任务，形成高效的反馈闭环。&lt;/p&gt;&lt;p&gt;过去，谷歌被戏称为「硅谷最大的养老院」，前 CEO 埃里克・施密特曾炮轰公司因过度追求「生活与工作平衡」而丧失斗志。&lt;/p&gt;&lt;p&gt;在这一点上，谷歌也有了转变。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;内部备忘录显示，谷歌联合创始人谢尔盖・布林曾在今年 2 月对 AI 部门表示，员工应每日到岗，每周 60 小时是「最佳效率区间」。布林强调，人工智能领域竞争迅猛，公司必须「全速推进」以维持领先。&lt;/p&gt;&lt;p&gt;Gemini 项目组遍布全球八个时区，数百个协作聊天室昼夜同步。&lt;/p&gt;&lt;p&gt;哈萨比斯长期以来习惯于在伦敦与家人共进晚餐，然后工作到凌晨 4 点，他说：「回想起来，每一天都感觉像过了一辈子。」&lt;/p&gt;&lt;p&gt;&lt;strong&gt;组织重组：成立新谷歌 DeepMind&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;谷歌内部，曾经有两支「神仙打架」级别的 AI 天团。&lt;/p&gt;&lt;p&gt;一支是伦敦的 DeepMind，掌门人是德米斯・哈萨比斯；另一支是山景城的 Google Brain，由传奇工程师杰夫・迪恩坐镇。&lt;/p&gt;&lt;p&gt;DeepMind 以强化学习和通用人工智能为愿景，偏向于基础科学突破，如 AlphaFold、AlphaGo；而 Google Brain 则更侧重于深度学习的基础设施建设以及与谷歌现有产品的深度集成。&lt;/p&gt;&lt;p&gt;两支队伍虽然同属谷歌体系，但往往在人才和算力分配上存在激烈的竞争，甚至在某些研究方向上重复造轮子。&lt;/p&gt;&lt;p&gt;2023 年 4 月，谷歌宣布组织大重组，将 Google Brain 和 DeepMind 合并，成立新的 Google DeepMind 部门，DeepMind 联合创始人德米斯・哈萨比斯出任 CEO，获充分自主权。&lt;/p&gt;&lt;p&gt;杰夫・迪恩出任谷歌首席科学家，他将从具体的部门管理中抽身，转而从技术架构的高度指导 Google DeepMind 和 Google Research 的研发方向。&lt;/p&gt;&lt;p&gt;谷歌通过这次合并，确立了哈萨比斯作为谷歌 AI 唯一统帅的地位，结束了两大实验室长达数年的资源内耗和技术分歧。&lt;/p&gt;&lt;p&gt;合作开始后，迪恩、哈萨比斯和&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;詹姆斯&amp;middot;马尼卡向董事会提交了一份计划，让两个团队联合打造迄今为止最强大的语言模型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;哈萨比斯想把这个项目命名为 Titan，但董事会不太喜欢，最终采纳了迪恩提出的 Gemini 这个名字。&lt;/span&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;统一后的团队获得了前所未有的资源支持：最优先的TPU集群、最自由的架构试验权，最强工程师与科学家。&lt;/p&gt;&lt;p&gt;2024年，Gemini产品团队从搜索部门转移到DeepMind，这是DeepMind首次直接负责面向消费者的产品。&lt;/p&gt;&lt;p&gt;2025年初，谷歌 AI Studio 团队和为该公司 Gemini 系列模型开发 API 的团队并入 Google DeepMind。&lt;/p&gt;&lt;p&gt;谷歌打破了部门墙，AI成为全公司的核心战略，而非某个研究部门的专属项目。搜索、云计算、广告、硬件等各个业务线都围绕AI进行重组，形成「AI优先」的全新文化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;创始人回归：打破官僚主义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2019 年，谢尔盖・布林辞去 Alphabet 的日常管理职务，虽然仍是董事会成员，但基本不再参与运营决策，&lt;span data-pm-slice="0 0 []"&gt;只是偶尔去硅谷办公室查看其「登月计划」项目的进展情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这种情况在 2023 年发生变化，已经退居幕后的布林被重新拉回战场。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;2023年1 月 24 日，布林提交了多年来的首次代码访问权限申请，该申请与谷歌的自然语言聊天机器人 LaMDA 有关。&lt;/p&gt;&lt;p&gt;据桑达尔・皮查伊透露，布林开始花费大量时间与谷歌 AI 团队在一起，并且亲自参与技术工作。&lt;/p&gt;&lt;p&gt;「谢尔盖现在花更多时间在办公室里，他真的在写代码，过去一年里我最美好的回忆之一就是和谢尔盖一起坐在大屏幕前，看着损失曲线训练这些模型。」&lt;a href="https://mp.weixin.qq.com/s/LcOa6SkvrEeRPzS1RRXkPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/58904077-be95-40d8-9e0f-7fd0915d078b/1767286301112.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;亲自审查 Gemini 模型的训练损耗曲线，这在大型科技公司的联合创始人中是极其罕见的 。&lt;/p&gt;&lt;p&gt;在神经网络训练中，损耗曲线反映了模型参数在迭代过程中的误差收敛情况，其形态直接预示了模型的最终性能。布林对这些底层细节的关注，迫使研发团队必须在每一个技术细节上追求卓越，而非仅仅满足于完成项目汇报。&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;据布林透露，他现在有个新习惯，喜欢一边开车一边与 Gemini 进行实时对话，讨论数据中心的电力和成本等问题。「&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":null,"data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;他车里用的 Gemini 型号比现在市面上的产品好得多。」这&lt;/span&gt;&lt;/span&gt;是典型的谷歌式「&lt;span data-pm-slice="0 0 []"&gt;dogfooding」。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice="0 0 []"&gt;（注：dogfood 是硅谷的行话，意思是在正式发布前让员工试用自家产品。）&lt;a href="https://mp.weixin.qq.com/s/LcOa6SkvrEeRPzS1RRXkPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7e7065f7-70f6-49f0-b093-94eceedb8172/1767286315611.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;除了直接参与到模型训练的技术细节中，布林还要和谷歌内部的官僚主义做抗争。&lt;/p&gt;&lt;p&gt;谷歌内部有份清单规定哪些工具可以用来写代码，而 Gemini 竟在禁止列表里，「理由是 Gemini 必须保持纯粹，不能用它&amp;hellip;&amp;hellip; 反正一堆特别奇怪的理由，让我完全无法理解」。&lt;/p&gt;&lt;p&gt;他与相关人员发生了激烈争执，最终通过皮查伊才解决了这个问题。&lt;a href="https://mp.weixin.qq.com/s/LcOa6SkvrEeRPzS1RRXkPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a2689a29-f8f4-4735-a302-9019c9eed236/1767286332527.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;布林还用 Gemini 进行了一次创新尝试。他在谷歌内部聊天中询问 Gemini：在这个聊天空间里，谁应该得到晋升？Gemini 选择了一位默默无闻的年轻女工程师。&lt;/p&gt;&lt;p&gt;布林表示，「&lt;span data-pm-slice="0 0 []"&gt;我甚至没注意到她，她平时并不太爱发言，特别是在那次PR评审时」，但 AI 检测到了她的实际贡献。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;布林随后找到该工程师的直属经理求证，得到回应：「你说得对，她一直在努力工作，做了很多事情。」最终&lt;/span&gt;这位工程师获得了晋升。&lt;a href="https://mp.weixin.qq.com/s/LcOa6SkvrEeRPzS1RRXkPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/bbf2eae3-2889-4185-8f0b-ab6d68b5f44d/1767286342948.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;此外，布林的存在极大地简化了招聘流程。&lt;/p&gt;&lt;p&gt;在硅谷，顶级 AI 研究员往往更倾向于与同样具备深厚技术底蕴的创始人对话，布林多次亲自给已经离职的顶级科学家打电话，邀请他们重返谷歌参与「决定人类未来」的 Gemini 项目。&lt;/p&gt;&lt;p&gt;创始人的回归意义重大。他重新聚焦 Gemini 等旗舰项目，参与技术开发和决策，同时直接介入打破内部的流程障碍。&lt;/p&gt;&lt;p&gt;当一个项目需要快速决策时，创始人可以直接拍板，而不是在各个部门之间反复协调。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人才召回：老兵的价值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 2023 年的大规模裁员和人才流失阴影下，外界曾一度认为谷歌的 AI 核心人才已经流失殆尽。&lt;/p&gt;&lt;p&gt;然而，谷歌在 2024 年和 2025 年实施了一场「回旋镖计划」。&lt;/p&gt;&lt;p&gt;据 2025 年底的内部数据，谷歌当年招聘的 AI 软件工程师中，有约 20% 是曾经在谷歌工作、后来离职或跳槽、最终又被请回来的「老兵」。&lt;/p&gt;&lt;p&gt;这些「老谷歌人」对公司文化、技术架构、内部系统都了如指掌，能够迅速上手，大幅降低磨合成本。&lt;/p&gt;&lt;p&gt;其中，最具代表性的就是 Transformer 论文作者之一 Noam Shazeer 的回归。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LQv0750xlaDggr25QN1LuXnLSgFDfnIZibx4BqA8u6fn7EHB2Mibo2fuw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" alt="Noam Shazeer" data-ratio="0.6650390625" data-type="jpeg" data-w="1024" data-backw="562" data-backh="374" data-imgfileid="503526322" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/94a7c34d-c6df-4411-b9b5-fd78087a2fd3/640.png" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;他曾因谷歌拒绝推出他的聊天机器人项目而于 2021 年离职创办了 Character.AI。&lt;/p&gt;&lt;p&gt;2024 年，谷歌支付了高达 27 亿美元的许可费给 Character.AI，实质上是为了将 Noam Shazeer 及其团队召回 DeepMind 。&lt;/p&gt;&lt;p&gt;这种近乎「赎身」式的召回，向外界传递了一个强烈的信号：谷歌愿意为顶级人才付出任何代价。&lt;/p&gt;&lt;p&gt;Shazeer 回归后被任命为 Gemini 项目的共同负责人，他的存在极大地增强了谷歌在复杂算法架构上的研发底气 。&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌吸引老兵回归的核心筹码被称为「基础设施羡慕」。虽然 Meta 等对手开出了高达 1 亿美元的签字费，但谷歌提供的条件是研究员在任何地方都无法获得的：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;能够直接调度拥有数十万个 TPU 节点的超级计算集群，以及处理来自搜索、YouTube 等九个拥有超过 10 亿用户的产品所产生的真实世界数据。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对于追求技术突破的高级研究员来说，这种级别的算力和数据资源，比单纯的薪酬更具诱惑力 。&lt;/p&gt;&lt;p&gt;为了留住这些召回的老兵，谷歌还彻底改革了激励机制和职级体系。&lt;/p&gt;&lt;p&gt;在 2025 年的薪酬改革中，谷歌将高绩效 AI 人才的报酬更多地与产品落地指标（如模型推理效率、用户活跃度）而非仅仅是论文发表量挂钩。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;竞争远未结束&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当 OpenAI 拉响「红色警报」时，外界惊呼谷歌已经完成了逆袭。&lt;/p&gt;&lt;p&gt;但竞争远未结束。&lt;/p&gt;&lt;p&gt;奥特曼在内部信中透露，OpenAI 即将发布一款性能超越 Gemini 3 的推理模型，同时正在研发代号为 Garlic 的新模型。&lt;/p&gt;&lt;p&gt;而 Anthropic 的 Claude 也在企业市场攻城略地，Meta 则以惊人的薪酬挖角顶尖人才。&lt;/p&gt;&lt;p&gt;当模型能力趋同时，竞争的焦点将从技术转向应用，谁能让 AI 真正融入用户的日常生活，谁能构建起难以复制的生态壁垒，谁能在监管和伦理的约束下持续创新。&lt;/p&gt;&lt;p&gt;从这个角度看，谷歌的翻身仗只是万里长征的第一步。在这场似乎没有终点的 AI 竞赛中，唯一确定的是：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;没有永恒的领跑者，攻守之势随时可能再次转换。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.businessinsider.com/google-isnt-launching-chatgpt-competitor-due-to-reputational-risk-2022-12?utm_source=chatgpt.com&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/Yuchenj_UW/status/2000068339104936058?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.wired.com/story/google-openai-gemini-chatgpt-artificial-intelligence/?utm_source=chatgpt.com&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.businessinsider.com/google-larry-page-sergey-brin-help-chatgpt-code-red-2023-1&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>系统学习Deep Research，这一篇综述就够了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 02 Jan 2026 00:46:38 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-02-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-02-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/23b5cb41-458e-4deb-892d-b1da65cf2043/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;近年来，大模型的应用正从对话与创意写作，走向更加开放、复杂的研究型问题。尽管以检索增强生成（RAG）为代表的方法缓解了知识获取瓶颈，但其静态的 &amp;ldquo;一次检索 + 一次生成&amp;rdquo; 范式，难以支撑多步推理与长期研究流程，由此催生了 Deep Research（DR）这一新方向。&lt;/p&gt;&lt;p&gt;然而，随着相关工作的快速涌现，DR的概念也在迅速膨胀并趋于碎片化：不同工作在系统实现、任务假设与评价上差异显著；相似术语的使用进一步模糊了其能力边界。&lt;/p&gt;&lt;p&gt;正是在这一背景下，来自山东大学、清华大学、CMU、UIUC、腾讯、莱顿大学等机构共同撰写并发布了目前最全面的深度研究智能体综述《Deep Research: A Systematic Survey》。文章首先提出一条由浅入深的三阶段能力发展路径，随后从系统视角系统化梳理关键组件，并进一步总结了对应的训练与优化方法。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrHWNDxkXExHRYgEgjNibGOI1e3O6RCiaF9jZ5QDDrYLJY94z70YvGzGLQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.6925925925925925" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525956" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6cb3c4b0-2b63-4fcc-b5e0-6129fac080c3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GitHub：https://github.com/mangopy/Deep-Research-Survey&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Website：https://deep-research-survey.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://deep-research-survey.github.io/static/doc/Deep-Research-Survey.pdf&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;什么是 Deep Research&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DR 并非某一具体模型或技术，而是一条逐步演进的能力路径。综述刻画了研究型智能体从信息获取到完整科研流程的能力提升过程。基于对现有工作的梳理，可将这一演进划分为三个阶段。&lt;/p&gt;&lt;p&gt;阶段 1：「Agentic Search」。模型开始具备主动搜索与多步信息获取能力，能够根据中间结果动态调整查询策略，其核心目标在于持续地找对关键信息。这一阶段关注的是如何高效获取外界信息。&lt;/p&gt;&lt;p&gt;阶段 2：「Integrated Research」。模型不再只是信息的收集者，而是能够对多源证据进行理解、筛选和整合，最终生成逻辑连贯的报告。&lt;/p&gt;&lt;p&gt;阶段 3：「Full-stack AI Scientist」。模型进一步扩展到完整的科研闭环，具备提出研究假设、设计并执行实验，以及基于结果进行反思与修正的能力。这一阶段强调的不仅是推理深度，更是自主性与长期目标驱动的科研能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrWqZ6V046ZFKeySpmib1mQw6YldZeuushialOVT1lgoSvOOf1jQuEIC7g/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.475" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525754" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c938eb8d-8377-4e7b-be57-7be6efc4ce37/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Deep Research 的四大核心组件&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvry2IBN0pPujT15UsK2DrfmDjeNTjFxoHHDQ1K5x4nA4kxDwf0fCWfpQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525758" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/d60688f5-c081-4fc8-9ef0-1b6dfb2d32eb/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;1. 查询规划&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;查询规划主要负责在当前状态下，决定下一步应该查询什么信息。具体分为三类规划策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;顺序规划，将复杂问题拆解为线性的子问题序列，模型根据前一步的检索结果逐步推进，适用于依赖关系明确的研究任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;并行规划，同时生成多个相对独立的子查询，用于加速搜索或降低单一搜索路径带来的信息缺失。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;树状规划，显式建模子问题之间的层级与分支关系，允许模型在研究过程中进行探索与回溯。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;相比传统 RAG 中一次性生成查询的做法，DR 将 &amp;ldquo;如何提问&amp;rdquo; 本身纳入推理过程，使模型能够在多轮研究中动态调整推理路径。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvruuaqg2VCiavdGBAawUQyibotBibOXlIiaaYnJKOFmVd3pUhZzoSgzvuPtA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.2175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525743" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/c5ba6148-b738-4bd5-b059-76d034288187/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 信息获取&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;论文从三个维度对现有的信息获取方法进行归纳。&lt;/p&gt;&lt;p&gt;（1）何时检索：不同于固定步数或每轮必检索的策略，DR 智能体需要根据当前不确定性与信息缺口，动态判断是否触发检索，以避免冗余查询或过早依赖外部信息。&lt;/p&gt;&lt;p&gt;（2）检索什么： 在确定检索时机后，从 Web 或外界知识库中做检索，包括多模态和纯文本信息。&lt;/p&gt;&lt;p&gt;（3）如何过滤检索信息：面对噪声较高的检索结果，系统通常引入相关性判断、一致性校验或证据聚合机制，对外部信息进行筛选与整合。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 记忆管理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在开放任务中，智能体往往需要跨越多轮交互、多个子问题与不同信息源。记忆模块是支撑 DR 系统长期运行与持续推理的核心基础设施，为系统提供状态延续和经验累积，使模型能够使用长期长线推理任务。现有工作通常将记忆管理过程拆解为四个相互关联的阶段：记忆巩固、记忆索引、记忆更新与记忆遗忘。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr5Jn9992VTjCQia7PzG7srSs8DibzMib1US80PG9iakrWbzTj0GMlhj4Igg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.42685185185185187" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525760" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/b0a24b7c-5ed1-4e1a-a564-d8e5ee775a26/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;4. 答案生成&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与传统生成任务不同，DR 场景的问答更强调结论与证据之间的对应关系，以及整体论证过程的逻辑一致性。因此，通常需要智能体显式整合多源证据与中间推理结果，使输出不仅在语言层面连贯，还能够支持事实核验与过程回溯。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrN3zsHzic9M126Hklb3t3Q9xH8GZEQtPAkQSmnjPtgDibryEffyk6TOQw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525761" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/04d9a22b-8613-4e2b-a84a-523a1b6f4d7c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;如何训练与优化 Deep Research 系统？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;文中总结了三类具有代表性的方法：&lt;/p&gt;&lt;p&gt;提示工程：通过精心设计的多步提示构建研究流程，引导模型执行规划、检索与生成等步骤，适合快速构建原型。其效果高度依赖提示设计，泛化能力有限。&lt;/p&gt;&lt;p&gt;监督微调：利用高质量推理轨迹，对智能体进行监督微调。该方法直观有效，但获取覆盖复杂研究行为的标注数据成本较高。&lt;/p&gt;&lt;p&gt;智能体强化学习： 通过强化学习信号直接优化 DR 智能体在多步决策过程中的行为策略，无需复杂人工标注。主要细分为两种做法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;端到端优化：输入到输出的完整决策过程，联合优化查询规划、检索、信息整合与报告生成等多个环节。这种方式有助于智能体学会协调各个模块，但是面临奖励稀疏、训练不稳定以及采样成本高等问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;优化特定模块：仅对查询规划或调度等关键模块施加强化学习信号。在保持系统其他模块稳定性的同时，学习何时检索、如何推理等单一策略。这种模块化训练显著降低了训练难度，更易于在现有系统中落地。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Deep Research 真正难在哪里？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Deep Research 的核心挑战并不在于单一能力的提升，而在于如何在长期、开放且不确定的研究流程中，实现稳定、可控且可评估的系统级行为。现有工作主要面临以下几方面的关键难题。&lt;/p&gt;&lt;p&gt;（1）内部知识与外部知识的协同： 研究型智能体需要在自身参数化知识与外部检索信息之间做出动态权衡，即在何时依赖内部推理、何时调用搜索工具。&lt;/p&gt;&lt;p&gt;（2）训练算法的稳定性：面向长线任务的训练往往依赖强化学习等方法，但优化过程中容易出现策略退化或熵坍缩等问题，使智能体过早收敛到次优行为模式，限制其探索多样化的推理路径。&lt;/p&gt;&lt;p&gt;（3）评估方法的构建： 如何合理评估研究型智能体仍是开放问题。综述系统梳理了现有 benchmark。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrURbXicOqiajJAM2AnsUX3EtI0PvGR8GuRED6KBSOriaC89JTxbhqd4icTQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="1.1277777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525762" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/c64966e1-af78-4c88-8c2a-afd49cdc94fb/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;尽管相关数据集不断涌现，构建可靠且高效的评估方法仍有待深入探索，尤其是在开放式任务中如何对 report-level 的模型输出进行全面评估。当前广泛采用的 LLM-as-a-judge 范式在实践中展现出便利性，但仍不可避免地受到顺序偏差，偏好 hacking 等问题的影响，限制了其作为测评方法的可靠性。&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（4）&lt;/span&gt;记忆模块的构建：记忆模块的构建是 DR 系统中最具挑战性的部分之一。如何在记忆容量、检索效率与信息可靠性之间取得平衡，并将记忆机制稳定地融入端到端训练流程，仍是当前研究中的关键难题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语 Deep Research&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Deep Research 并非对现有 RAG 的简单扩展，而是智能体在能力、动作空间以及应用边界上的一次转变：从单轮的答案生成，走向面向开放问题的深度研究。目前，该方向仍处于早期阶段，如何在开放环境中构建既具自主性、又具可信性的 Deep Research 智能体，仍是未来值得持续探索的重要问题。本文的 survey 也会持续更新，总结最新的进展。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>英伟达、AMD本月起或涨价，5090两千美元变五千</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 02 Jan 2026 00:43:25 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-02-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-02-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9de92c75-cca1-42e7-833a-2f52b199f21d/1767285712803.png" style="width: 700%;" class="fr-fic fr-dib"&gt;GPU 涨价看来正在变成定局。&lt;/p&gt;&lt;p&gt;据一些科技媒体及供应链报告，英伟达、AMD 将在 2026 年初上调 GPU 价格。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIufy6yH40OicYB6UcVQ5Uo8kBVhwKc44xItERnokXCxgMLsmGvvBh8FfQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.2401960784313726" data-s="300,640" data-type="jpeg" data-w="1020" type="block" data-imgfileid="503526477" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/c1ed6517-865e-459d-9497-a47a10892cae/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;来自 Board Channels、Wccftech 等消息源的报道称，英伟达、AMD 计划在未来几个月内对旗下的在售 GPU 逐步涨价。其中 AMD 预计在 1 月份开始上调价格，英伟达预计在 2 月份涨价。&lt;/p&gt;&lt;p&gt;预计此次涨价将首先影响部分消费级 GPU，如英伟达的 GeForce RTX 50 系列和 AMD 的 Radeon RX 9000 系列。英伟达的旗舰 GPU 产品 RTX 5090 于 2025 年 1 月发布，官方建议售价为 1999 美元，但预计今年的实际价格将飙升至 5000 美元。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIuok71ibj50YhvGibvTSkib11UVZkUVXp9OVcIOl0f6ujic0TggicxulqM8Rg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526478" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4b82c91d-b555-41d8-bc2d-f9639d502103/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;以此开始，涨价很可能涵盖两家公司的所有产品线，不仅包括消费级 GPU，还包括用于 AI 数据中心和服务器的 GPU。消息人士透露，显卡厂商何时想要提高 GPU 价格取决于他们自己。但很明显，如果他们从 AMD 和英伟达那里以更高的价格采购包含显存的模块，他们将别无选择，只能尽快提高 GPU 价格。&lt;/p&gt;&lt;p&gt;显然，此次提价的驱动因素是 GPU 成本结构中内存占比的快速增长。由于最近几个月来 GPU 内存价格飙升，维持现有 GPU 价格已十分困难。&lt;/p&gt;&lt;p&gt;一位业内人士解释说：「最近，内存成本在 GPU 整体制造成本中的平均占比已超过 80%。」&lt;/p&gt;&lt;p&gt;比如 RTX 5070 Ti 上搭载的 16GB GDDR7 内存，其采购成本已经从 2025 年 5 月的 65-80 美元，涨到了 12 月的 210-260 美元。英伟达及其合作伙伴（华硕、微星、七彩虹等）在 2025 年上半年执行的是 2024 年底签下的长协合同。当时显存价格还处于合理区间，因此 RTX 5070 Ti 能以 749 美元左右的官方建议零售价平稳上市。&lt;/p&gt;&lt;p&gt;然而，绝大多数旧合约在 2025 年底到期（英伟达的合同截止到今年 1 月，AMD 的合同截止到去年 12 月）。厂商在续签 2026 年采购协议时，面临的是已经翻了数倍的现货价格。&lt;/p&gt;&lt;p&gt;在科技公司对于 AI 芯片的旺盛需求推动下，内存生产商三星和 SK 海力士正在将原本属于 GDDR7 的生产线改造，用于生产利润更高的 HBM4（用于 Blackwell Ultra AI 芯片）。&lt;/p&gt;&lt;p&gt;类似的，AI 数据中心使用的 GPU 也是通过长期合同供应的，而内存价格的上涨可能会反映在 2026 年签订的新合同上。英伟达旗舰级 AI GPU H200 的售价在 3-4 万美元之间，预计今年价格还会进一步上涨。每块 H200 都包含六颗第五代高带宽显存（HBM3E）。&lt;/p&gt;&lt;p&gt;由于内存的涨价，今年的笔记本电脑整机价格也可能会面临调整，一些反向升级的 8GB 机型将会上市，16GB 及以上内存的机型价格将大幅上涨。研究机构 TrendForce 表示，DRAM 内存供应非常紧张，以至于各大品牌正在重新设计产品线并提高价格以保护库存。&lt;/p&gt;&lt;p&gt;最近的反应来自华硕。根据官方声明，其将于 1 月 5 日起上调部分产品价格，理由是人工智能需求推动 DRAM 和存储成本上涨。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic4ataLIR7Ge5M8ia2JGDOIu6fl7YgldSfpcGAjvI9u5fQ2gibD7b2Dxt1iadoGVPa4qJmsXR3V93ScQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.261637239165329" data-s="300,640" data-type="png" data-w="623" type="block" data-imgfileid="503526479" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/46e1ea6b-1c11-4a2f-8d0b-6b079b96aeee/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;虽然华硕尚未透露具体涨幅，但可以观察类似的情况：戴尔此前已宣布涨价 30%。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考内容：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;http://www.boardchannels.com.cn/thread-130155-1-1.html&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://mobile.newsis.com/view/NISX20251229_0003458273&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://wccftech.com/amd-and-nvidia-are-expected-to-hike-gpu-prices-early-2026/&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AAAI 2026 Oral | 给多流数据配「私教+外援」，漂移来了也不慌</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 02 Jan 2026 00:40:29 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-02</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-02</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/d125788c-92c1-47b3-bf0e-27c61aedbf53/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; line-height: 1.75em; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;本文作者为：En Yu, Jie Lu, Kun Wang, Xiaoyu Yang, Guangquan Zhang。所有作者均来自于悉尼科技大学（UTS）澳大利亚人工智能研究院（AAII）。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在智慧城市、社交媒体、工业物联网等真实开放动态环境中，数据往往以多流（Multistream）形式并发产生。然而，现实世界并非完美的实验室，这些数据流往往存在异构性，且分布变化各不相同，伴随着复杂的异步概念漂移。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如何让模型既能 &amp;ldquo;专精&amp;rdquo; 于单一流的特性，又能 &amp;ldquo;博采众长&amp;rdquo; 利用流间相关性，同时还能自适应分布变化？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;悉尼科技大学（UTS）研究团队提出了一种全新的&lt;strong&gt;漂移感知协作辅助混合专家学习框架 &amp;mdash;&amp;mdash; CAMEL (Collaborative Assistance Mixture of Experts Learning)&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;CAMEL 巧妙地将混合专家模型（MoE）引入流式学习，通过 &amp;ldquo;私有专家&amp;rdquo; 与 &amp;ldquo;辅助专家&amp;rdquo; 的协作机制，以及自动化专家生命周期管理，完美解决了异构多流学习中的关键问题。该工作已被 AAAI 2026 接收为 Oral 论文。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibelibiasu1y2RjTzoiahh8ic7t97kLNNILTRZBKibaBIA2qe27TueelPnWTPw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.21388888888888888" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526085" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/27dc75a4-6cb6-4b5b-9af8-bb703cb8efe8/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2508.01598&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;01 引言&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在真实应用场景中，数据通常以连续且无限的数据流形式产生，其生成机制往往呈现显著的非平稳性，即数据的联合概率分布随时间发生不可预测的概念漂移。这一特性与经典机器学习所依赖的独立同分布（I.I.D.）假设存在根本冲突。&lt;/p&gt;&lt;p&gt;然而，现有研究大多聚焦于单一或同构数据流的漂移建模，难以应对真实世界中普遍存在的多源异构数据流情形。以智能城市为例，交通传感器、气象观测、公共交通记录及社交媒体等信息流在时间尺度与演化模式上彼此独立，却潜藏着重要的动态关联。若能在概念漂移过程中有效挖掘并利用这些跨流关系，将显著提升决策的准确性与鲁棒性。&lt;/p&gt;&lt;p&gt;现有的方法往往陷入两难：要么假设所有流是同构的，强行统一处理导致模型失配；要么采用静态模型，一旦某个流发生漂移，重新训练会导致 &amp;ldquo;灾难性遗忘&amp;rdquo;，而增量微调又可能因为流之间的不同步演化而引发 &amp;ldquo;负迁移&amp;rdquo; 。&lt;/p&gt;&lt;p&gt;为此，作者正式定义了异构多流学习（HML）问题，并提出 CAMEL 框架。这是一种动态的、通过协作辅助的混合专家学习框架，通过模块化设计在 &amp;ldquo;专精 &amp;mdash; 协作 &amp;mdash; 适应&amp;rdquo; 之间取得平衡。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe5zxVsYB4kXpG1ZiaoXBBRQjkMDib9AjOaVxx1zdvy3Jib6aXfy4aaUWAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.46574074074074073" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526089" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/a6d0bdc4-537d-4470-83ed-ec66fb3dd731/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 1：CAMEL 整体框架。每个流的 MoE 模块利用动态的私有专家库和专用的辅助专家，通过多头注意力进行协作融合。系统遵循 &amp;ldquo;测试 - 诊断 - 适应&amp;rdquo; 的循环，通过自主专家调节器动态管理专家生命周期以响应漂移信号。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;02 方法论与架构设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队面向 HML 场景下的三大核心挑战：内在异构性、多流知识融合以及异步概念漂移，设计了一套模块化的漂移感知架构。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战一：内在异构性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统的多流学习方法通常假设所有流共享特征空间与标签空间，但现实中不同流可能具有不同维度 (&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeKIVQKt36SmeLAYXHP1CsicKHjS5rDappz1LicamTVskThmQk5jIw7mRA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3287671232876712" data-s="300,640" data-type="png" data-w="146" type="block" data-imgfileid="503526093" data-aistatus="1" data-original-style="width:52px;height:20px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/c3eccf4f-219e-4954-b588-8ea6b00d1222/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dii" style="width: 8.43%;"&gt;) 与任务目标 (&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe9ujQibJ25VQYuwXhNGBxmxjl9a8tibl39QnjRA2uKia3Nk3PKZtPF2xbA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.35036496350364965" data-s="300,640" data-type="png" data-w="137" type="block" data-imgfileid="503526094" data-aistatus="1" data-original-style="width:42px;height:20px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/c1ef033b-7a5b-40fd-9e3f-728dc639645b/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 7.33%;"&gt;&amp;nbsp;)。&lt;/p&gt;&lt;p&gt;CAMEL 为每个流配置异构感知的 &amp;ldquo;独立系统&amp;rdquo;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;特征对齐： 为每一个数据流&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeqQIVicZiap7DJ7Rlayoicibo8eRtmzr2HZznGRib4m8x7VosoDM6CFFHe6w/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1" data-s="300,640" data-type="png" data-w="42" type="block" data-imgfileid="503526095" data-aistatus="1" data-original-style="width:20px;height:20px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/91da0b76-58d3-4f28-b231-e049d3591a9f/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dii" style="width: 2.71%;"&gt;配备了专属的 特征提取器&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibevn0vjmG418UpFM4aLpUEpenuoq5AjzicmJsia2mfsnicpy0RISd2iaLeUQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.7090909090909091" data-s="300,640" data-type="png" data-w="55" type="block" data-imgfileid="503526096" data-aistatus="1" data-original-style="width:24px;height:20px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/bae9d826-c8f6-4247-988e-e44dddca8336/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 3.92%;"&gt;，将不同维度的原始输入映射到一个公共的潜在空间&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeKJYex0GicoWyc85aCxiasRiajNFibicm0nOjg7suwfgfGfAGfOAhRzApKLw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.2909090909090909" data-s="300,640" data-type="png" data-w="165" type="block" data-imgfileid="503526097" data-aistatus="1" data-original-style="width:53px;height:20px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/8a77aafd-cb6a-4681-929d-188eaf6f0631/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dii" style="width: 9.99%;"&gt;，为后续的特征交互奠定基础。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;任务专精：在输出端，配备了任务特定分类头&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeZQcZgtlLy62XCc9b6hFvGeXzgl3uDdyh4QRqbiaOLI89mkJNZeDfzLw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.7090909090909091" data-s="300,640" data-type="png" data-w="55" type="block" data-imgfileid="503526098" data-aistatus="1" data-original-style="width:21px;height:20px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/55b3c2bb-1705-494e-b5bf-6e31b90af1a8/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dii" style="width: 3.92%;"&gt;，独立处理各自的分类任务，确保决策层与标签空间的语义对齐。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;挑战二：多流知识融合&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;多流数据的核心价值在于流之间的潜在相关性，但盲目融合所有流的信息会导致负迁移。&lt;/p&gt;&lt;p&gt;CAMEL 除了为每个流维护一组捕捉自身特性的私有专家库外，还引入了一个辅助专家，它利用多头注意力机制：以当前流的特征&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibezBic182pCmgtmgKByr6ZQVDKFHn8klQWLDEn5WtL7M5I8sJBicB7Z8Mw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.8727272727272727" data-s="300,640" data-type="png" data-w="55" type="block" data-imgfileid="503526099" data-aistatus="1" data-original-style="width:25px;height:22px;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e6b806bf-4a12-4e46-aec6-6b09d83e378a/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dii" style="width: 3.3%;"&gt; 作为 Query；以所有其他并发流的特征&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeaVEibrGhaeQSfibB6yyOwsAwTrJClEaqBiaYibuZz8DvdZ6u9gibZtbb3Dg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.3582089552238806" data-s="300,640" data-type="png" data-w="134" type="block" data-imgfileid="503526100" data-aistatus="1" data-original-style="width:53px;height:20px;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/8f2b453c-f29a-42d7-afbd-564ba187841c/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dii" style="width: 8.89%;"&gt; 作为 Key 和 Value；生成上下文向量&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe7GDIv9lvN5Ys7NqUSVJZ2GSU9qB1Fp6LWLQavxqKUsAaTFuaHdqQXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.11062906724511931" data-s="300,640" data-type="png" data-w="461" type="block" data-imgfileid="503526101" data-aistatus="1" data-original-style="width:199px;height:22px;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/6716a8fe-4a1d-474a-9ae3-23538774c270/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 32.08%;"&gt;。通过这种机制，模型能够自主决定从哪些流中借力。如果其他流无帮助时，注意力权重会自然衰减，从而自适应抑制负迁移。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战三：异步概念漂移&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对数据分布的非平稳性，CAMEL 设计了自主专家调优器 ，在专家粒度上实现模型容量的在线伸缩，遵循 &amp;ldquo;测试 - 诊断 - 适应&amp;rdquo; 的闭环逻辑：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;漂移检测&lt;/strong&gt;：利用基于最大均值差异（MMD）的漂移检测器监控特征分布变化。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;增量式扩展（Add &amp;amp; Freeze）&lt;/strong&gt;： 当检测到漂移且伴随性能显著下降时， 实例化一个新的私有专家学习新概念，并冻结旧专家以规避灾难性遗忘。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;自适应剪枝（Prune）&lt;/strong&gt;： 对于长期利用率（由路由网络权重决定）低下的冗余专家，&amp;quot;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibepTzBVe3xgCsIHwD1fC8gFz5YvTxPzNB9PPd3ZO73UNsDFla6WP9mqA/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.46875" data-s="300,640" data-type="png" data-w="64" type="block" data-imgfileid="503526155" data-aistatus="1" data-original-style="width:27px;height:20px;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/f98bace6-71e4-45fb-9611-882973c02a3c/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dii" style="width: 4.95%;"&gt;&amp;quot; 执行剪枝操作，维持模型的稀疏性与推理效率。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于每个流拥有独立的&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe82XApicEwCJmDZAvkibat7a1Hw14BJMb05ek0FbiaCYt1pXlHBwGbDQfQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.46875" data-s="300,640" data-type="png" data-w="64" type="block" data-imgfileid="503526104" data-aistatus="1" data-original-style="width:29px;height:20px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/cc90d191-bd36-4828-a54a-bee9e5c2a685/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dii" style="width: 4.95%;"&gt;&amp;nbsp;，CAMEL 能够自适应地处理多流之间的异步漂移，即只在需要的时候对相关流进行架构调整。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;03 理论分析与实验验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;理论分析&lt;/strong&gt;：基于多任务学习理论，论文证明了 CAMEL 的泛化误差上界。 定理 1 表明，CAMEL 的期望风险由平均经验风险、流间不相似度 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibePSPayXnun9MMp529UtIMMySHY4kUU92YREic8bFzicCT0S3aEgNkz0dg/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.35036496350364965" data-s="300,640" data-type="png" data-w="137" type="block" data-imgfileid="503526108" data-aistatus="1" data-original-style="width:48px;height:20px;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/a11f58f2-186f-44c1-9bba-2b925f1a132f/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dii" style="width: 7.15%;"&gt;以及样本复杂度项构成。这意味着，辅助专家通过注意力机制最小化了流间的不相似度代价，而路由网络平衡了协作与专精。这为 CAMEL 在复杂环境下的鲁棒性提供了数学解释。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeHRTO8GiboLyicpFWK6dqJv002de3J5Xm9ibAUiaVBOnG4HPib2gjDCGxYtw/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.39166666666666666" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526090" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/773d1d89-d05e-431e-a62f-7c31a0704529/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验验证&lt;/strong&gt;：为了验证 CAMEL 的有效性，研究团队构建了包含 12 个合成流和 4 个真实数据集（涵盖了天气、新闻、信用卡信息等）的 8 大基准场景。&lt;/p&gt;&lt;p&gt;表 1 中的结果表明，CAMEL 在几乎所有场景中实现了最先进的平均准确率，显著超越了单流基线（SRP、AMF、IWE）和多流方法（MCMO、OBAL、BFSRL）。CAMEL 的优越性在异构环境中尤为明显，现有的多流方法由于依赖共享特征或标签空间而失败。相比之下，CAMEL 的流特定模块能够在输入异构下实现稳健的性能。该框架还通过其协作辅助机制有效利用潜在的流间相关性，超越了单流方法。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibekqYBepkM7J5ia9XiaXjtYB2ykxKf1U6pGZJ9B7Mjkuj4A6DbuUuUDhjw/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.45092592592592595" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526091" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/0500874d-6195-4167-ae44-af8182ebfdef/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表 1：各方法在所有基准上的分类准确率（%）。红色代表最优，蓝色代表次优。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;04 结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;CAMEL 的提出标志着多流学习从 &amp;ldquo;静态同构&amp;rdquo; 向 &amp;ldquo;动态异构&amp;rdquo; 迈出了关键一步。 该框架以私有专家保障流内专精，以辅助专家挖掘跨流关联，并通过自动化的专家生命周期管理在漂移下实现持续适应与效率控制，为复杂、动态演化的异构多流场景提供了一种可扩展的解决方案。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>超DeepEP两倍！无问芯穹FUSCO以「空中变阵」突破MoE通信瓶颈，专为Agent爆发设计</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 17:56:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-10</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/fc5e277e-d4f4-4723-9276-10496196c2ad/1767174672284.png" style="width: 700%;" class="fr-fic fr-dib"&gt;随着 ChatGPT、Gemini、DeepSeek-V3、Kimi-K2 等主流大模型纷纷采用混合专家架构（Mixture-of-Experts, MoE）及专家并行策略（Expert Parallelism, EP），MoE 技术已在产业应用中逐渐成为主流。&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;与此同时，以代码智能体、Cursor 类对话式 IDE 为代表的新型应用，&lt;strong&gt;一方面显著推高了用户请求规模，另一方面大幅拉长了单次推理的上下文长度，两者均呈现出一个数量级以上的增长。&lt;/strong&gt;在 MoE 架构下，这种变化不仅线性放大了计算开销，还显著增加了跨专家的通信与调度成本，使得整体系统压力接近一个数量级提升，并在规模化服务场景中进一步被放大。&lt;/section&gt;&lt;p&gt;MoE 模型因其结构上的稀疏性与专家并行特性，天然引入了频繁且规模庞大的全局分布式数据交换。而&lt;strong&gt;当前主流通信库及解决方案（如 DeepEP）&lt;/strong&gt;仍基于 “通信与数据布局解耦” 的传统设计假设，难以高效应对实际生产中的跨设备、非连续、动态重排的数据访问模式，在高并发、长上下文与大规模专家配置的场景下，DeepEP 性能已逐渐趋近瓶颈，直接制约了 MoE 大模型的持续落地、系统稳定扩展与经济性运行。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LictTuJfnD7iaNSwyf2MwHicM6FaMqO8b72ZIJEqMvzpavNAbERkuuoIxg/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.24814814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526329" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/c5165c2a-7868-4a12-9ce9-13163196b61b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文地址：https://www.arxiv.org/abs/2512.22036&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;开源地址：https://github.com/infinigence/FUSCO&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;基于此，无问芯穹联合清华大学、中关村学院、上海交大及南加州大学，面向 MoE 模型结构和 EP 并行策略场景，推出高效通信库 “FUSCO”。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这是一种全新的融合式通信优化路径：将通信过程与数据底层布局主动协同，在数据搬运的同时完成布局转换，从而彻底消除冗余的数据重排操作。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这一设计将融合优化的边界从传统的计算算子之间融合，拓展至通信与数据操作之间的跨层融合，揭示了大模型训练与推理中一个此前未被充分挖掘的优化新空间。在此基础上，FUSCO 可自动实现负载均衡与冗余通信消除，并在不同 GPU 架构与网络拓扑下保持良好的可移植性，为大规模模型的端到端执行提供了一种更具系统性的融合优化路径。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验表明，相较于 NCCL 和 DeepSeek 的 DeepEP 通信库，FUSCO 的通信性能可最高分别提升 3.84 倍和 2.01 倍。且在实际部署场景中，随着并发请求数和文本长度（例如达到 2048K tokens）的增加，其性能优势将进一步扩大。&lt;/strong&gt;这为基于 MoE 模型的推理、训练的各类 Agent 场景提供了有力支持。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L9Y6wmxvmAoZqGOPq9shTS36rH4ualNiaI2Qe03ibrf2mc77iawAthlo1w/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=2" data-ratio="1.3359375" data-s="300,640" data-type="png" data-w="1024" type="block" data-imgfileid="503526331" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f964268d-d216-4535-9aee-35e2ed641ff6/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;背景 &amp;nbsp;MoE 专家并行架构下的通信与数据重排瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在大规模 MoE 模型的训练和推理中，单个 GPU 往往无法承载完整模型权重或处理全部 token。因而系统通常引入&lt;strong&gt;专家并行（Expert Parallelism）&lt;/strong&gt;，将不同专家分布在多个 GPU 上，以提升计算吞吐并扩展模型容量。尽管该策略有效提升了可扩展性，但也引入了新的性能瓶颈：&lt;strong&gt;token 需要在不同专家所在的 GPU 之间进行跨设备的数据重排与通信，形成分布式数据重排（Distributed Data Shuffling）过程&lt;/strong&gt;，其典型执行流程包括：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;通信前的 token 重排&lt;/strong&gt;：根据 token–expert 的映射关系确定目标 GPU，并将 token 按目标 GPU 的通信布局重新排列，以满足 All-to-All 的数据组织要求；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;跨 GPU 的 All-to-All 通信&lt;/strong&gt;：重排后的 token 通过 All-to-All 通信发送至对应专家所在的 GPU；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;通信后的 token 重排&lt;/strong&gt;：每个 GPU 根据其本地承载的专家集合，对接收到的 token 进一步按专家进行排列，并完成对应专家的计算；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;镜像式的合并 (Combine) 过程&lt;/strong&gt;：在专家计算完成后，系统按与上述过程相反的顺序，依次执行本地逆向重排、All-to-All 通信以及最终的 token 顺序恢复，以保证输出与原始 token 顺序一致。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;传统集合通信库遵循 “通信与数据布局解耦” 的设计范式：通信被视为对连续数据块的被动搬运，而数据在模型执行过程中所固有的布局语义（如视图变换、维度重排与切片关系）通常被忽略。这一抽象虽然简化了接口，却在大模型训练与推理中引入了大量隐式的中间张量重排与内存拷贝，成为制约端到端效率的重要瓶颈。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LJpNk1T25uRiaPMfrXxnJOXIkwTG0HN9OAmERqJss0ppmO5kxfBLkDqQ/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=3" data-ratio="0.7824074074074074" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526336" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4b8d54f8-14cc-4b87-b2dd-738da91e2b8c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;随着专家并行规模的扩大，上述过程的开销呈上升趋势&lt;/strong&gt;。训练和推理的吞吐虽然随更多设备的参与而提升，但分布式数据重排在端到端总延迟中所占比例总体上不断增加。&lt;/p&gt;&lt;p&gt;这一现象主要源于随着专家分布在更多设备上，token 在设备间的传输量增加，同时全局同步成本也随之上升。每个 token 都必须在参与 GPU 间交换和重排，这相对于计算增加了额外的延迟。尽管专家内部的前馈计算仍然高效，但在更高的专家并行度和更大集群规模下，分布式数据重排已成为端到端性能的重要瓶颈。&lt;/p&gt;&lt;p&gt;为量化这一过程的开销，我们进一步对&lt;strong&gt;一次通信前后的数据重排与通信本身&lt;/strong&gt;进行了细致分析。以 32 MB 数据为例，使用 PyTorch 的 index_select 算子模拟本地重排操作，并分别在机内（NVLink）和跨机（RoCE）环境下，结合 NCCL 的 send/recv 原语测量通信延迟。&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;机内 (NVLink)&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;跨机 (RoCE)&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;总时延&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.349&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.96&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;通信时延&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.109&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.72&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;重排时延&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.24&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;0.24&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;重排占比&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;68.8%&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="191"&gt;&lt;section&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;25%&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;结果显示，重排操作在总 shuffle 时间中的占比分别高达 68.8%（机内）和 25%（跨机）。&lt;/strong&gt;这说明 MoE 中的数据移动瓶颈不仅来自网络带宽限制，还受限于内存绑定的数据重排操作。并且，随着互联效率不断提升，通信本身变得更快，若数据重排开销保持不变，其在总执行时间中的占比将更突出。&lt;/p&gt;&lt;p&gt;此外，&lt;strong&gt;传统的 All-to-All 通信对 token 冗余和网络层次缺乏感知。&lt;/strong&gt;在实际 MoE 工作负载中，同一 token 可能被路由到同一节点上不同 GPU 的多个专家，但在当前通信实现中，这些重复 token 会被序列化发送多次，造成带宽浪费和通信效率下降。现有优化方案如 DeepEP 虽然引入了跨机去重，但高度依赖特定网络硬件，部署范围有限，且未消除通信前后的数据重排，在通用 MoE 场景中的优化效果仍有限。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;FUSCO 设计解析 &amp;nbsp;如何让大规模的分布式数据交换既高效又轻量？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;FUSCO 的核心思路在于认识到：&lt;strong&gt;数据重排本质上就是一次数据布局的变换，而通信本身已经定义了数据该如何被拆分、发送和放置。因此，与其在通信前后引入额外的布局调整，不如顺着通信过程本身来完成布局变换。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于这一观察，我们提出了一种数据与通信协同设计的方法，在数据传输的过程中同步完成布局变换，从而避免将数据通信与数据重排变换分离执行的传统做法。每个数据段（LLM 中的 token）在传输的过程中即完成排列和发送，从而既减少了额外拷贝，也最大化利用了 GPU 和网络带宽。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L5M6yYfrpPmg3ibCkOichfqvGBxLCTLsaBrAaNlgFXvIJVQkG3SskRwDA/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=4" data-ratio="0.45" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526338" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/15aab0d3-a81d-4afd-a9ce-32656130f317/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;融合重排的通信：让数据在传输中一步到位完成布局变换&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为实现数据在传输过程中即完成重排，&lt;strong&gt;FUSCO 打破了将重排视为独立步骤的传统思路，从上到下协同设计通信接口和底层算子&lt;/strong&gt;：接口层负责精确表达数据 “从哪里来、到哪里去” 的布局语义，而算子层则负责在一次通信执行路径中高效地落实这些语义。&lt;/p&gt;&lt;p&gt;通过将布局描述与通信执行紧密绑定，&lt;strong&gt;FUSCO 构建了一条从接口到算子的贯通路径，使数据重排不再是独立的前后处理，而是被自然地融合进通信过程本身&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（1）通信接口设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在专家并行中，各个设备上的原始数据通常是一个大的连续张量，由多个 token 组成。经过 MoE 路由后，不同 token 可能被分配到不同的设备，而路由到同一设备的 token 往往在张量中呈离散分布，而非连续的一块。每个 token 的大小通常在 4KB 到 14KB 之间，需要发送到该设备上的不同专家。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;所谓 “数据重排”，本质上是在通信前，将这些离散 token 按目标设备和对应专家进行组织，并在通信完成后将它们正确地放置到各自的目标位置。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了简化讨论，先考虑两个设备之间的一次单向传输。为精确描述这些离散数据的布局，我们将通信数据抽象为一组逻辑段。每个段对应内存中一小段连续数据，FUSCO 用一个称为段描述符的数据结构记录其起始地址。在通信过程中，一端并不直接操作原始张量，而是根据连续的段描述符序列，从张量的对应位置读取或写入数据，从而实现对离散 token 的精确访问和操作。&lt;/p&gt;&lt;p&gt;在发送端，这个描述符序列规定了通信负载如何从源张量中被逐段读取；在接收端，对应的描述符序列则明确了每一段数据在目标内存中的落点。&lt;/p&gt;&lt;p&gt;基于上述段描述符序列的创新设计，FUSCO 以两个互补的通语实现其通信接口：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;gather-send&lt;/strong&gt;：发送端依据本地的段描述符序列，按顺序从多个不连续位置读取段数据并发起发送；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;scatter-recv&lt;/strong&gt;：接收端依据自身的段描述符序列，将接收到的段数据直接写入目标布局中的对应位置。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个原语在语义上是一一对应的：每一个逻辑段在发送端和接收端都有明确匹配的描述符，从而保证数据在端到端传输过程中被精确放置，无需任何额外的中间缓冲或后处理重排。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（2） 高效通信算子&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管前面通过描述信息已经可以精确表达 “哪些 token 从哪里来、到哪里去”，但一个更现实的问题随之而来：&lt;strong&gt;在引入细粒度重排语义之后，通信还能否保持原有的吞吐效率？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;FUSCO 的答案是：&lt;strong&gt;通过一套流水线化的执行方式，将布局整理与数据传输紧密地绑定在一起。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在机内通信场景下，发送端和接收端位于同一台机器，FUSCO 直接使用 GPU 到 GPU 的点对点拷贝。关键在于，描述信息的解析被嵌入到拷贝路径之中：GPU 在执行数据拷贝的同时，按照描述信息从分散的位置读取数据，并直接写入目标布局对应的位置。整个过程中不会引入额外的中间缓冲或额外的内存遍历。&lt;/p&gt;&lt;p&gt;跨机通信则需要经过网卡，而要充分利用网络带宽，必须持续提供足够大的发送数据。为此，FUSCO 并不会把每个数据段单独进行一次发送，而是将多段数据组织成较大的发送单元，每个发送单元包含多个逻辑段。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LRjWWibhVh2mGC5azyH1E0y6yOLibLiaKJup6PxCKIZFfqlJzUZNwzldxw/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=5" data-ratio="0.6611111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526330" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f2f48952-ce2a-4f71-b658-2bc566a7790d/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;FUSCO 跨机通信流水线执行路径&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在此基础上，FUSCO 将跨机通信组织为一条清晰的流水线执行路径：GPU 作为生产者，按照描述信息依次收集数据、完成布局整理，并将结果写入发送缓冲区；网卡作为消费者，一旦发现缓冲区中有就绪的数据单元，便立即发起 RDMA 传输。&lt;/p&gt;&lt;p&gt;由于单个发送单元的网络传输时间通常长于 GPU 准备该单元所需的时间，GPU 侧的内存操作可以稳定地与网络传输重叠，使通信链路始终保持高利用率。&lt;/p&gt;&lt;p&gt;通过这种设计，&lt;strong&gt;数据重排不再是通信前后的附加步骤，而是被直接嵌入到一次点对点通信的执行过程中完成。在引入灵活重排能力的同时，FUSCO 依然能够维持与高性能通信库相当的带宽效率。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通信调度和策略：跨机优化与负载均衡的 token 传输&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;FUSCO 的通信调度优化围绕两种数据重排操作展开：gather-send 和 scatter-recv。&lt;strong&gt;其核心目标是在消除重排的基础上，减少跨机传输量并平衡各设备通信负载。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为此，系统会先生成一份详细的执行计划，将 MoE 的 token 路由信息转化为可直接执行的低层指令。计划中明确了每个 token 的来源、目标 GPU 以及目标节点的位置，使 gather-send 和 scatter-recv 能直接利用这些元数据，无需在通信前、通信中、通信后进行额外的数据重排操作。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LX2yAkQA0I4NTibhcx9BHbQk8MY3HnOT1KGVbjkiaCwhjccjsveaJcGMA/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=6" data-ratio="0.6787037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526337" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/8f38c713-d466-47cc-a5ca-ba0963034e78/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; FUSCO 通信调度策略&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在生成执行计划时，FUSCO 首先考虑了&lt;strong&gt;跨节点通信的效率问题&lt;/strong&gt;。直接将每个 token 发送到目标节点的所有 GPU 会导致重复传输。为解决这一问题，FUSCO 为每个发送 GPU 在每个目标节点指定一个 “转发 GPU”：当某个 GPU 需要向同一节点的多个 GPU 发送相同 token 时，转发 GPU 会先接收发送端的数据，然后通过节点内部高速链路（如 NVLink）将数据分发给该节点的其他 GPU。这样不仅减少了跨节点传输，也充分利用了节点内的高速网络，让数据流动更顺畅。&lt;/p&gt;&lt;p&gt;同时，FUSCO 还考虑了&lt;strong&gt;转发 GPU 的选择&lt;/strong&gt;。如果总是集中在少数 GPU 上，容易形成网络热点。FUSCO 通过将转发 GPU 组织成通信组来解决这一问题，确保高负载 GPU 分散在不同组中，实现跨节点负载均衡。这样每块 GPU 都不会因数据过多而成为瓶颈，整个网络的利用率也更高。&lt;/p&gt;&lt;p&gt;总结来看，FUSCO 的通信调度策略主要通过三方面提升效率：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;精确执行计划&lt;/strong&gt;：每个 token 直接到达目标 GPU 的对应内存位置，无需额外重排。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;分层转发&lt;/strong&gt;：跨节点只传输一份，节点内快速分发，减少重复传输。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;在线负载均衡&lt;/strong&gt;：在运行时根据实际 MoE 路由流量动态构建通信组，高负载 GPU 均匀分布。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们基于 NCCL 实现了 FUSCO，在保持与 NCCL 相同网络兼容性的同时，复用了其高效通信能力，让 FUSCO 可以专注于算法优化。FUSCO 为 MoE 层提供了简单直观的 dispatch/combine 接口，可无缝接入现有 LLM 训练和推理框架。&lt;/p&gt;&lt;p&gt;不同于 DeepEP 仅能在特定网络环境（ibgda, NVLink, RDMA）下工作，FUSCO 能在多种网络环境下高效运行，无需针对网络做额外调优。&lt;/p&gt;&lt;p&gt;简而言之，FUSCO 可以作为 MoE 框架中 AlltoAll 通信的高效解决方案，同时兼顾性能与易用性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结果与分析 &amp;nbsp; FUSCO 的性能与优势&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通信性能：完全消除 MoE 模型通信数据重排开销，效率 2 倍优于 DeepEP&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在应用上，与现有的通信库相比，FUSCO 的最大特点在于&lt;strong&gt;完全消除了 MoE 模型通信中的数据重排开销，并在此基础上实现跨节点 token 去重和节点内高速分发，从而显著提升通信效率。系统适配主流 MoE 训练和推理框架和 GPU 架构，在各种典型的 MoE 路由流量场景都能够稳定降低延迟和提升吞吐。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在量化评测中，我们构造了三种具有代表性的 MoE 通信流量配置进行测试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;第一种是真实推理流量，直接采用大模型推理过程中实际产生的 MoE 路由结果，能够反映真实场景下的通信特征；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第二种是单节点路由流量，即一个 token 被路由到的 topk 个 expert 都在同一节点上，此时跨节点只需要传输一份数据，主要考察系统对冗余跨节点通信的消除能力；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第三种是负载不均衡流量，不同 GPU 间通信量差异显著，用于模拟热点 GPU 和通信倾斜严重的极端情况，重点评估系统的负载均衡能力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这三种配置均使用 64 张 GPU 进行性能测试，分别测试每卡文本长度 4K/8K/16K/32K 的情况，总文本长度最大可达 2048K。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LHDL9h4HEkOibRz0Zznfuib1LXh7ZEUyxugbmNWIu7OYB72YVNUNy5R3Q/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=7" data-ratio="0.5502846299810247" data-s="300,640" data-type="png" data-w="1054" type="block" data-imgfileid="503526333" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/a4a9b036-830c-4b59-9927-2a4d41ed0ba4/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 真实通信数据重排负载下的性能对比（64 GPUs，文本长度可达 32K*64，下同）&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L45gw4uDEaEKGDCFy34wia2tXk6VUTJficejAZOxTvFpDyO5RZjuJlMOA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=8" data-ratio="0.5137614678899083" data-s="300,640" data-type="png" data-w="872" type="block" data-imgfileid="503526332" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/4f783d42-6592-49de-a5c1-2b83083652fa/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 每个 token 仅会被路由到一个设备上的多个 expert 下的性能对比&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LrTLvqY7NHyNpEr1sRDicCsVJqVEGvr1xIyORb0Fnjp1he05sEyiaeMRA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=9" data-ratio="0.5" data-s="300,640" data-type="png" data-w="872" type="block" data-imgfileid="503526334" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/c7eb2799-ff58-47c5-9696-93fd84f8d891/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 设备之间负载不均衡情况下的性能对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;实验结果表明，在上述三种典型流量配置下，&lt;strong&gt;FUSCO 相比 NCCL 和 DeepEP 均能取得更高的通信效率。相较于 NCCL 和 DeepSeek 的 DeepEP 通信库，FUSCO 的通信性能最高可分别提升 3.84 倍和 2.01 倍，而且文本长度越长加速越明显。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;无论是在真实推理环境、跨节点通信最小化的场景，还是存在严重负载不均衡的情况下，&lt;strong&gt;FUSCO 都能稳定降低通信开销，为 MoE 模型的训练与推理提供更加高效、可靠的通信支持。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;端到端性能：训练与推理效率全面提升，最高 1.39 倍优化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在全模型训练和推理中，FUSCO 同样展现出明显优势。我们在 64 张 GPU 上对 Qwen3-235B-A22B 和 DeepSeek-V3 两种代表性 MoE 模型进行了评测，涵盖模型单轮训练时间和推理首 token 响应时间。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LicxVC041yGS9PKTF6YPFcLfZRGvrKxFcoCxdQ3cOnlItLOO0gtYwibxw/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=10" data-ratio="0.5518518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526335" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/9d44b97b-7dca-4b74-9ecb-1dcfdf11da31/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; FUSCO 带来的端到端训练与推理的性能提升&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果显示，在训练任务中，FUSCO 相较于 NCCL 性能最高提升 1.39 倍，相较于 DeepEP 性能最高提升 1.19 倍 ；在推理任务中，FUSCO 相较于 NCCL 性能最高提升 1.25 倍，相较于 DeepEP 性能最高提升 1.16 倍。且在实际部署中，模型规模越大，性能提升越显著。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;FUSCO 通过将 MoE 模型的 token 路由信息直接转化为可执行的 gather-send 与 scatter-recv 通信原语策略&lt;strong&gt;，彻底消除了传统通信前后的数据重排开销&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在多节点 64 GPU 测试中，相较于 NCCL 和 DeepEP，FUSCO 的通信性能分别提升了 3.84 倍和 2.01 倍，同时端到端性能增幅最高达 40%。&lt;/p&gt;&lt;p&gt;无问芯穹这一创新方案不仅&lt;strong&gt;为大规模 MoE 模型提供了可扩展、低成本的通信支持，为大规模 MoE 模型的通信优化提供了可验证的创新示范&lt;/strong&gt;。更有力推动了面向 Agent 的硬件效率潜能的释放，加速智能体的规模化高效落地。&lt;/p&gt;&lt;p&gt;相关代码和使用示例现已开源，欢迎在实际项目中下载测试与应用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;开源地址：https://github.com/infinigence/FUSCO&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://www.arxiv.org/abs/2512.22036&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>「视频世界模型」新突破：AI连续生成5分钟，画面也不崩</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 17:48:45 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-9</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/b2395540-7872-4a01-8c0f-9c42e8cddeae/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当 Sora 让世界看到了 AI 生成视频的惊艳效果，一个更深层的问题浮出水面：如何让生成的视频不只是「看起来像」，而是真正理解并遵循物理世界的规律？这正是「视频世界模型」（Video World Model）要解决的核心挑战。当生成时长从几秒扩展到几分钟，模型不仅要画面逼真，更要在长时间尺度上保持结构、行为与物理规律的一致性。然而，误差累积与语义漂移往往导致长视频出现画面退化与逻辑崩坏 &amp;mdash;&amp;mdash; 这已成为衡量世界模型能力的关键瓶颈。&lt;/p&gt;&lt;p&gt;围绕这一挑战，上海人工智能实验室联合复旦大学、南京大学、南洋理工大学 S-Lab 等单位提出了&lt;strong&gt;&amp;nbsp;LongVie 2&lt;/strong&gt;&amp;mdash;&amp;mdash; 一个能够生成长达 &lt;strong&gt;5 分钟高保真、可控视频的世界模型框架&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrlRbtCU8udhlgPCzzUqpPj0KYbicFRZGIgVyDdvOLEy5ia4fu5ISFvXJw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.6638888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525990" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ec107e81-cdd0-472a-8bf2-f74ea3442fa9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; LongVie 2 可自回归生成 3-5 分钟的超长可控视频&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="2 2 []"&gt;论文：https://arxiv.org/pdf/2512.13604&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://vchitect.github.io/LongVie2-project/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub：https://github.com/Vchitect/LongVie&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;视频演示：https://www.youtube.com/watch?v=ln1kMNYj50Y&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/oMWv6P6mm21XMk9bpZtKXg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/26bb2f16-70f3-492a-9a5e-e504780e4f52/1767174301445.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;strong&gt;什么是理想的视频世界模型？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;一个理想的视频世界模型，不应只是「生成得更久」，而应同时具备以下三项核心能力：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;全面可控性（Comprehensive Controllability）：能够在长时间生成过程中稳定响应多种控制信号，保持场景结构与运动意图不漂移；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;长期视觉保真（Long-term Fidelity）：随着时间推进，画面质量不发生明显退化，不出现纹理崩塌或细节丢失；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;长程上下文一致性（Long-context Consistency）：跨片段、跨时间保持语义、身份与物理规律的一致，避免「换世界式」断裂。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;现有世界模型的瓶颈在哪里？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本文系统调研了当前主流的视频世界模型，发现一个共同问题：随着生成时长的增加，模型的可控性、视觉保真度与时间一致性会同步下降。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrKYa72CzzbvIcoXw4ImLlft3opNGgDXhNaxwP0IqBOIVgt7n8F6ibE2Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.2824074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525992" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/43cfe190-bf6c-4ff9-b0dd-90390be02ef3/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 现有模型在长时间生成时的退化问题&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;LongVie 2：三阶段递进式训练&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为系统性解决上述挑战，LongVie 2 设计了一套逐层递进的三阶段训练策略，从控制、稳定性到时间一致性层层强化：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrEibYQ0VibHwSj2s8E9kFT3tyaQjxmic0WvGEq7kRicAgmfOQYw8jnVAflA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.20462962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525996" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ae91c531-35eb-47e4-a417-982737e18278/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; LongVie 2 三阶段训练流程&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;阶段一：Dense &amp;amp; Sparse 多模态控制&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过引入稠密信号（如深度图）与稀疏信号（如关键点轨迹），为模型提供稳定且可解释的世界约束。这使生成过程不再完全依赖隐式记忆，从源头提升长程可控性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;阶段二：退化感知训练（Degradation-aware Training）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;长视频生成中，质量衰减几乎不可避免。LongVie 2 的核心创新在于：在训练阶段主动「制造困难」&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr87gjI76SaVtjosaZjpUMhXibDiaDvYICvoQEa4toUazY4VWrOIaqys0A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.42314814814814816" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525999" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/ef79ec55-997c-4a4d-9360-e6e1423ad636/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 退化感知训练示意图&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;利用 VAE 的多次 encode-decode 模拟重建误差；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;通过 加噪 + Diffusion 去噪 构造退化图像。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以此作为训练信号，使模型学会在不完美输入下保持稳定生成，显著增强长期视觉保真度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;阶段三：历史上下文建模&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在生成过程中显式引入历史片段信息，并通过针对性 loss 约束相邻片段的衔接，使跨片段过渡更加自然顺畅，有效缓解长视频中的语义断裂与逻辑跳变问题。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrys4yLGuRsWIYPUdEEr1MOKGIibE0Yf4RHf4WHBFaltlFsH50muNq0VQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.31851851851851853" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526003" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/464d0593-861b-4e94-bdff-49221d7d8864/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 三阶段训练效果对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一图看懂 LongVie 2 框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过多模态控制、退化感知训练与历史上下文建模的协同设计，LongVie 2 将长视频生成从「片段拼接」提升为持续演化的世界建模过程：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrdgKTDN4syCdCTZbbKnplv6U1thn84yseZyRoXfvuD8zK2BeOuAMAEg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.3333333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526006" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/20186ee2-83b0-4589-a136-479d08970f16/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; LongVie 2 整体框架&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;从左至右，LongVie 2 首先将跨片段的稠密（深度）与稀疏（关键点）控制视频做全局归一化，并为所有片段采用统一的噪声初始化。随后在每一片段生成时，将全局归一化后的控制信号、上一片段的末帧与文本提示送入模型，逐步生成完整的长视频。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;LongVie 2 能力展示&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该研究将&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;LongVie 2&lt;/span&gt;与 Go-With-The-Flow 和 Diffusion As Shader 进行了对比。结果显示，LongVie 2 在可控性方面表现显著优于现有方法：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvryd0bCnLSkzkaaCgyzxzKonb9a2B3oJdLG0cf85ooIGRyzSazCQVgow/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.33055555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526007" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/77e6838e-3663-4f0f-805a-35ec84d0a1a5/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 与现有方法的可控性对比&lt;/sup&gt;&lt;a href="https://mp.weixin.qq.com/s/oMWv6P6mm21XMk9bpZtKXg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/98e05880-2b4b-461b-86e9-f6cb067b414f/1767174384227.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/oMWv6P6mm21XMk9bpZtKXg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/dd08b91d-ff10-405a-add9-a7648175e6ce/1767174400600.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;消融实验也充分验证了三阶段训练的有效性：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrJF6gYQswzziagrXlQcXCJwZLt249VrGJANnic2by0t2o71y4Le4uWQZw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.22777777777777777" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526020" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/d4853a60-ae19-49f6-b35e-4d8bf45162ce/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 消融实验结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;LongVGenBench&amp;nbsp; 首个可控超长视频评测基准&lt;/p&gt;&lt;p&gt;当前缺乏面向可控长视频生成的标准化评测。为此，本文提出 &lt;strong&gt;LongVGenBench&lt;/strong&gt;&amp;mdash;&amp;mdash; 首个专为超长视频生成设计的基准数据集，包含 &lt;strong&gt;100 个时长超过 1 分钟&lt;/strong&gt;的高分辨率视频，覆盖真实世界与合成环境的多样场景，旨在推动该方向的系统研究与公平评测。&lt;/p&gt;&lt;p&gt;定量评估与用户主观测评结果显示，LongVie 2 在多项指标上达到 &lt;strong&gt;SOTA 水平&lt;/strong&gt;，并获得最高用户偏好度：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrvRjk8Uoldia3iaZWewjNnWcTm3ZnQ5LyQiaah4k6q2bnEFOzcpRO4YkMQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.35648148148148145" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526018" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/ca54fca3-c428-4543-a40d-c2c4b3e618e2/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrHm8SCCEWolUQJ4Er2ZEbuaWssq0cHIyRFZEDceicjqc5dItNVTlSfVg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.46835443037974683" data-s="300,640" data-type="png" data-w="948" type="block" data-imgfileid="503526019" data-aistatus="1" data-original-style="width:472px;height:221px;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/3cf8e219-f29f-425e-921f-5d708e10c2c4/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 定量评测结果与用户研究&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，稚晖君发布的人形机器人Q1，小到能塞进书包</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 16:39:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-8</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/792d8dab-7b17-4a86-a434-9bfe2eb54f7a/1767170079344.png" style="width: 700%;" class="fr-fic fr-dib"&gt;这就是 2025 科技界最后一个重磅产品？&lt;/p&gt;&lt;p&gt;12 月 31 日下午，稚晖君（彭志辉）的机器人公司智元机器人正式发布了&lt;strong&gt;全球首款、全身力控的小尺寸人形机器人 Q1&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L4qSPzEqy30NTU4ziaK95hnNJr19EyL9rFEYDhJkicQUeJca6ibn8BcS0A/640?wx_fmt=gif&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.752" data-type="gif" data-w="500" data-width="500" data-height="376" data-backw="500" data-backh="376" data-imgfileid="503526413" data-aistatus="1" data-original-style="width:100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f18616c1-0ffd-4cbe-b935-47c372236dbf/640.gif" data-order="0" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;17 年前，乔布斯把笔记本电脑塞进了信封；17 年后，稚晖君把完整的人形机器人塞进了书包。&lt;/p&gt;&lt;p&gt;Q1 的出现，似乎预示着稚晖君「极客精神」的回归：它试图将全尺寸机器人的动力性能与智能化压缩至背包大小，从而定义未来个人机器人的基本范式。&lt;br&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lv1R5pheOvTejszCk5FiaOwzB5ePLBMcPQgbkjGR72ibHSjibZC47icmlEg/640?wx_fmt=gif&amp;amp;from=appmsg#imgIndex=2" data-ratio="0.5634847080630213" data-type="gif" data-w="1079" type="block" data-backw="562" data-backh="317" data-imgfileid="503526396" data-aistatus="1" data-original-style="width:100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/39f5c449-7946-4694-bd47-c051a4ab95b2/640.gif" data-order="1" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;它并不是为了小体积而缩小 ——Q1 的设计初衷，是为了在保持全尺寸人形机器人能力的条件下，极大降低科研成本与物理交互的门槛。用稚晖君的话说：「当机器人变小，世界的物理法则也变得温柔。」&lt;/p&gt;&lt;p&gt;换句话说，正常尺寸机器人能做的它都能做，还能做得更好。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lc8E9Z3vXHicFWbhOTsYpyVkhj81sb7p6S7icBJNItzNLjF0dsENv51Pg/640?wx_fmt=gif&amp;amp;from=appmsg#imgIndex=3" data-ratio="0.58" data-type="gif" data-w="500" data-width="500" data-height="290" data-backw="500" data-backh="290" data-imgfileid="503526415" data-aistatus="1" data-original-style="width:100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/57239511-f39a-4ab3-9875-6e3d21b027a9/640.gif" data-order="2" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Q1 面向高校、创业公司的科研团队，开发工具包和接口完全开放；同时又面向硬核潮玩市场，极客们可以利用 3D 打印技术为其打造专属的皮肤和装甲，不论是猫娘还是蒸汽朋克，颜值完全由用户决定。&lt;br&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Ly5WSQPX4jFChFUKibdTCuCwZt3AaZA6oehmX6icdcN7BtzXsbqQhtUvg/640?wx_fmt=gif&amp;amp;from=appmsg#imgIndex=4" data-ratio="0.8733333333333333" data-type="gif" data-w="450" type="block" data-backw="450" data-backh="393" data-imgfileid="503526414" data-aistatus="1" data-original-style="width:100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/07eb4821-e69e-415a-bbc4-564bb209ad73/640.gif" data-order="3" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;稚晖君也表示，机器人也可以与机器人合作，就像 AI 大模型的智能体一样。（机器狗：你礼貌吗）&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LYejSeRcCxf3vGAyXtTeS3AicMCTmKZaUgzT6ocLaMBjjKn11dLw3Y3A/640?wx_fmt=gif&amp;amp;from=appmsg#imgIndex=5" data-ratio="0.77" data-type="gif" data-w="500" type="block" data-backw="500" data-backh="385" data-imgfileid="503526424" data-aistatus="1" data-original-style="width:100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/0bc2dce0-b0b0-4ef5-a02c-d09f434d86a7/640.gif" data-order="4" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;Q1采用了模块化的硬件设计，支持头部等位置的整体更换，配合零门槛的动作编排。它就像一个等待提示词的AI，你可以让它在物理世界完成很多任务。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;极致微型化与动力学的革命&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;人形机器人在今年属实是大放异彩。&lt;/p&gt;&lt;p&gt;从春晚舞台上机器人扭秧歌的趣味亮相，到王力宏演唱会上它们与人类伴舞并完成高难度动作、甚至引来马斯克点赞，再到社交平台上各种关于机器人乌龙笑话的刷屏……&lt;/p&gt;&lt;p&gt;但这些机器人都是全尺寸的「等身」大小。&lt;/p&gt;&lt;p&gt;为什么做小这么难？&lt;/p&gt;&lt;p&gt;小尺寸人形机器人Q1有个关键词&lt;strong&gt;：&lt;/strong&gt;全身力控。&lt;/p&gt;&lt;p&gt;机器人领域中的&lt;strong&gt;全身力控（Whole-Body Control, WBC）&lt;/strong&gt;是一种高级控制技术，用于协调机器人全身多个自由度（如关节、腿、臂等）在与环境接触时的力与运动，同时综合考虑平衡、姿态、任务优先级等因素。相比传统只控制末端执行器运动或位置，全身力控让机器人不仅实现「去哪儿」，还要精确「怎么用力」去完成任务。&lt;/p&gt;&lt;p&gt;为了实现精准的全身力控，在主流的人形机器人设计中，普遍采用QDD关节来打造机器人本体，是高性能机器人的核心组件。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LLAx7uSQC8qGs2RqZfEdrIPVxiao0DOcotMYMggGwC8ZjEicOib2X5OqNg/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=6" data-ratio="0.4842592592592593" data-type="png" data-w="1080" data-width="3506" data-height="1698" data-backw="562" data-backh="272" data-imgfileid="503526412" data-aistatus="1" data-original-style="width:100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e79439b0-8437-4867-b04d-8f6c01454437/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;QDD（准直驱）&lt;/strong&gt;是一种介于传统高减速比伺服驱动与「真正直驱」之间的执行器设计：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;使用低减速比的减速结构连接电机和输出轴；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;结合高扭矩密度电机和低传动比，保留电机侧低惯性、良好背驱性，有利于高带宽力控和动力反馈；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;相比传统方案（如高比减速+伺服电机），QDD 在力量感知、外力互动、动态响应方面有明显优势。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;1、高扭矩密度电机难以缩小体积&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;QDD 的核心是强力矩但低传动比的电机，这要求在有限的体积下产生高扭矩，同时保持低反射惯量。&lt;/p&gt;&lt;p&gt;但电机本身的扭矩密度受电磁材料、磁路尺寸、绕组填充率等物理极限制约。在缩小尺寸时，线圈填铜率下降、散热变困难、磁通密度不足都会导致输出扭矩显著下降。&lt;/p&gt;&lt;p&gt;2、&lt;strong&gt;热管理难题更突出&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;高扭矩密度意味着大电流密度，而小体积里散热条件非常受限。在 QDD 里没有传统高减速比「缓冲&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;，电机必须自己承担大部分工作，这对热管理提出了非常高的要求。&lt;/p&gt;&lt;p&gt;3、&lt;strong&gt;制造和材料工艺挑战更高&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;小尺寸制造对工艺要求极高：磁体、绕组、轴承都要极高精度；编码器、高性能驱动电子器件要集成在狭小空间。&lt;/p&gt;&lt;p&gt;这种高集成度本身就是技术门槛极高的交叉学科工程。&lt;/p&gt;&lt;p&gt;正如电子产品的发展规律所揭示的那样——&lt;strong&gt;尺寸越小，水平越高&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;智元机器人 Q1 在全身力控人形机器人领域创造了新的&lt;strong&gt;「迷你皇冠」&lt;/strong&gt;，将原本结构复杂、制造门槛极高的 QDD 关节压缩至鸡蛋大小的体量，不仅刷新了工程极限，也集中展现了其卓越的技术创新能力与扎实的工业实力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;属于你的个人机器人来了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在国内具身智能领域，智元机器人作为明星公司一直处于聚光灯下。该公司成立于 2023 年 2 月，在不到三年的时间里估值已飙升至 150 亿元人民币。&lt;/p&gt;&lt;p&gt;今年 7 月，智元机器人相关主体宣布完成了对 A 股科创板上市公司上纬新材的控股，原主营新材料业务的上市公司开始转向具身智能机器人布局。今年11月，智元团队成员（包括稚晖君）已进入董事会，并推动业务转型与机器人板块落地。&lt;/p&gt;&lt;p&gt;与资本运作同样快速的是智元机器人的技术发展与落地。目前智元已完成机器人本体、三大智能技术的快速迭代，灵创、灵心、精灵平台及灵渠OS等技术生态也逐渐完善。智元机器人已经构建了核心零部件、传感器等供应链。&lt;/p&gt;&lt;p&gt;智元机器人的产品远征、灵犀、精灵三大系列，分别面向工业、服务与消费端，构建了全场景覆盖的格局。在今年 12 月初，该公司已经达到了5000 台机器人量产下线的里程碑。&lt;/p&gt;&lt;p&gt;如今 Q1 的问世，势必会让人形机器人成为更多普通用户的伙伴。&lt;/p&gt;&lt;p&gt;未来的 AI 交互终端真的是个大屏设备吗？或许，正确的答案是一个「跟你长得很像」的机器人。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>7B扩散语言模型单样例1000+ tokens/s！上交大联合华为推出LoPA</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 16:31:48 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-7</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/ec55e9db-9e8b-41d7-b248-bcbf30d2d013/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;a href="https://mp.weixin.qq.com/s/9PrF80XDWTxF8aj7jcWQjQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8d643edd-4608-4d4f-a66d-5e23163a9783/1767169769443.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;sup&gt;视频 1：单样例推理速度对比：SGLang 部署的 Qwen3-8B (NVIDIA) vs. LoPA-Dist 部署 (NVIDIA &amp;amp; Ascend)（注：NVIDIA 平台相同，配置对齐）&lt;/sup&gt;&lt;/section&gt;&lt;p&gt;在大语言模型（LLMs）领域，扩散大语言模型（dLLMs）因其并行预测特性，理论上具备超越传统自回归（AR）模型的推理速度潜力。然而在实践中，受限于现有的解码策略，dLLMs 的单步生成往往局限于 1-3 个 Token，难以真正释放其并行潜力。&lt;/p&gt;&lt;p&gt;近期，&lt;strong&gt;上海交通大学 DENG Lab 联合华为的一项新研究&lt;/strong&gt;打破了这一瓶颈。该工作提出了一种名为 &lt;strong&gt;LoPA （Lookahead Parallel Decoding） 的无需训练的解码算法&lt;/strong&gt;，通过主动探索最优填词顺序，显著提升了 dLLMs 的推理并行度和吞吐量。&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;本文作者团队来自上海交通大学 DENG Lab 与华为。该研究由徐晨开、金义杰同学等人共同完成，指导教师为邓志杰老师。DENG Lab 隶属上海交通大学，致力于高效、跨模态生成模型的研究。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503525993" data-ratio="0.23796296296296296" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrmCIDZcbftwiaiasvc5pOHa2rlg8dqYWIWSfEasH95NHUfAq9pia9YLPfQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/581a3570-86a2-49b9-a273-8b58f716889a/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="2 2 []"&gt;论文地址：https://arxiv.org/abs/2512.16229&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码地址：https://github.com/zhijie-group/LoPA&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;博客地址：https://zhijie-group.github.io/blogs/lopa&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实验显示，LoPA 将 D2F-Dream 在 GSM8K 基准上的单步生成 Token 数（TPF）从 3.1 提升至 10.1，并行度提升超 3 倍。配合团队自研的 LoPA-Dist 分布式推理系统，在华为 Ascend 910C 平台上实现了 1073.9 tokens/s 的单样本吞吐量，不仅大幅超越基线模型，更将 dLLMs 的推理效率推向了新高度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr40mkVZlMSHBImjrqBgb0HsABY9dJhrb5kUFRjDkg4bBkFwgevAibGwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3787037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525995" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d44d52ee-12e3-4ae3-8e2e-f994d1582aaa/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 1：LoPA 的吞吐量结果展示。LoPA 将 D2F-Dream 的单样本吞吐量在 MBPP 和 GSM8K 上分别提升至高达 1073.9 和 856.5 个 token/s，显著优于基线方法。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;简单来说，LoPA 为 dLLMs 赋予了以下核心特性：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 极高的并行度&lt;/strong&gt;：首次将 dLLMs 的每步生成数量（TPF）提升至 10 Token 量级，突破了传统方法的效率瓶颈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 无需训练&lt;/strong&gt;：作为一种即插即用的解码算法，无需对模型进行重训或微调。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 前瞻并行解码&lt;/strong&gt;：通过引入分支并行机制，主动探索不同的填词顺序（TFO），避免模型陷入低置信度的局部最优。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4. 系统级加速&lt;/strong&gt;：配套设计的 LoPA-Dist 系统，支持 CUDA 和 Ascend 双平台，通过分支并行最大化硬件利用率。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrAVTa0Av4JokQ6bkQWjtDOW3Y2WuG2Qw6HPh0Vg0NNXLCXOyrGefTvQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3101851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525997" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/e5dc36aa-3c77-4e93-95a4-0f6d1ec6aff5/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;sup&gt;图 2：对不同分支数的 D2F-Dream 进行 LoPA 扩展性分析。结果表明，LoPA 能有效扩展 D2F 的 TPF，使其峰值超过 10，从而显著减少解码总步骤数。&amp;nbsp;&lt;/sup&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;问题的根源：填词顺序限制并行潜力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;dLLMs 理论上支持全序列并行生成，但在实际应用中，现有的主流模型（如 Fast-dLLM, D2F, SDAR）普遍采用置信度驱动采样（Confidence-Driven Sampling）。这种策略倾向于贪婪地优先填充当前置信度最高的位置。&lt;/p&gt;&lt;p&gt;研究团队发现，并行度的高低与填词顺序（Token Filling Order, TFO）高度相关。贪婪策略虽然在当前步骤保证了准确性，但并不考虑后续步骤的预测置信度，导致模型在后续迭代中并没有充分释放并行度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrOlCXrE4oWvNSUMa2ohllta6fjOEQVOTbWz07nLSfjRHIul59rJ1faQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5962962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525998" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b2afb370-6502-43ec-a3fe-74c55c4c976c/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 3：LoPA 算法流程概览。在每次迭代中，LoPA 通过独立采样高置信度位置，生成一个锚定分支以及多个前瞻分支。然后，分支置信度验证机制并行评估所有分支，以选择最优路径。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;LoPA 的核心设计：前瞻并行与分支验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了解决上述问题，LoPA 引入了前瞻并行解码机制。其核心思想是：利用少量的额外计算开销，同时探索多种填词顺序，从而找到一条能让未来预测 &amp;ldquo;更自信&amp;rdquo; 的路径。&lt;/p&gt;&lt;p&gt;LoPA 的工作流程包含三个关键阶段：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 多分支并行探索&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LoPA 在保留标准锚点分支（Anchor Branch，即常规贪婪策略）的同时，额外对当前的最高置信度的 k 个位置分别采样得到 k 个前瞻分支（Lookahead Branches）。每个分支代表一种不同的填词顺序尝试。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 分支置信度验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队设计了分支置信度（Branch Confidence）指标，用于量化分支中剩余未填位置的平均预测置信度。较高的分支置信度意味着该路径在下一轮迭代中能填充更多的 Token，具备更高的并行潜力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 并行验证与复用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过隔离不同分支的注意力设计，所有候选分支（锚点 + 前瞻）可以在一次前向传递中并行完成验证。系统最终选择未来潜力最大的分支作为本次迭代结果。验证过程中计算的 Logits 被直接复用于下一步生成，无需额外前向传播。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526000" data-ratio="0.6018518518518519" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrsicUUFoiaNv6tjWJbTL9MJOqREBLXq64ownxGVSoBAoRRuX5n9zJRicZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/fbf85e22-09d3-4b27-b0f2-7a5073221868/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 4：LoPA 分支并行分布式推理系统设计展示。关键区别在于针对不同后端定制的键值缓存管理协议：LoPA-Dist-NV 采用稳健的两阶段更新机制以确保一致性，而 LoPA-Dist-Ascend 则采用精简的单阶段更新策略以优化服务效率。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;系统级创新：LoPA-Dist 分布式推理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了承载 LoPA 的多分支计算，团队设计了 LoPA-Dist 分布式推理系统，引入了全新的分支并行（Branch Parallelism, BP）策略，可与张量并行（Tensor Parallelism，TP）等现有并行机制混合使用。&lt;/p&gt;&lt;p&gt;该系统针对不同硬件平台进行了定制优化：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. LoPA-Dist-NV（CUDA）&lt;/strong&gt;：面向低延迟场景。采用静态 KV Cache 和独创两阶段更新协议（Pre-Write &amp;amp; Commit-Winner-Cache），确保分支切换时的缓存一致性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. LoPA-Dist-Ascend（Ascend 910C）&lt;/strong&gt;：面向高吞吐服务场景。采用混合并行策略（TP+BP），结合图编译技术融合算子，异步调度，以及量化机制，大幅降低 Kernel 启动开销。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrBOnCKoqzdiaOWLcMWku94icCuQgQOiamYek68GM0CVKFsQkqfY7LUCwYg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.6398148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526001" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/2c172334-72b4-484c-a917-17c4aa2e7765/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 5：LoPA 的并行度扩展曲线。在 GSM8K 和 HumanEval+ 上，LoPA 分别将 D2F-Dream 和 D2F-DiffuCoder 的 TPF 分别扩展至高达 10.1 和 8.3，并保持和基线相当的性能。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果：速度与质量的双重提升&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;并行度：单步突破 10 Token&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LoPA 在 SOTA 扩散语言模型 D2F 上进行了实验。实验结果表明，随着前瞻分支数量的增加，模型的 TPF 呈现显著上升趋势。在 GSM8K 任务上，LoPA 将 D2F-Dream 的 TPF 推高至 10.1，大幅缩短了总推理步数。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrpsaicH7e317ShaFC3ZgNnXwu3rgntSJBdV5VGlOEs5fXjvSTGiaA9aPw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.25277777777777777" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526002" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/b7843eb4-40f4-48bf-abf2-4e908fc194b1/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表 1：LoPA 集成 D2F-Dream 的性能。LoPA 集成的 D2F-Dream 在多个基准测试中实现了保持精度的 TPF 提升。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrlz2ognLMLN0TK7BEV9n7JgrlibN6557NKx1eC93q0l0ekhVkH5lzibrw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.2919847328244275" data-s="300,640" data-type="png" data-w="1048" type="block" data-imgfileid="503526004" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/0d187380-47ee-461d-90c7-3ce04790c11e/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表 2：LoPA 集成 D2F-Diffucoder 的性能。LoPA 集成的 D2F-DiffuCoder 在代码任务中实现了保持精度的 TPF 提升。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;系统吞吐量&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在系统层面，LoPA-Dist 展现了优异的扩展能力。在华为 Ascend 910C 平台上，系统实现了 1073.86 tokens/s 的峰值吞吐量。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrw8CG2V57w9a1PriaEHoGssiadefeg6vWkgtLffsCtWLxlSRKWRFbXByw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.18703703703703703" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526005" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/803b5af7-b684-4225-a4a4-20a47669414c/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表 3：LoPA 系统性能。结果表明，我们的系统能够有效地将算法并行性（高 TPF）转化为显著的实际运行时间加速，在专用的 LoPA-Dist-Ascend 引擎上实现了超过 1000 token/s 的平均吞吐量。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LoPA 通过算法与系统的协同设计，成功突破了 dLLM 推理的并行度瓶颈，证明了非自回归模型在保持高性能的同时，能够实现远超传统模型的推理速度。团队表示，未来将进一步探索 LoPA 在 SDAR 等更多 dLLM 架构上的应用，推动高效生成模型的落地。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>卓驭打造科技平权，端到端辅助驾驶延伸至重卡高速、无人物流</title>
      <description>&lt;![CDATA[卓驭科技在步入第十年之际，由公司CEO沈劭劼进行了一场题为《行者》的演讲。]]&gt;</description>
      <author>李泽南</author>
      <pubDate>Wed, 31 Dec 2025 16:18:42 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-6</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;12 月 30 日，卓驭品牌盛典 2025 在深圳举行。&lt;/p&gt;&lt;p&gt;不同于常规的技术参数发布会，本次卓驭科技的发布更侧重技术路线阐释、工程哲学与未来布局。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/451dfb1d-22e4-4bcf-8346-e497dd023433/QQ20251231-161006.jpg" style="width: 67.52%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;作为从大疆独立仅一年多的智能驾驶公司，卓驭首次系统对外解读了其从规则驱动到数据驱动的技术转型、软硬一体的工程能力，以及从乘用车向商用车等多场景延伸的战略路径。&lt;/p&gt;&lt;p&gt;卓驭科技 CEO 沈劭劼在演讲中系统回顾了卓驭团队自 2016 年创立至今、从大疆车载迈向独立主体的发展历程，介绍了多模态端到端世界模型体系，并宣告了其 &amp;ldquo;数据驱动的空间智能移动基座&amp;rdquo; 正式成型。&lt;/p&gt;&lt;p&gt;卓驭的技术根基源自大疆的机器人工程基因，早期沿用规则驱动方案。但随着智能驾驶场景复杂度提升，基于规则的辅助驾驶系统陷入了 &amp;ldquo;解决一个问题，冒出十个新问题&amp;rdquo; 的工程困境。&lt;/p&gt;&lt;p&gt;2024 年 10 月 14 日，卓驭做出关键决策：全删原有代码库，全面转向端到端架构。在转型初期，团队面临模型不成熟、交付压力大、输出不稳定等多重挑战，但最终探索出一条差异化的技术路径。&lt;/p&gt;&lt;p&gt;借鉴 &amp;ldquo;巧力出奇迹&amp;rdquo; 的总体思路，卓驭不盲目堆算力与参数量，而是将视觉 - 语言 - 动作（VLA）模型拆解为多个可解释、可分工的模块。他们以较低成本攻克了行业公认的 &amp;ldquo;因果推理&amp;rdquo; 与 &amp;ldquo;低频数据生成&amp;rdquo; 两大难题，实现了端到端系统在中等算力平台上的可用性与可部署性。&lt;/p&gt;&lt;p&gt;卓驭提供高质量、低价格的辅助驾驶解决方案，通过极致优化，能够让 10 万元级车型也能具备城市领航辅助驾驶能力：其在地平线征程 8650 芯片上实现性能媲美 Orin X，通过网络压缩与优化，将端到端网络成功部署在 TI TDA4 等中算力平台，实现了 &amp;ldquo;中算力城市 NOA&amp;rdquo;。&lt;/p&gt;&lt;p&gt;与此同时，卓驭推出高通 8775 舱驾一体方案，以单芯片同时驱动智能座舱与智能驾驶系统，推动了整车架构向中央计算的方向演进。&lt;/p&gt;&lt;p&gt;历时九年，卓驭的辅助驾驶系统已历经千万公里真实道路的考验，卓驭也已经从行业新生力量成长为业界领军企业，乘用车领域覆盖 9 大客户 15 大品牌，量产覆盖 50 多个车型和所有动力构型，此外还有 30 + 款即将量产车型。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/73703826-6a81-4e1d-ac18-a52da2ee00ac/QQ20251231-160528.jpg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;卓驭着重建设 &amp;ldquo;全场景兜底能力&amp;rdquo;，希望通过持续建设全流程产业链能力、完善的软硬件能力，以及对客户 &amp;ldquo;产品交付&amp;rdquo;、对用户 &amp;ldquo;不放弃每一位&amp;rdquo; 的兜底能力。&lt;/p&gt;&lt;p&gt;基于这一判断，卓驭选择在统一架构上向上延展，而非重新搭建高算力方案。从 TDA4（中算力）到 8650 及更高算力平台，再到 8775（舱驾一体），其核心架构保持一致且可规模化复用。&lt;/p&gt;&lt;p&gt;目前，卓驭已推出两大高算力方案：一是 L3/L4 方案，搭载两块 Thor 芯片，配合自研激目前向感知系统和知周补盲雷达；二是舱驾一体方案，采用高通 SA8797，将 VLA 融入统一架构。基于对算力的极致运用，卓驭的工程实力获得英伟达、德州仪器等芯片厂商认可 &amp;mdash;&amp;mdash;&amp;ldquo;同样的芯片，在我们手里能跑出更高的效率&amp;rdquo;。&amp;quot;&lt;/p&gt;&lt;p&gt;面向未来，卓驭希望构建空间智能的移动基座，引领自主移动机器人时代，其技术能力将不再局限于乘用车辅助驾驶，而是依托数据驱动的开发范式、成熟的基座模型及软硬一体的工程能力，将移动智能的边界拓展至商用车等更广泛的业务场景。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2e0a68be-8bec-42f3-8929-5ed313113a8d/QQ20251231-160615.jpg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;卓驭宣布，在 2026 年上半年会上线重卡 NOA 辅助驾驶。&lt;/p&gt;&lt;p&gt;目前，卓驭已启动重卡高速 NOA 项目，旨在解决重卡司机长时间驾驶疲劳的痛点，提升干线物流的安全与效率水平，且已与徐工、陕汽、重汽三大业界头部客户确立合作，首批重卡车型将于 2026 年上半年正式量产。&lt;/p&gt;&lt;p&gt;基于端到端的辅助驾驶方式，正在为智能的泛化提供便利。卓驭透露，从乘用车到重卡的能力迁移，所需的时间不超过 1 个月。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/4cacd87b-d570-4a4f-9085-b1675e333c75/QQ20251231-160722.jpg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;同时，卓驭正联合商用车头部企业，共同设计和定义无人物流车，应用于矿山、港口等场景，这意味着在该项目中卓驭将不只是 tier1 供应商，而会参与到产品设计等更多环节。&lt;/p&gt;&lt;p&gt;昨天，卓驭发布了全新的使命、价值观与愿景：使命从原本的 &amp;ldquo;让给所有人带来安全轻松的出行体验&amp;rdquo;，升级为 &amp;ldquo;为世界提供安全、轻松的移动智能&amp;rdquo;，价值观升级为 &amp;ldquo;激极尽志、求真品诚、用户为本、成就客户&amp;rdquo;，并希望以此实现 &amp;ldquo;引领自主移动机器人时代&amp;rdquo; 的愿景。&lt;/p&gt;&lt;p&gt;未来，卓驭将会把移动智能推向更多场景。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>视远 · 正心明智——「AI 中国」机器之心2025年度评选正式揭晓</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 13:19:44 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-5</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9EricQXXByphb4tN0ha6mibe5QJ8IBG8nibRpDVIUB4tAsZ02f1uXn6OseqhOA9HB2UicJdIT3bNwI6A/640?wx_fmt=webp&amp;from=appmsg#imgIndex=0" data-ratio="0.5712962962962963" data-s="300,640" data-type="webp" data-w="1080" type="block" data-imgfileid="503526129" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/372907fd-2b19-4d58-842e-65673e69045d/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;2025 年的日历已经翻到最后一页。&lt;/p&gt;&lt;p&gt;这一年里，大模型的演进速度被不断推高：新的模型架构、训练范式与推理策略轮番登场，技术边界一次次被向前推移。&lt;/p&gt;&lt;p&gt;放眼海外，GPT-5、Gemini 3 等新一代模型相继亮相，在理解、生成与推理等核心能力上持续抬升上限，通用智能的轮廓愈发清晰。&lt;/p&gt;&lt;p&gt;回到国内，2025 年的 AI 场面同样热闹。国产大模型一边在核心能力上不断拉近与国际头部模型的差距，甚至在个别方向上实现反超，另一边也在开源、工程化和应用适配上明显提速。&lt;/p&gt;&lt;p&gt;然而，在技术浪潮起伏中，我们更需要清醒识别真正具备长远价值的 AI 力量。因为决定行业走向的，从来不是某一次参数翻倍、某一项榜单刷新，而是哪些能力能够在真实世界中持续发挥作用 &amp;mdash;&amp;mdash; 它们是否真正重塑了生产方式，并在时间的检验中沉淀为基础能力。&lt;/p&gt;&lt;p&gt;正是基于这样的判断与追问，我们尝试把目光从短期热度中抽离，去辨认那些真正值得被记录的技术进展与创新路径。&lt;/p&gt;&lt;p&gt;带着这些思考与期待，机器之心精心策划了 2025 年度榜单，记录中国人工智能奋进的这一年，勾勒技术创新的璀璨未来。&lt;/p&gt;&lt;p&gt;今日，「AI 中国」机器之心 2025 年度评选正式揭晓：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;最强技术实力企业/机构 TOP 10&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeawtmGqASNYD9ApHpmsb1ab5GBuOc9YFODDpxGUkSWZJvxAKVcBy4lA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.3601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526132" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f0f76124-6dca-4227-80bc-9c5c7fb636b9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;人工智能领军企业 TOP 20&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibend6ZuXedZQGToQroeiafFc5jLuDC2O5MHCvGzib4EfLhXeY3iaFgumU7A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="1.8953703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526133" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/99207199-8921-491d-857e-a4b4aab8e143/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;最佳大模型 TOP 20&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe8udYv7NMF1V9Q8uicLITatvWwTs703BicQ9ACaicOVOeOSN2VDZu73IoQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="2.448148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526134" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/dae6faf7-5bae-469a-bfd9-26abf9c23c02/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;最佳大模型产品 TOP 20&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibes10ZjuzKmBHS1N4C5UcG6via2U83Wia4cxhvuVmcoPrZEmUMczj1icBOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="2.448148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526135" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/9a0df25c-1f70-4cf7-b82d-a128721702dd/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;具身智能领军企业 TOP 20&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeUQVbvUWibhPu4bFP8WzDMCvJ4GAMY9UYOvGq8SibGp5QwKa8RHib3zFAw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.8953703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526136" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/2e90595e-dd2d-4428-b14e-73ae6504dba3/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;span data-pm-slice="0 0 []"&gt;&lt;strong&gt;ScienceAI 领军企业/机构 TOP 10&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeD0UiceN1D29jo7o5u1cHYFXxYespdHmaexN0YXSZkkXQeIlkibZicqicfw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="1.3601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526137" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9c019211-7f58-4fdd-8abe-be7a41bba7ac/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>NUS尤洋教授深度探讨智能增长的瓶颈：或许我们将这样实现AGI？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 31 Dec 2025 13:16:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-31-4</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-31-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;2026 年即将到来，AI 的发展也已经进入了一个新的阶段：我们已经取得了惊人成就，却同时面临进一步增长的瓶颈。&lt;/section&gt;&lt;p&gt;新加坡国立大学（NUS）的尤洋教授近期发表了一篇深度分析：《&lt;strong&gt;智能增长的瓶颈&lt;/strong&gt;》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LOniaWe4cQKOzUoUhicTOxStOI0kc6svMJxboh81K966XlZJvuuYmbkmw/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.40063091482649843" data-s="300,640" data-type="jpeg" data-w="634" type="block" data-imgfileid="503526311" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f6bd305d-ff9e-4e9a-81e0-3e72b54679ca/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;原文链接：https://zhuanlan.zhihu.com/p/1989100535295538013&lt;/p&gt;&lt;p&gt;在这篇分析文章中，尤洋教授从技术本质出发，直指智能增长的核心矛盾，为我们揭示了 AGI（通用人工智能）的可能路径。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;观点速览&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;✅&lt;strong&gt;&amp;nbsp;智能增长的本质不是架构变革，而是算力如何转化为智能&lt;/strong&gt;：AI 的核心智能来自于预训练及其 Loss 结构（例如 GPT 的 Next-Token Prediction）。这些机制更像是把算力转化为智能的方法，而非智能本身。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;✅ 现有智能增长遇到瓶颈的根源&lt;/strong&gt;：当前范式（Transformer + 超大算力）在面对进一步增长时， 难以充分消化不断增长的算力资源，这导致了所谓 “预训练红利递减”。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;✅ 算力并不是无限扩展就能解决问题&lt;/strong&gt;：即使算力指数级增长，如果现有算法无法有效利用这些计算资源，智能提升仍将受限。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;✅ 未来方向不在于工程优化，而是底层范式突破&lt;/strong&gt;：文章探讨了更高精度计算、更高阶优化器、更灵活的 Loss 设计、超大规模训练策略等潜在突破点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;✅ AI 未来仍然乐观&lt;/strong&gt;：智能增长瓶颈虽强，但仍有可能通过更好的算力利用方式被克服。预训练可能才刚刚开始，大模型智能仍有巨大的发展空间。&lt;/p&gt;&lt;p&gt;AGI 的未来将如何发展？让我们拭目以待。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9EricQXXByphb4tN0ha6mibeTWEreV5NLd1rQ2ASjPBb10pQCTARuib8FAZ7YSNA11TaiaXUtXLShAwQ/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=2" data-ratio="1.5" data-s="300,640" data-type="jpeg" data-w="1000" type="block" data-imgfileid="503526216" data-aistatus="1" data-original-style="width: 296px;height: 444px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c8963d7f-778f-42e2-90d6-ef48a9999f72/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 尤洋教授，《智能增长的瓶颈》作者&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;以下为其分享原文：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智能增长的瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2026 年已至。在 ChatGPT 诞生三年多后的今天，关于我们的智能水平是否令人满意，以及未来是否还能强劲增长，笔者想分享一些个人的看法。如有谬误，恳请大家指正。&lt;/p&gt;&lt;p&gt;为了能深入探讨智能的本质，本文将不涉及产品易用性、成本等商业化或落地问题，因为这些本质上与智能突破本身无关。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 智能的现状&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;什么是智能？其实目前并没有一个明确的定义。&lt;/p&gt;&lt;p&gt;从最近图灵奖得主 Yann LeCun 和诺贝尔奖得主 Demis Hassabis 关于 AGI 的争论中，我感受到即便是世界上最顶尖的&lt;strong&gt;专家&lt;/strong&gt;也无法准确定义智能。&lt;/p&gt;&lt;p&gt;个人感觉，AGI 很难定义，其标准也会随着时代的变化而变化。我依然记得十几年前，普通人对人脸识别技术感到不可思议。如果把今天的 ChatGPT 拿到 2006 年，相信那时候的很多人会毫不怀疑地认为我们已经实现了 AGI。&lt;/p&gt;&lt;p&gt;我觉得智能的核心是&lt;strong&gt;预测&lt;/strong&gt;和&lt;strong&gt;创作&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;我认为如果达到以下这种状态，那么就离 AGI 不远了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;如果你选择接受哪个工作 Offer，完全听从 AI 的意见。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果你买足球彩票预测世界杯冠军，完全听从 AI 的意见。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果你有健康问题，会完全采用 AI 制定的方案去治疗。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;你分辨不清楚一部奥斯卡最佳电影是否是由 AI 生成的。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;石油公司的勘探团队用 AI 替代了所有数值算法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;AI 能指导初级高铁工程师在 5 分钟内排除高铁的疑难故障。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;AI 能研制出一款专杀癌细胞且不破坏好细胞的药物。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;AI 能通过某区域的地下结构数据，精准预测地震的时间。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;等等……&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;今天，我们显然还没实现这些。未来能否实现，取决于我们能否克服智能发展的瓶颈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 智能发展的瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;今天，我们经常听到一些关于智能发展遇到瓶颈，或者预训练红利已尽的观点。何为瓶颈？我们先探讨一下智能从何而来。&lt;/p&gt;&lt;p&gt;过去 10 年，AI 大模型的技术本质，是把电力能源通过计算过程转化为可复用的智能。技术的好坏取决于这个转化效率的高低。类似的表述，我也听月之暗面的朋友提及过。&lt;/p&gt;&lt;p&gt;今天模型的智能本身，最主要还是来自预训练（往往是自监督方法），仅有少量来自微调或强化学习。&lt;/p&gt;&lt;p&gt;为什么？先算一笔浅显的经济账：因为预训练消耗的算力最多，消耗的能源也最多。&lt;/p&gt;&lt;p&gt;当然，预训练、微调、强化学习本质上都是在计算梯度以更新参数。如果有合适的海量数据和 Loss 函数，未来在预训练阶段采用 SFT（监督微调）或特殊的强化学习方法也有可能。&lt;/p&gt;&lt;p&gt;从智能增长的角度，我们甚至不用刻意区分预训练、SFT 和强化学习。它们的区别主要在于更新参数的次数与规模。&lt;strong&gt;从计算本质上看：预训练、微调、强化学习（比如 GRPO）都是在计算梯度的类似物，并用它来更新参数。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;那么，能源从何而来呢？这就是 GPU 或算力。英伟达在这点上做了最大的贡献。虽然英伟达有很多先进的技术，比如更强的 Tensor Cores、Transformer Engine、互联技术（NVLink / 网络化 NVLink）、软件栈等，但我先试图用一句话说清楚英伟达过去几年在技术上做的最重要的事情，即其 GPU 设计的核心思路。&lt;/p&gt;&lt;p&gt;简而言之，英伟达过去几年最重要的路线是：&lt;strong&gt;在同样的物理空间里堆更多 HBM（高带宽内存）。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;HBM 虽然带宽很高，但依然是计算核心之外的内存（Off-chip from logic die），与计算核心存在不可忽略的物理距离。为了掩盖内存访问延迟，GPU 只能依赖超大的 Batch Size（批处理量）和大规模并行来处理数据。英伟达 GPU 本质上就是一台并行计算机。&lt;/p&gt;&lt;p&gt;因此，英伟达对算法层和软件层的要求非常明确：必须提供足够大的 Batch Size 或并行度。&lt;/p&gt;&lt;p&gt;面对英伟达的要求，很多研究团队都提出了自己的方案。比如 RNN、Transformer、卷积序列模型（CNN for Sequence）等等。甚至有人尝试用 SVM 来处理大规模序列数据。&lt;/p&gt;&lt;p&gt;那为什么 Transformer 率先脱颖而出？因为 Transformer 也是一台并行计算机。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9EricQXXByphb4tN0ha6mibe1zVVszeSf8HcLzhoFWnss4rObllWFmwFKjERQuPxb46iarzCZzVZsrw/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=3" data-ratio="1.4559322033898305" data-s="300,640" data-type="jpeg" data-w="590" type="block" data-imgfileid="503526226" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/24e3ce8a-5297-4d2a-83ba-a7d43f9cb147/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 原初的 Transformer 架构&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这里我引用一下 Ilya Sutskever 的一句话：“Transformers: parallel computers in disguise”，直白的意思是：Transformer 本质上是一个被神经网络外壳包裹起来的并行计算机。这也是 Transformer 最先能够显现智能的核心原因，因为&lt;strong&gt;它的并行计算特性完美匹配了 GPU 的并行计算单元&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe74VN9hKHibtLDjZfxHoumJs5QKT9N1MbjgFETcbO9ZgUTzWWggHmuPA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=4" data-ratio="0.43333333333333335" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526092" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/54926c94-fc79-4fc8-bb77-1679e570b877/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;同时，OpenAI 完美地实现了 &lt;strong&gt;Next-Token Prediction&lt;/strong&gt; 这个 Loss 函数，它给了 AI 大模型近乎无限的训练数据。理论上 BERT 的 Loss 函数（完形填空和 Next Sentence Prediction）也可以提供近乎无限的数据，但在实践中，Next-Token Prediction 的效果明显更好。&lt;/p&gt;&lt;p&gt;我推测，这个 Loss 函数最小化了人类的干预 —— 它不是人为设计的，而是大自然在进化过程中赋予人脑的逻辑。并且，Next-Token Prediction 其实是&lt;strong&gt;预测未来&lt;/strong&gt;，而 BERT 的完形填空其实是把过去的信息和现在的信息串联起来。这就好比让一个足球专家根据历史数据和当天的比赛结果去解释合理性，几乎所有专家都能做到；但是，如果让专家去预测每一场比赛的精准比分，他们会经常出错。这再次说明了，&lt;strong&gt;预测 (Prediction) 是智能的核心能力体现，难度远高于解释 (Explanation)&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其实我挺佩服 OpenAI 团队能够坚持下来的勇气。2018 年时，BERT 在媒体上的影响力几乎完全碾压了 GPT，且当时 OpenAI 的 AI 研发团队体量跟 Google 比起来微不足道。很佩服他们没有放弃 Next-Token Prediction，也没有转向类 BERT 的训练方式。真理往往需要时间去检验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9EricQXXByphb4tN0ha6mibe1ZT4VndxCK3n8phMO6pZVQX90icquXrCovlG5rxYI32Wmias4OY0WU1A/640?wx_fmt=webp&amp;amp;from=appmsg#imgIndex=5" data-ratio="0.5" data-s="300,640" data-type="webp" data-w="800" type="block" data-imgfileid="503526223" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/1350a0e1-c403-4655-9406-c6e2c6926123/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; BERT 对比 GPT&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;同时，以 Transformer 为核心的方案收获了 “一箭双雕” 的双重优势：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;模型的每层参数量越多，并行度就越高 (Tensor Parallelism)&lt;/strong&gt;。 所以，只要通信代价不显著增加，能同时利用的算力就越多。这点需要点赞行业领导者的先见之明。几年前，我看到 CNN 时代有研究人员试图把模型往深度发展，比如设想 1000 层的神经网络。其实非常深（层数非常多）的神经网络是不利于有效利用算力的，因为流水线并行提供的并行度上限不高。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Transformer 的不同 Token 可以同时计算&lt;/strong&gt;。 序列长度越长，并行度就越高，只要通讯代价不显著增加，能同时利用的算力就越多。Sequence Parallelism 与 Data Parallelism 互补，进一步提供了更多的并行度。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;就这样，我们见证了 GPT-1、BERT、GPT-2、GPT-3、ChatGPT、Gemini 一步一步把智能提升到了今天的高度。&lt;/p&gt;&lt;p&gt;到这里，大家大概也清楚为什么 AI 模型的智能增长会遇到瓶颈了 —— 因为&lt;strong&gt;我们现在的范式无法充分消化持续增长的算力&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;假定一次模型训练和微调消耗的浮点数计算次数（即程序员面试中的计算复杂度的具体值）从 10&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;ⁿ&lt;/span&gt; 变成 10ⁿ⁺³ 时，我们是否获得了一个显著更好的模型？&lt;/p&gt;&lt;p&gt;其实，很多时候我们把 “效率优化技术” 和 “智能提升技术” 混淆了。比如，明天我提出一个新的架构，实验发现达到跟 GPT-5 类似的效果，只需要 20% 的参数量或计算量。这其实更多是落地或商业化问题；智能的终极问题是：使用同样的浮点数计算次数（而非 Token 量），能否获得一个更好的模型。&lt;strong&gt;浮点数计算次数，才是算力最基本、最本质的计量单位。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 未来的方法探讨&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;首先从硬件层来看，我们&lt;strong&gt;需要持续产生更大的绝对算力&lt;/strong&gt;，这不一定局限于单位芯片上的算力提升。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9EricQXXByphb4tN0ha6mibehp5PDHhwdbWXPpaSqxLx8D5ZrD5awkiaMCeWGqzSfyiagXjyj6e4fTBg/640?wx_fmt=jpeg&amp;amp;from=appmsg#imgIndex=6" data-ratio="0.4861111111111111" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526228" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/8026f37c-8700-4096-8342-5e6dc2027b49/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 前沿规模机器学习模型训练所用计算量的趋势，图源：Epoch AI&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;即便单位芯片上的算力没有大幅度提升，我们通过集群的方式也能构建更大的绝对算力。这里需要平衡的是：聚集芯片带来的性能增长，要高于 “芯片或服务器之间通信增长带来的负担”。&lt;/p&gt;&lt;p&gt;所以，具体的硬指标就是：增长或至少维持住 “计算开销/通信开销” 这个比值。这是整个 AI 基础设施层最核心的技术目标。要想实现这个目标，我们需要扩展性更好的并行计算技术，无论是软件还是硬件。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;更上层&lt;/strong&gt;的探索中，我们需要让 AI 模型在单位时间内 “吃下” 更多能源，并真正将其转化为智能。个人感觉大概有以下几点方向：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更高精度的计算能力。&lt;/strong&gt; 今天，从 FP16 到 FP32，甚至 FP64，模型智能并未出现明显跃升。这本身就是一个瓶颈。理论上，更高精度应当带来更可靠的计算结果，这一点在传统科学计算中早已得到验证。这个观点可能与主流机器学习共识并不一致，而且真正发生可能需要很长时间，但从本质上看，智能仍然需要更精准的计算。这与过拟合并无直接关系，过拟合的根源在于数据规模不足或参数与数据不匹配。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更高阶的优化器。&lt;/strong&gt; Google 的朋友告诉我，他们有时候已经不用类 Adam 优化器，而是用更高阶的优化器在训练模型。高阶优化器理论上能在学习过程中给模型更好的指导，算出更好的梯度，这是模型智能提升的本质。当然，高阶优化器的全面替代可能需要很长的时间。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;扩展性更好的模型架构或 Loss 函数。&amp;nbsp;&lt;/strong&gt;我们仍然需要一种扩展性更好的整合和利用算力的方式。这点我们需要注意：优化效率不一定能提升智能。比如 Mamba 出来的时候，宣传重点是吞吐量的提升，用更小的模型获得同水平的智能。但是，本文关注的是：在最健全的 AI 基础设施上，用最大的可接受成本，能否训出更好的模型，获得更高的智能。比如，今天 Google 告诉你：预算 300 亿美元，半年内给我训出一个更好的模型，不考虑省钱问题，花 10 亿和花 100 亿没区别。在这个场景下，你最终是否会用 Mamba 这样的架构？你是否需要设计更好的 Loss 函数？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更多的 Epoch 和更好的超参数。&amp;nbsp;&lt;/strong&gt;迫于成本压力，我们今天其实并没有对 AI 模型进行深度优化，甚至没有深度搜索超参数。这其实也是我之所以对 AI 模型的智能继续增长有信心的原因。我这里的意思不是直接训练更多的 Epoch。明知无效却生硬地跑更多 Epoch 其实是方法不对（比如参数量和数据量不匹配）。但是，根本上，更多的 Epoch 代表更多的浮点数、更多的能源。我们需要找到方法去 “吃下” 更多能源，并转化出更高智能。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;有些技术对大规模落地 AI 非常重要，比如低精度训练、剪枝、量化、蒸馏、PD 分离等推理优化技术。但是，在一个 “算力转智能” 极端有效的情况下，这些技术跟&lt;strong&gt;提升智能上限&lt;/strong&gt;无关。笔者对这些技术的贡献者非常尊重，它们在实际落地中至关重要，只是与本文探讨的主题无关。&lt;/p&gt;&lt;p&gt;智能增长归根到底还是算力利用问题。假定算力无限大，比如一个集群的算力达到今天的万亿倍，可能我们会发现更简单的模型结构比 Transformer 和 Next-Token Prediction 的扩展性更好。从 SVM 到 CNN、LSTM、BERT、GPT、MoE：我们始终在寻找能更高效利用算力且具备更好扩展性的方法。这个过程中，&lt;strong&gt;核心原因是问题的规模在不断扩大&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;我们在 AI 时代到来之前便已实现天气预报，然而至今仍未能攻克地震预报，尽管两者本质上都是针对地球数据的研究。究其原因，地下结构涉及比大气更加错综复杂、且变量规模呈指数级庞大的动态多模态数据。这种传统计算模式难以驾驭的高维复杂性，恰恰是未来 AI 技术大有可为的机遇所在。&lt;/p&gt;&lt;p&gt;所以，我有信心我们未来会不断找到更高效的算力使用方式。虽然过程中可能会有很多困难和低潮，但大趋势不可阻挡。&lt;/p&gt;&lt;p&gt;最后，借用 Richard Sutton 教授的一句话收尾：&lt;strong&gt;人工智能 70 年的研究留给我们最大的经验教训是，依托计算能力的通用方法才是最终的赢家，且具备压倒性的优势。&lt;/strong&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
