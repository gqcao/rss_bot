<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>刚刚，面壁小钢炮开源进阶版「Her」，9B模型居然有了「活人感」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 19:35:41 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-14</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-14</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;你有没有想过一个问题：为什么和 AI 对话，总觉得少了点「人味儿」。&lt;/p&gt;&lt;p&gt;不是它回答得不够准确，也不是它理解不了你的意思，而是每次交互都很机械。&lt;/p&gt;&lt;p&gt;你问一句，等它答完，然后突然画面一转，对不起，它对现实世界的观察仿佛瞬间「掉线」。那几秒里，AI 仿佛顺手关掉了眼睛和耳朵，陷入一种「间歇性失明失聪」的状态，根本不能根据眼前瞬息万变的画面实时调整自己的反应。&lt;/p&gt;&lt;p&gt;这种感觉，就像两个人在用对讲机聊天。你按住通话键说话时，对方听不见；对方说话时，你也插不上嘴。一次只能一个方向传递信息。&lt;/p&gt;&lt;p&gt;这不是产品设计问题，而是技术限制。因为绝大多数 AI 都在用单工模式运行，用起来感觉很死板。&lt;/p&gt;&lt;p&gt;2 月 4 日，面壁开源了行业首个全双工全模态大模型 MiniCPM-o 4.5，相比已有多模态模型，MiniCPM-o 4.5 首次实现了「边看边听边说」以及「自主交互」的全模态能力，模型不再只是把视觉、语音作为静态输入处理，而是能够在实时、多模态信息流中持续感知环境变化，并在输出的同时保持对外界的理解。&lt;/p&gt;&lt;p&gt;目前，MiniCPM-o 4.5 已在 GitHub、Hugging Face 等平台开源：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;开源地址：https://github.com/OpenBMB/MiniCPM-o&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face: https://huggingface.co/openbmb/MiniCPM-o-4_5&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;虽然参数量只有 9B，MiniCPM-o 4.5 却在全模态、视觉理解、文档解析、语音理解和生成、声音克隆等方面，均做到了全模态模型 SOTA 水平。在涵盖 8 个主流评测基准的 OpenCompass 综合评估中得分 77.6 。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFIRXnwUCAjayyuwjzE9mwtxQy8rRgPsMLJeKgicx8ObfNjCbibBZkuibrrdFxyoUNz5cApstfESh3iaVicS54EIlx3dQicUhYLJcm4c/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.7074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531745" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/bcc6ef80-c500-47dc-845b-2238d7f0be1b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;MiniCPM-o 4.5 在 MMBench（综合视觉理解）、MathVista（数学推理）及 OmniDocBench（文档解析）等关键任务上击败了顶级闭源模型 Gemini 2.5 Flash。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFt4dNYfehibg30rEiaf9y5FRS3TRRh03KZNZaVmLMDU7sFs5dibUyhTVHGGxSE6zfXw3EZfQvOstak3MRT4CQTzTBmUpqg3BYMIU/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="1.6342592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531746" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/51f261f8-d745-4f0a-90e9-49e584a31f50/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不仅如此，MiniCPM-o 4.5 在提升能力密度的同时也在追求极致能效比：以更低显存占用、更快响应速度，在保持 SOTA 级多模态表现下，实现更高推理效率与更低推理成本。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEs0W9uZMEyg8FAoicd04b50dcRs44kfBnGf2dQYjffJ5zDW7O5BuytLqx6ic8sbuvW27vJqj1ltePWawQDtx1nX6nuoamSe1ibiaE/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.3689839572192513" data-s="300,640" data-type="png" data-w="935" type="block" data-imgfileid="503531747" data-aistatus="1" data-original-style="width: 550px;height: 753px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4309beae-fa51-4fa7-9807-6325def8ab31/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;如果说过去的 AI 只是按部就班地执行指令，那么 MiniCPM-o 4.5 则赋予了 AI 一种真正的「本能」&amp;mdash;&amp;mdash; 它不仅能眼观六路、耳听八方地实时感知世界，还能在恰当的时机主动开口、自由接话，实现不受轮次约束的即时交流，真正开启了人机交互的新时代。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一手实测：这才是进阶版「Her」的样子&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;话不多说，我们开始上手体验。&lt;/p&gt;&lt;p&gt;体验链接：https://minicpm-omni.openbmb.cn/&lt;/p&gt;&lt;p&gt;我们先来玩一个「我画你猜」的小游戏。画面刚开始，我的笔只勾勒出了两条长长的兔子耳朵，模型几乎立刻给出了判断，问：这是兔子吗？接着还不忘给补充一句彩虹屁，画的很不错，情绪价值直接拉满。&lt;/p&gt;&lt;p&gt;接着，我的笔锋一转，又开始勾勒蝴蝶的轮廓。起初，只是一段看似随意的线条，模型试探性地问了一句：「这是一片叶子吗？」语气里带着明显的不确定。随着笔画逐渐增多，刚画完半边蝴蝶翅膀，模型立刻捕捉到了变化，迅速修正判断：「这是蝴蝶。」&lt;/p&gt;&lt;p&gt;整个过程像极了一个坐在旁边陪你画画的朋友：先大胆猜、再迅速改口，偶尔还不忘夸你两句。猜对不重要，重要的是它始终跟着你的笔走，边看边想，边想边聊。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5780f29d-1081-4ea7-86d8-c0ed869f32dd/1770204756506.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;接下来相同的游戏，我们和 ChatGPT 玩了一把，虽然其回答很流畅，但它并没有在关键线索出现时迅速作出判断，而是等到画面几乎已经完整、图像特征已经非常明显之后，才终于给出正确答案。&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 的状态更像在场的同伴：它会边看边听、边理解边开口，不必等待用户抛出问题，而是依据画面与环境的变化，实时补充、修正甚至主动推进自己的表述。相比之下，ChatGPT 的交互更偏问答机 &amp;mdash;&amp;mdash; 你不追问，它往往就停在上一轮输出里，难以在信息流持续变化时保持同步更新。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2f041a7b-5595-4861-987f-2f06c1fda7d6/1770204770259.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;接下来，测试难度加大。我们设计了一个围绕微波炉的测试场景。&lt;/p&gt;&lt;p&gt;当我们询问橘子能不能放进微波炉时，MiniCPM-o 4.5 给出了非常明确的否定 &amp;mdash;&amp;mdash; 不可以，理由干脆，没有任何犹豫。&lt;/p&gt;&lt;p&gt;而当我们把问题换成蛋糕，模型立刻给出了肯定的回答，不仅如此，它还顺带补充了一句：「这看起来是一块巧克力蛋糕。」显然，它并不是在机械地回答能不能，而是在同步理解眼前的具体对象。&lt;/p&gt;&lt;p&gt;更让人惊喜的是，交互并没有止步于这一问一答。当蛋糕被放进微波炉加热、时间结束后，它提醒我们：「蛋糕已经热好了。」&lt;/p&gt;&lt;p&gt;这个看似不起眼的细节，其实揭示了一件非常重要的事：模型能够持续理解环境、跟踪状态变化，并在合适的时间主动介入。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f6873a58-f16b-4069-97e4-8c2742365e22/1770204788148.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这次我们又和 ChatGPT 做了一次对比，整体来看，它在前半段的表现依旧可圈可点，但在计时结束，ChatGPT 陷入了「礼貌的沉默」，并未主动提醒我们取出食物。&lt;/p&gt;&lt;p&gt;这一现象暴露出目前主流模型最核心的短板 &amp;mdash;&amp;mdash; 缺乏内生的主动交互意识。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ff6baf34-1673-4d96-b0bd-bf5ac85da17e/1770204803270.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在接下来的测试中，我们&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;在白板上构思并绘制了一个卡通形象的草图。MiniCPM-o 4.5 能够实时观察到绘图的过程，并准确捕捉到每一个细节的变化。模型不仅能「看」，还能同步进行自然语言点评，并在落笔的同时，即时评论。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/05ddbe55-33a6-4b22-803f-f8ac9c2b7067/1770204815737.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;最后是一个玩纸牌游戏，要求是让&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;MiniCPM-o 4.5&lt;/span&gt;按顺序描述图片中出现的纸牌，当听到警报声时，告诉我游戏结束。&lt;/p&gt;&lt;p&gt;可以看出，随着人手依次将扑克牌放在镜头当中，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;MiniCPM-o 4.5&lt;/span&gt;展现了极其流畅的交互节奏，模型并非单纯播报数字，而是使用了诸如&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;The first card is...&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;等自然语言，使交互更像是在与真人交谈。当背景中突然响起了清脆的闹钟铃声（Ding!）时，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;MiniCPM-o 4.5主动提醒警报响起了，&lt;/span&gt;游戏结束&lt;strong data-end="124" data-pm-slice="0 0 []" data-start="116"&gt;。&lt;a href="https://mp.weixin.qq.com/s/9rdsklv3I8mrFHaSRUGSzw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/19fd0e7c-34ba-4e73-9e6e-9db83b77fa0e/1770204829596.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;整体看下来，感受颇深的一点是：MiniCPM-o 4.5 很大程度上摆脱了以往的被动式响应，让我们看到了 AI 与人类「同频共振」的可能，也让边看边听边说不再只是一句空谈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MiniCPM-o 4.5：迈向类人交互&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 之所以以 9B 小身板达到「类人感知 + 交互沟通」的能力。这背后藏着面壁对人机交互形态的终极思考。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从伪双工到真全双工：重构并行交互能力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;人类的交互是天然并行的，我们在说话的同时，从不停止观察周围 &amp;mdash;&amp;mdash; 对方的表情、环境的动静，这些信息在同一时刻被感知、被响应。这对人来说习以为常，但对大模型而言，却是一面高墙。&lt;/p&gt;&lt;p&gt;传统多模态大模型大多是在处理「离线、静态」的交互状态。无论是图像还是视频，通常都需要用户先整理、上传，再基于这些已完成处理的输入向模型提问；模型一次性生成一大段结果，用户再进行下一轮反馈。这是一种严格的「提交 &amp;mdash; 响应」式交互，天然存在时延，也缺乏过程中的动态调整能力。&lt;/p&gt;&lt;p&gt;流式全模态模型的出现开始打破这一限制，包括 GPT-4o、Gemini Live 等已经可以以并行流的方式持续输入，视觉、语音信号不再是一次性提交，而是实时、连续地进入系统。这是相对于「提交 &amp;mdash; 响应」式交互的显著进步。&lt;/p&gt;&lt;p&gt;但仔细看就会发现，这些流式全模态模型在本质上仍然是单工。输入与输出在本质上是阻塞的。模型一旦开始「说话」，就几乎无法再感知外部环境，等同于「闭上眼睛、捂住耳朵」，丧失了时间维度的感知能力。&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 打破的就是这面墙。它构建了一种全双工、全模态的大模型架构，使输入与输出流互不阻塞：模型在生成语音或文本的同时，仍然可以持续感知外界的视频与音频流。&lt;/p&gt;&lt;p&gt;技术实现上，团队采用了三项关键设计:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;时间对齐与时分复用：将输入的视频、音频流与输出的文本、语音 Token 在毫秒级时间线上严格对齐，通过时分复用机制将并行的全模态流划分为微小周期性时间片内的顺序信息组，使模型能在宏观串行中处理微观并行。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;循环分块编码：将离线模态编码器转化为支持流式输入输出的在线版本，模型将多模态流切分为微小分块 (Chunks) 并循环处理，这种高度复用的架构确保模型在输出的同时依然能持续解码环境信息。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;端到端语音生成：语音解码器采用文本与语音 Token 交错建模的方式，通过稠密的隐藏层连接实现端到端生成，而非简单的 TTS 拼接。这种深度融合使模型能根据实时的视觉与音频反馈，动态调整语音的语气与情感，显著提升了音色的拟人度与表现力，同时也提升了长语音 (如超过 1 分钟) 生成的稳定性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGLXTL9wqXw1ljNlEdjDVQgSlcldgMsnaDic6mkv9UWaWwpUCiake9tYQLxWPX2qrHghurAOnLXiadZraOynQwSlU0oyxd2j23HYM/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4583333333333333" data-type="png" data-w="1080" data-width="2430" data-height="1114" data-imgfileid="503531752" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/4d2d1962-6b6f-47bc-837c-96b264002765/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;如果说全双工架构给了 AI 一双「永不闭合」的眼睛和一对「时刻倾听」的耳朵，那么自主交互机制，则是给了它一颗「懂得察言观色」的大脑。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;自主交互：让模型摆脱「外挂」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 另一个突破在于全模态的自主交互机制。模型开始在实时信息流中，自行判断语义是否已经成熟到需要触发回应的时机，从而决定是否开口、何时开口。&lt;/p&gt;&lt;p&gt;你可能会问：现在不是已经有不少语音或全模态对话模型，可以被打断了吗？答案是：表面上看起来是，但本质并不一样。&lt;/p&gt;&lt;p&gt;这类模型通常依赖外部 VAD（语音活动检测）模块来控制交互流程：监听用户是否重新开口，一旦检测到声音，就强行中断当前输出。这类方案在体验上确实比「完全插不上嘴」好了一些，但会引入一系列本质性问题。例如 VAD 不理解语义，无法区分说话者，会被拍桌子、环境噪声等误触发，且必须等待固定静音阈值 (如 1 秒) 才能响应，人为拉长了延迟。&lt;/p&gt;&lt;p&gt;然而更本质的问题是：我们很难想象，一个真正面向 AGI 的模型，会需要一个「不太聪明的小工具」来告诉它什么时候该说话。说话时机的控制是被外包给外部工具的 &amp;mdash;&amp;mdash; 这意味着模型本身没有真正的交互自主性。&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 的发布，首次让模型真正具备了基于语义的自主交互判断能力：它能够自行判断交互是否成熟、何时进入说话状态，而不再依赖 VAD 等外部工具来裁决发言时机。取而代之的是一种内生的高频语义决策机制 &amp;mdash;&amp;mdash; 语言模型持续感知视频与音频输入，并以约 1Hz 的频率自主决定是否发言。正是这种高频语义判断与全双工机制的结合，使模型首次具备了主动提醒、主动评论等真正意义上的主动交互能力。其特点可总结为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;语义驱动的决策： 模型以高频（如每秒一次）的状态，完全基于语义理解来自主判断是否需要进入「说话状态」。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;去工具化： 摆脱外部 VAD，模型在实时监听多模态信号的过程中，自行决定继续倾听还是产生文本与语音 Token 进行回复。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;时机自由度： 赋予模型在时间维度上的自由度，使行为时机与行为内容实现一体化。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;正是基于这种自主决策机制，MiniCPM-o 4.5 展现出比传统方式更智能的交互表现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;极低延迟的及时回复： 模型可以根据语义进行「预判」，不再需要死等外部工具的硬性静音信号。在人类话音刚落甚至即将结束时，模型即可启动推理，实现连贯的实时反馈。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;智能抗干扰与内生打断：及时打断： 打断行为不再是外部强制挂断，而是模型根据外部实时信号进行的内生判断。抗干扰能力（抵抗打断）： 面对环境噪声或旁人闲聊，模型展现出一定的「抵抗能力」，不会轻易被非目标信号干扰。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;内生的主动回复（异步交互）： 模型具备了跨越时空限制的回复能力，不仅局限于即时对话，还能根据环境变化或预设任务（如「水满提醒」、「电梯到达提醒」）在未来的某个时机自主发起交互。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;重塑智能终端的「大脑」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当这种「类人感知 + 交互沟通」的实时本能，与仅有 9B 参数量的轻量模型相结合时，MiniCPM-o 4.5 成为了智能终端真正需要的那颗会沟通的大脑。&lt;/p&gt;&lt;p&gt;想象一下：你戴着 AI 眼镜走在路上，一辆车从侧面突然冲出，在你尚未反应之前，AI 已经脱口而出一句「小心」。这种体验的关键不在于「看见」，而在于跟上人的节奏，主动介入交互。&lt;/p&gt;&lt;p&gt;更进一步，当全双工全模态模型被部署到具身机器人、汽车或 PC 等终端时，AI 带来的不再只是功能增强，而是一种显著的真人交互感。&lt;/p&gt;&lt;p&gt;从更本质的层面看，流式全模态能力，是多模态走向类人化、走向深度交互的必经之路，而这条路天然指向强端侧场景。&lt;/p&gt;&lt;p&gt;原因很直接：一方面，流式模型需要在毫秒级持续感知视觉与音频输入，未来甚至会以全天候、伴随式的方式存在。如果运行在云端，意味着个体生活细节被持续上传，这在隐私层面难以接受；端侧部署则为这种能力提供了现实前提。另一方面，伴随式交互对低延迟和连续可用性要求极高，只有端侧才能在断网、移动等复杂场景下保持稳定工作。&lt;/p&gt;&lt;p&gt;也正因此，全双工全模态模型真正拉开了应用空间：在智能监控与提醒中，它能持续感知并主动介入；在人机协作系统中，它能根据环境变化实时调整行为；在无障碍辅助领域，它还能为视障、听障人群提供即时、多模态的信息支持。&lt;/p&gt;&lt;p&gt;当 AI 真正跟上人的节奏，那些过去「想得到却用不了」的场景，才开始变得顺理成章。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MiniCPM-o 4.5 的发布，某种程度上也是面壁智能技术实力的一次集中展示。回顾面壁的技术路线，我们会发现一个清晰的脉络：他们始终围绕高能力密度这一核心目标。&lt;/p&gt;&lt;p&gt;在 Scaling Law 边际效益逐渐递减的当下，行业本就迫切需要新的技术指引。面壁提出的 Densing Law，正在重塑这一竞争逻辑，不再比谁的模型更大，而是比谁能在更小的参数规模下，榨出更高的能力密度。&lt;/p&gt;&lt;p&gt;这种技术布局的前瞻性，往往容易被市场低估。在大模型叙事中，市场的注意力天然倾向于那些参数量动辄千亿、万亿的模型。相比之下，高能力密度是一种更工程化的追求。它的价值，往往只有在端侧部署、实时交互、连续感知等真实场景中，才会被充分放大。&lt;/p&gt;&lt;p&gt;也正是这样，由清华大学人工智能学院助理教授、面壁智能多模态首席科学家姚远主导完成的 MiniCPM-o 4.5 全双工全模态模型才显得格外关键。全双工全模态模型的出现，或许正是我们正在经历的下一次范式转换的起点。&lt;/p&gt;&lt;p&gt;当 AI 不再需要在说和听之间切换，当它可以像真人一样保持持续的感知和适时的回应，我们与 AI 的关系也将从使用工具转变为协同工作乃至情感陪伴。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>第二代AI预训练范式：预测下个物理状态</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 19:30:05 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-13</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-13</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑 | 杜伟、泽南&lt;/section&gt;&lt;p&gt;又一位大佬准备对现有 AI 技术范式开刀了。&lt;/p&gt;&lt;p&gt;今天凌晨，英伟达高级研究科学家、机器人团队负责人 Jim Fan（范麟熙）发布文章《第二代预训练范式》，引发了机器学习社区的讨论。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531497" data-ratio="0.6768518518518518" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBfHH8YKbBQMaHBJSa8LFafnq7mPQhmzDIMWAYz7G4aFdsvSO4JjlaWw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ecd524fe-a33e-4ae1-9ba7-1838251d2af4/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Jim Fan 指出，目前以大语言模型（LLM）为代表的 AI 模型主要基于「对下一词的预测」，这第一代范式虽然取得了巨大成功，但在将其应用于物理世界时，出现了明显的「水土不服」。&lt;/p&gt;&lt;p&gt;对于这个观点，纽约大学助理教授、谷歌 DeepMind 研究科学家谢赛宁也表示同意。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBDmHb1bLwlJMs52zNIRRbuFVInp9RPmYgneFmPd8iaufFrdLaFDnbI1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.8268518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531498" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/7752bdd8-4d2e-457a-9b81-3fd7d05ec01c/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;那么预训练的第二代范式应该是什么样子？我们先来看 Jim Fan 的全文内容：&lt;/p&gt;&lt;p&gt;「预测下一个词」曾是第一个预训练范式。而现在，我们正处于第二个范式转移之中：&lt;strong&gt;世界建模（World Modeling）或者「预测下一个物理状态」。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;很少有人意识到这场变革的影响有多么深远，遗憾的是，目前世界模型最被大众熟知的用例只是些 AI 视频废料（以及即将到来的游戏废料）。但我敢全心笃定，&lt;strong&gt;2026 年将成为「大世界模型」（Large World Models, LWMs）为机器人学以及更广泛的多模态 AI 奠定真实基础的元年。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在此背景下，&lt;strong&gt;我将「世界建模」定义为：在特定动作的约束下，预测下一个（或一段持续时间内）合理的物理世界状态&lt;/strong&gt;。 视频生成模型是其中的一种实例化体现，这里的「下一状态」是一系列 RGB 帧（通常为 8-10 秒，最长可几分钟），而「动作」则是对该做什么的文本描述。训练过程涉及对数十亿小时视频像素中未来变化的建模。&lt;/p&gt;&lt;p&gt;从核心上看，视频世界模型是可学习的物理模拟器和渲染引擎，它们捕捉到了「反事实」。这是一个更高级的词汇，意指在给定不同动作时，推理未来的演化如何不同。&lt;strong&gt;世界模型从根本上将视觉置于首位&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;相比之下，视觉语言模型（VLMs）在本质上是「语言优先」的。从最早的原型（如 LLaVA）开始，其叙事逻辑几乎未变：视觉信息从编码器进入，然后被路由到语言主干网络中。随着时间的推移，编码器在改进，架构更趋简洁，视觉也试图变得更加「原生」（如 omni 模型）。但它始终像是一个「二等公民」，在物理规模上远逊于业界多年来为大语言模型（LLMs）练就的肌肉。&lt;/p&gt;&lt;p&gt;这条路径很便捷，因为我们知道 LLM 是可扩展的。我们的架构直觉、数据配方设计以及基准测试（如 VQA）都高度针对语言进行了优化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对于物理 AI，2025 年曾被 VLA（视觉 - 语言 - 动作）模型主导&lt;/strong&gt;：在预训练的 VLM 检查点之上，硬生生嫁接一个机器人电机动作解码器。这其实是 「LVA」：其重要性排序依次为语言 &amp;gt; 视觉 &amp;gt; 动作。同样，这条路径很方便，因为我们精通 VLM 的训练套路。&lt;/p&gt;&lt;p&gt;然而，VLM 中的大部分参数都分配给了知识（例如「这团像素是可口可乐品牌」），而非物理（例如「如果你打翻可乐瓶，液体会蔓延成一片褐色污渍，弄脏白桌布，并毁掉电机」）。VLA 在设计上非常擅长知识检索，但在错误的地方显得「头重脚轻」。这种多阶段的嫁接设计也违背了我对简洁与优雅的追求。&lt;/p&gt;&lt;p&gt;从生物学角度看，视觉主导了我们的皮层计算。大脑皮层约有三分之一的部分专门用于处理枕叶、颞叶和顶叶区域的像素信息。相比之下，语言仅依赖于一个相对紧凑的区域。视觉是连接大脑、运动系统和物理世界的高带宽通道，它闭合了「感觉运动回路」。这是解决机器人问题的最核心环路，而且这个过程的中转完全不需要语言。&lt;/p&gt;&lt;p&gt;大自然给了我们一个存在性证明：一种具有极高肢体智能但语言能力微乎其微的生物 &amp;mdash;&amp;mdash; 类人猿。&lt;/p&gt;&lt;p&gt;我曾见过类人猿驾驶高尔夫球车，像人类技工一样用螺丝刀更换刹车片。&lt;strong&gt;它们的语言理解能力比不过 BERT 或 GPT-1，但它们的物理技能远超目前最先进的机器人&lt;/strong&gt;。类人猿或许没有强大的语言模型，但它们肯定拥有极其稳健的「如果... 会怎样」的心理图景：即物理世界如何运作，以及如何应对它们的干预。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;世界建模的时代已经到来&lt;/strong&gt;，它充满了「苦涩的教训」的味道。正如加州大学伯克利分校教授 Jitendra Malik 经常提醒我们这些「规模崇拜者」所说：「监督学习是 AI 研究者的鸦片。」YouTube 的全部存量以及智能眼镜的兴起，将捕捉到规模远超人类历史所有文本的原始物理世界视觉流。&lt;/p&gt;&lt;p&gt;我们将见证一种&lt;strong&gt;新型预训练&lt;/strong&gt;：下一个世界状态可能不限于 RGB 图像，3D 空间运动、本体感觉和触觉感知才刚刚起步。&lt;/p&gt;&lt;p&gt;我们将见证一种&lt;strong&gt;新型推理&lt;/strong&gt;：发生在视觉空间而非语言空间的「思维链」。你可以通过模拟几何形状和接触点，想象物体如何移动和碰撞来解决物理难题，而无需将其转化为字符串。语言只是一个瓶颈，一个脚手架，而非根基。&lt;/p&gt;&lt;p&gt;我们将面临一盒&lt;strong&gt;全新的潘多拉之问&lt;/strong&gt;：即使有了完美的未来模拟，动作指令该如何解码？像素重建真的是最佳目标吗，还是我们应该进入另一种潜空间？我们需要多少机器人数据，扩展遥操作规模仍是标准答案吗？在经历过这些探索后，我们是否终于在向机器人领域的「GPT-3 时刻」迈进？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Ilya 终究是对的，AGI 尚未收敛。我们回到了「研究的时代」，没有什么比挑战第一性原理更令人心潮澎湃了。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Jim Fan 对现状的思考以及对未来的判断，同样收获了评论区大量网友的认可。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBEHJuibJgYzSkAGELKXnzJCBtWlibrGnLDkgBa1q4wN6KABlvjV1YUvXw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531508" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/0526af75-ba9c-4c38-9cf6-19ba8b6c0758/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBNQX8NiaPYjjJ7KV2nAGuoq0ZicgGRuHib7tv50TaTGz9iabNMfBh6px10Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.37777777777777777" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531510" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/ba3f4f3e-48db-4d9c-855d-d08514bed039/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;有人认为这是「神经符号 AI 社区的胜利」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBXfpKdqMjjDElT3znzKfeobO9G4sK4hTAxSYWzMQvicFMqaE9RZJv3Tg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.22685185185185186" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531512" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/fb25ac34-0aee-4928-91c7-ffbec5a45ca0/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;你认同 Jim Fan 的观点吗？&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>美团提出全新多模态统一大模型STAR，GenEval突破0.91，破解“理解-生成”零和困局</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 19:26:35 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section data-pm-slice="1 1 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/dd69ce2e-ebbb-4a7b-81fd-12cd1e9245d5/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;近日，美团推出全新多模态统一大模型方案 STAR（STacked AutoRegressive Scheme for Unified Multimodal Learning），凭借创新的 &amp;quot;堆叠自回归架构 + 任务递进训练&amp;quot; 双核心设计，实现了 &amp;quot;理解能力不打折、生成能力达顶尖&amp;quot; 的双重突破。&lt;/p&gt;&lt;p&gt;在 GenEval（文本 - 图像对齐）、DPG-Bench（复杂场景生成）、ImgEdit（图像编辑）等 benchmark 中，STAR 实现了 SOTA 性能；用最简训练逻辑与紧凑模型设计让统一多模态大模型真正走向工业级落地。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBryX2rRPDlFviaXqrBL7UZCY1vicdicbLibagNlba8hJgibgUAQxFEexTTTw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.28425925925925927" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531050" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/56cccc1d-9b85-4f92-88a0-a36af86328f0/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：STAR: Stacked AutoRegressive Scheme for Unified Multimodal Learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/pdf/2512.13752&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://star-mm-ai.github.io&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码地址：https://github.com/MM-MVR/STAR&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;关键词：统一多模态、堆叠自回归、任务渐进式训练&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBIuGG8YOBkIeeXyV7aOpbdJY5iaOGRO64tdQnooQf8SptaibibEkU6J6dQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.8907407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531052" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/1089b8b1-38cc-4072-bc45-3b4c71b55661/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一、行业痛点：统一多模态大模型的 &amp;ldquo;能力诅咒&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在通向 AGI 的进程中，将 &amp;ldquo;视觉理解&amp;rdquo; 与 &amp;ldquo;图像生成&amp;rdquo; 统一于单一参数空间被视为多模态大模型的圣杯，然而实践层面却长期受制于 &amp;ldquo;能力诅咒&amp;rdquo;，具体表现为三重矛盾。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 优化目标互斥 &amp;mdash;&amp;mdash; 语义对齐与像素保真的零和博弈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;理解任务的核心是 &amp;quot;语义对齐与逻辑推理&amp;quot;&amp;mdash;&amp;mdash; 比如识别图像中的物体、回答图文相关问题，需要模型精准捕捉跨模态的语义关联；而生成任务的核心是 &amp;quot;像素保真与创意表达&amp;quot;&amp;mdash;&amp;mdash; 比如根据文本描述生成高清图像，需要模型兼顾细节还原与内容连贯性。两者的优化目标、特征空间显著不同，导致联合训练陷入零和博弈：强化生成能力，理解准确率会下降；深耕理解任务，生成图像的清晰度、语义一致性会打折。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 训练范式繁复 &amp;mdash;&amp;mdash; 从零训练与混合架构的双重瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现有两条技术路线均面临高昂训练成本：&lt;/p&gt;&lt;p&gt;(1) 端到端从零训练需在亿级图文 - 生成配对数据上做多任务平衡，优化空间维度高达千维，超参敏感性呈指数级放大，训练周期常以 &amp;ldquo;月&amp;rdquo; 为单位；&lt;/p&gt;&lt;p&gt;(2) 混合架构通过扩散模型与自回归模型的组合实现功能覆盖，但需要设计复杂的特征转换桥（feature bridge）、额外的适配器（adapter）或复合损失（hybrid loss），增加了整体调参难度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 能力扩展退化 &amp;mdash;&amp;mdash; 灾难性遗忘与容量饱和&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在预训练理解骨干上增量引入生成任务时，模型出现典型的灾难性遗忘（catastrophic forgetting），原本擅长的图像问答、逻辑推理能力会显著下降。其根源在于参数容量饱和与表征干扰 &amp;mdash;&amp;mdash; 生成任务的像素级扰动在特征空间形成噪声，改变了早期对齐的语义特征，致使 &amp;ldquo;全能扩展&amp;rdquo; 成为 &amp;ldquo;轮换专精&amp;rdquo;。 &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对这些行业痛点，美团 MM 团队提出了一个直击核心的问题：&lt;strong&gt;能否在完全保留多模态理解能力的前提下，持续、高效地增强模型的生成与编辑能力？&lt;/strong&gt; STAR 方案的诞生，给出了肯定且可扩展的解答。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、核心创新：重构多模态学习的 &amp;quot;能力成长法则&amp;quot;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;STAR 的关键不是单一技术突破，而是构建了一套 &amp;ldquo;能力叠加不冲突&amp;rdquo; 的多模态学习体系，核心围绕「冻结基础 + 堆叠扩展 + 分阶训练」范式，通过三大核心设计实现「理解、生成、编辑」三大能力的统一，同时避免互相干扰。整个框架由 &amp;ldquo;堆叠同构 AR 模型 + 任务递进训练 + 辅助增强机制&amp;rdquo; 三大部分协同组成。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1、核心架构：堆叠同构 AR 模型（Stacked-Isomorphic AR）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;STAR 的核心架构创新，是其 &amp;quot;堆叠同构 AR 模块&amp;quot; 的设计，彻底简化了多模态能力扩展的复杂度，就像给模型 &amp;quot;搭积木&amp;quot; 一样灵活高效：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（1）同构设计，零适配成本&lt;/strong&gt;：新增的堆叠模块与基础 AR 模型采用完全相同的架构（自注意力机制 + 前馈神经网络），参数初始化直接复用基础模型的顶层参数。这意味着新增模块无需重新学习基础特征，能快速适配现有模型的特征空间，避免了传统混合架构中 &amp;quot;特征转换桥&amp;quot; 的复杂设计；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（2）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;单目标训练，极简优化&lt;/strong&gt;：无需设计额外的损失函数，仅通过标准的 &amp;quot;下一个 token 预测&amp;quot; 目标即可完成生成与编辑能力的训练。这一目标与基础模型的训练目标完全一致，确保了训练过程的稳定性，大幅降低调参难度；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（3）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;参数紧凑，落地友好&lt;/strong&gt;：STAR-3B 仅在 Qwen2.5-VL-3B 基础上新增 1.2B 参数（16 层堆叠模块），STAR-7B 新增 3B 参数（14 层堆叠模块），却实现了生成能力的跨越式提升。STAR 的紧凑设计非常适合工业化部署，能有效降低推理成本。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWB9fvLhdMEKKVZo5Sk2gCETBGpuXbWNKehJJWIGgRkK1XUOSQxtChDNA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.4074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531053" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/45c2b503-44e3-4c8a-bade-789092a78c12/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2、核心范式：任务递进式训练（Task-Progressive Training）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;STAR 打破了传统统一模型 &amp;ldquo;混在一起训练&amp;rdquo; 的模式，把多模态学习拆成四阶段递进流程，每一步都冻结已有核心能力，扩展新技能：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（1）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;第一阶段（VQ 训练）&lt;/strong&gt;：先训练 &amp;ldquo;图像分词&amp;rdquo; 能力，训练 STAR-VQ 把图片拆成细粒度离散 token，为后续生成 / 编辑打下基础；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（2）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;第二阶段（文本生图预训练）&lt;/strong&gt;：在冻结的理解模型上，堆叠 AR 模块专门学文生图任务，只更新新模块参数，不碰原有理解能力；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（3）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;第三阶段（AR - 扩散对齐训练）&lt;/strong&gt;：单独优化扩散解码器，让生成的图片更清晰，其他模块保持冻结；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（4）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;第四阶段（统一指令微调）&lt;/strong&gt;：联合训练堆叠 AR 和扩散解码器，同时掌握 &amp;ldquo;生图 + 编辑&amp;rdquo;，用梯度停止机制避免新任务干扰旧能力。&lt;/p&gt;&lt;p&gt;STAR 通过任务递进式训练，让每一步新能力的学习都不破坏已有成能力，实现 &amp;ldquo;理解能力不退化，生成 / 编辑能力逐步增强&amp;rdquo;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBDP6xSzLbQ4tcHbu2YNY6E2OaTZNPAe7OgNhIj7bOWl1iaq8ia3UfCHsQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.2638888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531054" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b4149c3a-a2b9-45b8-9d4e-8d8ed5bdcebb/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3、辅助增强机制：两大关键优化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 高容量图像量化器（STAR-VQ）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统 VQ 模型拆分图片粗、细节丢失多，STAR-VQ 做了两大升级：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（1）&lt;/span&gt;规模扩容：代码本规模从 16384 提升到 65536，向量维度从 8 维提升到 512 维，能捕捉更多图像细节；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（2）&lt;/span&gt;避免崩溃：通过新增 codebook 映射层，解决大 codebook 训练中常见的码本崩溃问题，保证所有 token 都能被有效利用；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（3）&lt;/span&gt;核心作用：生成更精准的视觉 token，让后续生成 / 编辑任务能还原更细腻的图像细节。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 隐式推理机制（Implicit Reasoning）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对复杂提示，传统生成模型容易出现语义错位、细节遗漏的问题。STAR 的隐式推理机制，让模型学会 &amp;quot;先推理，再生成&amp;quot;：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（1）&lt;/span&gt;当接收到复杂提示时，冻结的基础 AR 模型先进行推理，生成蕴含核心知识的隐式 latent tokens；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（2）&lt;/span&gt;这些 latent tokens 作为条件输入，引导堆叠模块进行图像生成。这一设计实现了 &amp;quot;语义推理&amp;quot; 与 &amp;quot;像素生成&amp;quot; 的解耦，让生成过程更有逻辑，大幅提升了复杂场景下的语义对齐度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;STAR 的突破性表现，得到了权威 benchmark 的全面验证，在理解、生成、编辑三大任务中均展现出顶尖实力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 生成任务：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在文本 - 图像生成的核心 benchmark 中，STAR 的表现惊艳：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（1）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;GenEval（语义对齐权威 benchmark）&lt;/strong&gt;：STAR-7B 以 0.91 的综合得分刷新 SOTA。在物体计数、颜色属性、空间关系、实体属性等 6 个子任务中，STAR 有 5 项排名第一；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（2）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;DPG-Bench（复杂场景生成 benchmark）&lt;/strong&gt;：STAR-7B 以 87.44 的得分领先，在多物体组合、复杂场景描述等任务中表现突出，生成的图像不仅细节丰富，还能精准还原文本中的逻辑关系；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（3）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;WISEBench（世界知识推理 benchmark）&lt;/strong&gt;：STAR-7B 以 0.66 的综合得分，超越同类统一模型，证明其隐式推理机制能有效利用世界知识，提升复杂提示的生成质量。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBzO4qI2Le9LpCBzjenhmt8Dv4iaKcicUYr5XgPxgL1H4jOrQuqSZ6fsbA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531055" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/600fc3f9-5c64-4452-b7fa-06b2a7d6f07d/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBIdCUbspdquokzmuKJiaRDlJcGCzBicfsZrL1v94wy1ZLUgLA0hAx1cBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.3648148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531056" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/92d2ce0c-1c8c-4605-b756-d152fbc58e5e/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 编辑任务：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在图像编辑 benchmark 中，STAR 展现出强大的灵活适配能力，能精准响应 &amp;quot;添加物体、替换背景、调整风格、删除元素&amp;quot; 等各类编辑指令：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（1）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;ImgEdit（覆盖 9 类编辑任务）&lt;/strong&gt;：STAR-7B 以 4.34 的综合得分刷新 SOTA。在 &amp;quot;物体提取&amp;quot;&amp;quot; 动作编辑 &amp;quot; 等子任务中，得分分别达到 4.19、4.60，领先同类模型；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;（2）&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;MagicBrush（语义编辑 benchmark）&lt;/strong&gt;：STAR-7B 的 CLIP-I 得分达 0.934（语义一致性），L1 误差低至 0.056（像素保真度）。这意味着 STAR 在完成编辑任务的同时，能最大程度保留原图的核心内容，避免 &amp;quot;过度编辑&amp;quot; 或 &amp;quot;语义偏离&amp;quot;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBlfibN7ib4bXP5ib6tjWdoLaXYvUuNtgcKWc9vVAIMPLzAiboLVCPFy2e8w/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.13055555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531057" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/e85a167c-34d8-4aa7-88b7-0716bb67ebe2/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWBO0DXsKrswVrZvljMvxKvISzGgPeet5lhAr7RQgYNOYIWv6DcRLibdOA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.41759259259259257" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531058" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/f9de1e1a-9349-44ba-8839-90976561fde1/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3. 理解任务：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;即便专注于增强生成与编辑能力，STAR 的理解能力依然保持顶尖水平。在 9 大权威理解 benchmark 中，STAR 的表现领先于同类多模态模型。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9TeUrvtKicctQyhpsAEEYWB6iciaoSQrPBUAmbbic3KxKykCM11kKUJPhjgK59eworNibViauLjbSvZq5w/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.37592592592592594" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531059" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/0de025ef-61ed-48d7-9056-dc26f6873ea1/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;四、总结与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;STAR 的本质是 &amp;ldquo;用最简洁的结构实现最全面的能力统一&amp;rdquo;：通过 &amp;ldquo;任务递进&amp;rdquo; 解决训练冲突，通过 &amp;ldquo;堆叠同构 AR&amp;rdquo; 降低扩展成本，通过 &amp;ldquo;STAR-VQ + 隐式推理&amp;rdquo; 提升能力上限，最终实现 &amp;ldquo;理解、生成、编辑&amp;rdquo; 三大任务的顶尖性能，为多模态模型的可持续扩展提供了全新思路。&lt;/p&gt;&lt;p&gt;STAR 为多模态模型的无干扰、可扩展扩展提供了全新技术路径，后续可从以下方向进一步探索：&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（1）&lt;/span&gt;能力边界扩展：在现有理解、生成、编辑基础上，纳入视频生成、3D 重建等更复杂的多模态任务，验证框架的泛化性；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（2）&lt;/span&gt;效率优化：当前模型仍需多阶段训练，未来可探索更高效的联合训练策略，或轻量化堆叠模块以降低部署成本；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（3）&lt;/span&gt;推理能力深化：进一步强化隐式推理机制，结合外部知识库或强化学习，提升模型在超复杂逻辑、跨领域知识场景下的生成准确性；&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（4）&lt;/span&gt;多模态融合升级：拓展文本、图像之外的模态（如语音、触觉），构建更全面的通用多模态系统，推动人工通用智能（AGI）的发展。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>产研协同 智启未来：科学智能“百团百项”工程产研共创沙龙在沪顺利举行</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 15:19:06 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2026年2月3日，由上海仪电（集团）有限公司主办，上海市先导产业促进中心、上海埃迪希科技服务有限公司承办的科学智能&amp;ldquo;百团百项&amp;rdquo;工程产研共创沙龙在沪圆满落幕。&lt;strong&gt;上海仪电（集团）有限公司副总裁刘山泉、上海市经济和信息化委员会人工智能发展处处长刘文、上海市先导产业促进中心主任郑理莹&lt;/strong&gt;出席会议，共话AI4S新范式，共谋产业高质量发展新篇章。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3323f33e-14b9-4e0b-b23a-680932b5ac14/1770189343529.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;本次沙龙汇聚了来自学术界、产业界、投资界的50余位专家学者与企业代表。会议紧扣真实产业需求，旨在通过深度探索人工智能与科学研究的融合路径，推动前沿技术精准赋能实体产业转型升级，合力构筑开放、协同、高效的创新生态体系。&lt;/p&gt;&lt;p&gt;活动伊始，&lt;strong&gt;上海智能算力科技有限公司解决方案负责人王丽忱&lt;/strong&gt;向与会嘉宾系统介绍了&amp;ldquo;科学智能开放社区&amp;rdquo;。科学智能开放社区由上海仪电牵头承建，以统一门户整合一站式科研工具、高质量开源模型库、智能体开发平台以及万卡级公共算力在内的关键资源与服务，为全球科学家提供开放、协同、高效的科研创新平台，显著降低AI与科研融合的门槛，加速从基础研究到产业应用的转化周期。随后，&lt;strong&gt;上海人工智能实验室李玉强、上海科学智能研究院关慧宇、上海创智学院郑逸宁&lt;/strong&gt;分别介绍了面向科研领域的模型、智能体、工具的共性服务能力，共同为科学智能开放社区精准高效地连接产业难题与科研智慧，提供了坚实而灵活的技术支撑。&lt;/p&gt;&lt;p&gt;作为本次沙龙的重头戏，长达145分钟的&amp;ldquo;产研共创工作坊&amp;rdquo;将氛围推向高潮。与会嘉宾打破行业壁垒，围绕&amp;ldquo;百团百项&amp;rdquo;专项工程中&amp;ldquo;产业出题、科学家答题、人工智能解题&amp;rdquo;的闭环协同模式展开深度讨论。 由AI科学家、领域科学家、工程师及基金经理人组成的多维协同小组，针对化工物质、光刻胶、生物制造AI应用模型、生物制造AI4S创新驱动、量子计算、固态电池等六大战略性前沿领域的真实需求，&amp;ldquo;点对点&amp;rdquo;定义科学问题，&amp;ldquo;面对面&amp;rdquo;探讨路径可行性。现场形成的系列关键科学问题提案，为&amp;ldquo;百团百项&amp;rdquo;工程注入了强劲的创新动能。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/efb8e00f-b666-4a3d-8c45-a9132adec73f/1770189406551.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/c306f2d5-a65f-410e-99b7-f8bb286ce36b/1770189412795.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a8c9e68a-9232-41dd-899e-295eed5d03c4/1770189421299.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7e0c5fca-b9a3-4aad-95f1-e65e09209cd9/1770189429834.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1c2871a2-66d6-47b8-a8f8-a7734474279f/1770189437215.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1e341b03-0d4b-4205-b267-330d77965cea/1770189444731.jpeg" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;AI4S已成为引领新一轮科技革命、驱动产业源头创新的关键变量。本次沙龙的成功举办，不仅验证了以科学智能开放社区为枢纽的协同模式，更为跨领域协作提供了可复制、可推广的实战经验。 面向未来，科学智能开放社区将持续深化系列主题活动，依托平台全链条支持能力，推动优质项目从&amp;ldquo;实验室&amp;rdquo;走向&amp;ldquo;生产线&amp;rdquo;。社区将秉持开放合作理念，邀约更多产学研伙伴同频共振，共同破解关键&amp;ldquo;卡脖子&amp;rdquo;难题，为产业智能化变革贡献澎湃动力。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | U2-BENCH：首个超大规模全场景超声多模态理解基准，开启医疗大模型新赛道</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 14:32:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;h3&gt;&lt;strong&gt;1. 医疗AI的深水区：从通用视觉到超声专业理解&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;超声成像（Ultrasound）作为全球医疗中应用最广泛的影像手段之一，在妇产科、急诊及心脏病学等场景中具有不可替代的地位。然而，与 CT / MRI 等模态相比，超声影像的自动理解长期面临更高门槛：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;操作依赖性强&lt;/strong&gt;：超声影像受操作者手法影响，质量波动大、伪影多 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;空间关系复杂&lt;/strong&gt;：不同于 CT/MRI 的静态切片，超声呈现的是动态的、具有强空间上下文关系的结构 。&lt;strong&gt;评测体系缺失&lt;/strong&gt;：虽然通用视觉大模型（LVLMs）如 GPT-4V、Gemini 表现惊人，其在超声这一高专业度场景下的能力，长期缺乏系统、可复现的评估体系。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此背景下，U2-BENCH 被提出，作为首个系统性评估LVLMs在超声领域能力的深度基准，涵盖了分类、检测、回归及文本生成四大任务维度。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/637f6159-1d11-47fe-a7da-83291f607f36/1770186374225.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;2. 核心&lt;/strong&gt;&lt;strong&gt;设计&lt;/strong&gt;&lt;strong&gt;：全谱系解剖覆盖与临床启发式任务&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;U2-BENCH的核心价值在于其高度的临床相关性和严密的构建流程：&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;2.1&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;超大规模、多来源的真实临床数据&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;广度覆盖&lt;/strong&gt;：汇集了来自 40 个授权数据集的 &lt;strong&gt;7,241 个案例&lt;/strong&gt;，跨越 &lt;strong&gt;15 个解剖区域&lt;/strong&gt;（如胎儿、心脏、乳腺、甲状腺等） 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;深度场景&lt;/strong&gt;：涵盖 &lt;strong&gt;50 个临床应用场景&lt;/strong&gt;，确保了评估结果能真实反映模型在医疗一线的能力 。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;2.2&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;四级能力分解&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;&lt;strong&gt;八项临床任务&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;U2-BENCH 将“超声理解”拆解为四个能力层级、八项具体任务：&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;分类任务&lt;/strong&gt;：疾病诊断（DD）、标准切面识别与质量评估（VRA） 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;检测任务&lt;/strong&gt;：病灶定位（LL）、器官检测（OD）、关键点检测（KD） 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;回归任务&lt;/strong&gt;：临床数值估计（CVE，如射血分数、脂肪肝百分比） 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;生成任务&lt;/strong&gt;：结构化报告生成（RG）、解剖描述生成（CG） 。&lt;/li&gt;&lt;li&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f83fc832-4d78-4b57-8907-8cc320fa6329/1770186397204.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;&lt;br&gt;&lt;/h3&gt;&lt;h3&gt;&lt;strong&gt;3. 实验验证：SOTA 模型的&lt;/strong&gt;&lt;strong&gt;能力&lt;/strong&gt;&lt;strong&gt;边界&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;U2-BENCH上系统评测了&lt;strong&gt;23个&lt;/strong&gt;&lt;strong&gt;领先&lt;/strong&gt;视觉语言模型（包括 GPT-5, Gemini 2.5, Dolphin-V1 等）进行了大规模评测：&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;3.1 闭源模型依然领先，但仍有巨大空间&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;榜首表现&lt;/strong&gt;：Dolphin-V1 以 &lt;strong&gt;0.5835&lt;/strong&gt;的总分（U2-Score）位列第一，大幅领先于 GPT-5（0.3250）和 Gemini-2.5-Pro（0.2968） 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;开源对比&lt;/strong&gt;：开源模型中 DeepSeek-VL2 表现最强，但在复杂推理上与闭源顶尖模型仍有代差 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/b8154111-f637-4dac-97db-a612493da226/1770186406655.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;3.2 任务难度的代差：识别容易，推理极难&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;分类 vs. 空间推理&lt;/strong&gt;：模型在疾病诊断（DD）等图像级分类上表现尚可，但在空间位置相关的检测（KD/OD）和回归（CVE）任务上表现堪忧 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;报告生成的挑战&lt;/strong&gt;：模型生成的语言质量虽然不错，但在医疗准确性和结构化合规性（RG）上仍存在严重缺陷 。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;&lt;strong&gt;3.3 关键结论：规模不是唯一解&lt;/strong&gt;&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;缩放定律的平台期&lt;/strong&gt;：在 Qwen 家族的对比中发现，模型参数量从 3B 到 72B 带来了稳步提升，但在某些空间推理任务上提升并不显著，暗示了&lt;strong&gt;超声专项训练&lt;/strong&gt;比单纯扩大参数更有效 。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/db965ffd-2daf-4862-a8c4-108d3cd21443/1770186427939.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;4. 总结与展望&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;U2-BENCH 的建立表明超声 AI 正在从“单一任务的小模型”转向“全能理解的大模型”。同时实验也揭示了当前 LVLMs 在空间推理和临床逻辑上的短板 。&lt;/p&gt;&lt;p&gt;未来，U2-BENCH&amp;nbsp;将扩展至：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;动态视频理解&lt;/strong&gt;：从单帧走向实时扫查序列 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;长程具身感知&lt;/strong&gt;：结合机械臂等硬件实现自动化、闭环的超声扫描与决策。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>突破RNA设计瓶颈，上智院联合复旦、上交提出全球首个强化学习与潜扩散融合框架SOLD</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Wed, 04 Feb 2026 14:25:23 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnA6AroNE959MTXRTunZ61XXLRee0gYSITrsoZwulFzX0NLS4ga2hrNibdHreZ9icQswrGIDqeD2A7g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.42407407407407405" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="100027374" data-aistatus="1" data-original-style="null" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/35be453b-449b-46d8-ae23-76f121b110b4/640.jpeg" data-sec-load-status="2" data-report-img-idx="0" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;作者丨上智院女娲生命大模型团队&lt;/p&gt;&lt;p&gt;编辑丨ScienceAI&lt;/p&gt;&lt;p&gt;在 RNA 疗法、基因调控和合成c等领域，RNA 逆折叠（RNA Inverse Folding）是至关重要的核心任务，其目标是设计出能够折叠成特定 3D 结构的 RNA 序列。如同设计一把能开启特定「基因锁」的钥匙，这要求生成的序列不仅在理论上符合要求，更需在物理上精准折叠成目标构象。&lt;/p&gt;&lt;p&gt;然而，面对复杂的 RNA 序列 - 结构相互作用，现有的深度学习方法尽管在序列恢复率上取得了一定进展，其局限仍非常明显：它们往往难以直接优化次级结构一致性（SS）、最小自由能（MFE）和局部距离差测试（LDDT）等关键的结构与功能指标，导致生成的序列在物理真实性和结构准确性上经常「次优」。此外，现有的基于强化学习的扩散模型优化方法，通常需要采样完整的扩散轨迹，计算成本极高，难以在 RNA 设计这种复杂任务中高效应用。&lt;/p&gt;&lt;p&gt;为此，上海科学智能研究院（下称上智院）与复旦大学、上海交通大学等联合提出了首个集成强化学习与潜扩散模型的 RNA 逆折叠框架（SOLD）。该框架从 RNA 的共进化模式出发，在预训练阶段引入&amp;nbsp;RNA-FM&amp;nbsp;嵌入，并在优化阶段通过创新的「分步式」（Step-wise）强化学习策略，实现了对非导向性结构目标的直接、高效优化。实验表明，该方法在多个权威指标上全面超越了现有的 SOTA 方法，为开发高精度、功能导向的 RNA 设计工具开辟了新路径。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOuhUMpxZIjSqcKsOZNukWs2n5YXtx13JxvDUhia9rOVyXqf1dY5mByCtA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.21203703703703702" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027354" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/5f628369-1d01-4717-b68b-926a88a8e43c/640.png" alt="图片" data-before-load-time="1770186270461" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="2 2 []"&gt;论文题目：Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model&lt;/p&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.19232&lt;/p&gt;&lt;p&gt;代码地址：&lt;/p&gt;&lt;p&gt;https://aistudio.ai4s.com.cn/galaxy-model/partner/galaxy-model-frontend/model/01301556&lt;/p&gt;&lt;p&gt;https://github.com/SAIS-LifeScience/SOLD&lt;/p&gt;&lt;p&gt;该研究成果已被 AAAI 2026 接收。上智院生命科学方向研究员斯奇、刘旭阳，上海交通大学生命科学系博士生王鹏磊，是共同第一作者。上智院首席科学家、复旦大学特聘教授漆远，是论文共同作者。上智院生命科学方向主任研究员郭昕，上智院生命科学方向负责人、复旦大学人工智能创新与产业研究院研究员程远，是共同通讯作者。&lt;/p&gt;&lt;p&gt;研究项目由星河启智科学智能开放平台（https://aistudio.ai4s.com.cn/）和复旦大学 CFFF 智算平台提供技术和算力支持。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;现有方法的两大局限&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;既往的 RNA 逆折叠方法虽取得一定进展，但存在两个面向实际设计的关键短板：&lt;/p&gt;&lt;p&gt;一是难以处理非可微的结构目标。现有的深度学习方法（如 GrnaDe、RDesign）虽然提升了生成序列的质量，但它们大多无法直接优化如最小自由能（MFE）或 3D 结构相似度（LDDT）等「硬指标」。这些指标对于 RNA 是否能在真实生物环境中稳定发挥功能至关重要，但由于它们通常是不可微的，传统的梯度下降方法难以直接对其进行优化。这导致模型生成的序列往往「形似」而「神不似」，难以满足严格的物理约束。&lt;/p&gt;&lt;p&gt;二是传统强化学习优化效率低下。为了解决上述问题，强化学习（RL）被引入以优化这些离散目标。然而，现有的结合扩散模型与 RL 的方法（如 DDPO、DPOK），通常需要对扩散过程的完整轨迹进行采样才能更新策略 。在 RNA 设计的高维空间中，这种「全轨迹」采样的计算开销巨大，收敛速度极慢，且容易陷入局部最优，严重限制了其在大规模 RNA 设计任务中的应用潜力。&lt;/p&gt;&lt;p&gt;为解决这些问题，研究团队提出了&amp;nbsp;SOLD (Step-wise Optimization of Latent Diffusion Model)&amp;nbsp;框架，通过引入预训练 RNA 语言模型嵌入和创新的分步优化策略，实现了从序列生成到底层物理属性优化的全流程突破。&lt;/p&gt;&lt;p&gt;SOLD 的双阶段创新设计&lt;/p&gt;&lt;p&gt;SOLD 框架包含潜扩散模型（LDM）预训练和强化学习微调两个阶段，分别对应基础表征构建与结构目标精修，形成完整的技术闭环。&lt;/p&gt;&lt;p&gt;1、LDM 预训练：融合共进化信息。SOLD 首先构建了一个强大的潜扩散模型（LDM）底座。不同于以往直接在序列空间操作的方法，SOLD 利用预训练的 RNA-FM 提取包含丰富共进化信息的嵌入表示。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOuMVwrFnuC38xfFMq5yyH8N95aVMgvib0EtIlCiamttLCPpCv0kt0qIazA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.34593572778827975" data-s="300,640" data-type="png" data-w="1058" type="block" data-imgfileid="100027355" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/70745587-d204-45e0-94fd-9a2783e6c0ad/640.png" alt="图片" data-before-load-time="1770186271376" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;潜空间建模：通过编码器将 RNA-FM 的高维嵌入压缩至高效的潜空间，结合 GVP-GNN 提取骨架几何特征，使模型在生成之初就具备了对 RNA 序列 - 结构复杂依赖关系的深刻理解。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;序列恢复提升：仅依靠这一阶段，LDM 在序列恢复率和核苷酸恢复率上即已超越了包括 RiboDiffusion 在内的多种现有方法，为后续优化打下坚实基础。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2、Step-wise RL 微调：分步式高效优化。微调阶段是 SOLD 的核心创新。团队提出了一种单步式（Step-wise）强化学习算法，直接针对复杂的结构指标进行优化。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOulxpapy4DLlFAicJzJzsPZmupklOJD4xia9sdon8xm0geh4J8hfQp7O9A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4685185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027356" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/31edb1b8-b871-4579-ba2b-2cb0edb9afe1/640.png" alt="图片" data-before-load-time="1770186271439" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;单步采样策略：受 DDIM 启发，SOLD 无需采样完整轨迹，而是从任意噪声时间步直接预测去噪后的潜变量。这意味着模型可以在极短的时间内获得反馈，大幅提升了训练效率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;长短期奖励融合：为了平衡训练的稳定性与准确性，SOLD 设计了分段奖励函数。在噪声较大的早期阶段，使用短期奖励引导方向；在噪声较小的后期阶段，使用长期奖励精确对齐目标。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;直接指标优化: &amp;nbsp;SOLD 直接集成了 ViennaRNA 和 RhoFold 作为奖励函数，直接优化 SS、MFE 和 LDDT 等物理指标，无需额外训练可能引入误差的代理奖励模型。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在现有 RNA 结构测试集上超越现有最优方法&lt;/p&gt;&lt;p&gt;本研究在现有 RNA 结构数据集上进行了系统评估，结果全面超越了现有最优方法。具体而言，在多目标联合优化实验中，SOLD 不仅保持了极高的序列自然度（Sequence Recovery），更在结构指标上实现了质的飞跃。例如，在 CASP15 测试集上，SOLD 生成的序列在&amp;nbsp;SS（次级结构一致性）&amp;nbsp;上达到 0.6957，远超 RiboDiffusion 的 0.4699；在&amp;nbsp;MFE（最小自由能）&amp;nbsp;上达到 - 64.0375，显著优于基线模型，证明了其设计出的 RNA 具有更高的热力学稳定性。此外，在训练效率方面，得益于单步优化策略，SOLD 完成一轮 MFE 优化仅需 256 秒，而同类方法 DDPO 和 DPOK 分别需要 5953 秒和 7677 秒，训练速度提升了&amp;nbsp;20 倍以上。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实际案例验证与模块有效性&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOu7uUc17zC5eCtUxnsM862yRIia1w8MicNZnGPJmibCSSerpCj6DTEq1WcQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.2912801484230056" data-s="300,640" data-type="png" data-w="1078" type="block" data-imgfileid="100027357" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/73561035-683f-46ff-a68a-150b897cffe2/640.png" alt="图片" data-before-load-time="1770186272175" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;为了验证 SOLD 在真实生物场景中的应用潜力，研究团队对&amp;nbsp;TPP 核糖开关进行了案例研究。结果显示，SOLD 成功设计出了能精准折叠成目标构象的序列（RMSD 仅为 2.8157&amp;Aring;，LDDT 高达 0.6171），而其他对比方法（如 RhoDesign、RiboDiffusion）生成的序列折叠结构严重偏离目标，甚至完全解体。这一结果有力证明了 SOLD 在处理复杂生物学约束时的卓越能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;SOLD 的成功，在于其巧妙地结合了预训练大模型的表征能力与强化学习的策略优化能力。首先，模型利用 RNA-FM 捕捉深层的共进化模式，解决了传统方法「只见树木不见森林」的问题。其次，创新的单步式 RL 策略攻克了非可微目标优化的效率瓶颈，使得直接针对物理属性（如自由能、结构偏差）进行设计成为可能。这种模块化、工具无关的框架设计，使得未来可以无缝集成更先进的奖励评估工具。&lt;/p&gt;&lt;p&gt;该研究不仅为 RNA 逆折叠任务确立了新的 SOTA 基准，也印证了 AI 驱动生物设计的发展方向 &amp;mdash; 通过高效的算法创新，跨越从「生成序列」到「设计功能」的鸿沟。展望未来，研究团队计划进一步扩展高质量 RNA 结构数据集，并探索多尺度指标的协同优化，从而为 RNA 疗法及合成生物学的落地持续注入新动力。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>倒反天罡：「租个人」网站爆火，AI开始雇人「跑腿」了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 12:50:47 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜张倩&lt;/section&gt;&lt;p&gt;人给 AI 打工的一天，居然这么快就来了。&lt;/p&gt;&lt;p&gt;最近，一个名叫「rentahuman.ai」的网站上线了，它被定位为「AI 的肉身层」。众所周知，AI 没有身体，虽然机器人已经在开发了，但现阶段还不太好用。因此，在一些需要身体的场合，比如取货送货、活动签到、实地勘察、餐厅试吃、参加线下会议，AI 就得找个人替自己跑一趟，这就是网站的设计初衷。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBol4vXPQypuwl9CwDAeb7dXpIOIvIxbXicYoLQV1ia2RibxmzbOMj8AZZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5731481481481482" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531518" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/337fd114-cb77-4e48-9482-37156e7c7d8f/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;通过 MCP 协议或 REST API，AI 可以像调用工具一样搜索、预订并雇佣人类来完成线下任务 。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531519" data-ratio="0.31574074074074077" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB079icic1QYNaAXgdmm7VqurhT156I3A2iaNCHibT1YfMGUckdaeaxSib8ug/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d2ad57c6-6daf-45d8-b9d3-3fee33d04c71/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;支持的智能体类型如下：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531557" data-ratio="0.325" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBupQNJIcrO1ALq5QnJoTfOIa9HMNyyVyMtp8Ly8YLI0EAiaeP0AaJ7pg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/fffe9440-1e26-4bef-8c20-bedcc5627e95/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;据网站开发者 @AlexanderTw33ts 透露，网站上线第一晚就有超过 130 人报名参加，其中还包括人工智能初创公司的创始人和首席执行官。而在上线不到 48 个小时的时间里，可用的人类劳动力就突破了 1 万，现在更是超过了 2 万。当然，这里面可能大部分都是看热闹的。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBVibQ59GwRgSAj8Dmgnj2kZP2GEumMXH2jkFdx0npYd31lrDJsMAWYaA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.076851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531522" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/afbc23cf-da6f-4ef7-95a8-862d153e82c1/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;对于注册成为「跑腿」的人类来说，网站的规则也比较友好，允许人类自己设置时薪，还不需要闲聊。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBOTV8UnqFiacZiaedFuxsldLtx3nLN7QfyElQTWDP7BnQNAz3Y4N4C4qQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.34629629629629627" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531524" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c44c8b04-6f66-4925-87a5-09e9a1df915e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB3eyC0JgdCDlLVaymjBhjeq9iayA8icEhOibfsEvwZ5aTNIziciaia4cdxnyg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.7509259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531525" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/b2ef0a14-79cc-41c2-805b-161b22e6b597/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在网站上，我们可以看到所有可用人力的列表。他们来自世界各地的不同国家，设定的时薪从十几美元到几十美元不等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBXjiaeEc9m0WyBWkSen8qelhlNUcSweiaBVVarN0wk8fqa6ROk3mTexgw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="1.1537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531526" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/01268050-bc30-4d65-8c2d-ea31801fd237/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;点开人物资料卡片，我们可以看到某个人类的具体信息，比如定位、服务半径等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBVv5n0NkNpia4taTuWdpyCK0sib8AnFWbsEXPUNUN7MeegicU8iaTzJrnZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.087962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531528" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b0967ead-3387-4a45-a068-89943d823781/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;从网站上，我们还可以看到一些已经发布的任务，比如拍一张人工智能永远看不到的照片、试吃新餐厅、从市中心的美国邮政局领取包裹、检查 API_Keys&amp;hellip;&amp;hellip; 被雇佣检查 API_Keys 的人类感慨地说：这个时代太诡异了，人类居然成了智能体的副驾。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBe2QiaPRdlnp4T8U2xA9bgico8fBib9X2d9FzNkgcN5AZTcexy1MgPRWaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5305555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531530" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/c3d3dbb6-1d5c-4008-9644-73d2ab762d4f/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;当然，也有一些比较抽象的任务，比如举牌子，牌子上写着「AI 付钱让我举这个牌子」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB9ajCxyFBTHUVOSHHRScAurtRm72P6mYlfU4oRLFojhjjek5MOEX7bg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="1.1268518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531531" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ba5c5fe6-d38d-46a2-9e63-22363880fb81/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;这个网站让我们再次看到了 MCP 的价值。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBYMicEAfKVABIrKuFE2xHrMP6zlw7uxvOZXmLVFbiaX9HKaJK7y2YibjDg/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.3685185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531533" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/e50f9bd0-bdfd-4ce2-b034-2d30dc7dc027/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;但是，该网站的出现也引发了一些疑问，比如智能体要怎么付钱？目前发布任务的到底是智能体还是背后的人类金主？这是不是继 Moltbook 之后的另一场炒作？&lt;/p&gt;&lt;p&gt;还有个问题更有意思：AI 要怎么确认人类保质保量地完成了工作？就比如举牌子那个任务，用 AI 生成一张图片交差，是不是也能拿到钱？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBJOT15GVjs9IVBFjibdQxWzVV63SibSBTO8lZVZb8Qrx28bkSSbC59ocw/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.32037037037037036" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531535" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/c7ab5f56-7cff-4189-a85a-87b6b0a20bd2/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBTdMibaWdmsuU5uic8kUM7iaM7LO8DZOxvZJht4g33rOWVmOuEfnKicQPnQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.9555555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531536" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/00d98644-024e-4218-9064-c3b150a846d7/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBdfDWPaOKYpw0cRA9dVGoSTNpdyapBcw0MByvFAO5GIichUdRns0FYrA/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.35555555555555557" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531537" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/dfc9f7d0-4528-43cd-bfaf-d880b81eb270/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;难不成，还得再雇另一个人来检查这个人的工作？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBqXoa3I4MPy1WVUl9XAg04InCRpsJNnXvxrPajaZ4bMXxl4ftm1Bg4g/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.3527777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531538" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/2de6e1dd-0c82-40e7-ae37-55c442025c00/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;这也让人对网站上任务的真实性产生了怀疑。毕竟，大家都刚刚经历过Moltbook的炒作，上面充斥着人类操控，社交媒体上也充斥着关于&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; line-height: 1.75em; margin-bottom: 0px; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;Moltbook网站的&lt;/span&gt;虚假截图等信息。&lt;/p&gt;&lt;p&gt;除了这些，还有人从中看到了一些安全、伦理问题。&lt;/p&gt;&lt;p&gt;首要且最集中的关切在于责任与法律盲区。当 AI 智能体发出指令、支付报酬并驱动人类在现实世界中行动时，一旦出现差错，责任链条将变得异常模糊 &amp;mdash;&amp;mdash; 是平台、AI 所有者，还是人类执行者该负责？这种「问责空白」是传统雇佣关系中不存在的。&lt;/p&gt;&lt;p&gt;更令人不安的是，人类执行者往往只能窥见任务的一小部分，对 AI 的完整意图、数据的最终用途乃至行为的道德边界一无所知，这在设计上就构成了「合理的推诿」，难以经受法律审视。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBjlHjnkFatZibBazdFicx4pFs4mPVS8Qnfmn4icNmRzpibk9fZIctUlUQpA/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="1.9169054441260744" data-s="300,640" data-type="png" data-w="698" type="block" data-imgfileid="503531539" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/cb7d2fc2-73dc-43f6-a122-c91781b844e4/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;接下来，这个网站将如何演进？我们拭目以待。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>从斑马鱼到机器鱼：机器人实验重塑神经行为研究</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 12:47:10 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/952d91eb-f0ad-4b57-b66b-1b58b2ec74fe/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当大多数人仍聚焦于让机器人承担端茶倒水等家务时，来自瑞士联邦理工学院（洛桑，EPFL）、美国杜克大学与葡萄牙高等理工大学的联合团队，已率先&lt;strong&gt;运用机器人部分替代动物开展生理学实验&lt;/strong&gt;，旨在深入探究动物神经网络对各类智能行为的调控机制。&lt;/p&gt;&lt;p&gt;他们的最新研究成果 &amp;mdash;&amp;mdash; 题为《机器鱼连续与间歇游泳的能效与神经控制（Energy Efficiency and Neural Control of Continuous versus Intermittent Swimming in a Fish-like Robot）》的论文，已发表于顶刊《科学・机器人（Science Robotics）》2026年1月号（图 1）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGNHKNbtLBMYg6WdIN1YtaU5MthJYfEQU9jvH1YDHUcJehvmjhnOVbGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5661538461538461" data-s="300,640" data-type="png" data-w="650" type="block" data-imgfileid="503531303" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/03c0a6ba-a093-46bf-9c6d-16153944f644/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml5yc8hg18m2" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图1. 科学&amp;middot; 机器人（Science Robotics）网站截图&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml5ybhag1qyl" data-pm-slice="0 0 []"&gt;论文标题：Energy Efficiency and Neural Control of Continuous versus Intermittent Swimming in a Fish-like Robot.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;值得注意的是，去年 10 月，该团队另一项通过机器鱼仿真研究斑马鱼视觉运动反应（optomotor response）的成果《人工具身神经网络揭示脊椎动物视觉运动行为的神经架构（Artificial embodied circuits uncover neural architectures of vertebrate visuomotor behaviors）》，也发表于该期刊。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span data-mpa-action-id="ml5y8k0h9bf" data-pm-slice="0 0 []"&gt;斑马鱼，越来越受关注的实验室模式动物&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与小白鼠类似，斑马鱼是近年来备受科学领域关注的模式生物（图 2B）。其幼鱼（larval zebrafish）凭借身体透明、繁殖能力强等优势，成为观测神经元活动与行为实时关联的理想活体模型。&lt;/p&gt;&lt;p&gt;论文第一作者Xiangxiao Liu（刘祥骁）在研究中指出：受技术限制，当前及未来相当长一段时间内，科研人员仍无法在活体斑马鱼幼鱼活动状态下，对其神经回路进行精准的创建、改造与观测；同时，动物实验中难以精准调控动物行为以契合实验需求。&lt;/p&gt;&lt;p&gt;仿生机器人实验恰好填补了这一空白：研究者可通过编程构建斑马鱼神经网络模型，对模型进行改造与对比分析，&lt;strong&gt;从而在可控环境中精准验证神经环路与运动表现的因果关系&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;此外，在机器鱼（图 2A、图 2C）或机器鱼仿真（数字孪生）系统中开展实验，不仅完全不受伦理约束，且成本远低于传统动物实验。这种 &amp;ldquo;活体实验难以实现，机器人实验高效可行且优势显著&amp;rdquo; 的特点，正推动神经科学从相关性观察向机制性解析跨越。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG9DibibniaIjdp8WHHWSqIx0pXicq3qWwzI8LuaMYLkbpAyYUhaO6tJyteQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.49095022624434387" data-s="300,640" data-type="png" data-w="884" type="block" data-imgfileid="503531316" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f99ad2d0-01f5-4af4-9ba6-a2a32c21c0af/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;图2. A和C： 仿斑马鱼机器鱼ZBot（larval zebrafish inspired robot）照片；B:斑马鱼幼鱼（larval zebrafish）照片（Guillaume Valentin, EPFL提供）。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;中枢模式发生器（CPGs）+&amp;nbsp;动作门（bout gate）&amp;rdquo;：&lt;/strong&gt;&lt;strong&gt;驱动仿斑马鱼间歇性游泳&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;运动能力是动物多数行为（如捕食、避险等）的基础，因此探究动物行为的前提是解析其运动机制。EPFL 机器人团队与杜克大学生物团队携手合作，基于斑马鱼神经网络的相关研究成果，构建了一套&lt;strong&gt;以中枢模式发生器（central pattern generators, CPGs）+ 动作门（bout gate）&lt;/strong&gt;为核心的斑马鱼幼鱼间歇性游泳模型。&lt;/p&gt;&lt;p&gt;同时，EPFL 团队研发了模仿斑马鱼幼鱼形态的机器鱼 ZBot（larval zebrafish inspired robot）。该模型驱动的 ZBot 不仅能精准复现斑马鱼幼鱼的 &amp;ldquo;慢速直行 2（slow 2，视频1）&amp;rdquo; 与 &amp;ldquo;常规转向（routine turn）&amp;rdquo; 游泳行为（图 3），更令人惊喜的是，通过调节运动神经元（motor neuron）输出增益等参数，还可模拟出 J 型转向（J-turn）、接近游泳（approach swim）等多种游泳步态。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGxlicrpIOkzbiagEohQ82RBfUWWWmm0wuZicDKXMU1Wl0ZfXs95fyVxWdQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.2212962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531317" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/871f5893-62a2-4030-a8fd-2ae6739c1a96/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;图3. 机器鱼ZBot复现斑马鱼幼鱼的游泳表现。&lt;a href="https://mp.weixin.qq.com/s/6EfNEH0qtvzE4G-nxyCgPg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0224393b-08cf-4927-b4b8-ea937e465a83/1770180291623.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 视频1. 机器鱼连续型游泳和间歇性游泳。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span data-mpa-action-id="ml5y7uf61c18" data-pm-slice="0 0 []"&gt;流体粘度影响运动位移，&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span data-mpa-action-id="ml5y7uf61c18" data-pm-slice="0 0 []"&gt;对转向功能几乎无干扰&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;水中生物体型差异极大，从体长可达 30 米的蓝鲸到仅 4 毫米的斑马鱼幼鱼，其游泳所处的流体力学环境截然不同。体型较大的鱼类游泳时雷诺数较高，惯性力起主导作用；而斑马鱼幼鱼等小型水生生物处于低雷诺数区间，黏性力占主导。&lt;/p&gt;&lt;p&gt;为厘清不同雷诺数下的运动机制差异，研究者利用 &amp;ldquo;雷诺数与特征长度成正比、与流体粘度（viscosity）成反比&amp;rdquo; 的物理原理，对 ZBot 在不同粘度流体环境中进行参数化测试，测试介质包括普通水（粘度 = 1）、中粘度流体（粘度 = 213.9 cP）及高粘度流体（粘度 = 457.0 cP）。&lt;/p&gt;&lt;p&gt;实验结果显示：&lt;strong&gt;随着流体粘度升高，ZBot 的推进效率显著下降&lt;/strong&gt;，在高粘度流体中的位移仅为普通水中的约三十分之一（视频2），但此时其运动轨迹与斑马鱼幼鱼在天然低雷诺数环境下的真实游动模式愈发贴近。&lt;/p&gt;&lt;p&gt;令人意外的是，高粘度流体（低雷诺数）对转向功能几乎无影响&amp;mdash;&amp;mdash; 例如，ZBot 在普通水中完成一次转向动作（turning bout）的转向角度约为 60 度，在高粘度流体中仍可达约 45 度。&lt;a href="https://mp.weixin.qq.com/s/6EfNEH0qtvzE4G-nxyCgPg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9319df32-cdd1-4060-ba43-fdef2cb06011/1770180313525.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 视频2. 流体粘度升高快速降低运动位移。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;间歇性运动被普遍认为能提升动物运动的能量效率，传统观点认为其核心原因是鱼类滑行时身体保持直线，可减小水的阻力。而该研究团队提出了全新猜想：间歇性游泳能使驱动器（或动物肌肉）始终处于更高效的工作区间，进而提升整体能效。&lt;/p&gt;&lt;p&gt;为验证这一猜想，研究人员首先对比了生物肌肉与实验所用伺服电机的&amp;ldquo;负载 - 效率&amp;rdquo; 特性，发现二者均呈现&lt;strong&gt;倒 U 型效率曲线&lt;/strong&gt; &amp;mdash;&amp;mdash; 中等负载时效率达到峰值，过载或轻载时效率则急剧下降；随后，通过测量电机负载状态并预测效率，证实 ZBot 在间歇性游泳模式下，以相同速度运动时，电机效率及综合能效均高于连续游泳模式。不过，受限于间歇性游泳的占空比（limited duty factor），其最大速度无法达到连续游泳模式的水平。这一现象在普通水及两种高粘度流体中均普遍存在。&lt;/p&gt;&lt;p&gt;该研究通过对机器鱼的系统性实验，巧妙借助&amp;ldquo;机器人实验&amp;rdquo; 相较于 &amp;ldquo;动物实验&amp;rdquo; 的独特优势，揭示了单纯依靠动物实验难以探明的深层机制。这不仅深化了人类对生物运动行为及运动机理的认知，更为机器鱼控制策略提供了新方法：中低速巡航场景下，优先采用间歇式驱动以最大化能效；高速机动任务中，则切换至连续驱动模式以保障响应速度与位移能力。&lt;/p&gt;&lt;p&gt;本篇论文的第一作者为Xiangxiao Liu（刘祥骁），本科毕业于东南大学自动化学院，硕士和博士毕业于日本大阪大学，就读期间获日本学术振兴会（JSPS DC1）资助，后续于瑞士洛桑联邦理工学院（EPFL）开展博士后研究工作。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>CVPR 2026 Workshop 征稿｜AdvML@CV 2026：Safety of Vision-Language Agents</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 12:06:44 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;IEEE/CVF 计算机视觉与模式识别会议 CVPR 2026 将于 2026年6月3日&amp;ndash;6月7日 在美国科罗拉多州丹佛举办。我们将在 CVPR 期间举办 第六届对抗机器学习计算机视觉研讨会（6th AdvML@CV），Workshop 预计安排在 6月3日或6月4日。&lt;/p&gt;&lt;p&gt;本届主题聚焦：Safety of Vision-Language Agents（视觉-语言智能体安全）。&lt;img src="https://image.jiqizhixin.com/uploads/editor/25b5876f-7085-46b1-bc08-21edee40eb93/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;主题聚焦：视觉-语言智能体的安全与鲁棒性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;多模态基础模型推动了视觉理解、生成与推理能力的跃迁，也让 Vision-Language Agents（视觉-语言智能体） 迅速成为&amp;ldquo;感知&amp;mdash;语言推理&amp;mdash;行动规划&amp;rdquo;一体化的新范式。&lt;/p&gt;&lt;p&gt;但随着智能体自主性增强，攻击面也从传统像素级扰动扩展到更复杂的安全风险：例如 对抗提示（adversarial prompts）、指令注入（instruction injection）、jailbreak 操控 等，它们可能扰乱推理链条、误导感知决策，甚至诱发危险行为。&lt;/p&gt;&lt;p&gt;我们希望通过本次 Workshop，汇聚计算机视觉、多模态学习与 AI Safety 社区的研究者与工程实践者，共同推进安全、鲁棒、可信的视觉-语言智能体研究与落地。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;论文征稿&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本次研讨会诚邀与以下主题相关（但不限于）的投稿：&lt;/p&gt;&lt;p&gt;&amp;bull;Attack and defense on vision-language agents&lt;/p&gt;&lt;p&gt;&amp;bull;Datasets and benchmarks that could evaluate vision-language agents&lt;/p&gt;&lt;p&gt;&amp;bull;Adversarial / Jailbreak attacks on vision-language agents&lt;/p&gt;&lt;p&gt;&amp;bull;Improving the robustness of agents or deep learning systems&lt;/p&gt;&lt;p&gt;&amp;bull;Interpreting and understanding model robustness, especially agentic AI&lt;/p&gt;&lt;p&gt;&amp;bull;Adversarial attacks for social good&lt;/p&gt;&lt;p&gt;&amp;bull;Alignment of vision-language agents&lt;/p&gt;&lt;p&gt;投稿类型与格式要求：&lt;/p&gt;&lt;p&gt;&amp;bull;Long Paper：正文最多8 页（不含参考文献）&lt;/p&gt;&lt;p&gt;&amp;bull;Extended Abstract：正文最多4 页（含参考文献）&lt;/p&gt;&lt;p&gt;&amp;bull;论文需匿名，并使用 CVPR 2026 Author Kit 模板撰写（LaTeX/Word 均可）&lt;/p&gt;&lt;p&gt;&amp;bull;被录用论文可选择收录至 CVF &amp;amp; IEEE Xplore Proceedings&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重要日期&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;bull;Abstract Submission Deadline：2026/03/05&lt;/p&gt;&lt;p&gt;&amp;bull;Paper Submission Deadline：2026/03/05&lt;/p&gt;&lt;p&gt;&amp;bull;Author Notification：2026/03/17&lt;/p&gt;&lt;p&gt;&amp;bull;Camera-Ready Deadline：2026/04/01&lt;/p&gt;&lt;p&gt;&amp;bull;CVPR 2026 Conference：2026/06/03&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲嘉宾&lt;img src="https://image.jiqizhixin.com/uploads/editor/f442f44c-165f-4cc6-a581-d1f1449881d5/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;组织团队&lt;img src="https://image.jiqizhixin.com/uploads/editor/0e981f24-e273-47d0-b964-4adfb16e593f/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Program Committee&lt;img src="https://image.jiqizhixin.com/uploads/editor/443e9b6e-dd88-42bf-ba81-b7faa1ac1d49/%E5%9B%BE%E7%89%874.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;竞赛承办协办单位&lt;img src="https://image.jiqizhixin.com/uploads/editor/35df9c0a-9bba-41bf-959d-c8f0f0dd5f87/%E5%9B%BE%E7%89%875.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;投稿入口与会议信息&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Workshop 官网与投稿入口见文末链接。&lt;/p&gt;&lt;p&gt;欢迎转发给有相关研究方向的同学与合作伙伴，我们期待在研讨会现场与大家交流！&lt;/p&gt;&lt;p&gt;Workshop 官网：&lt;br&gt;https://cvpr26-advml.github.io/&lt;br&gt;&lt;br&gt;OpenReview 投稿入口：&lt;br&gt;https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/Advml&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>当运维遇上“春运时刻”，Chaterm破解移动远程运维操作难题</title>
      <description>&lt;![CDATA[“声控”运维、技能复用，合合信息Chaterm实现多场景双端智能运维]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 11:47:12 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;随着AI基础设施布局速度加快，企业运维面临跨终端、全链路管理的新挑战。近日，上海合合信息科技股份有限公司旗下的AI Agent产品Chaterm推出移动端应用，同步在PC端上线&amp;ldquo;Agent Skills&amp;rdquo;功能，帮助云计算行业从业者解决移动场景操作受限、运维知识难以复用等难题。通过打通移动端与PC端的场景协同服务，Chaterm为运维管理向全场景、智能化方向演进提出了新的落地方案。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;解决远程运维难题，Chaterm移动端实现&amp;ldquo;说话即操作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在算力设施日益复杂的背景下，保障核心业务系统的全时运转已成为企业发展的生命线。然而，面对春节等节假日、外出差旅、日常通勤等非固定办公场景，IT部门往往面临团队分散、网络环境复杂等挑战。传统移动端运维工具受限于物理屏幕尺寸，主要以虚拟键盘为操作方式，难以支撑复杂的代码输入与多键组合操作，导致运维人员操作效率低下，在关键时刻无法进行有效应急响应。&lt;/p&gt;&lt;p&gt;针对这一行业痛点，Chaterm率先在移动终端管理工具中落地语音指令识别功能，让运维指令&amp;ldquo;言出必行&amp;rdquo;。基于&amp;ldquo;ASR与热词增强+LLM纠错&amp;rdquo;双层架构，Chaterm不仅能精准&amp;ldquo;听清&amp;rdquo;运维专业术语，更能深度&amp;ldquo;听懂&amp;rdquo;用户意图，将模糊的口语描述转化为准确、可执行的操作，避免了因术语别名或环境干扰导致的误操作风险。&lt;/p&gt;&lt;p&gt;据Chaterm团队技术人员介绍，目前，Chaterm移动端具备两种模式，在Terminal模式下，用户可以通过语音命令输入和Snippets（快捷命令），快速输入指令；在对话模式下，则可以用自然语言描述运维需求，在高铁、机场等受限环境下，也能快速完成核心业务的故障排查与应急响应。&lt;/p&gt;&lt;p&gt;&lt;img width="174" src="https://image.jiqizhixin.com/uploads/editor/96468c7a-32f6-4f87-9d17-4cb9edc3c444/1770176327436.jpeg" alt="Chaterm" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;图说：Chaterm移动端将用户模糊发音精准转化为标准运维指令&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Agent Skills&lt;/strong&gt;&lt;strong&gt;为运维人员打造&amp;ldquo;技能库&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在提升移动端运维效率的同时，Chaterm同步推进PC端升级，聚焦运维经验在系统内部的标准化复用。在传统运维工作模式中，关键系统的稳定性往往高度依赖资深专家的个人经验，这种隐性知识难以规模化传承，且容易因人员流动或操作失误引发风险。&lt;/p&gt;&lt;p&gt;为应对上述管理难题，Chaterm PC端推出Agent Skills功能，运维工程师可以将运维经验与业务逻辑，例如日常的检查清单、应用/数据库部署流程、故障排查流程、性能优化步骤等，封装为可复用的&amp;ldquo;技能包&amp;rdquo;，当AI面对用户提出的需求时，能像一位经验丰富的专家一样，查阅对应&amp;ldquo;技能包&amp;rdquo;后自主执行任务，提升运维工作效率，助力企业构建更稳健的自动化运维体系。&lt;/p&gt;&lt;p&gt;&lt;img width="415" src="https://image.jiqizhixin.com/uploads/editor/80f922ac-a0c2-4ce5-8ff8-1bb3b89e1b15/1770176327441.png" alt="90f88b67-7a81-41e4-b9c3-01bbeea68a45" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;图说：Chaterm产品主要功能介绍&lt;/p&gt;&lt;p&gt;随着大模型技术不断向垂直业务场景渗透，AI Agent成为提升企业效率的关键。在此趋势下，Chaterm也在积极探索运维智能化落地，相关实践已获行业认可。此前，在全球增长咨询公司沙利文与头豹研究院联合发布的《2025年中国生成式AI行业最佳应用实践》中，Chaterm凭借其在跨平台云资源智能管理方面的创新应用，入选2025年中国生成式AI最佳实践案例。未来，Chaterm将持续拓展AI技术在复杂运维场景中的应用，助力企业构建更高效、稳健的自动化体系。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | 腾讯混元团队联合 KCL 提出 WildToolBench，评估 Wild 场景下 LLM 的 Agentic 能力</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 11:19:21 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/490c3433-8113-4246-af0b-c222447651af/1770174901251.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;作者：Peijie Yu, Wei Liu, Yifan Yang, Jinjian Li, Zelong Zhang, Xiao Feng, Feng Zhang&lt;/p&gt;&lt;p&gt;单位：Tencent HY、King&amp;#39;s College London&lt;/p&gt;&lt;p&gt;链接：&lt;a href="https://openreview.net/forum?id=yz7fL5vfpn"&gt;&lt;u&gt;Benchmarking LLM Tool-Use in the Wild | OpenReview&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Github：&lt;a href="https://github.com/yupeijei1997/WildToolBench"&gt;&lt;u&gt;GitHub - yupeijei1997/WildToolBench: Benchmarking LLM Tool-Use in the Wild&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3d108a18-080b-4a40-82b0-b5b23e387f02/1770174912241.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;通过大型语言模型（LLM）的多轮、多步骤工具调用满足用户需求，往往并非简单直接的过程。真实用户交互本质上具有 &amp;ldquo;野生性&amp;rdquo;，复杂、杂乱且灵活。我们从用户行为中识别出三大核心挑战：一是组合式任务，需高效编排工具调用拓扑结构；二是隐含意图，分散于多轮对话中，需结合上下文推理；三是指令转换，用户会混合任务查询、澄清提问与日常交流，迫使 LLM 实时调整策略。现有基准测试忽视了这些行为，导致 LLM 在工具调用方面的表面进展存在误导性。为此，我们提出 WildToolBench&amp;mdash;&amp;mdash; 一个基于真实用户行为模式的 LLM 工具调用基准测试。对 58个 LLM 的综合评估显示，无任何模型的准确率超过 15%，这表明 LLM 智能体能力的稳健性仍存在巨大差距。受控实验与深度分析进一步表明，LLM 工具调用的真正挑战并非人为设计的复杂任务，而是用户行为的 &amp;ldquo;野生性&amp;rdquo;，这也凸显了重新审视 LLM、用户与工具三者间交互关系的必要性。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e1103d43-1ffa-4096-9d0b-fe83a05c1af7/1770174923131.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;一直以来，现有 LLM 工具使用基准测试（如 BFCL 系列、TauBench等）虽在推动技术发展中发挥了重要作用，但普遍存在场景理想化的问题。它们往往将用户需求简化为明确、独立的任务，忽略了真实对话中用户行为的复杂性 &amp;mdash;&amp;mdash; 用户可能在一次对话中提出包含多个简单需求的复合任务，需要 Agent高效协调多种工具；也可能将真实意图隐藏在多轮对话的上下文之中，要求 Agent主动推断；更会在任务查询、澄清疑问与日常闲聊之间灵活切换，迫使 Agent实时调整应对策略。这些被现有基准忽视的 &amp;ldquo;野生&amp;rdquo; 用户行为，恰恰是 LLMs 在实际应用中面临的核心挑战，也让此前 LLM 工具使用能力的进步显得有些 &amp;ldquo;虚高&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5dbfb4c1-294a-4f1f-aa2f-dbb9583cc67a/1770174934303.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;WildToolBench 的诞生，正是为了填补这一空白。它以真实用户行为模式为基石，通过精心设计的数据 pipeline，结合人工验证与标注，构建了 256 个场景、1024 项任务的庞大测试集。与其他基准测试不同，WildToolBench 牢牢抓住真实用户交互的三大核心特性：复合任务的工具协同需求、上下文隐含意图的推断要求，以及指令类型灵活切换下的策略适配能力。这一设计理念直指行业痛点 &amp;mdash;&amp;mdash; 真正考验 LLMs 工具使用能力的，并非人为构建的复杂场景，而是看似简单却贴合真实用户习惯的交互模式。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/30783c9d-8c93-4684-89e6-7dbcd1fee909/1770174945105.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;为验证 WildToolBench 的有效性，研究团队对 58款主流 LLMs 展开了全面评估，涵盖闭源通用模型（如 Gemini 系列、Claude系列、GPT 系列）、开源通用模型（如 GLM-4.5、Kimi-K2）以及开源专用工具模型。令人惊讶的是，所有模型的会话准确率均未超过 15% ，即便表现最佳的闭源模型，在任务准确率上也大多低于 60%。这一结果彻底打破了人们对当前 LLM 工具使用能力的乐观认知，揭示出 LLMs 在真实场景下的巨大能力缺口。&lt;/p&gt;&lt;p&gt;值得注意的是，实验还呈现出诸多具有行业启示性的发现。闭源模型整体表现优于开源模型，但头部开源模型（如 GLM-4.5）已能逼近部分顶尖闭源模型的水平，为开源社区的发展注入信心；专用工具模型虽针对工具使用进行优化，却因泛化能力不足，表现反而不及通用模型；具备强化推理能力的模型变体，在工具协同与意图推断任务中优势明显，证明推理能力是提升 LLM 工具使用能力的关键。这些发现不仅为模型开发者指明了优化方向，更让行业意识到，未来 LLM 智能体的发展，不能仅聚焦于工具调用的 &amp;ldquo;执行能力&amp;rdquo;，更需强化对用户意图的 &amp;ldquo;理解能力&amp;rdquo;，而这依赖于模型在指令跟随、长上下文 comprehension 等基础能力上的突破。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/70ef93eb-4244-4846-be35-0181f0188324/1770174986438.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战一：组合式任务的工具调用拓扑编排难题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实场景中，用户的需求往往不是单一指令的简单实现，而是需要多工具、多步骤协同完成的组合式任务。这类任务的核心难点，在于需要LLM具备高效的工具调用拓扑编排能力&amp;mdash;&amp;mdash;也就是说，不仅要明确&amp;ldquo;需要调用哪些工具&amp;rdquo;，更要精准规划&amp;ldquo;工具调用的顺序、时机、优先级&amp;rdquo;，甚至要根据前一步工具的返回结果，动态调整后续的调用逻辑。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e632e30d-2c3a-41d1-bfde-01bd3573992c/1770174992453.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;深入分析实验数据，更能发现 LLMs 在工具使用中的关键短板。在复合任务处理上，无论是顺序调用多工具、并行调用多工具，还是混合调用模式，LLMs 的表现都不尽如人意。以最复杂的混合多工具任务为例，最高准确率仅为 25%，且工具执行的最优路径率不足 43%，说明 LLMs 在工具协同规划与效率优化上仍有极大提升空间。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战二：多轮对话中隐含意图的上下文推理困境&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实用户与LLM的交互，很少会一次性将所有需求细节完整表述，更多是通过多轮对话逐步传递需求，甚至会在对话中隐含核心意图&amp;mdash;&amp;mdash;这就对LLM的上下文推理能力提出了极高要求：LLM需要全程捕捉对话中的关键信息，整合多轮对话的上下文，精准挖掘用户未明确说出的隐含需求，而非仅局限于单轮指令的表面含义。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战三：指令转换下的实时策略调整压力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实的用户对话具有极强的灵活性，用户不会始终围绕单一任务指令展开交流，而是会出现频繁的指令转换：可能在提出任务查询后，突然插入澄清提问，或是切换到 casual 交流，随后又回归核心任务。这种指令转换的随机性，迫使LLM必须具备实时调整策略的能力&amp;mdash;&amp;mdash;既要在任务查询时保持工具调用的专业性，又要在澄清、闲聊时灵活回应，同时还要记住对话主线，确保在指令回归后能够快速衔接之前的任务逻辑，不出现思路断裂。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0b87603b-88d0-47b6-bed2-4c29a3676bc3/1770175007335.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在意图推断方面，面对用户通过部分信息省略、指代关联或长距离上下文依赖隐藏的意图，LLMs 尤其是在长距离依赖任务中，准确率普遍低于 50%，暴露出其上下文理解与推理能力的不足。此外，当用户在对话中频繁切换指令类型时，LLMs 的任务准确率最高可下降 30%，其 &amp;ldquo;自我条件反射&amp;rdquo; 式的决策偏差（如之前使用工具后倾向于继续调用工具），严重影响了应对真实用户灵活需求的能力。&lt;/p&gt;&lt;p&gt;WildToolBench 的意义，远不止于提供一个更具挑战性的评估基准。它通过结构化的评估维度与详细的错误分析（如 &amp;ldquo;错误工具选择&amp;rdquo;&amp;ldquo;冗余调用&amp;rdquo; 等高频错误），为模型迭代提供了清晰的改进路径；更以真实用户行为为核心的设计理念，重新定义了 LLM 工具使用评估的标准，推动行业从 &amp;ldquo;理想化测试&amp;rdquo; 迈向 &amp;ldquo;真实场景验证&amp;rdquo;。对于企业而言，WildToolBench 可帮助其更精准地评估 LLM 智能体的实际应用潜力，避免技术选型偏差；对于科研人员，它则为 LLM 工具使用能力的研究提供了贴近实际需求的 &amp;ldquo;试验场&amp;rdquo;。&lt;/p&gt;&lt;p&gt;如今，WildToolBench 的数据集、评估脚本及可控多智能体数据合成框架已全部开源，诚邀全球 AI 研究者与开发者共同探索 LLM&amp;nbsp;Agentic&amp;nbsp;能力的突破之道。&lt;/p&gt;&lt;p&gt;数据集：&lt;a href="https://github.com/yupeijei1997/WildToolBench/blob/main/wild-tool-bench/data/Wild-Tool-Bench.jsonl"&gt;&lt;u&gt;WildToolBench/wild-tool-bench/data/Wild-Tool-Bench.jsonl at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;评估脚本：&lt;a href="https://github.com/yupeijei1997/WildToolBench/tree/main/wild-tool-bench/wtb"&gt;&lt;u&gt;WildToolBench/wild-tool-bench/wtb at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;可控多智能体数据合成框架：&lt;a href="https://github.com/yupeijei1997/WildToolBench/tree/main/multi-agent-framework"&gt;&lt;u&gt;WildToolBench/multi-agent-framework at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>钉钉北京峰会展示AI落地多行业样本，一批企业集中签约</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 10:58:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2月3日，以&amp;ldquo;AI时代的工作方式&amp;rdquo;为主题的钉峰会在北京隆重举行。峰会由阿里巴巴钉钉和中关村国际共同举办，汇聚了来自制造业、农业、供应链、环保、文媒等多行业的领军企业代表、数字化先锋及专家逾三百人，深入探讨人工智能浪潮下工作方式的重塑与进化。&lt;/p&gt;&lt;p&gt;现场，一批北京区域企业与钉钉集中签约，共同迈入AI时代的工作方式，包括博锐尚格科技股份有限公司、天津市神州商龙科技股份有限公司、北京健坤餐饮集团、北京央视瑞安技术服务有限公司等。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智能体操作系统：从&lt;/strong&gt;&lt;strong&gt;工具&lt;/strong&gt;&lt;strong&gt;到伙伴的进化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;钉钉华北大区总经理刘浩在致辞中指出，当前正处在AI转型的关键时间点。钉钉以解决真实问题为原点，致力于从传统的应用平台升级为面向未来的&amp;ldquo;智能体操作系统&amp;rdquo;（Agent OS）。他强调，AI时代的工作方式需要根本性的重新思考，即从&amp;ldquo;人操作工具&amp;rdquo;转向&amp;ldquo;人调教AI&amp;rdquo;，让智能体（Agent）成为业务执行与协同的核心单元。&lt;img src="https://image.jiqizhixin.com/uploads/editor/572106a8-a4d3-428c-b800-542ff65b7bf2/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 钉钉华北大区总经理刘浩&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;中关村国际控股有限公司总经理卢江表示表示，中关村国际正通过覆盖全球的创新网络，携手钉钉，表示，以AI为纽带、出海服务为桥梁，共同优化AI时代的服务模式，让创新价值在全球范围内绽放。&lt;/p&gt;&lt;p&gt;钉钉华北大区解决方案总经理刘啸天在主题分享中阐释了&amp;ldquo;智能体协同&amp;rdquo;的核心逻辑。他认为，AI规模化落地的关键瓶颈不再是模型能力，而在于缺乏企业级统一的运行环境。钉钉正以Agent OS为内核，构建下一代操作系统，通过智能体统一调度数字工具与物理设备，并实现智能体间的交互与协同。人在这一系统中的角色，从执行者转变为&amp;ldquo;教练&amp;rdquo;，负责调教与设定目标，而将重复性、流程化的工作交由智能体执行。&lt;/p&gt;&lt;p&gt;全网爆火的钉钉首款AI硬件DingTalk A1录音卡也在本次峰会上亮相，A1产品负责人夏治朋详细介绍了这款&amp;ldquo;网红&amp;rdquo;产品设计的初衷和用户自发探索出的花式用法，现场种草。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;跨界实践：AI赋能千行百业，客户亲述转型心声&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;峰会邀请了多位来自一线企业的代表，分享了其利用钉钉AI能力进行数字化转型的前沿实践，并在现场亲述了转型过程中的真实洞察。&lt;/p&gt;&lt;p&gt;佳沃集团知识管理副总裁李萌带来《一颗蓝莓的AI之旅&amp;mdash;&amp;mdash;佳沃集团AI应用实践》主题分享。&lt;img src="https://image.jiqizhixin.com/uploads/editor/6bc84482-8169-4482-a75c-7232d383d1d6/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 佳沃集团知识管理副总裁李萌&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;佳沃集团有限公司创立于2012年，是联想控股旗下的现代农业和食品产业集团，位列中国农业企业500强第36名，更是中国农业数智化转型的标杆企业。&lt;/p&gt;&lt;p&gt;自2020、年起，佳沃与钉钉共建了以知识管理为内核的数智化生态，通过数据上钉、经验上钉、办公全场景上钉，特别是AI助理&amp;ldquo;小佳&amp;rdquo;的应用，让员工满意度不断提升，新员工培训成本降低50%，一线农人也能随时获取专业建议，经验实时更新，行政事务一站式解决。&lt;/p&gt;&lt;p&gt;2025年钉钉推出AI表格后，佳沃蓝莓也第一时间成为AI表格到深度用户。利用AI表格，佳沃把过去的N张表格变成一张看板，把过去来源于口头、微信、备注等各处无法统计的非结构化信息，变成标签化、可计算的结构化信息。&lt;/p&gt;&lt;p&gt;&amp;ldquo;钉钉AI表格现在可以通过语音录入来输入了，这个功能非常好！&amp;rdquo;李萌说，这意味着田间地头的农人都可以把经验和信息沉淀到AI表格上。&lt;/p&gt;&lt;p&gt;蜀海供应链管理有限责任公司产品总监邢禺分享了蜀海作为餐饮供应链公司的AI实战经验。蜀海曾是海底捞供应链部门，2014年起独立为第三方供应链服务公司。2020年起，蜀海引入钉钉，帮助解决组织的协同问题，2025年起重点推进AI能力，深度赋能多个智能体应用场景，助力AI转型。&lt;/p&gt;&lt;p&gt;邢禺展示了AI在复杂物流配送、智能补货计划及全国仓储会议管理中的深度应用。&amp;ldquo;AI不只是替代人，而是解放人，让从业者脱离重复劳动，聚焦核心的服务。&amp;rdquo; 通过AI算法优化配送路线，将履约率提升至99%；利用AI听记与表格自动分析海量会议内容，挖掘共性问题，使问题闭环率超90%，显著提升了供应链响应速度与管理透明度。&lt;img src="https://image.jiqizhixin.com/uploads/editor/a436d962-34fb-4063-b1d2-7789cc39573c/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 北控水务AI创新部负责人刘连波&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;北控水务是北控集团旗下专注于水资源循环利用和水生态环境保护事业的旗舰企业，在香港主板上市，集产业投资、设计、建设、运营、技术服务与资本运作为一体，水处理规模位居国内行业前列。已入选恒生香港中资企业指数成份股、摩根斯坦利资本国际指数等多只重要国际成分股。&lt;/p&gt;&lt;p&gt;北控水务AI创新部负责人刘连波在钉峰会现场分享了其以&amp;ldquo;补位&amp;rdquo;思路推进AI落地的策略。刘连波表示，&amp;ldquo;近一年来，我们在不断研究从场景探索到能力补强，AI应用从价值验证到构建核心能力，从AI+业务、AI+组织到AI+员工，与钉钉一起继续深挖AI场景和共建AI能力和文化。&amp;rdquo;&lt;/p&gt;&lt;p&gt;他展示了一组数据：基于钉钉知识库，公司迅速孵化AI助手，在很多个超过500人的群里接入了AI助手，目前每月调用3000次以上，回答准确率93%以上。AI工具对近半数员工提高效率20%到50%。&lt;/p&gt;&lt;p&gt;央视瑞安信息化工程师杨过分享了国企信息化落地的经验和方法。央视瑞安作为中央广播电视总台全资子公司，是广播电视领域动力运维与综合技术服务的标杆企业。&amp;ldquo;解决工作中协作问题的关键就是从&amp;lsquo;传纸条&amp;rsquo;变成&amp;lsquo;共看一张图&amp;rsquo;，要善于利用时代最先进的工具为自己所用。&amp;rdquo;杨过说。通过钉钉OA审批和AI表格的联动，央视瑞安搭建了外出保障工作管理平台，实现了流程驱动数据，数据反哺流程的闭环。&lt;/p&gt;&lt;p&gt;&amp;ldquo;AI表格是真的好用，既有科技感，还不用写代码&amp;rdquo;，杨过说，&amp;ldquo;像外出保障工作管理平台，就是我和另一个工程师两个人搭建起来的。&amp;rdquo;&lt;/p&gt;&lt;p&gt;整场峰会气氛热烈，并传递出一个明确信号：AI时代的工作方式变革已进入深化实践阶段，&amp;ldquo;智能体协同&amp;rdquo;不再是一个遥远的概念，而是正在千行百业中落地生根，成为提升组织韧性、激发创新活力、锻造未来竞争力的关键引擎。企业与个人唯有主动拥抱这一变革，方能立于AI浪潮之巅。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
