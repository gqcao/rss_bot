<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>密瓜智能获数千万元天使轮融资：植根开源基因，驱动异构算力效率跃升</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 06 Jan 2026 18:15:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;div&gt;&lt;img src="https://dynamia-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=MDNkYTAyYzY3ZjhiNTVkYzI4MGU1MDU0MDZmNzgyMzlfamdrNEt1ZTAzVUdxOU5hckF2OWhEWjlya2F2R0c1NDhfVG9rZW46QWZUeWJseGVvb09NVzB4SDFPRGN6TG5wbkRiXzE3Njc2OTQzODE6MTc2NzY5Nzk4MV9WNA" data-single-block="true" data-snapshot="eyJ0eXBlIjoiaW1hZ2UiLCJwYXJlbnRfaWQiOiJWTnFrZHdQT3BvcXNWbnhUNlFmY3JoQkdub2QiLCJjb21tZW50cyI6W10sInJldmlzaW9ucyI6W10sImxvY2tlZCI6ZmFsc2UsImhpZGRlbiI6ZmFsc2UsImF1dGhvciI6Ijc1NTI5NTk0MTk1NjAxMTYyMjUiLCJhbGlnbiI6IiIsImltYWdlIjp7InRva2VuIjoiQWZUeWJseGVvb09NVzB4SDFPRGN6TG5wbkRiIiwibWltZVR5cGUiOiJpbWFnZS9qcGVnIiwic2l6ZSI6NDM0NTIsInNjYWxlIjoxLCJ3aWR0aCI6MTA4MCwiaGVpZ2h0Ijo0MzAsIm5hbWUiOiJBZlR5Ymx4ZW9vT01XMHhIMU9EY3pMbnBuRGIuanBnIiwiY3JvcCI6WzAsMCwwLDBdLCJyb3RhdGlvbiI6MCwiY2FwdGlvbiI6eyJ0ZXh0Ijp7ImFwb29sIjp7Im5leHROdW0iOjAsIm51bVRvQXR0cmliIjpudWxsfSwiaW5pdGlhbEF0dHJpYnV0ZWRUZXh0cyI6eyJhdHRyaWJzIjpudWxsLCJ0ZXh0IjpudWxsfX19fSwiYXJlYV9jb21tZW50cyI6e319" data-suite="eyJmaWxlVG9rZW4iOiJBZlR5Ymx4ZW9vT01XMHhIMU9EY3pMbnBuRGIiLCJvYmpUeXBlIjoiZG9jeCIsIm9ialRva2VuIjoiRHRqWGRvYmQ5bzRiYkl4NExrUWNNc3Rnbk1mIiwib3JpZ2luU3JjIjoiaHR0cHM6Ly9pbnRlcm5hbC1hcGktZHJpdmUtc3RyZWFtLmZlaXNodS5jbi9zcGFjZS9hcGkvYm94L3N0cmVhbS9kb3dubG9hZC9wcmV2aWV3L0FmVHlibHhlb29PTVcweEgxT0RjekxucG5EYi8/cHJldmlld190eXBlPTE2In0=" data-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/all/AfTyblxeooOMW0xH1ODczLnpnDb/?mount_node_token=DtjXdobd9o4bbIx4LkQcMstgnMf&amp;mount_point=docx_image" data-width="1080" data-height="430" data-lark-image-uri="drivetoken://AfTyblxeooOMW0xH1ODczLnpnDb" data-lark-image-width="1080" data-lark-image-height="430" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/div&gt;专注于异构算力调度和虚拟化的 AI 初创企业&lt;strong&gt;上海密瓜智能科技有限公司（&amp;ldquo;密瓜智能&amp;rdquo;）已于近期完成数千万元的天使轮融资&lt;/strong&gt;，本轮融资由&lt;strong&gt;复星创富&lt;/strong&gt;领投，&lt;strong&gt;拙朴投资、种子投资人及产业方&lt;/strong&gt;强力跟投。自去年 3 月获得超五百万元种子轮融资以来，密瓜智能在不足一年时间内已迅速完成 2 轮融资，展现出强劲的发展势能，其技术前景与商业价值备受市场认可。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;strong&gt;密瓜智能：让异构算力因开源而好用&lt;/strong&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;密瓜智能始终致力于&lt;strong&gt;打造全球领先的开源异构算力调度方案，以技术驱动 AI 时代的算力效率革命&lt;/strong&gt;。该公司&lt;strong&gt;主导并发起了 CNCF 项目 HAMi&lt;/strong&gt;，这是目前行业内&lt;strong&gt;唯一专注于异构 GPU 资源虚拟化、高效调度的开源项目&lt;/strong&gt;，通过灵活、可靠、按需、弹性的 GPU 虚拟化来提升资源利用率，可以插拔式、轻量化、无侵入地部署在任意公有云、私有云、混合云环境中。在生态兼容层面，HAMi 已全面支持 NVIDIA、昇腾、沐曦、寒武纪、海光、摩尔线程、天数智芯、昆仑芯、燧原及 AWS 等国内外主流 GPU 芯片。同时无缝支持 vLLM, Volcano, Koordinator, Kueue 等上下游主流开源项目，逐步形成稳定的异构算力生态。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;目前，HAMi 已集结了来自&lt;strong&gt;&amp;nbsp;16 个国家的 360 余位贡献者&lt;/strong&gt;，社区活跃度持续提升，&lt;strong&gt;最终用户已经超过 200 家企业&lt;/strong&gt;，覆盖国内主流云厂商、互联网以及 KA 用户，并已拓展至东南亚、欧洲等海外地区，行业影响力十分广泛。2025 年，由密瓜智能发起的&amp;ldquo;不卷算力卷效率&amp;rdquo;系列 Meetup 亦备受关注。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;在短短一年的商业化发展中，&lt;strong&gt;密瓜智能获得多个头部客户商业订单，渠道合作体系迅速拓展&lt;/strong&gt;，正逐步获得市场的广泛认可。基于现有生态发展与广泛的用户反馈，密瓜智能将进一步推动 HAMi 与更多 GPU 厂商实现兼容适配，持续挖掘高价值的场景需求，推动其在大模型推理侧的生态适配，构建上下游协同的算力服务体系。与此同时，将基于 HAMi 推出面向企业级市场的核心产品，为客户的生产级场景持续赋能。&lt;div&gt;&lt;img src="https://dynamia-ai.feishu.cn/space/api/box/stream/download/asynccode/?code=NTY2MmI1ODQyZmU1ZWQ5MWZlNDAxNDg4NWVlMTYzMzRfYmtKZUdtQ2xMYnFGQmxidmRCc3g0M3BXQlBvcnJvQ0VfVG9rZW46RlZQSWJhbUl3bzlodDd4Vmc2MGNxZGZIbmdjXzE3Njc2OTQzODE6MTc2NzY5Nzk4MV9WNA" data-single-block="true" data-snapshot="eyJ0eXBlIjoiaW1hZ2UiLCJwYXJlbnRfaWQiOiJWTnFrZHdQT3BvcXNWbnhUNlFmY3JoQkdub2QiLCJjb21tZW50cyI6W10sInJldmlzaW9ucyI6W10sImxvY2tlZCI6ZmFsc2UsImhpZGRlbiI6ZmFsc2UsImF1dGhvciI6Ijc1NTI5NTk0MTk1NjAxMTYyMjUiLCJhbGlnbiI6ImNlbnRlciIsImltYWdlIjp7InRva2VuIjoiRlZQSWJhbUl3bzlodDd4Vmc2MGNxZGZIbmdjIiwibWltZVR5cGUiOiJpbWFnZS9qcGVnIiwic2l6ZSI6MTUwNzQ1LCJzY2FsZSI6MSwid2lkdGgiOjE5MjAsImhlaWdodCI6MTI4MCwibmFtZSI6IuW+ruS/oeWbvueJh18yMDI2LTAxLTA1XzE1MjIzNV8wODUuanBnIiwiY3JvcCI6WzAsMCwwLDBdLCJyb3RhdGlvbiI6MCwiY2FwdGlvbiI6eyJ0ZXh0Ijp7ImFwb29sIjp7Im5leHROdW0iOjAsIm51bVRvQXR0cmliIjpudWxsfSwiaW5pdGlhbEF0dHJpYnV0ZWRUZXh0cyI6eyJhdHRyaWJzIjpudWxsLCJ0ZXh0IjpudWxsfX19fSwiYXJlYV9jb21tZW50cyI6e319" data-suite="eyJmaWxlVG9rZW4iOiJGVlBJYmFtSXdvOWh0N3hWZzYwY3FkZkhuZ2MiLCJvYmpUeXBlIjoiZG9jeCIsIm9ialRva2VuIjoiRmx4NmQxcUtCb3JPbWF4U3Rwa2N3elF2bjhnIiwib3JpZ2luU3JjIjoiaHR0cHM6Ly9pbnRlcm5hbC1hcGktZHJpdmUtc3RyZWFtLmZlaXNodS5jbi9zcGFjZS9hcGkvYm94L3N0cmVhbS9kb3dubG9hZC9wcmV2aWV3L0ZWUEliYW1Jd285aHQ3eFZnNjBjcWRmSG5nYy8/cHJldmlld190eXBlPTE2In0=" data-src="https://internal-api-drive-stream.feishu.cn/space/api/box/stream/download/all/FVPIbamIwo9ht7xVg60cqdfHngc/?mount_node_token=Flx6d1qKBorOmaxStpkcwzQvn8g&amp;mount_point=docx_image" data-width="1920" data-height="1280" data-lark-image-uri="drivetoken://FVPIbamIwo9ht7xVg60cqdfHngc" data-lark-image-width="1920" data-lark-image-height="1280" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/div&gt;密瓜智能的创始团队成员均来自于&lt;strong&gt;「DaoCloud 道客」、第四范式、百度等 AI 基础设施领域代表厂商&lt;/strong&gt;以及&lt;strong&gt;清华大学、浙江大学等国内高校&lt;/strong&gt;，在云计算、云原生以及 GPU 共享与调度技术上具备丰富的技术积累和行业经验。&lt;strong&gt;创始人张潇和联合创始人李孟轩是 HAMi 的作者&lt;/strong&gt;，团队其他成员也都是 HAMi 的核心贡献者和维护者，拥有广泛的技术影响力与行业前瞻性。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;strong&gt;投资人说：从开源到产业，他们以创新赋能异构算力&lt;/strong&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;strong&gt;复星创富投资执行总经理叶丽娟&lt;/strong&gt;表示，异构将成为算力市场的长期格局，无论是 GPU 还是新型算力芯片，是 AI 最重要的底座，密瓜智能在 AI 大生态中不可或缺地链接算力端与应用端，为客户极大程度提升算力效率，节省昂贵的算力成本。开源的 HAMi 已建立起颇具规模的开发者与用户生态&amp;mdash;&amp;mdash;这一路径也与 AI 行业开源化、协同化的发展趋势高度契合。HAMi 提供的灵活、弹性、按需且可靠的虚拟化技术，能够实现算力的高效切分与调度，显著提升算力利用率，从而为全球客户带来极具竞争力的投资回报率（ROI）。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;strong&gt;拙朴投资总监陈敏洁&lt;/strong&gt;在与密瓜智能的沟通中提到，在上一代以 CPU 为核心的云计算时代，诞生了像 VMware 这样的虚拟化巨头。如今到了以 GPU 为核心的 AI 智算时代，AI 任务负载对算力的需求与底层硬件分配方式之间也同样存在巨大的错配，虚拟化是通向 AI 普惠的核心钥匙。国产算力多元异构百家争鸣的现状，也赋予 HAMi 开源更深远的意义，开源不再只是情怀，而是生存发展的必需，是对当前算力秩序的重塑。HAMi 想要打破硬件的藩篱，让算力成为像水一样随手可得的公共基础设施，帮助多元异构芯片与全球生态共振。在这一趋势中，HAMi 有望成为异构算力调度虚拟化的全球通用标准。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;strong&gt;孵化股东「DaoCloud 道客」创始人兼 CEO 陈齐彦&lt;/strong&gt;表示，HAMi 已在 AI 基础设施中形成关键生态位。「DaoCloud 道客」期待密瓜智能将其能力打造为统一、通用的异构 GPU 管理与虚拟化标准。随着 AI 技术向开源社区收敛，密瓜智能具备成为该领域领军者的潜力，双方产品体系已与 HAMi 企业版深度结合，实现协同赋能。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;strong&gt;持续深耕开源，加速企业级产品布局&lt;/strong&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;strong&gt;密瓜智能创始人兼 CEO 张潇&lt;/strong&gt;对各位新老投资人的信任与支持表示衷心感谢。他提到，密瓜智能自创立之初，便&lt;strong&gt;以开源为技术基因，聚焦于异构算力这一核心赛道&lt;/strong&gt;。过去一年，凭借这一关键的生态卡位和在开源社区的持续深耕，密瓜智能在技术沉淀与产品迭代上取得了扎实的进展，&lt;strong&gt;这既离不开团队的努力，也受益于投资人在战略与资源上的持续赋能&lt;/strong&gt;。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;本轮融资将主要用于公司发展的三个方面。&lt;strong&gt;第一，深化开源生态建设&lt;/strong&gt;，公司将持续贯彻开源战略，保持在这一路径上的领先投入。&lt;strong&gt;第二，加速团队建设与全球化布局&lt;/strong&gt;，重点扩大核心技术研发与生态营销团队，并构建完善的渠道体系。（如果有想要加入密瓜智能全球渠道体系的伙伴，欢迎与我们取得联系）&lt;strong&gt;第三，企业级产品研发&lt;/strong&gt;，密瓜智能将围绕弹性扩容、任务优先级调度、显存超配及全链路可观测性等企业级能力，推出企业核心产品，进一步推动开源项目的企业赋能及商业化变现。&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;&lt;br&gt;&lt;/div&gt;&lt;div data-docx-has-block-data="true" data-lark-html-role="root" data-page-id="VNqkdwPOpoqsVnxT6QfcrhBGnod"&gt;从技术创新到生态共建，从开源社区到全球客户，密瓜智能身上丰富的开源基因与创新力让它在短短一年间展现出惊人的成长速度，印证了其以开源驱动算力变革的前瞻性。未来，随着推理需求持续深化与应用场景不断拓展，密瓜智能将持续为企业提升算力效率，在通向AGI的道路上注入核心动力。&lt;/div&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，智元提出SOP，让VLA模型在真实世界实现可扩展的在线进化</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 18:11:21 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e017edb5-947a-4abf-ab17-a1d7dab2ed00/1767693873504.png" style="width: 700%;" class="fr-fic fr-dib"&gt;对于电子产品，我们已然习惯了「出厂即巅峰」的设定：开箱的那一刻往往就是性能的顶点，随后的每一天都在折旧。&lt;/p&gt;&lt;p&gt;但对于通用机器人来说，这个设定必须被颠覆。&lt;/p&gt;&lt;p&gt;试想，如果一个在实验室里完成训练的 AI 机器人，一进家门面对光线稍暗的房间或堆满杂物的茶几就大脑宕机，那它就永远只能是一个昂贵的实验品。这正是当前具身智能面临的尴尬真相：我们在互联网知识里训练出了博学的预训练模型，可一旦让它们走进充满未知的物理世界，这些「理论巨人」往往会因为环境变化而束手无策：「懂」很多道理，却依然干不好家务。&lt;/p&gt;&lt;p&gt;通用机器人的出路，绝不应是被困在出厂设置里的「静态标品」，而应当是能在真实部署中、在每一次失败和纠正中持续变强的生命体。&lt;/p&gt;&lt;p&gt;为了实现这一跨越，智元具身研究中心提出了 &lt;strong&gt;SOP（Scalable Online Post-training）框架&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNJ23GOdCQ83XLUvovIkK9viaRGSy57lb6e3ynbwEwCnkic9TDq8F7Tia7A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.2324074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527026" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/4eb3d760-2808-441d-a94f-3aea4fe797ff/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：SOP: A Scalable Online Post-Training System for Vision-Language-Action Models&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;官方博客：https://www.agibot.com/research/sop_zh&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;SOP，即&lt;strong&gt;可扩展在线后训练&lt;/strong&gt;，是一种颠覆性的机器人学习新范式。据了解，&lt;strong&gt;这是业界首次在物理世界的后训练中深度整合了在线、分布式和多任务机制&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;通过 SOP 框架，智元具身研究中心构建了一个「多机平行现实」与「云端集中进化」的闭环，进而打破了机器人认知的时间边界，让智能的进化不再止步于出厂的那一刻。&lt;a href="https://mp.weixin.qq.com/s/3I-zhRIZe6gPk_wR2GklcA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d3573f40-602b-4550-b415-d9c2527fbefe/1767693912163.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;SOP：让机器人实现在真实世界中的分布式持续学习&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在过去几年里，基于互联网海量数据预训练的 VLA（视觉 - 语言 - 动作）模型，虽然赋予了机器人一定的通用泛化能力，但始终面临一个难以逾越的鸿沟：&lt;strong&gt;「懂」不代表「能」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;预训练模型或许「懂」什么是叠衣服，但当它真正面对一件材质松软、光照复杂的真实衣物时，往往会因为&lt;strong&gt;分布偏移&lt;/strong&gt;而束手无策。&lt;/p&gt;&lt;p&gt;为了解决这个问题，传统的做法是&lt;strong&gt;后训练（post-training）&lt;/strong&gt;。但这通常是一条&lt;strong&gt;离线、单机、顺序&lt;/strong&gt;的漫漫长路：采集数据、离线训练、更新模型、再次部署。这种模式下，机器人探索慢、迭代慢，且很容易在学习新任务时遗忘旧能力。&lt;/p&gt;&lt;p&gt;智元具身研究中心提出的 SOP 颠覆了这一陈旧范式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNIAT13cBBwmchqbbSBpEQ5TicUibrsFgO8TFibadAzTe4T5TsCTHL9qgVA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527003" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/09e5f2fb-77d8-4292-bbc1-44b40f309e7a/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;它将 VLA 的后训练从「单机单打独斗」转变为「&lt;strong&gt;在线、集群、并行&lt;/strong&gt;」的集团军作战。形象地说，SOP 构建了一个「&lt;strong&gt;多机平行现实 &amp;rarr; 云端集中学习 &amp;rarr; 模型即时回流&lt;/strong&gt;」的超级闭环。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNfNoykDHTEBveWjhO6fHvCS2S9Ex5oXhDMlax25jJ5KmFmhKEUnaDBg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-ratio="0.45732838589981445" data-s="300,640" data-type="gif" data-w="1078" type="block" data-imgfileid="503527004" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/fe832591-58e3-4cdb-81e7-96126c40b07e/640.gif" data-order="0" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;分布式机器人队伍：构建「平行现实」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 SOP 架构下，不再是一台机器人在苦苦探索，而是多台机器人组成集群，共享同一个 VLA 策略。&lt;/p&gt;&lt;p&gt;这就好比在同一时间开启了多个「平行现实」：有的机器人在尝试叠衣服，有的在整理杂货，有的在处理纸盒。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNHZJw8ianCuFBKwVssOmZYwFCH6RMY4uO14q58DzF764qmFibwBO35hgQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="0.546875" data-s="300,640" data-type="gif" data-w="640" type="block" data-imgfileid="503527005" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/6b0f0e48-6a2d-4b5a-b17d-795a6ea0b027/640.gif" data-order="1" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这种空间上的并行，大幅拓宽了真实世界中状态 - 动作分布的覆盖面，让系统能瞬间接触到极其广泛的场景，直接避开了单机学习容易陷入的局部瓶颈。&lt;/p&gt;&lt;p&gt;值得注意的是，&lt;strong&gt;人类还可以通过施加少量的干预性修正来加速学习过程。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNbz9TozgdSkV4uTpJrmylJHMOmsOMVHNlR3vnuGySBb3BAW5ibKLV19g/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-ratio="0.54125" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503527006" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/ea8ab8aa-6a83-4bd1-9357-fff4401cd7b8/640.gif" data-order="2" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;云端集中在线更新：分钟级的进化速度&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;所有的运行轨迹、奖励信号甚至人工纠正信息，都会被实时流式上传至云端 GPU 集群。在这里，一个&lt;strong&gt;通才学习器（Generalist Learner）&lt;/strong&gt;夜以继日地运转，持续对策略模型进行在线更新。&lt;/p&gt;&lt;p&gt;为了支撑这种大规模的实时并发，SOP 在底层架构上搭建了一套&lt;strong&gt;工业级的分布式数据基座&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;系统采用了先进的 &lt;strong&gt;Actor-Learner 分离架构&lt;/strong&gt;，通过消息队列完全解耦了数据生产与消费。这意味着系统具备了「零配置」的&lt;strong&gt;弹性水平扩展能力&lt;/strong&gt;：新的机器人加入集群无需修改任何代码或停机配置，只需连接消息队列即可即插即用，自动分担数据采集任务。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNbTKZMDq2OZavkIEDjSn4cR1dbLUpsUrAmhLgV9S7v9nGUQ6fic1fbAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.28888888888888886" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527007" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/b1bd801b-b523-43bd-abe4-97f3f4a9141a/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;同时，针对物理世界复杂的网络环境，SOP 建立了严苛的容错与数据原子性机制。依靠本地缓冲和对象存储的原子写入特性，确保了即便在网络波动或节点故障时，数据要么完整保存，要么完全回滚，绝不让脏数据污染核心训练池。&lt;/p&gt;&lt;p&gt;为了让学习更高效，SOP 内置了一个聪明的&lt;strong&gt;动态采样器（Adaptive Sampler）&lt;/strong&gt;。它不像传统模型那样盲目混合数据，而是能根据任务的实时训练损失「查漏补缺」，也就是&lt;strong&gt;自动加大对当前薄弱环节的在线数据训练权重&lt;/strong&gt;。这种有的放矢的学习策略，让位于边缘端的机器人能在数秒至数十秒内获得云端最新进化的大脑，真正实现了群体智能的实时同步。&lt;/p&gt;&lt;p&gt;这意味着，如果一台机器人在北京学会了某个抓取动作的微调，几分钟后，位于上海的另一台机器人就能用上这套最新的记忆。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;破解灾难性遗忘：泛化与精度的共存&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统的单机在线训练往往面临一个两难：为了精通某项任务（如叠衣服），模型很容易退化成只懂这一件事的专家，丧失了通用的 VLA 能力。&lt;/p&gt;&lt;p&gt;SOP 通过&lt;strong&gt;多任务并行&lt;/strong&gt;巧妙化解了这一矛盾。因为它是在更广阔的分布中同时进行多任务学习，而非按顺序一个个学，从而确保了 VLA 的通用性不会因针对某一任务的性能提升而受损。&lt;/p&gt;&lt;p&gt;下面展示 SOP 的伪代码：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNClicsf9N1AWPdbCZMviad8bm2krkab1SiaezC0z5p0yD3N1UzwgdJ2G0A/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.7851851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527008" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/f31d729e-6d13-4cad-a8d3-08abd02f5a68/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;有效性验证：从鲁棒性涌现到具身智能的 Scaling Law&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了验证 SOP 的有效性，智元具身研究中心团队思考了三个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SOP 对于预训练 VLA 的性能究竟有多大的提升？跟之前的一些离线方案相比呢？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;分布式机器人队伍的数量规模扩展会如何影响性能？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于不同质量的预训练模型，SOP 能否提供一致的性能增益？&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了解答这三个问题，智元具身研究中心基于自家的&lt;strong&gt;智元精灵 G1（Agibot G1）&lt;/strong&gt;机器人平台进行了实验验证。这是一款拥有双臂 14 个自由度的移动操纵机器人，其头顶与手腕配备的「三目」RGB 视觉系统，配合 7 自由度的灵活手臂和 30Hz 的高频控制，使其具备了在复杂非结构化环境中执行精细微操的硬件基础。&lt;/p&gt;&lt;p&gt;结果呢？相当亮眼！下面我们将深入挖掘实验数据，你将看到：SOP 的技术可行性不仅得到了验证，更展示了极高的「训练性价比」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;超越离线：不仅是成功率的提升，更是鲁棒性的涌现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;首先看看实验所选的任务 &amp;mdash;&amp;mdash; 可以说极具挑战性：从杂货补货任务中涵盖的 500 多种不同形态商品，到叠衣服任务中涉及的柔软易变形物体，甚至包括协同打开冰柜门等复杂动作。这些场景不仅考验机器人的认知能力，更对操作的鲁棒性提出了严苛要求。&lt;/p&gt;&lt;p&gt;在有效性验证中，团队选择了&lt;strong&gt;&amp;nbsp;HG-DAgger（典型的单机在线算法）和 RECAP（最新的 SOTA 离线方法）&lt;/strong&gt;作为对比基准。实验设计非常直观：先看基线模型表现，再看经过这些算法打磨后的效果，最后看接入 SOP 框架后的「终极形态」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNTc2xA1JKUmWjEMYao3nPHEEf1ib4kiasGREHeNA45QWEb015ib8I3SQQg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.5" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527009" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/3fbe8df3-5107-4f29-942d-307bc8b98733/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在各类测试场景下，结合 SOP 的在线多机方案全面碾压了传统单机或离线方法。更令人惊喜的细节出现在「叠衣服」和「叠纸盒」这类长序列任务中：SOP 训练出的模型展现了显著的「&lt;strong&gt;恢复行为&lt;/strong&gt;」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNM8PQuC55eCfBSu9f4A6xlHE95soWjkdyFBR4eKu5umlS7NJMSDGODg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=9" data-ratio="0.49875" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503527010" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/3d7164f7-d71a-4ccb-a10b-1cd8864a6794/640.gif" data-order="3" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNe90c6aPKeFibnOL3icaNumUMEruGBibxrwmTXdoDkBZgtM7RHLLYI32HA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=10" data-ratio="0.5" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503527012" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ab35de7b-4f39-4e45-9e7b-4aa230bae728/640.gif" data-order="4" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;这意味着，当机器人在操作中出现细微偏差时，它不再像过去那样直接导致任务失败或中止，而是学会了类似人类的微调动作进行补救。&lt;/p&gt;&lt;p&gt;这种在动态交互中获得的鲁棒性，直接经受住了极限压力的考验：&lt;strong&gt;在叠衣服和组装纸盒的长程评估中，SOP 系统实现了超过 36 小时的连续运行且无性能衰减&lt;/strong&gt;。这种稳定性同时转化为效率的质变，特别是在叠衣服任务中，SOP 将系统的吞吐量直接翻倍，从每小时 21 件提升至 45 件。&lt;/p&gt;&lt;p&gt;以下视频展示了配备了 SOP 的智元精灵 G1 连续 36 小时叠衣服与叠纸盒的视频片段（已加速）：&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gWicXs2As91d0ugEiarVKrDQiaNibkBR0yTmsnc6sXABFOf692qw132V7iba58zHkPKJLgYbVYN3B35n8AQ%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4330074355775782924" data-ratio="1.7777777777777777" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4330074355775782924" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="1920" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4330074355775782924"&gt;&lt;div data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5="" data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;a href="https://mp.weixin.qq.com/s/3I-zhRIZe6gPk_wR2GklcA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/dc271be0-0c45-4db6-84c2-be292ec01ed4/1767694133763.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;具身智能的 Scaling Law：用硬件换时间，效率达到原来 2.4 倍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说单机试验只是「小试牛刀」，那么关于扩展性的实验则回应了工业界最关心问题：堆机器人数量，真的有用吗？&lt;/p&gt;&lt;p&gt;团队设置了单机、双机和四机三种配置。实验结果（见下表）展现了一个清晰的趋势：&lt;strong&gt;随着分布式集群规模的扩大，模型性能呈现出近乎线性的增长&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNsUGjZnOiauxxK3AOn0YVvuiaDrP7sbcAlrRhftl4S0vl5vvicf3TdJNuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.22030981067125646" data-s="300,640" data-type="png" data-w="581" type="block" data-imgfileid="503527013" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/2c55bc9e-8cb6-42fc-b08f-fad94c5603ab/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在严格限制总训练时长为 3 小时的前提下，四机并行学习的最终成功率达到了 &lt;strong&gt;92.5%&lt;/strong&gt;，比单机提升了 12%。更关键的是，SOP 成功将硬件的扩展转化为了学习时长的极致压缩。要达到 80% 的性能基准线，单机苦练需要 174 分钟，而四机战队仅需 72 分钟，&lt;strong&gt;训练速度达到原来的 2.4 倍&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这表明，多机并行采集不仅能防止模型对单机特征的过拟合，也证实了在物理世界中，&lt;strong&gt;通过增加设备数量来加速模型进化的 Scaling Law 是真实有效的。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;突破预训练瓶颈：3 小时实战 &amp;gt; 上百小时数据堆砌&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;最后一组实验揭示了 SOP 在训练成本上的优势。&lt;/p&gt;&lt;p&gt;团队对比了分别使用 20 小时、80 小时和 160 小时数据预训练的模型。数据显示，虽然预训练规模决定了模型的初始能力，但 SOP 给所有不同基础的模型都带来了稳定的提升。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNicDtBjria7u5OPfsDq8ibywqYSDvWy4A5F30YdnSMllibCK5kiaBPrYGjdA/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.6324074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503527015" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/85400766-916b-476b-85c5-87a247c3ad9a/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;关键的对比出现在投入产出比上：当预训练数据从 80 小时增加到 160 小时，巨大的算力和数据投入仅带来了 4% 的性能提升，明显的边际效应递减已经出现。然而，在同样的瓶颈期，&lt;strong&gt;SOP 仅用了 3 小时的在轨经验，就换来了约 30% 的性能提升&lt;/strong&gt;。这一数据有力地证明：部署后的在线学习不是对预训练的简单重复，而是更高维度的优化。&lt;/p&gt;&lt;p&gt;但也需要指出，SOP 并非万能药。实验发现，&lt;strong&gt;最终的性能上限依然被预训练模型的初始规模所锚定&lt;/strong&gt;。这表明在线学习本质上是既有知识的超级优化器，而非大规模预训练的完全替代品。&lt;/p&gt;&lt;p&gt;因此，对于追求极致性能的具身智能系统而言，在解决特定长尾问题和弥合「仿真 - 现实」差距时，几小时的真实场景交互，往往比单纯增加几十小时的离线数据更为关键。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;当机器人开始进化&amp;hellip;&amp;hellip;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当我们重新审视 SOP 时，会发现它改变的不仅仅是某一项具体的训练技巧，而是整个通用机器人系统的生命周期。在传统的工业逻辑中，产品交付即意味着研发的终点，但在具身智能时代，这个逻辑正被反转。&lt;/p&gt;&lt;p&gt;智元具身研究中心通过 SOP 传达了一个核心理念：&lt;strong&gt;通用机器人应当是一个在真实运行中持续进化的「生命体」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种范式转变意味着机器人可以带着尚不完美的初始模型上线。对于产业而言，这极大地降低了落地的门槛：我们不再需要等到模型完美无缺才敢让机器人走出实验室，因为&lt;strong&gt;部署就是通往完美之路&lt;/strong&gt;。SOP 能让机器人的每一次任务执行、每一次失败后的纠正都转化为宝贵的训练数据。部署不再是技术迭代的终点，而是更大规模学习的起点。&lt;/p&gt;&lt;p&gt;随着远征、灵犀、精灵、Q1 等机器人走入真实世界，分布式集群的规模将呈指数级增长，我们也将见证一种前所未见的群体智能增长速度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN2rcrNaeTJgtlg47QuWh2UezgCckgT5WQ6zGzfn9c6emiawfe4XrFYdg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=13" data-ratio="0.5614285714285714" data-s="300,640" data-type="gif" data-w="700" type="block" data-imgfileid="503527048" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/de4dd2cb-164f-45f6-8e25-3acd8ee38959/640.gif" data-order="5" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;如果说 VLA 模型让机器人第一次具备了通用的理解与行动能力，那么 SOP 所做的是让众多机器人的经验共同驱动智能的快速成长。它让训练不再被锁死在过去的数据集中，而是让机器智能在每一次交互中不断成长。这或许就是通用机器人走向大规模真实世界部署的关键一步。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>独家解读｜2025年AI五大趋势与底层数据革命</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 18:02:24 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/15c39015-80af-413d-b5bd-01d327a53689/1767693480979.png" style="width: 700%;" class="fr-fic fr-dib"&gt;2025 年，人工智能的发展重心正在发生一次根本性转移：从追求模型的规模，转向构建其理解与解决复杂现实问题的能力。在这一转型中，高质量数据正成为定义 AI 能力的新基石。作为人工智能数据服务的前沿探索者，数据堂深度参与并支撑着这场变革的每一个关键环节。本文将深入解读 2025 年 AI 五大技术趋势及其背后的数据需求变革。&lt;/p&gt;&lt;p&gt;&lt;img data-aistatus="1" data-imgfileid="503526739" data-ratio="0.562037037037037" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibjOGZCP2wb1mhrJrYiaZ4KpWlcxlafr8UCjupOXPy5uVWgwfCSYJ1P1A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/94c8ea6a-e606-401c-bfa2-57e7e6963912/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势一：多语种 TTS 与全双工交互 &amp;nbsp;「人情味」与「实时性」革命&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势解码：追求更细腻的情感与更自然的实时互动&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当前，语音合成技术已超越追求「清晰准确」的基础阶段，正同时向两个深度智能化维度演进：一是为合成语音注入情感、个性与文化适配性，让虚拟助手、数字人、有声内容更具感染力和亲和力；二是从单向反应升级为支持实时打断、重叠对话与上下文连贯的全双工自然交互，这已成为高端智能座舱、实时翻译、拟真客服等前沿场景的刚需。技术的核心挑战在于，让 AI 不仅能「读」出文字，更能「理解」语境与情绪，并像真人一样实时聆听、思考与回应，实现有情感、有逻辑的连续对话。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据需求跃迁：从「清晰样本」到「生动语料」与「交互流」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;训练数据的重心正经历双重跃迁。一方面，需构建服务于音色、韵律、情感和风格精细控制的「表现力语料库」，包括覆盖多语种、多方言、多年龄层的音色基底，以及蕴含欢笑、叹息等副语言特征的语音样本。另一方面，为实现全双工交互，迫切需要多通道、真实、带有自然打断与话题转换的对话语音数据，以及对应的精确文本转录与对话状态标注，以训练模型理解对话逻辑、管理话轮并生成即时、恰当的语音响应。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibvibVOkMA6rv4PCv4ibobxlmNVKexkU1cpSglDt4TiauOibUKgLKghkXzUA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562037037037037" data-type="png" data-w="1080" data-imgfileid="503526740" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f00952b8-9ae9-4f9b-99ae-5d43b2d9031e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;为高效赋能下一代语音交互模型，数据堂提供从标准化成品数据集到深度定制服务的完整方案。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据堂提供可直接用于模型训练的成熟数据集。&lt;/strong&gt;核心数据资产包括：为高自然度合成准备的 100 万小时多语种自然对话语音数据集与 300 万条前端文本库；为情感合成优化的 2000 小时多情感普通话合成数据集；以及为训练实时交互模型关键的 1 万小时全双工多语种自然对话数据集。这些高质量数据资产，为客户模型的快速启动与效果优化提供了坚实基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;依托覆盖全球 200 + 语种及方言的庞大语音资源网络与专业声优库，数据堂能够为各类定制化项目提供强大支持。&lt;/strong&gt;无论是潮汕语、客家语等特定方言，貂蝉、温柔白月光等特定音色与情感，还是多种场景下的全双工对话交互数据，数据堂均可通过专业的采集标注流程进行高效生产，精准匹配客户独特的模型训练与产品落地需求。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势二：多模态大模型 &amp;nbsp; &amp;nbsp;从「识别」到「认知与推理」的跃迁&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势解码：DeepSeek-OCR 引爆多模态认知热潮&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2025 年，以 DeepSeek-OCR 模型的开源为标志性事件，揭示了多模态大模型发展的核心方向：其价值远不止于文字识别的精度提升，更在于推动 AI 从处理单一模态信息，迈向对图像、文本、表格、图表、GUI 界面等多元信息进行统一理解、关联分析与深度推理的新阶段。其目标是让 AI 能像专家一样，解读混合图文的研究报告、理解软件界面的操作逻辑，或根据一份试卷推理解题步骤。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据需求跃迁：跨模态关联与推理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统针对单一模态的训练数据已无法满足需求。要训练出具备「认知」能力的多模态模型，数据必须能够刻画不同模态元素之间的复杂关联与深层语义逻辑。这要求数据形态朝着跨模态语义对齐、深度结构化与语义图谱化的方向演进：不仅需要标注图像中的文字、界面元素，更需要建立「图表－总结文字」、「试题－解题步骤」、「图标－操作指令」之间的关联，甚至提供围绕整体任务的推理链条描述。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibNbQwGqlg8ZbcOVO8SolRLLa5bPbKLpGUUtDPLvU1zibOjrD2Wc2zIlg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.562037037037037" data-type="png" data-w="1080" data-imgfileid="503526741" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/6da016cb-5aaf-4c94-8886-f8dfb087b573/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据堂提供覆盖多模态认知全链条的高质量数据，支撑客户模型实现从精准感知到深度理解的全面进阶。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;千万级 OCR 数据、百万级 GUI 界面，多领域专业文档等为模型认知世界提供了丰富的「原材料库」。300 万组涵盖动作、场景、建筑等的图文理解数据，直接助力模型学习「看图说话」与语义推理。而 20 万组 OCR 问答及图像视频编辑数据，则瞄准未来交互范式，训练模型理解指令并执行任务，真正推动 AI 从「看懂」走向「会做」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势三：大模型的深度演进 &amp;nbsp; 推理能力与专业精度的提升&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势解读：通用思维的「升维」与垂直领域的「深耕」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当前大模型的发展呈现出两条清晰且并行的路径：一方面，主流研究持续追求更强大的通用推理与复杂常识能力；另一方面，产业应用落地则驱动模型向金融、法律、生物医药等垂直领域深入，追求高度的专业精度与可靠性。未来的成功模型，必然是强大的通用智能底座与深度领域知识融合的产物。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据需求跃迁：从「规模优先」到「质量与结构驱动」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;高质量训练数据的需求正高度集中于金融、法律、生物医药及科学研究等知识密度高、容错率低的专业领域。其核心已转变为获取能直接赋能模型专业推理与精准判断能力的关键数据资产，主要包括三大类：揭示复杂逻辑链条的「过程型数据」、经领域专家深度校验的「精标知识数据」，以及用于校准专业判断的「对齐与偏好数据」。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaib2PYkZvSSPLzoyVby6LPYf3qofRJl4As5NVFRcib1HvMm5UPYQkiclPSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.562962962962963" data-type="png" data-w="1080" data-imgfileid="503526742" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/0ca45c39-1bdf-4c9b-b8ea-cd716db871e4/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;为应对大模型从通用智能迈向垂直领域深化的双轨需求，数据堂提供从标准化数据产品到深度定制服务的完整解决方案，以高质量数据驱动模型能力的精准进化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;基于大规模、高质量的成品数据集，数据堂为不同训练阶段的模型提供可直接部署的「标准燃料」。&lt;/strong&gt;包括 5000 万条新闻文本、3 亿条 STEM 试题等为预训练奠基的高质量无监督数据，以及 70 万组指令微调与 150 万条安全内容等为指令对齐提供关键支撑的 SFT 指令微调数据，确保模型获得广泛且专业的知识基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据堂组建了覆盖金融、医疗、法律、教育、电力、稀土工业等十余个领域的超 500 人专家团队&lt;/strong&gt;，所有成员均具备专业资质与大模型项目经验，已成功支持超 100 个大模型数据项目，能够高效交付高准确率、强场景适配的专业数据，助力模型实现从「通用智能」到「领域专家」的精准跃迁。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势四：具身智能 &amp;nbsp; AI 加速从数字世界迈向物理世界&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势解码：从「纸上谈兵」到「动手实践」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身智能成为 2025 年焦点，源于对 AI 本质缺陷的突破：传统大模型在纯数字环境中训练，缺乏物理交互经验，无法建立真实世界的因果认知。人类婴儿通过抓握、推拉等身体交互才能构建物理知觉。同样，机械臂面对杂乱抽屉时，仅靠视觉无法判断「能否伸手进入缝隙」，因为空间可感性取决于材质形变、摩擦系数等连续物理变量，必须通过实时交互感知。赋予 AI 物理载体，已成为突破认知天花板的必然选择。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据需求跃迁：构建物理交互的闭环数据&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身智能的核心在于让 AI 通过数据习得物理世界的因果规律，这需要严格对齐时序的高维交互数据，其必须完整融合多视角视频、高精度力 / 触觉传感器流、动作指令序列及最终任务结果，以构成「感知－决策－行动－结果」的完整因果链。&lt;/p&gt;&lt;p&gt;当前，这类高质量数据的获取主要通过真机物理采集、高保真仿真环境生成以及人类行为视频记录等方式实现。然而，真实物理世界的交互数据获取成本极高，往往需要构建专业的采集环境及团队，在严格的安全约束下进行，这导致了能够直接驱动模型进化的高质量数据依然极度稀缺。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibM9Z7lCUZVibBq0uI2dEIpYOohZp3oyFeuDJhNxU7Qz9RV685ZY1bkrg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.562037037037037" data-type="png" data-w="1080" data-imgfileid="503526743" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/6fc43ea6-74d8-44e2-8569-1a0d54ba1315/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;为高效支持具身智能的研发，&lt;strong&gt;数据堂提供从标准化数据集到深度定制采集的完整服务&lt;/strong&gt;。目前已构建数亿组 3D 环境数据、第一人称任务视频、机器人抓取数据集等在内的完整体系，覆盖从环境理解、决策规划到动作执行的全链路，为模型提供高质量的训练起点。&lt;/p&gt;&lt;p&gt;此外，&lt;strong&gt;数据堂在中、美、日、韩、德等全球布局超过 20 个专业采集场，单个面积最大超 4000 平方米，部署有包括人形机器人&lt;/strong&gt;、机械臂、机械狗在内的 70 余台各品牌机器人，可在家居、工厂、商超等多样场景中，执行物体抓取、导航避障、人机交互等复杂任务。采集过程遵循严格的运动平稳性、操作成功率等质量规范，并同步输出多模态传感器数据。&lt;/p&gt;&lt;p&gt;同时，&lt;strong&gt;数据堂专业标注平台与团队能够完成从感知数据的目标检测、分割，视频分割，任务描述，COT 等全类型标注任务&lt;/strong&gt;，确保数据能直接用于算法迭代。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势五：自动驾驶的技术范式转移 &amp;nbsp; 从模块化到端到端&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;趋势解码：自动驾驶 VLA：从「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-indent: 0px;text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;割裂模块&lt;/span&gt;」到&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-indent: 0px;text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-indent: 0px;text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;统一认知&lt;/span&gt;&lt;/strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-indent: 0px;text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;」&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;2025 年，自动驾驶系统正经历一场深刻的技术范式变革。核心架构正从传统的 「感知－规划－控制」模块化设计，向数据驱动的「端到端」一体化模型演进。这一转变的本质，是将驾驶任务视为一个整体，让单一模型直接从传感器输入（如图像、激光雷达点云）映射到控制输出（如方向盘转角、油门），从而避免了模块化架构中固有的信息损失、误差累积与系统复杂性问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据需求：从「感知信号」到「因果阐释」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;以特斯拉 FSD v12 为代表的经典端到端方法，核心在于获取海量真实驾驶视频与同步车辆控制信号。这类数据需求侧重于对「老司机」驾驶行为的模仿，依赖影子模式积累海量，尤其是覆盖边缘场景的未标注或轻标注数据，本质是以数据驱动的行为克隆。&lt;/p&gt;&lt;p&gt;而新一代的 VLM/VLA 多模态大模型路径则提出了颠覆性需求。其目标不仅是控制车辆，更要让模型具备推理、解释与人机交互能力。因此，训练数据必须实现视觉（图像 / 视频）、语言（指令 / 描述 / 问答）与行动（控制信号）三者在时序上的精细对齐与深度耦合。这催生了对高质量、强逻辑的标注数据的极度依赖，例如为视频中的每个决策匹配「为何如此驾驶」的语言解释，其复杂度和标注成本远超以往。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaiboFV9s12B9xaH9icCVdq5UISrk0iaRx5zwGYJziafDovXmPnyZ7Ih7TLLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.562962962962963" data-type="png" data-w="1080" data-imgfileid="503526744" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/eb00e860-76f6-4bb2-8536-4d372af9a3b6/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;面对端到端驾驶模型对复杂逻辑标注的海量需求，数据堂的解决方案聚焦于专业标注实力与规模化交付的核心优势。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据堂能够对驾驶场景同步执行端到端的精确坐标标注与粗粒度的语义说明标注，并融合场景描述、决策依据、反思过程等深度逻辑，构建「感知－决策」闭环的训练数据对。&lt;/strong&gt;这一高质量产出得益于自研平台集成的预识别接口、自动化工具以及严格的一致性培训体系。&lt;/p&gt;&lt;p&gt;基于高效的标注工具及成熟的流程管理，数据堂具备稳定的规模化标注产能，可高效处理长时序驾驶视频流，其中&lt;strong&gt;车辆路线判断与行驶意图等关键任务的量产交付能力均达到每月 40 万组&lt;/strong&gt;，持续为客户的端到端模型从「行为模仿」到「因果理解」的进化提供可靠数据支撑。&lt;/p&gt;&lt;p&gt;2025 年人工智能的深入发展，其效能瓶颈与差异化优势，将日益取决于高质量、专业化、场景化数据的获取与构建能力。数据堂始终站在这一变革的前沿，从前沿趋势研判，到定制化采集方案设计，再到严格的质控体系，致力于为每一波技术浪潮构建坚实、精准、可扩展的数据基础设施。&lt;/p&gt;&lt;p&gt;欲了解更多数据服务，敬请关注数据堂公众平台：&lt;a href="https://mp.weixin.qq.com/s/IoP38i_T_fv_P94d_kjHFg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/da8e3110-38a7-4dea-afb6-1c1c3243f4a5/1767693459051.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>开源1万小时具身智能数据，这家公司是为了什么？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 17:56:16 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0a984921-a6ba-4869-989d-ea9b3dacbd3b/1767693107422.png" style="width: 700%;" class="fr-fic fr-dib"&gt;想象一下，你正在训练一个未来的家庭机器人。你希望它能像人一样，轻松地叠好一件衬衫，整理杂乱的桌面，甚至系好一双鞋的鞋带。但最大的瓶颈是什么？不是算法，不是硬件，而是数据 &amp;mdash;&amp;mdash; 海量的、来自真实世界的、双手协同的、长程的、多模态的高质量数据。&lt;/p&gt;&lt;p&gt;因此为了整个具身智能探索加速，开源集合成为了大家的共同选择，从谷歌 Open-X Embodiment、智元 AgiBot Digital World，到智源 RoboCOIN 与它石智航的 World In Your Hands，都在试图构建更庞大、更完善的数据集合，并开源给到全行业。&lt;/p&gt;&lt;p&gt;但在 1 月 6 日，有一家公司将这件事做到新高度，进行了超过 1 万小时、接近百万 clips 的具身数据集合开放，这是行业最大规模、也是泛化程度最高的开源数据集合，它就是&lt;strong&gt;简智机器人的 &amp;ldquo;10Kh RealOmni-Open DataSet&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526939" data-ratio="0.5564814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNTznkwlYXgOGMHHTEVtdNnbowDkXzGtCcsAxrWC6LeJ8KrNz36ibXn9g/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d48a5d4e-035f-4ac1-9bbe-442fdaa8651f/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;（&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;下载地址为：https://huggingface.co/datasets/genrobot2025/10Kh-RealOmin-OpenData，其他数据正在陆续上传。国内也与阿里魔搭、百度百舸合作，方便国内用户下载。&lt;/span&gt;）&lt;/section&gt;&lt;p&gt;&lt;strong&gt;这批数据集合和之前不同点在哪儿？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;总体规模大，这个&lt;strong&gt;体量甚至已经超越很多具身公司自己所储备的数据&lt;/strong&gt;，而在量大的同时，这个数据集合&lt;strong&gt;还期望它更加&amp;ldquo;实用&amp;rdquo;&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;首先需要它具备&lt;strong&gt;足够强的 &amp;ldquo;技能深度&amp;rdquo;&lt;/strong&gt;，在简智开源数据集合中，没有选择去发散的扩充技能数量，而是聚焦在 10 个常见家庭任务集合中，从而对应&lt;strong&gt;每一项技能都有超过 1 万 Clips 规模&lt;/strong&gt;的数据覆盖，这使得其不只是总体规模的最大，也是&lt;strong&gt;单个技能的行业最多&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其次是&lt;strong&gt;质量、模态的要求&lt;/strong&gt;，这决定这些数据是否真正能被模型消化理解，而画面的超大 FOV、清晰的画质是基础，保证可以全方位录制到周围的环境和人的操作细节，&lt;strong&gt;简智这次数据集合的像素达到 &amp;ldquo;1600*1296&amp;rdquo;&amp;ldquo;30fps&amp;rdquo; 的水平&lt;/strong&gt;；&lt;/p&gt;&lt;p&gt;在这之上轨迹的精度是数据质量的关键，厘米级的轨迹精度对人来说可能足够精细，但对于机器人来说则需要达到毫米级别，因此简智这次开源数据对比行业，一方面具备了大多数不具备的轨迹信息，同时通过高精度 IMU 硬件和云端重建与还原，进一步&lt;strong&gt;将轨迹提升到亚厘米级别&lt;/strong&gt;。而在模态上，作为夹抓类的技能采集，夹抓的开合角度、位移也都在集合中包含。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNfRUx8fLdygiaHYLPx3tqNzqQcbiasK3nngEqbaV3etN9hcd3pCliccozw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7212962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526943" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f5822a99-4edb-472c-a277-8692826c14e5/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;而在技能方面，单手在实际场景中可以完成的任务优先，因此难得是在&lt;strong&gt;数据集中，99.2% 都是 &amp;ldquo;双手、长程任务&amp;rdquo;，这也让它变得更落地&lt;/strong&gt; &amp;mdash;&amp;mdash; 以第一批数据为例，平均 clips 长度为 1min37s。这意味着，它记录的不是一张张静态快照，而是从 &amp;ldquo;拿起散乱 T 恤&amp;rdquo; 到 &amp;ldquo;叠放整齐&amp;rdquo; 的完整过程，是动作逻辑与因果的连续学习。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNaSLXKTYUu2Z4x4ibhCviaqvSrEOtG1EGYY39ibNoENQca5uicOavs4SglQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5277777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526944" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/59b02ca8-5e0d-4ad4-b122-23d33d5cce90/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;最后则是在&lt;strong&gt;相同技能下，数据的场景、目标泛化上需要足够丰富，人员的操作要足够自然&lt;/strong&gt;，而非单一场景的重复、动作僵硬重复，这样才能让模型在真实的生活中，应对家庭环境、目标类型千变万化。&lt;strong&gt;简智这批数据来自 3000 个真实的家庭规模采集&lt;/strong&gt;，以叠衣服为例，不同的衣服种类、平铺的位置等多重因素变量都包含在其中，弥补了传统 &amp;ldquo;数采工厂&amp;rdquo; 方案过于单一的问题。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN4K1iakUDCWIj0MjrpQtGdRtLlfbXNtCnWYFz9wKrL3sm2nWr2eliacgw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-ratio="0.5546296296296296" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526946" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/3a755466-72ef-4fd6-a8a3-9a1b5280caa6/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;为什么有底气开源这么大批量数据？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这些大规模、高质量、泛化程度高数据的背后，其实一套完整的 &amp;ldquo;数据生产链条&amp;rdquo;，在这方面简智也有自己的一套方法论，&lt;strong&gt;完成从采集设备到云端平台，再到数据的二次迭代的闭环&lt;/strong&gt;，这也使得简智在 2 个月时间内就积累了近百万小时规模的数据。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNcPUd9XnCnSX9xnZZ0W9OrLDiaPldA2eMZ048DZJjicsSq0CgDCqNISlg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.31666666666666665" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526947" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/66dd8c08-eb62-4ede-9111-2b9bf2b1163c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这其中，&lt;strong&gt;Gen DAS Gripper &lt;/strong&gt;是能完成简智规模化采集的首要触点，它相比传统的数据采集、UMI 等方案来看，可以&lt;strong&gt;更容易、快速地部署&lt;/strong&gt;，不需要做任何的场地布置；同时&lt;strong&gt;全栈自研的 ISP 图像处理、CMOS 传感器&lt;/strong&gt;，保证图像高质量、清晰。&lt;/p&gt;&lt;p&gt;同时可以做到基于车规级 IMU、双手设备同步，实现&lt;strong&gt;双手技能的高精度坐标对齐，异构数据时间误差小于 1ms&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在设备端，具备超强压缩能力：将&lt;strong&gt;数据体积压缩至原大小的 2%&lt;/strong&gt;，同时打通在线上传通道，实现分钟级快速上传，大幅提升数据流转效率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Gen Matrix 则是中枢数据平台&lt;/strong&gt;，它将收集后数据进行高精准的轨迹还原、对齐、清洗处理：将众多分散设备数据收集，&lt;strong&gt;超强轨迹还原、环境重建能力，轨迹真值误差小于 1cm&lt;/strong&gt;，并将异构数据进行同步与清洗，保证数据质量，并具备&lt;strong&gt;自动化标注、切片等进阶能力&lt;/strong&gt;，可以高并发处理海量数据源。这在具身行业也是领先的数据平台基建。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Gen ADP（AI Data Pipeline）则是规模化、自动化数据产线&lt;/strong&gt;，它是将 DAS 的数据完成自动化的脉搏。它将标注、加工流程自动化，让高质量数据的产出像流水一样持续、高速，&lt;strong&gt;2h 内完成采集与处理全过程。目前据简智公开信息，已经完成百万小时规模数据累计，并且每天以接近万小时规模增长&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN4Y3BtKU3U9AZVTwIwguE7ItBdo5EqvJAHic0KyAv8xz0l1GhH0bqgLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.46296296296296297" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526949" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/661184dd-2153-4835-a707-bcca61dee190/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;开源是一件需要持续做、加速做的事情&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身智能的未来，建立在高质量数据的基石之上。在今天来看，大家对于数据的格式、规范还尚不成熟，这大大的影响了模型方案的进步速度，因此&lt;strong&gt;开源数据持续、加速推进，能快速填补数据鸿沟、统一技术标准、降低研发门槛、推动生态协同与自主可控&lt;/strong&gt;，最终加速具身智能从实验室走向规模化落地。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10Kh RealOmni-Open DataSet &lt;/strong&gt;的开放，不仅是一份海量数据资源，更是一种通过共享加速创新的可能性。简智团队后续将继续加强数据基建建设，推出更多行业有益的数据、服务，形成 &amp;ldquo;数据共享 &amp;mdash; 模型优化 &amp;mdash; 场景落地 &amp;mdash; 数据反哺&amp;rdquo; 的正向循环。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>别被室内基准高分骗了：大模型是在推理空间，还是在「背答案」？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 17:50:27 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;h1&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/3403b427-92eb-497d-a537-a99b539597d1/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/h1&gt;&lt;p&gt;2025 年，随着李飞飞等学者将 &amp;ldquo;空间智能&amp;rdquo;（Spatial Intelligence）推向聚光灯下，这一领域迅速成为了大模型竞逐的新高地。通用大模型和各类专家模型纷纷在诸多室内空间推理基准上刷新 SOTA，似乎 AI 在训练中已经更好地读懂了三维空间。&lt;/p&gt;&lt;p&gt;然而，这背后存在着隐忧：由于带有准确 3D 标注数据的稀缺，模型训练所用数据（如 ScanNet++、ARKitScenes）往往与测试基准高度同源。这种数据的 &amp;ldquo;近亲繁殖&amp;rdquo; 让我们不得不担忧：&lt;strong&gt;近期模型分数的飙升，究竟是真正习得了空间几何推理能力，还是仅仅因为 &amp;ldquo;看多了&amp;rdquo; 类似的室内数据分布，从而学会了 &amp;ldquo;背答案&amp;rdquo;？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了回答这个问题，&lt;strong&gt;中国科学院大学机器学习与感知实验室联合微软亚洲研究院以及苏黎世联邦理工大学共同发布了&lt;/strong&gt;全新空间智能基准 &lt;strong&gt;OSI-Bench&lt;/strong&gt;，从数据源头出发，基于自采开放世界中带有准确 3D 标注的视频数据，提供了对空间智能真正诊断的能力。由此出发，该工作重新审视了当前大模型的空间能力是否得到了发展。真正的空间智能鸿沟，或许无法在现有数据范式下仅靠简单的微调来填平。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibwYsVFM0W0tXqyqDVzGtAPIffmQ96zlXemrNzqib1d1OfOC4Bq195K7A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.22962962962962963" data-type="png" data-w="1080" data-width="1932" data-height="444" data-imgfileid="503526758" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/85072acf-569d-4c22-a5f8-c76668cf1b99/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;作者：Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2512.19683&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://mingrui-wu.github.io/osi-bench&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;室内场景的局限&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;近年来，空间智能的研究大多聚焦于室内场景。这很大程度上受限于源数据集的匮乏 &amp;mdash;&amp;mdash; 少数可用的室外数据集往往基于自动驾驶视角，与第一人称的行人视角存在本质差异。&lt;/p&gt;&lt;p&gt;这种对室内数据的过度依赖，不仅导致了训练集与测试集的高度同源，更因室内场景过强的&lt;strong&gt;语义先验&lt;/strong&gt;难以公平评估模型的空间感知和推理能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNH4fGMdUCbyApE2k2h7jcf1lPmciafcHyer1ZIqyt3ibHNqGZJK0kw9cg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526885" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4e269026-fd61-448b-b60d-cd80d83fe495/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当我们在室内场景提问时（例如：&amp;ldquo;浴缸和马桶之间相距多远？&amp;rdquo;），模型往往能基于 &amp;ldquo;典型浴室布局&amp;rdquo; 的先验知识做出合理推测。即便关闭视觉输入，模型也能仅从语言信息 &amp;ldquo;盲猜&amp;rdquo; 对部分此类问题。&lt;/p&gt;&lt;p&gt;OSI-Bench 选择的室外开放世界的一个核心优势在于其&lt;strong&gt;复杂性与随机性&lt;/strong&gt;。在这种环境下，语义先验变得微弱。面对 &amp;ldquo;告示牌和遮阳篷之间的距离是多远&amp;rdquo; 这样的问题，模型无法再仅凭语义关联获得正确答案，被迫回归到真正的视觉空间推理上来。这种&lt;strong&gt;对先验知识与视觉空间智能的解耦&lt;/strong&gt;，使得 OSI-Bench 可以评估模型的真实空间能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从数据到问答&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;OSI-Bench 摒弃了从现有数据集二次提取的路径，完全基于由多传感器平台（双目相机、LiDAR、IMU/GPS）采集的&lt;strong&gt;原始视频流&lt;/strong&gt;。这些数据自带精确的 3D 信息，覆盖了公园、步行街、古建筑、校园等丰富多样的开放世界场景。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibiacWYOVkwp66BE2kHCCFNt8POJoYs3LmU718XbwjnEajniaK7uQSDXXA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3277777777777778" data-type="png" data-w="1080" data-width="2212" data-height="726" data-imgfileid="503526760" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b3e6ca95-825a-42e6-9dc2-8a440334046e/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;我们的 Human-in-the-loop 流程从 20 小时的视频素材中生成约 9000 条高质量问答，涵盖 9 种任务。为了系统性评估模型能力，我们将这些任务划分为空间智能的三个层级：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 相对关系&lt;/strong&gt;&amp;nbsp;：针对空间位置的定性判断&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 静态尺度&lt;/strong&gt;&amp;nbsp;：针对静态空间物理量的定量估算&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 动态尺度 &lt;/strong&gt;：引入时间维度的动态物理量估计&lt;/p&gt;&lt;p&gt;&lt;strong&gt;评测结果：我们离空间智能还有多遥远？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 OSI-Bench 上的评测结果表明，当下的开源与闭源 SOTA 多模态大语言模型普遍在这些任务上失败了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN8IeAeNicanWIx1goE1wQ081ED394soLVCVN10WGZeUkU59EKQGCv1pg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7101851851851851" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526886" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7f3562ed-48eb-4905-a2b3-aa2dc8bc8b65/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;尽管 Gemini-2.5-Pro 在一众模型中取得了相对显著的优势，但整体表现仍远低于人类水平。然而，比低分更令人担忧的是，我们目前看到的所谓 &amp;ldquo;空间智能提升&amp;rdquo;，可能只是一场&lt;strong&gt;虚假的繁荣&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNH8jhGMib2lQCvvia130wZIJ99UtEpRcTC4b7GbN3pFxJSRAicKc2LFIrA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.19907407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526888" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/dc0fee4f-a99e-4060-9f67-0b13edaf6547/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;我们为此研究了在 2025 年发布新版本并报告在 VSI-Bench（室内基准）上取得巨大提升的两个模型家族：Qwen-VL 与 InternVL 系列。&lt;/p&gt;&lt;p&gt;这两个系列在加入更多空间数据训练后，其同尺寸新旧版本在 VSI-Bench 上的得分显著上升了&lt;strong&gt;约 24.1 分，性能几乎翻倍&lt;/strong&gt;。然而，这种惊人的增长并未出现在同样考察空间推理的 OSI-Bench 上。&lt;/p&gt;&lt;p&gt;另外，结果显示，在绝对距离任务上，更新后的各尺寸模型在 VSI-Bench 上一致涨点，却在 OSI-Bench 上一致退步。由于两个基准在这一任务上采用的提问模版完全相同（仅场景不同），这提供了直接的证据：模型在室内基准上的分数提升，本质上是对特定场景分布的过拟合，而非真正习得了可泛化的空间智能。&lt;/p&gt;&lt;p&gt;我们正在经历的这场 &amp;ldquo;空间智能刷点狂潮&amp;rdquo;，或许只是空中楼阁。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;语言先验：模型的捷径&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当面对空间任务时，相比于费力地进行视觉几何推理，模型更倾向于走 &amp;ldquo;捷径&amp;rdquo;&amp;mdash;&amp;mdash; 利用语言先验知识，基于平均值进行猜测。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibErNKC6GGfp7bdQeMfD0kyfRibrQS1jeDPQ6UzPvUQnZDWialxwsZnLqg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.4787037037037037" data-type="png" data-w="1080" data-width="1666" data-height="798" data-imgfileid="503526766" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e927d3bc-7050-4e19-9f10-acd69cd7469a/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;为了量化这一现象，我们设计了两组实验。&lt;/p&gt;&lt;p&gt;盲测实验结果显示，模型在有 / 无视觉输入的情况下的得分差距极小，视觉输入并没有被有效地在推理中使用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNj2IYmg0SeFAAZbZDtS90SIZQy2c57Vk31MIarZib5icSyb2BiaPTsczicg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.44074074074074077" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526890" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/8681e8d1-8d77-4d5d-9f0a-f1765549b2bd/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;我们构建了一组包含 &amp;ldquo;正常场景&amp;rdquo; 与 &amp;ldquo;反常场景&amp;rdquo;（物体尺寸被特意调整至违背常理）的合成数据。人类在面对反常场景时，空间判断力并未受太大影响；而模型在语言先验失效、常理不再适用的情况下，性能出现了断崖式下跌。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNoVMa57mNGHCT5kZ2De5AJ7cPScZocquW8UyicKjy93kdGovFRIfQ19g/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.687037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526892" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/24dbe9cf-7497-4a39-8b5f-af139b49ba7f/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;总结与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;OSI-Bench 暴露了现有大模型在空间智能层面与实际应用需求之间的巨大鸿沟，更让我们对当前模型是否真正具备可泛化的空间能力提出了质疑。&lt;/p&gt;&lt;p&gt;我们呼唤一种全新的空间智能范式，相较于 data-driven 的分布拟合，我们需要真正赋予模型在空间中感知、在空间中思考的工具与能力。&lt;/p&gt;&lt;p&gt;OSI-Bench 的基准与评测代码已全部开源。未来，我们将持续开源更多带有高精度 3D 信息的开放世界视频数据，推动空间智能从室内场景走向复杂的开放世界。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>1956-2026：人类与机器智能的七十年对话</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 06 Jan 2026 14:27:56 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1956年夏天，当约翰&amp;middot;麦卡锡（John McCarthy）、马文&amp;middot;明斯基（Marvin Lee Minsky）等先驱在达特茅斯学院首次提出&amp;ldquo;人工智能&amp;rdquo;这个概念时，他们乐观地预言：十年内机器将具备人类级别的推理能力。&lt;img src="https://image.jiqizhixin.com/uploads/editor/42d998f1-b637-414c-9af3-bb37eba164e6/%E5%9B%BE%E7%89%871.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;七十年过去了，这个预言虽未完全实现，但AI的演进轨迹却远比当初设想的更加波澜壮阔&amp;mdash;&amp;mdash;从符号推理的黄金时代到&amp;ldquo;AI寒冬&amp;rdquo;的沉寂，从机器学习的复兴到深度学习的爆发，再到2026年AI全面融入产业基础设施的当下。&lt;/p&gt;&lt;p&gt;这七十年的历史揭示了一个关键规律：AI的每一次突破，都源于思想碰撞、跨界融合与全球协作。1997年深蓝战胜卡斯帕罗夫，标志着暴力计算与精妙算法的混合突破；2010年后深度学习革命的爆发，则得益于大数据、GPU算力与神经网络架构创新的三重汇聚。而站在2026年这个新起点，当生成式AI、AI4S、具身智能等前沿趋势加速涌现，&lt;strong&gt;当国际竞合格局重塑全球创新生态，我们比以往任何时候都更需要一个能够汇聚顶级思想、链接全球资源、激发跨界创新的高浓度平台。&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&amp;ldquo;那么，东方为这场持续七十年的对话，按下了哪些关键按钮？&amp;rdquo;答案写在黄浦江畔。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;01&lt;/strong&gt;&lt;strong&gt;/&lt;/strong&gt;&lt;strong&gt;七十年的回响：浦江答卷&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上海的AI实践为这场全球对话提供了丰富的东方注脚。&lt;/p&gt;&lt;p&gt;这座城市不仅培育了像MiniMax、阶跃星辰这样的基础大模型先锋，在垂直领域，联影医疗将AI融入医疗影像诊断，松鼠AI打造个性化教育系统，小i机器人深耕政务智能化。&lt;/p&gt;&lt;p&gt;消费级市场则涌现出珞博智能与华为联合打造的&amp;ldquo;智能憨憨&amp;rdquo;情感陪伴硬件，探索人与AI之间超越工具性的&amp;ldquo;养成&amp;rdquo;关系；XREAL与Google合作深耕轻量级AR生态，推动AR设备从显示工具向日常化的空间计算终端演进。&lt;img src="https://image.jiqizhixin.com/uploads/editor/e4eee54c-6585-4ceb-92af-3be1aa417371/%E5%9B%BE%E7%89%872.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;更令人瞩目的是，沪产机器人&amp;ldquo;智元&amp;rdquo;打破了&amp;ldquo;人形机器人行走最远距离&amp;rdquo;的吉尼斯世界纪录，标志着中国在具身智能领域的硬核突破。这些看似分散的创新成果，共同勾勒出上海作为全球AI重要节点的完整生态图景。&lt;/p&gt;&lt;p&gt;然而，在AI这场高度依赖算力、数据与资本的长期竞赛中，任何单一创新节点都面临着资源整合与全球链接的挑战。特别是对于寻求国际化发展的AI企业而言，如何高效对接全球市场、资本与人才，成为必须跨越的门槛。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;02&lt;/strong&gt;&lt;strong&gt;/&lt;/strong&gt;&lt;strong&gt;七十年的回响：双城新篇章&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当上海的AI产业积累需要更广阔的国际化舞台时，中国香港以其独特的&amp;ldquo;超级联系人&amp;rdquo;角色进入了视野。&lt;/p&gt;&lt;p&gt;这座城市正在迅速崛起为亚洲AI枢纽，目前已汇聚约500个AI相关组织、290家AI企业及180家投资机构，形成了高密度的创新生态。&lt;/p&gt;&lt;p&gt;香港的优势远不止于数字。其资本市场在2025年以2860亿港元的IPO募资额位居全球第一，成为科技与AI企业上市的首选地之一。与此同时，香港政府宣布投入30亿港元设立人工智能专项资助计划，建设AI研发院与超算中心。&lt;/p&gt;&lt;p&gt;一边是扎实的产业&amp;ldquo;底座&amp;rdquo;，一边是强大的国际&amp;ldquo;接口&amp;rdquo;，两者的历史性握手，只差一个契机。这个契机，随着维多利亚港的海风如期而至。2026年AI领域首场高浓度思想盛宴&lt;strong&gt;&amp;ldquo;WAIC UP!全球年终盛会&amp;rdquo;&lt;/strong&gt;来到香港。这不仅是世界人工智能大会（WAIC）首次在港举办年度会议，更是上海AI产业实践与香港国际枢纽功能的一次历史性握手。&lt;/p&gt;&lt;p&gt;会议汇聚了WAIC旗下五大生态品牌&amp;mdash;&amp;mdash;&lt;strong&gt;创新孵化引擎WAIC Future Tech、产业对接枢纽WAIC CONNECT、思想启迪窗口WAIC UP!、青年科教阵地WAIC Young以及全球合作舞台AI GRAVITY&lt;/strong&gt;。这种多维度、立体化的平台设计，确保了不同背景的参会者都能找到自己的价值定位。&lt;img src="https://image.jiqizhixin.com/uploads/editor/67fb4174-ffff-4618-9006-e159c99e4efe/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;对于上海及内地的AI创业者而言，&lt;strong&gt;这场盛会提供的不仅是展示舞台，更铺设了一张更广阔的全球AI协作网络。&lt;/strong&gt;从港交所的上市通道，到香港投资推广署的落地支持，从科学园的研发设施，到数码港的孵化生态，这些以往需要数月才能打通的环节，如今在一天之内就能建立初步联系。然而，链接资源只是第一步。在范式转换的临界点上，比资源更稀缺的是洞察未来的&amp;ldquo;思想地图&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;03&lt;/strong&gt;&lt;strong&gt;/&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;WAIC UP!连接下一个七十年&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;国际级讲者阵容带来的历史纵深与前沿视野&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如今，我们正站在另一个范式转换的临界点：从单模态到多模态，从云端到边缘，从工具到伙伴&amp;hellip;&amp;hellip;这些AI趋势的把握，不能仅靠闭门研发，更需要站在巨人肩膀上的思想启迪。上午场&lt;strong&gt;&amp;ldquo;WAKE思想觉醒&amp;rdquo;&lt;/strong&gt;正是为此而设。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;皮埃罗&amp;middot;斯加鲁菲（Piero Scaruffi）&lt;/strong&gt;，作为硅谷人工智能研究院院长与硅谷精神布道师，见证了从专家系统兴衰到深度学习爆发的完整周期。他对AI演进周期性规律的洞察，将帮助参会者避免重蹈历史覆辙，在泡沫与实质之间保持清醒判断。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;史蒂夫&amp;middot;霍夫曼（Steve Hoffman）&lt;/strong&gt;，作为Founders Space创始人与硅谷创投教父，孵化过数百家AI创业公司。他将分享从实验室到市场的转化密码，揭示哪些技术趋势真正具备商业化潜力&amp;mdash;&amp;mdash;这正是避免1980年代专家系统式&amp;ldquo;虚假繁荣&amp;rdquo;的关键。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;朱晓波&lt;/strong&gt;教授，作为&amp;ldquo;祖冲之号&amp;rdquo;量子计算总师，代表着AI算力革命的下一个前沿。量子计算与AI的结合，可能重现2010年GPU为深度学习带来的颠覆性加速，这是理解未来十年AI发展的战略制高点。&lt;/li&gt;&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如AI历史所示，1997年深蓝的胜利不仅是技术突破，更重塑了人们对&amp;ldquo;智能&amp;rdquo;的理解；2023年ChatGPT的爆发不仅是产品成功，更引发了对AGI路径的全球讨论。这些思想领袖的价值不仅在于传递信息，更在于提供思维框架；参会者将获得的不是碎片化的技术细节，而是构建AI时代世界观的思想基石。&lt;img src="https://image.jiqizhixin.com/uploads/editor/7fee2783-b46d-49c6-932e-2d978070e15d/%E5%9B%BE%E7%89%874.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;产业全链条的立体化资源网络&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;低代码/无代码AI平台的普及，使得非技术专家也能构建智能应用。但真正的挑战在于：如何将技术能力转化为产业价值？如何在垂直领域找到AI的最佳应用场景？&lt;strong&gt;下午场&amp;ldquo;UP拓维跃迁&amp;rdquo;&lt;/strong&gt;精准回应这一需求，通过垂直应用案例、商业实战与出海战略的三维透视，构建从技术到商业的完整闭环。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;技术供给侧：&lt;/strong&gt;商汤科技、科大讯飞等头部AI企业展示最新解决方案，从计算机视觉到语音交互，覆盖AI技术的全栈能力。这些企业经历了从研发到规模化部署的完整历程，其经验教训价值千金。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;产业需求侧：&lt;/strong&gt;中国移动国际、神州数码、易鑫集团等传统行业巨头分享数字化转型实践。他们的痛点与需求，正是AI创业者与技术提供商的机遇所在。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;基础设施层：&lt;/strong&gt;算丰等算力提供商、RTE开发者社区等技术生态，构成AI应用的底层支撑。正如深度学习革命依赖GPU算力突破，下一代AI应用同样需要新型基础设施的支撑。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;资本催化剂：&lt;/strong&gt;孚腾资本、Atma Capital等投资机构带来资本视角。他们对赛道的判断、对商业模式的洞察，能够帮助创业者避免方向性错误，加速从0到1的突破。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种产、投、创、研的四维聚合，令参会者可以在一天内完成通常需要数月的生态链接：上午吸收前沿思想，下午对接产业资源，实现从&amp;ldquo;知道&amp;rdquo;到&amp;ldquo;做到&amp;rdquo;的跨越。&lt;img src="https://image.jiqizhixin.com/uploads/editor/8352c942-0222-4b1e-a49e-b44cbe54262b/%E5%9B%BE%E7%89%875.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI时代的&amp;ldquo;达特茅斯时刻&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;达特茅斯会议的真正价值，不仅在于定义了&amp;ldquo;人工智能&amp;rdquo;这个术语，更在于它创造了一个跨学科思想自由碰撞的场域。数学家、工程师、心理学家、语言学家齐聚一堂，在非正式讨论中激发出影响后世七十年的核心概念。&lt;strong&gt;夜晚场&amp;ldquo;MORE灵感迸发&amp;rdquo;&lt;/strong&gt;正是要重现这种魔力。当正式议程结束，当西装革履卸下，当不同代际、不同领域、不同文化背景的参与者在轻松氛围中交流，往往会产生最意想不到的化学反应。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;跨代际对话的独特价值：&lt;/strong&gt;青年科学家带来未被传统范式束缚的新鲜视角；资深专家提供历史纵深与战略判断；初创团队展现颠覆式创新的勇气；企业决策者贡献产业落地的实战智慧。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;跨领域融合的创新源泉：&lt;/strong&gt;夜晚场汇聚的多元群体&amp;mdash;&amp;mdash;从机器人工程师到AGENT开发者，从科技媒体到社区运营者，从BIM国际青年联盟到WAYtoAGI社区&amp;mdash;&amp;mdash;构成了一个思想熔炉。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;历史反复证明，AI的突破往往发生在学科交叉点。一个机器人工程师与一个内容创作者的对话，可能催生下一个爆款AI应用；一个出海企业家与一个国际组织代表的邂逅，可能开启跨境合作的新篇章。这种&amp;ldquo;计划外的收获&amp;rdquo;往往比正式议程更有价值。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;04&lt;/strong&gt;&lt;strong&gt;/人类与未来的永恒对话&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;以1956年达特茅斯会议正式定名&amp;ldquo;人工智能&amp;rdquo;为起点，AI的发展虽然历程尚短，却以远超预期的速度穿越了一个又一个技术关口，从逻辑推理、统计学习，到今天的大模型与多模态系统，智能以持续涌现的方式重塑着现实世界的运行节奏。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;而愈是演进加速的时刻，愈需要重新思考人的位置。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;算法可以更快，模型可以更大，但人的判断、情感与责任，从未可被替代。在碳基生命与硅基智能在文明进化的路口相遇时，我们亟需重新建立认知框架，回望我们想成为什么样的人类。&lt;/p&gt;&lt;p&gt;站在2026年的起点，我们更能体会这场演进的复杂与惊奇，每一次技术跃迁，像是文明的回响。正是在这样的时代拐点上，WAIC作为全球AI的重要思想交流平台，正持续拓展其&amp;ldquo;科技&amp;times;人文&amp;rdquo;的边界，推动议题，凝聚共识。&lt;img src="https://image.jiqizhixin.com/uploads/editor/402a7b98-5165-42fe-bce1-3bb5eecbaf21/%E5%9B%BE%E7%89%8711.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;无论是思想刊物《WAIC UP!》，还是连接创业者、产业方、政策制定者、青年一代的多维平台，WAIC一直以来都承载着一个宏大的命题：我们愿意与AI共同走向怎样的未来。而即将到来的&amp;ldquo;WAIC UP!全球年终盛会&amp;rdquo;，也将为新一年的探索翻开新的篇章。&lt;/p&gt;&lt;p&gt;回顾这七十年，一个有趣的对比是：1956年的达特茅斯会议只有几十位参与者，而今天的WAIC将连接成千上万的全球头脑。规模的变化背后，是AI从学术课题到文明议题的演进。&lt;/p&gt;&lt;p&gt;我们的使命无比清晰，只要答案未至，步履永远不停。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;即刻锁票，与下一个70年对话&lt;br&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/98786665-43e0-4fd4-934a-c132dbf7ab84/%E5%9B%BE%E7%89%8721.png" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考资料：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;[1] AI养成系潮玩受资本追捧，&amp;ldquo;不看好早期具身智能&amp;rdquo;的朱啸虎也出手了，每日经济新闻，202506.&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;[2] Hong Kong Cements AI Hub Status with 500 Organizations, 23% IPO Surge, and AI-for-Finance Ecosystemic Leadership，香港金融发展局，202511.&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;[3] 打造香港成全球AI重要枢纽，文汇网，202511.&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>当蛋白质组数据走向大规模计算，Frag’n’Flow给出组学流程的部署与扩展问题的解决方案</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Tue, 06 Jan 2026 11:57:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnzAGqfveATlN2NVOoVSExwDQVl9IYJo0CaFXVqftmmmTBwiahF8icqQsj6DloYAAkc0vTPf3nAmQVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5453703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="315" data-imgfileid="100027069" data-aistatus="1" data-original-style="width: 100%;" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/dc826614-da44-41cc-a816-cb8eef42cf40/640.png" data-sec-load-status="2" data-report-img-idx="1" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;编辑丨%&lt;/p&gt;&lt;p&gt;在蛋白质组学分析中，已有不少大模型发挥着它们各自的能力。但基于质谱的大规模复杂数据集常会让桌面计算资源不堪重负，且需要手动配置分析。&lt;/p&gt;&lt;p&gt;FragPipe 是目前应用最广泛的蛋白质组分析平台之一，以速度快、定量准确著称，支持多种采集模式。但即使如此，它也在&amp;nbsp;&lt;strong&gt;HPC（高性能计算）或云环境中的实际部署面临着系统性障碍&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在这一背景下，来自匈牙利任自然科学研究中心（HUN-REN Research Centre for Natural Sciences）与美国纽约大学（NYU）等提出研究团队提出了&amp;nbsp;&lt;strong&gt;Frag&amp;rsquo;n&amp;rsquo;Flow&lt;/strong&gt; &amp;mdash;&amp;mdash;一个专门为 FragPipe 设计的、面向大规模计算环境的自动化工作流。&lt;/p&gt;&lt;p&gt;相关研究内容以「&lt;em&gt;Frag&amp;rsquo;n&amp;rsquo;Flow&lt;/em&gt;:&amp;nbsp;automated workflow for large-scale quantitative proteomics in high performance computing environments」为题，于 2026 年 1 月 4 日发布在《BMC Bioinformatics》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnzAGqfveATlN2NVOoVSExwib0ClMgcT8ujrN2oZjGYg5CmOkuribvibIbwGSvVNIyXLc6Cra8fYNRDA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.31313131313131315" data-type="png" data-w="990" data-width="990" data-height="310" data-backw="546" data-backh="171" data-imgfileid="100027066" data-aistatus="1" data-original-style="width: 100%;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/15fb67e5-ca2b-4791-b5d6-8b0c80f6be55/640.png" alt="图片" data-before-load-time="1767671834150" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：&lt;em&gt;https://link.springer.com/article/10.1186/s12859-025-06305-y&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;拆解化的工作流系统&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在动辄几十几百 GB 的数据规模下，蛋白质组学分析所面临的问题往往不再是「能不能算出结果」，而是谁有能力把完整流程稳定地跑完。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnzAGqfveATlN2NVOoVSExwBLe1tKVbzYVIhxJv2nIicWHhOGrQ2qv7bz0Ng93icHxKrEOiaqN83Tu7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.48349514563106794" data-type="png" data-w="1030" data-width="1030" data-height="498" data-backw="546" data-backh="264" data-imgfileid="100027065" data-aistatus="1" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d95396e0-bfa4-4f6f-928f-0fe5c30a623e/640.png" alt="图片" data-before-load-time="1767671834330" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 1：Frag&amp;rsquo;n&amp;rsquo;Flow 流程概述。&lt;/p&gt;&lt;p&gt;研究团队选择 Nextflow 作为工作流编排框架，将 FragPipe 及其关键组件（MSFragger、IonQuant、diaTracer 等）封装进一个可移植、可复现的流程中。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;自动生成分析清单与流程配置&lt;/strong&gt;，避免手工编写 manifest 文件；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;通过容器化管理依赖&lt;/strong&gt;，在 HPC、云平台和本地集群间保持一致行为，集成相关数据库；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;支持 DDA、DIA 与 TMT 三类主流定量策略，&lt;/strong&gt;选择合适的工作流；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;配置工具下载模块，并内置下游统计分析模块&lt;/strong&gt;，直接输出差异蛋白与通路结果；&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在四个子模块就位后，、必要的输入文件、配置工具和环境设置会自动准备就绪，凭此无缝启动。因此，Frag&amp;rsquo;n&amp;rsquo;Flow 为高性能计算或云环境中部署提供了完全自动化的设置，最大限度地减少了人工干预，并确保了多次运行的一致性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;性能验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;实验验证部分专注于回答一个核心问题：在不牺牲定量准确性的前提下，Frag&amp;rsquo;n&amp;rsquo;Flow 是否真的提升了大规模分析效率？&lt;/p&gt;&lt;p&gt;因此，团队选择使用 quantms 来对 Frag&amp;rsquo;n&amp;rsquo;Flow的性能和输出结果进行基准测试。这两个工作流均使用公开的原始 DIA 数据集，采纳默认参数和标准的SLURM工作负载管理器。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnzAGqfveATlN2NVOoVSExwKomF47dV0sFlAIKGdiaOcT4qDMYdkJNQThS2uRpsxHibBHxd8T8icJaTw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.1986970684039089" data-type="png" data-w="614" data-width="614" data-height="736" data-backw="546" data-backh="654" data-imgfileid="100027067" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/703e434c-23a6-451f-b2d3-1cbef4b01a63/640.png" alt="图片" data-before-load-time="1767671834578" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 2：与竞争流程相比，Frag&amp;rsquo;n&amp;rsquo;Flow 具有高度一致性且运行时间更快。&lt;/p&gt;&lt;p&gt;该数据集大小大概在 58 GB，而处在这种数据规模下的 Frag&amp;rsquo;n&amp;rsquo;Flow 仍保持了定量准确性，且将运行时间几乎缩短了一半，同时缓解了内存和输入/输出瓶颈。团队在三个代表性数据集（无标记DDA、DIA和TMT）上验证了Frag&amp;rsquo;n&amp;rsquo;Flow 的结果，仅需极少的用户干预就能成功重现已发表的生物学特征。&lt;/p&gt;&lt;p&gt;通过整合 FragPipe-Analyst 的 R 实现版本，Frag&amp;rsquo;n&amp;rsquo;Flow 在主流程结束后自动完成数据归一化与缺失值处理；基于 limma 的差异表达分析；PCA、火山图、相关性热图等质量控制图；通路富集分析（Hallmark / KEGG）。这些结果以 CSV 与单一 PDF 报告形式输出，显著降低了分析门槛。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;工程问题的系统解法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过 Nextflow 的编排能力与 FragPipe 的高灵敏度结合，Frag&amp;rsquo;n&amp;rsquo;Flow 的研发团队将桌面导向的 FragPipe 转化为 HPC 适配的规模化工具，实现了从原始数据到生物学解释的端到端自动化分析。&lt;/p&gt;&lt;p&gt;该工作流程兼顾定量准确性与分析效率、支持多平台部署、遵循 FAIR 原则、操作门槛低，无需复杂手动配置。这是一种可扩展、可复现、对用户友好的实现路径，使大规模定量蛋白质组分析不再依赖复杂的人工配置。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>黄仁勋CES放出大杀器：下一代Rubin架构推理成本降10倍</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 10:29:12 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/cba512b8-400a-478d-a619-34375fdca2e9/1767666256601.png" style="width: 700%;" class="fr-fic fr-dib"&gt;「每隔 10 到 15 年，计算行业就会革新一次，每次都会催生出新形态的平台。现在，有两个转变在同时进行：应用将会构建于 AI 之上，你构建软件的方式也将改变。」&lt;/p&gt;&lt;p&gt;就在今天凌晨，在拉斯维加斯 CES 2026 展会现场，英伟达创始人黄仁勋身穿经典皮衣现身！&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNbsygCawhXjoy7wtjSWMh99k7yN8eFoyoNSjwuAWhp3EPksJVbdOE1g/640?wx_fmt=gif&amp;from=appmsg#imgIndex=1" data-ratio="0.5833333333333334" data-s="300,640" data-type="gif" data-w="600" type="block" data-imgfileid="503526871" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/cab191c0-9eab-4c0a-a455-f7a8e8e64f00/640.gif" data-order="0" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;黄仁勋展示的第一张幻灯片是：「人工智能的发展超越了大型语言模型。」&lt;/p&gt;&lt;p&gt;随着大语言模型技术的进步，未来的物理世界 AI 将可以理解真实世界的结构，独立完成任务，并随着时间的推移进行学习。他表示，「宇宙中任何存在信息、任何存在结构的地方」都可以用来训练人工智能。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNsjYLSpibnwTAmc6QP4YsiaN51NgibM7vQ6RtEd8q8f5ED3G8R6pzibWd7w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5472222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526872" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d712ff59-c896-4c50-9159-e6be0f698337/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;老黄分享了下一代加速计算与人工智能将如何变革每一个行业，并一一介绍了英伟达在芯片、人工智能模型、开源开放等领域的最新进展，主要包括如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;下一代 Rubin 平台；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;全新的视觉 - 语言 - 动作模型（VLA）&amp;mdash;&amp;mdash;Alpamayo 1；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;面向物理 AI 的新开放模型、框架和 AI 基础设施。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不仅包括新一代 GPU，也有引领业界的开源 AI 模型。可见到了 2026 年，英伟达正准备以全栈的形式引领技术发展。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rubin 平台问世 &amp;mdash;&amp;mdash; 六款全新芯片，一台划时代 AI 超算&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNPha14khESe4Xc7JrV0wHMeSfJib9u4ffWOjYBdRkNw6Fe4a9FPWicukg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" data-ratio="0.562962962962963" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526873" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8fdc2f2f-e9de-4b9e-8e19-c51d28603f08/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;首先，最引人关注的是下一代计算架构 &amp;mdash;&amp;mdash;NVIDIA Rubin 平台，刚刚推出的六款全新芯片，目标是构建一台在成本、性能与安全性上全面领先的 AI 超级计算机，加速 AI 在主流场景中的落地。&lt;/p&gt;&lt;p&gt;这六款芯片包括：NVIDIA Vera CPU、NVIDIA Rubin GPU、NVIDIA NVLink 6 Switch、NVIDIA ConnectX-9 SuperNIC、NVIDIA BlueField-4 DPU 和 NVIDIA Spectrum-6 Ethernet Switch，极致的协同设计，将大幅缩短训练时间，降低推理 Token 成本。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN43HibCI7IlticoXWDZF6P7xLJ1cNVsyatxMibj5ELhYAUqyF4RPGX3aZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.6898148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526874" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5712c4f8-a4b5-4daf-a218-404242f83f7b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;「Rubin 的到来恰逢其时，因为训练和推理的 AI 计算需求正在激增，」黄仁勋表示，「我们以每年一代 AI 超级计算机的节奏持续前进，而 Rubin 通过六款全新芯片的极致协同设计，向 AI 的下一个前沿迈出了关键一步。」&lt;/p&gt;&lt;p&gt;据了解，Rubin 平台以美国天文学家 Vera Florence Cooper Rubin 命名，她的研究彻底改变了人类对宇宙的认知。该平台包括 NVIDIA Vera Rubin NVL72 机架级解决方案和 NVIDIA HGX Rubin NVL8 系统。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNVMLRFVsNvScA69Olxpe1h0micUFOnGVOLgn3v2bUTiaqsUBTMWqs4GJw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5064814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526875" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/5750f298-f908-449d-971b-f5bd85c854ad/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Rubin 平台引入了五项创新，包括最新一代 NVIDIA NVLink 互连技术、Transformer 引擎、机密计算和 RAS 引擎，以及 NVIDIA Vera CPU。这些突破将加速智能体 AI、高级推理和大规模混合专家（MoE）模型推理，其每 Token 成本比 NVIDIA Blackwell 平台低高达 10 倍。与前代产品相比，NVIDIA Rubin 平台训练 MoE 模型所需的 GPU 数量减少了 4 倍，从而加速了 AI 普及。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;专为扩展智能而生&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;智能体 AI 和推理模型，以及最先进的视频生成工作负载，正在重新定义计算的极限。多步问题解决需要模型在长序列 Token 中处理、推理和行动。旨在满足复杂 AI 工作负载需求的 Rubin 平台包含五项突破性技术：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;第六代 NVIDIA NVLink&lt;/strong&gt;：提供当今大规模 MoE 模型所需的快速、无缝的 GPU 到 GPU 通信。每个 GPU 提供 3.6TB/s 的带宽，而 Vera Rubin NVL72 机架总带宽高达 260TB/s，比整个互联网的带宽还多。凭借用于加速集体操作的内置网内计算，以及用于增强可维护性和弹性的新功能，NVIDIA NVLink 6 switch 可实现更快、更高效的大规模 AI 训练和推理。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;NVIDIA Vera CPU&lt;/strong&gt;：专为智能体推理设计的 NVIDIA Vera 是大型 AI 工厂中最节能的 CPU，采用 88 个英伟达自研 Olympus 核心，完全兼容 Armv9.2，并具有超快的 NVLink-C2C 连接。Vera 提供卓越的性能、带宽和行业领先的效率，可支持全方位的现代数据中心工作负载。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;NVIDIA Rubin GPU&lt;/strong&gt;：配备具有硬件加速自适应压缩的第三代 Transformer 引擎，Rubin GPU 可为 AI 推理提供 50 petaflops 的 NVFP4 计算能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;第三代 NVIDIA 机密计算&lt;/strong&gt;：Vera Rubin NVL72 是首个提供英伟达机密计算的机架级平台，可在 CPU、GPU 和 NVLink 域之间维护数据安全，保护全球最大的专有模型、训练和推理工作负载。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;第二代 RAS 引擎&lt;/strong&gt;：Rubin 平台涵盖 GPU、CPU 和 NVLink，具有实时健康监测、容错和主动维护功能，可最大限度地提高系统生产力。机架的模块化、无线缆设计使组装和维护速度比 Blackwell 快高达 18 倍。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;AI 原生存储和安全、软件定义基础设施&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Rubin 平台引入了 NVIDIA 推理上下文内存存储平台，这是面向千亿级推理上下文规模（gigascale） 设计的新一代 AI 原生存储架构。&lt;/p&gt;&lt;p&gt;该平台由 NVIDIA BlueField-4 驱动，可在 AI 基础设施中实现 KV Cache 数据的高效共享和重用，提高响应能力和吞吐量，同时实现可预测、能效友好的智能体 AI 扩展。&lt;/p&gt;&lt;p&gt;BlueField-4 还引入了高级安全可信资源架构（ASTRA），这是一种系统级信任架构，可为 AI 基础设施构建者提供统一、可信的控制点，以便在不影响性能的情况下安全预置、隔离和操作大规模 AI 环境。&lt;/p&gt;&lt;p&gt;随着 AI 应用向多轮智能体推理发展，AI 原生组织必须在用户、会话和服务之间管理和共享更多推理上下文。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;针对不同工作负载的不同形态&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;NVIDIA Vera Rubin NVL72 提供了一个统一、安全的系统，集成了 72 个 NVIDIA Rubin GPU、36 个 NVIDIA Vera CPU、NVIDIA NVLink 6、NVIDIA ConnectX-9 SuperNIC 和 NVIDIA BlueField-4 DPU。&lt;/p&gt;&lt;p&gt;英伟达还将推出 NVIDIA HGX Rubin NVL8 平台，这是一款服务器主板，可通过 NVLink 连接八个 Rubin GPU，以支持基于 x86 的生成式 AI 平台。HGX Rubin NVL8 平台可加速 AI 和高性能计算工作负载的训练、推理和科学计算。&lt;/p&gt;&lt;p&gt;NVIDIA DGX SuperPOD 可作为大规模部署基于 Rubin 系统时的参考，它集成了 NVIDIA DGX Vera Rubin NVL72 或 DGX Rubin NVL8 系统，并搭配 NVIDIA BlueField-4 DPU、NVIDIA ConnectX-9 SuperNIC、NVIDIA InfiniBand 网络和 NVIDIA Mission Control 软件。&lt;/p&gt;&lt;p&gt;NVIDIA Spectrum-6 以太网是下一代 AI 网络以太网，旨在以更高的效率和更强的弹性扩展基于 Rubin 的 AI 工厂，并由 200G SerDes 通信电路、共封装光学器件和 AI 优化结构提供支持。&lt;/p&gt;&lt;p&gt;基于 Spectrum-6 架构，Spectrum-X 以太网光子共封装光交换系统可为 AI 应用提供 10 倍的可靠性和 5 倍的更长正常运行时间，同时实现 5 倍的更高能效，与传统方法相比，每瓦性能最大化。Spectrum-XGS 以太网技术是 Spectrum-X 以太网平台的一部分，可使相距数百公里甚至更远的设施作为一个统一的 AI 环境运行。&lt;/p&gt;&lt;p&gt;这些创新共同定义了下一代 NVIDIA Spectrum-X 以太网平台，该平台采用与 Rubin 极致协同设计，旨在实现大规模 AI 工厂，并为未来的百万 GPU 环境铺平道路。 Rubin 准备就绪&lt;/p&gt;&lt;p&gt;NVIDIA Rubin 已全面投产，基于 Rubin 的产品将于 2026 年下半年通过合作伙伴上市。&lt;/p&gt;&lt;p&gt;首批在 2026 年部署基于 Vera Rubin 实例的云服务提供商包括 AWS、Google Cloud、微软和 OCI，以及英伟达云合作伙伴 CoreWeave、Lambda、Nebius 和 Nscale。&lt;/p&gt;&lt;p&gt;CoreWeave 将与英伟达合作，帮助 AI 领域的先驱者充分利用 Rubin 在推理和 MoE 模型方面的进步，此外，思科、戴尔、HPE、联想和 Supermicro 预计将推出基于 Rubin 产品的服务器。&lt;/p&gt;&lt;p&gt;包括 Anthropic、Black Forest、Cohere、Cursor、Harvey、Meta、Mistral AI、OpenAI、OpenEvidence、Perplexity、Runway、Thinking Machines Lab 和 xAI 在内的 AI 实验室正在寻求利用 NVIDIA Rubin 平台来训练更大、功能更强大的模型，并以比前几代 GPU 更低的延迟和成本运行长上下文、多模态系统。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;增强自动驾驶推理， Alpamayo 1 开源模型来了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;英伟达认为，下一代面向 L4 的自动驾驶方案，需要基于拥有强推理性能的 VLA 模型。&lt;/p&gt;&lt;p&gt;英伟达今日发布了 &lt;strong&gt;NVIDIA Alpamayo 系列开源 AI 模型、仿真工具及数据集&lt;/strong&gt;，旨在加速下一代安全、基于推理的自动驾驶汽车（AV）开发。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNiclzAeSGOd7MsofeUMTFguX82ModyRcdLibI6miaUNAbY2PrPrGtfDhHg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.6175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526876" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/599c1ec0-5779-404b-a160-126e845aca1d/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;自动驾驶汽车必须在极其广泛的驾驶条件下安全运行。那些稀少且复杂的场景（通常被称为「长尾问题」），依然是自动驾驶系统安全掌控的最严峻挑战之一。&lt;/p&gt;&lt;p&gt;传统的自动驾驶架构将感知与规划分离，当遇到全新或异常情况时，这种方式会限制系统的可扩展性。&lt;/p&gt;&lt;p&gt;虽然端到端学习在近期取得了显著进展，但要克服这些长尾极端案例，仍需要模型能够针对因果关系进行安全推理，尤其是在情况超出模型训练经验时。&lt;/p&gt;&lt;p&gt;Alpamayo 系列引入了&lt;strong&gt;基于思维链推理的视觉语言动作（VLA）模型&lt;/strong&gt;，为自动驾驶决策带来了类似人类的思考方式。&lt;/p&gt;&lt;p&gt;这些系统可以分步骤思考新颖或罕见的场景，从而提升驾驶能力和可解释性。可解释性对于增强智能汽车的信任度与安全性至关重要。此外，该系列还得到了英伟达 Halos 安全系统的底层支持。&lt;/p&gt;&lt;p&gt;黄仁勋表示：「&lt;strong&gt;物理 AI 的 ChatGPT 时刻已经到来，机器开始理解、推理并对现实世界采取行动。&lt;/strong&gt;」&lt;/p&gt;&lt;p&gt;他接着说，Alpamayo 为自动驾驶汽车带来了推理能力，使它们能够思考罕见场景，在复杂环境中安全驾驶，并解释其驾驶决策。这些都是实现安全、可扩展自主驾驶的基石。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;构建基于推理的自主驾驶完整开源生态&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Alpamayo 将三大支柱（开源模型、仿真框架和数据集）整合为一个内聚的开放生态系统，任何汽车开发商或研究团队都可以在此基础上进行开发。&lt;/p&gt;&lt;p&gt;不过，Alpamayo 模型并非直接在车端运行，而是作为大规模的「教师模型」。开发者可以对其进行微调和蒸馏，转化为各自完整自动驾驶技术栈的核心骨架。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Alpamayo 1：全球首个面向自动驾驶汽车的开源大规模推理视觉语言动作（VLA）模型&lt;/strong&gt;，不仅能让车辆深度理解周围环境，还能对其采取的驾驶行为给出合理解释。现已在 Hugging Face 上线。&lt;/p&gt;&lt;p&gt;Alpamayo 1 采用 100 亿参数架构，通过视频输入生成行驶轨迹及推理痕迹，展示每项决策背后的逻辑。开发者可以将 Alpamayo 1 改编为适合车辆开发的小型运行模型，或将其作为自动驾驶开发工具（如基于推理的评估器和自动标注系统）的基础。&lt;/p&gt;&lt;p&gt;Alpamayo 1 提供开放的模型权重和开源推理脚本。该系列未来的模型将具备更大的参数量、更详细的推理能力、更灵活的输入输出选项以及商业化用途。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AlpaSim：一个完全开源的端到端高保真自动驾驶开发仿真框架&lt;/strong&gt;，可在 GitHub 上获取。它提供逼真感知的传感器建模、可配置的交通动态以及可扩展的闭环测试环境，能够实现快速验证和策略优化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;物理 AI 开源数据集&lt;/strong&gt;：英伟达提供了最多样化的大规模自动驾驶开源数据集，包含超过 1700 小时的驾驶数据。这些数据采集自极其广泛的地域和环境，涵盖了对于推进推理架构至关重要的稀有且复杂的现实极端案例。这些数据集现已在 Hugging Face 上线。&lt;/p&gt;&lt;p&gt;这些工具共同构成了一个自我强化的开发闭环，助力构建基于推理的自动驾驶技术栈。&lt;/p&gt;&lt;p&gt;Alpamayo 已经得到了自动驾驶行业的广泛支持。包括 Lucid、捷豹路虎（JLR）、Uber 和 Berkeley DeepDrive 在内的出行领军者，都对利用 Alpamayo 开发基于推理的自动驾驶技术栈表示了浓厚兴趣，以实现 L4 级自动驾驶。&lt;/p&gt;&lt;p&gt;在 Keynote 上，老黄展示了奔驰新款 CLA 在旧金山市区点到点的全自动驾驶，英伟达表示，国内的一些汽车厂商如吉利和小米也会在晚些时候接入英伟达的智能驾驶模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;全新物理 AI 模型，与全球合作伙伴推出新一代机器人&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNUKkM0Gp7mke4yehCJHIgKyPMmiaIHAThjyyibvU1Gb5bljceK9uA6ncw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-ratio="0.562962962962963" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526877" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/468429c7-d1bd-48b1-94f6-9f8138f54ca8/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;英伟达宣布推出针对物理人工智能（Physical AI）的全新开源模型、框架及 AI 基础设施，并携手全球合作伙伴展示了涵盖各行各业的机器人。&lt;/p&gt;&lt;p&gt;这些新技术加速了机器人开发全生命周期的工作流，助力开启下一波机器人浪潮，其中包括构建能够快速学习多项任务的通用型专家机器人。&amp;nbsp;&lt;/p&gt;&lt;p&gt;包括波士顿动力、Caterpillar、Franka Robotics、Humanoid、LG 电子和 NEURA Robotics 在内的全球行业领军企业，正利用英伟达机器人技术栈推出全新的 AI 驱动型机器人。&lt;/p&gt;&lt;p&gt;黄仁勋表示：机器人的「ChatGPT 时刻」已经到来。物理 AI 领域的突破 &amp;mdash;&amp;mdash; 即能够理解现实世界、进行推理并规划行动的模型 &amp;mdash;&amp;mdash; 正在开启全新的应用场景。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNqvf4NIXiaibTfScGDBDguF4sWo0iaKvXh0RTibpYxgZvN02emhZvIHmj5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.5648148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526878" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/db1720e1-994e-43bd-8abd-07f36775c465/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;新型开放模型推动机器人学习与推理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;将当今成本高昂、任务单一且编程困难的机器转变为具有推理能力的通用型专家机器人，需要巨大的资本投入和构建基础模型的专业知识。&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达正在构建开源模型，让开发者能够绕过耗费资源的预训练阶段，专注于创造下一代 AI 机器人。这些模型均可在 Hugging Face 上获取，包括：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;NVIDIA Cosmos Transfer 2.5 与 NVIDIA Cosmos Predict 2.5&lt;/strong&gt;：开源、完全可定制的世界模型，可生成符合物理定律的合成数据，并在模拟环境中对物理 AI 的机器人策略进行评估。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;NVIDIA Cosmos Reason 2&lt;/strong&gt;：一款开源推理视觉语言模型（VLM），使智能机器能够像人类一样观察、理解并在物理世界中采取行动。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;NVIDIA Isaac GR00T N1.6&lt;/strong&gt;：一款专为人形机器人设计的开源推理视觉语言动作（VLA）模型，可实现全身控制，并利用 NVIDIA Cosmos Reason 获得更好的推理和情境理解能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;助力机器人开发的全新开源模拟与计算框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;可扩展的模拟对于机器人的训练和评估至关重要，但当前的工作流依然零散且难以管理。基准测试通常依赖人工，难以规模化，而端到端流水线则需要在不同的计算资源之间进行复杂的协调。&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达今日在 GitHub 上发布了全新的开源框架，简化了这些复杂的流程，加速了从研究到实际应用场景的转化。&amp;nbsp;&lt;/p&gt;&lt;p&gt;NVIDIA Isaac Lab-Arena 是一个在 GitHub 上提供的开源框架，为模拟环境中的大规模机器人策略评估和基准测试提供了一个协作系统，其评估层和任务层是与 Lightwheel 紧密合作设计的。它连接了 Libero 和 Robocasa 等行业领先的基准，实现了测试标准化，确保机器人技能在部署到物理硬件之前稳健可靠。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaNXPvRayHKHFR0VYGYJk1lriaOVN5tZlNUTvvpQBXsBicuAa4eBRtU7fAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.7657407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526879" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/4e59b171-5223-4a90-a196-4be30696da17/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; Isaac Lab-Arena 框架概览&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;NVIDIA OSMO 是一款云原生编排框架，将机器人开发统一到一个易于使用的中心控制台中。OSMO 允许开发者在从工作站到混合云实例的不同计算环境中，定义并运行合成数据生成、模型训练及软件在环测试等工作流，从而缩短开发周期。&amp;nbsp;&lt;/p&gt;&lt;p&gt;OSMO 正在被 Hexagon Robotics 等开发者使用，并已集成到微软 Azure Robotics Accelerator 工具链中。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicXs2As91d0ugEiarVKrDQiaN7sFUw06xWxibknbQkiahoq10RXU1wzO1a9A58684iaicibDIz0YnlxK6y2Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5027777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526880" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/b592dd20-8df7-499c-8731-3aad013e7884/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; OSMO 框架概览&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;携手 Hugging Face 加速开源物理 AI 发展&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;机器人目前是 Hugging Face 上增长最快的类别，英伟达的开源模型和数据集在蓬勃发展的开源社区中下载量遥遥领先。&lt;/p&gt;&lt;p&gt;为了进一步支持该社区，英伟达正与 Hugging Face 合作，将开源的 Isaac 和 GR00T 技术集成到领先的 LeRobot 开源机器人框架中，提供更便捷的软硬件工具访问，加速端到端开发。&lt;/p&gt;&lt;p&gt;此次合作将英伟达的 200 万机器人开发者与 Hugging Face 的 1300 万 AI 构建者连接在一起。 GR00T N 系列模型和 Isaac Lab-Arena 现已在 LeRobot 库中上线，方便用户进行微调和评估。&lt;/p&gt;&lt;p&gt;Hugging Face 的开源人形机器人 Reachy 2 将与 NVIDIA Jetson Thor 机器人计算机完全互操作，支持开发者运行包括 GR00T N1.6 在内的任何 VLA 模型。&lt;/p&gt;&lt;p&gt;此外，Hugging Face 的开源桌面机器人 Reachy Mini 也与 NVIDIA DGX Spark 完全互操作，可利用本地运行的英伟达大语言模型、语音及视觉模型构建自定义体验。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人形机器人开发者采用 NVIDIA Jetson Thor&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;NVIDIA Jetson Thor 能够满足人形机器人推理所需的庞大算力。在 CES 上，人形机器人开发者展示了集成 Jetson Thor 的最新顶尖机器人。&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中，NEURA Robotics 推出了保时捷设计的三代人形机器人，以及一款针对灵巧控制优化的迷你人形机器人。Richtech Robotics 推出了 Dex，这是一款可在复杂工业环境中进行精细操作和导航的移动人形机器人。&lt;/p&gt;&lt;p&gt;智元机器人（AGIBOT）介绍了面向工业和消费领域的人形机器人，以及集成了 Isaac Sim 的机器人仿真平台 Genie Sim 3.0。&lt;/p&gt;&lt;p&gt;LG 电子则展示了一款旨在执行多种室内家务的新型家用机器人。 波士顿动力、Humanoid 和 RLWRLD 均已将 Jetson Thor 集成到现有人形机器人中，以增强其导航和操作能力。&lt;/p&gt;&lt;p&gt;更多细节信息请参考英伟达官方博客。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://nvidianews.nvidia.com/news/rubin-platform-ai-supercomputer&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://nvidianews.nvidia.com/news/nvidia-releases-new-physical-ai-models-as-global-partners-unveil-next-generation-robots?linkId=100000401170428&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.youtube.com/watch?v=0NBILspM4c4&amp;amp;t=3s&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>检索做大，生成做轻：CMU团队系统评测RAG的语料与模型权衡</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 06 Jan 2026 10:21:55 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-06</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-06</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/735cef40-a694-443c-900f-b30333bc85f7/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在检索增强生成中，扩大生成模型规模往往能提升准确率，但也会显著抬高推理成本与部署门槛。CMU 团队在固定提示模板、上下文组织方式与证据预算，并保持检索与解码设置不变的前提下，系统比较了生成模型规模与检索语料规模的联合效应，发现扩充检索语料能够稳定增强 RAG，并在多项开放域问答基准上让小中型模型在更大语料下达到甚至超过更大模型在较小语料下的表现，同时在更高语料规模处呈现清晰的边际收益递减。更进一步，研究不仅刻画了随语料扩容而变化的性能增益，也揭示了若干相对稳定的不变规律。&lt;/p&gt;&lt;p&gt;在开放域问答等知识密集型任务中，检索增强生成（RAG）已经成为主流范式之一。它通过先检索外部文档，再让大语言模型基于证据生成答案，从而缓解纯参数记忆带来的幻觉与事实错误。然而，近年来提升 RAG 的常见路径往往集中在扩大生成模型规模，准确率确实会上升，但推理成本与部署门槛也随之显著提高。对于希望在有限算力下落地的系统而言，一个更现实的问题是：在不继续扩大模型参数的前提下，是否还有同样有效的提升空间。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibRJicC4EauATIPzAicM69uEwSH2lHW6qLugM52G8zs1XMG0DVc2ekUhuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.37407407407407406" data-type="png" data-w="1080" data-width="1955" data-height="732" data-imgfileid="503526749" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/fcbc88b6-13c4-4e5a-952a-36cbde736579/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;卡内基梅隆大学计算机学院团队&lt;/strong&gt;在最新 ECIR 接收论文中给出了一个清晰的回答。他们把关注点从更大的模型转向更大的检索语料，系统评估了语料规模与生成模型规模之间的替代关系，并提出了可操作的权衡框架。核心观点为，扩大检索语料通常可以显著增强 RAG，且在不少设置下，这种增强效果可以部分替代扩大模型参数带来的收益，但在更大语料规模处会出现边际收益递减。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibxJGr8px19b2LxtGe7CxGj3jrUf46vpib7gULqC5cYa83nAnrmR3MYxw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.36036036036036034" data-s="300,640" data-type="png" data-w="777" type="block" data-imgfileid="503526754" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/76a260c5-477e-42f9-8280-01683bccc45e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Less LLM, More Documents: Searching for Improved RAG&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/pdf/2510.02657&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;从问题出发：RAG 的另一条扩展轴&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;RAG 的效果由两部分共同决定。检索模块负责把可能包含答案的证据送到模型上下文中；生成模型负责理解问题、整合证据并形成答案。扩大模型参数能够提升推理与表达能力，但检索端提供的证据质量与覆盖范围，往往直接决定模型是否有机会看到答案线索。CMU 团队指出，检索语料的规模本身就是一条独立的扩展轴，但长期以来缺少与模型规模联合控制变量的系统研究，因此语料扩容能否补偿小模型仍缺乏定量结论。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验设计：只让两个变量变化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为得到可解释的权衡曲线，研究采用了全因子设计，只让语料规模与模型规模变化，其余保持一致。检索语料选用大规模搜索引擎数据集 ClueWeb22-A 的英文子集，总计包含约 2.64 亿真实网页文档，并将其随机均衡切分为 12 个 shard。语料规模用激活 shard 的数量表示，逐步从 1 个 shard 扩展到 12 个 shard。检索端使用 MiniCPM-Embedding-Light 做稠密向量编码，后端采用 DiskANN 构建多 shard 近邻检索，固定 top 文档数、切块与重排策略，最终向生成模型提供固定数量的 top chunk 作为 LLM 答案生成证据。&lt;/p&gt;&lt;p&gt;生成端选用最新 Qwen3 同一模型家族的不同尺寸，覆盖从 0.6B 到 14B 的 Qwen3 模型，并固定提示模板与解码设置，以确保比较只反映规模变化带来的差异。评测任务覆盖三个开放域问答基准：Natural Questions、TriviaQA 与 Web Questions，指标采用最常用的 F1 与 ExactMatch。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键发现一：语料扩容可以让小模型追上大模型（变）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;实验结果展示了明确的补偿效应。以 Natural Questions 为例，随着语料从 1 个 shard 扩展到更大规模，较小模型的 EM 与 F1 持续提升，并在一定语料规模后达到或超过更大模型在小语料上的基线表现。研究用 n 星指标刻画补偿阈值，即小模型需要多少倍语料才能追平大模型在 1 个 shard 下的成绩。在三个数据集上，这一阈值呈现出稳定模式：中等规模模型之间的追平往往只需要把语料扩大到 2 倍或 3 倍，而最小模型想追平下一档模型则需要更高倍数的语料扩容。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaib7JekFMn2uRUfINF9YQicTee5wtY6qb2PeRyaaHHCnSunFoKmicXy1G9Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.35462962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526750" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8938b776-1af0-44ef-b871-e52157a7a05c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;更重要的是，这种追平并非个别现象。研究在 TriviaQA 与 WebQuestions 上观察到相同趋势，并给出了跨数据集的阈值表，显示语料扩容在多数设置下都能把性能缺口缩小到一个模型档位，甚至两个档位。对部署而言，这意味着当推理预算难以支撑更大参数模型时，把资源投入到更大语料与更强检索，可能是更务实的提效方向。&lt;/p&gt;&lt;p&gt;在增长形态上，研究观察到几乎与模型规模无关的共同曲线。最显著的提升发生在从无检索到有检索的第一步，随后随着语料继续扩大，收益逐步下降，并在约 5 到 6 倍语料规模附近出现饱和趋势。这一现象对工程实践具有直接意义：检索能力的从无到有往往带来最大增益，但在较高语料规模处继续无上限扩容并不划算，应该结合吞吐、延迟与存储成本做更精细的预算分配。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键发现二：提升主要来自证据覆盖，而非模型更会用证据（不变）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;语料变大为什么能带来提升？论文给出的机制解释相对直接且符合直觉预期：语料扩容提高了检索到含答案片段的概率。当语料规模较小时，检索到的片段经常只与主题相关，但不包含关键事实；随着语料扩大，更容易检索到明确包含答案字符串的证据片段，生成模型因此获得更可靠的落脚点。&lt;/p&gt;&lt;p&gt;为把这种直觉量化，研究定义了 Gold Answer Coverage Rate，用于统计传入生成模型的 top chunk 中至少有一个包含标准答案字符串的概率。结果显示，覆盖率随语料规模增长而单调上升，并在不同数据集上体现出差异性，例如 TriviaQA 的覆盖率整体更高，反映其信息需求与网页语料的重合度更强。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibjibUZEicNfvsnKx7zq6e36SicDqTibKLxnZQlbu3Q4PUxr29df7esYXwMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5" data-type="png" data-w="1080" data-width="3000" data-height="1500" data-imgfileid="503526751" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/65d3b0c8-be7d-4cf0-8509-30a391090ee3/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;进一步地，研究提出 Context Benefited Success Rate，用于衡量那些在无检索时无法答对的问题，在加入检索证据后被答对的比例，并用 Utilization Ratio 将其与覆盖率相除，以刻画模型把可用证据转化为正确答案的效率。实验显示，Utilization Ratio 在不同语料规模下整体保持稳定，且在不同模型尺寸之间差异有限。结合无检索设置下的基线表现可以看到，不同大小模型的主要差别更多来自其参数中可直接调用的内部知识储备，使其在无需外部证据时也能回答一部分问题；而对于那些无法仅凭内部知识答对的问题，一旦检索端提供了包含答案线索的证据，不同模型将证据转化为正确答案的效率整体相近。因此，语料扩容带来的关键收益主要体现在提高含答案证据进入上下文的概率，而非显著提升模型对既有上下文的利用能力。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibPZCsUwHMeichict28WobRaC6EU2oqwh6icCktTiaSaON88ibAe3s1pvbVbw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5" data-type="png" data-w="1080" data-width="3000" data-height="1500" data-imgfileid="503526752" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/ec6de7f3-0e37-41f2-b328-8923f6467a04/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;工程启示：如何在预算约束下分配投入&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;综合实验结论，论文给出了一条可执行的系统设计建议。当推理资源受限时，优先考虑扩大检索语料与提升覆盖率，常常能让中等规模生成模型达到接近更大模型的表现。相比之下，极小模型需要更激进的语料扩容才能追平下一档，收益效率偏低；而极大模型在更大语料下的增益也相对有限，体现出利用效率并不会随着参数规模单调上升。对系统优化而言，跟踪答案覆盖率与利用率可以作为诊断指标，帮助判断瓶颈更偏检索端还是生成端，从而指导下一步应该扩语料、调检索，还是换模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这项研究把 RAG 的规模讨论从单一的模型参数扩展到语料与检索能力，给出了可复现的控制变量实验与清晰的机制解释。其结论可以概括为两点：扩大语料通常有效，但收益存在边际递减；提升主要来自更高的答案证据覆盖，而非模型利用证据能力的跃迁。在面向真实部署的 RAG 系统中，这提供了一条更可控、更具性价比的提升路径。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;作者简介：&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibcdBp0xYuX7nc1ibnMjJGXvlDY3FplrhQrhm6F6IKc5srcKMia7AVJcsw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="1" data-type="png" data-w="1080" data-width="2730" data-height="2730" data-imgfileid="503526753" data-aistatus="1" data-original-style="width: 160px;height: 160px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/56b93f3e-781d-48d0-877a-5e8a222e7b51/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;本论文第一作者为卡内基梅隆大学计算机学院语言技术研究所硕士研究生 Jingjie Ning，研究方向聚焦信息检索、DeepResearch、Query 理解与强化、推荐系统 Benchmark 等工作。Jingjie Ning 师从 Jamie Callan 教授，后者为卡内基梅隆大学计算机学院语言技术研究所教授，曾任 SIGIR 大会主席，同时担任系博士项目主任，长期引领搜索与信息检索领域研究，在学术界与工业界具有广泛影响力。在卡内基梅隆大学前，Jingjie 曾在腾讯任职 Senior Data Scientist。个人主页：https://ethanning.github.io&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AI Shortlist上线｜研究值得关注的AI企业</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 05 Jan 2026 18:06:35 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-05-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-05-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d19de98d-e2ab-4149-979c-4a0200b9c7ac/%E5%B0%B1%E5%BE%88%E9%9A%BE%E5%8A%A0%E4%B8%8D%E5%8A%A0%E7%8F%AD_jpg.jpg" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7b170419-48dc-4a0e-8cdf-930a34cd03df/0.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Claude Code 一小时「复刻」谷歌一年成果，那一年能读完五年半的博士吗？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 05 Jan 2026 17:09:43 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-05-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-05-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2d0f00db-74f0-4a7e-9426-9f433b0179f8/1767604027650.png" style="width: 700%;" class="fr-fic fr-dib"&gt;近日，X 知名博主、Hyperbolic 联创 &amp;amp; CEO Yuchen Jin 发帖称，如果在他读博士的时候就有 Claude Code、Gemini 和 ChatGPT 等各类 AI 工具出现，那么也许只要一年就能毕业，而不是用了 5.5 年。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibiatqp1y35rY1vNNcjHanIx0Jx99oI6OckibrvlHXSApb0b49LrJ7NXEA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5888888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526769" data-aistatus="1" data-original-style="width:508px;height:299px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ed42f2fe-50ae-46c5-9cff-ac83cde39845/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;而他之所以发出这个感慨，缘由是最近一些硅谷 AI 大厂工程师表示，在用了 AI 工具后，项目完成时长被大幅压缩&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;先是谷歌首席工程师、Gemini API 负责人 Jaana Dogan 在 X 上发文称：「我不是在开玩笑，这也不好笑。从去年开始，我们就在谷歌内部尝试构建分布式 Agent 编排器。有多种选择，大家并没有完全认同&amp;hellip;&amp;hellip; &lt;strong&gt;我只是向 Claude Code 描述了问题，它就在一小时内生成了一个东西，而这几乎就是我们去年一年所做的东西。&lt;/strong&gt;」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibbibGW1z083WOBic2KRP2Mfhp6rI8NysAw98HfficiagFjry59Hz1FPCs5w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.32037037037037036" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526778" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/02bc0c99-53d8-4eae-ad7d-d793934b77ea/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;随后，她又发文补充，提示内容不算详细，也没有具体细节，只是一段三段式的描述。但由于不能分享任何东西，也不好具体展示出来，总结来说就是在现有一些想法基础上构建一个玩具版本，用以评估 Claude Code。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibDqmwBROfzYfottmYKxh7U2KwKUEib56vyauhjCa5wxic26kX3SjusfHQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.31574074074074077" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526779" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/51352749-485f-435f-ba4e-f6caccd0461e/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;帖子一经发布，迅速引起网友讨论，有人感慨没想到当前的 AI 编码工具已经发展到如此强大的地步，也有人惊叹，原来即便是强大如谷歌这样的 AI 大厂，也会使用 AI 编码工具，更意外的是，居然允许员工使用其他公司产品，而非强制大家使用自家旗下的 Gemini、Gemini CLI 或 Antigravity&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;其中，一位名为 Rohan Anil 的 X 网友评论道：自己以前也是谷歌工程师，在职期间一路晋升，&lt;strong&gt;但如果当时就有 Agent coding 的话，尤其是 Opus，也许就能把前六年的工作压缩在几个月内完成&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibDT39f7CdVibpkXYopzHDTUvOWXaN2y4nqnoTBgfGiamHtOocqVfC9LWg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.31666666666666665" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526782" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7a480fe3-f046-4553-942b-467c3993f2b8/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;随后此推文获得了上百次的浏览，而该网友也发文认真做起了自我介绍，原来 Rohan Anil 曾是前谷歌和 Meta 杰出工程师，在 Google Brain 期间从事基础研究，重点包括训练算法和基础设施方面的工作。例如谷歌 Google 内部首个 Transformer 推理系统，以及首批大规模 TPU 训练与推理系统的落地与上线。&lt;/p&gt;&lt;p&gt;之后他在 Google DeepMind 负责领导 Gemini 模型相关工作，且因在 Gemini 预训练方面的贡献，获得杰出工程师称号。去年一月，他从谷歌离职，目前在 Anthropic 工作。&lt;/p&gt;&lt;p&gt;而谈及帖文中所说的「Opus 这项技术可以把我六年的工作压缩到几个月」时，他表示，&lt;strong&gt;这主要指的是工程层面：性能优化、在真实约束条件下拼装分布式系统等&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;也正是这些内容，让 Yuchen Jin 有感而发：「这与我的亲身经历完全一致。」&lt;/p&gt;&lt;p&gt;他认为，&lt;strong&gt;当前 AI 正在显著压缩学习曲线，并以惊人的速度把初级工程师「拉升」为高级工程师&lt;/strong&gt;。在大型代码库中的新员工入职熟悉周期，已经从过去的几个月缩短到现在的几天。曾经需要花上数小时在 Google 和 Stack Overflow 上查资料的问题，如今往往只需要一个 Prompt 就能解决。同时，AI 也正在成为一位优秀的导师和结对编程伙伴。&lt;/p&gt;&lt;p&gt;「现在真正稀缺的，只剩下主动性。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibjF8apvicib4DgLaVYNX0I3Diaez3n02LLBExwgHGwwI2HSMdibzSLAHeMA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.9194444444444444" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526783" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/15550f19-2c18-4883-86d4-93eb3ea0365a/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;更进一步地，他认为不仅是在工作中，如果再早一些，在读博期间就会各种 AI 工具出现的话，那么，可能毕业时间也会大大缩短，也就是在文章开头所发的帖子。&lt;/p&gt;&lt;p&gt;在他看来，&lt;strong&gt;「当前的教育模式还处于人工智能出现之前，需要根本性的更新。」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不过，不同于对 AI 工具在工作中所带来影响的积极认可，关于 AI 工具在教育中的作用，大家开始出现了分歧。&lt;/p&gt;&lt;p&gt;有网友认为，就像 Yuchen Jin 所言，博士期间约有 25% 的时间是用来阅读大量论文的，但 AI 的出现让这一部分变得不一样了。「我以前需要花几个小时来解析晦涩的论文，而现在只要请 Claude 帮忙解释关键见解，并对照实际论文进行验证就可以了。」&lt;/p&gt;&lt;p&gt;当然，他也坦言，虽然现在还不能算是完美，但这只是时间的问题。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibLia0MI4K9ePVcjrZiamqQOUrhBuUlJhLZFdoeYluibkFVII9PzkjXxqBg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.2712962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526775" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/19bc1802-a481-4802-9157-4b27c4e20d23/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;网友 Thierry Laurent 也持相似看法，他说自己目前正在攻读遥感硕士学位，以往需要几个月才能积累的脚本素材，现在在 Codex/CC 里只需要几天时间就可以了。「我们目前正处于一个转折点，但大学尚未准备好。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibMibG6vthTCH6TWW7sTfukor4ddLvsPyyxQTicOASGv1Xefl0akdbV7rA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.1648148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526771" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/6a424ab3-4e84-44a2-9d54-a06aefa1d4f3/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;但一位名为 alyxya 的网友并不这样认为。在他看来，对于学生来说，仍然需要花费时间来学习批判性思维、推理行业理解能力。「AI 就像个人导师一样，我可以不断地向他提问，但 TA 不能强迫我立刻内化这些知识。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibYqH4b6Z2S4lfVYcZrlguAYoI40OkeyvD00ibRribSnwUFgJsMbiafAiaQQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.17222222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526776" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/8e888b24-f541-41fb-b6de-50253e011a7a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;网友 Palmi 也有类似的看法，他认为，也许 AI 工具的接入会让毕业时间加快，但是，「你会像现在这样优秀吗？」AI 确实加快了工作、学习进程，但对于个人来说，并不会获得任何（处理过程中）的知识。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibOtK8SXhhJYpaIQBfE9JiariazG0P674vrXaXpEicU6tk7DDRcBHUib9xSg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.12407407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526784" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/6fdb0b2d-927f-45ae-8ac3-e3be85a185c1/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;网友 Burhan 甚至觉得那些曾经「浪费」的时间让自己成长了。他内心深处开始思考一个问题：这「5 年半的挣扎」是否真的在我们身上构建了一些「1 年冲刺」所无法带来的东西。换句话说，这种用「笨办法」死磕硬磨所带来的阻力，是否反而锻造出了一种更深层的专业造诣？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibjaV0fPibmXN3Su6I2F3Q3GBWVD6ZY7ejUZwqzB9nahJmwJbwA5Nic4rQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.16574074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526777" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/bf7c30d5-ca76-4c79-a4f3-cb565befb520/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;还有网友调侃称，&lt;strong&gt;「兄弟，不过别忘了，你不是唯一有AI工具的人，也许你的导师很快就会把研究生要求定为：需要50篇第一作者论文。」&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaiblM9pIs4ndficyict7BnQHC5IopDzn4iaDpLyGp6oxsUNbYwJflyv83nJw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.13240740740740742" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526770" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/9faa2b8d-f17b-445c-80d5-dd968e222d1c/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;最新信息是，Yuchen Jin 又发了一个案例，说他一位从事 AI 研究的朋友正在教自己 8 岁的孩子用 Claude Code 写 PyTorch 代码。&lt;/p&gt;&lt;p&gt;他表示，自己对这种虎妈式的做法感到震惊。但真正的信号是：当一个 8 岁的孩子能建造出需要多年学校和训练的东西时，「高等教育」就变得过时了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「经验年限现在远不如品味、好奇心、主动性，以及与人工智能合作的能力重要。」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;目前，关于这一观点的争论还在继续，各路网友都在基于自身的经验或认知来阐述自己的立场，帖子热度也在继续高涨。&lt;/p&gt;&lt;p&gt;那么你呢，你如何看待持续快速发展的 AI 对教育带来的影响？&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/QuanquanGu/status/2007947084608246188&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/Yuchenj_UW/status/2007512853625090095&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>空间智能终极挑战MMSI-Video-Bench来了，顶级大模型全军覆没</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 05 Jan 2026 17:04:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-05-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-05-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/65ff1982-f67e-4877-9013-0c5ba8f160e9/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;空间理解能力是多模态大语言模型（MLLMs）走向真实物理世界，成为 &amp;ldquo;通用型智能助手&amp;rdquo; 的关键基础。但现有的空间智能评测基准往往有两类问题：一类高度依赖模板生成，限制了问题的多样性；另一类仅聚焦于某一种空间任务与受限场景，因此很难全面检验模型在真实世界中对空间的理解与推理能力。&lt;/p&gt;&lt;p&gt;要真正走入现实世界，模型不仅需要看得见，更要看得懂空间： 它需要在复杂、多变的真实场景中理解空间布局、感知运动变化、进行时空推理，并基于这些信息做出合理决策，与环境产生有效交互。&lt;/p&gt;&lt;p&gt;为此，&lt;strong&gt;上海人工智能实验室 InternRobotics 团队&lt;/strong&gt;近日推出了一套&lt;strong&gt;全面而硬核的空间智能视频基准 &amp;mdash;&amp;mdash; MMSI-Video-Bench&lt;/strong&gt;，对当前主流多模态大模型精心打造了一场挑战系数极高的 &amp;ldquo;空间智能大考&amp;rdquo;。&lt;/p&gt;&lt;p&gt;本工作由上海人工智能实验室、上海交通大学、香港中文大学、浙江大学、香港大学、北京航空航天大学、西安交通大学、复旦大学、加州大学洛杉机分校 的研究者们共同完成。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaib3rJBXyrkemHibYYwI3UUIej2qRHtKvTY9nUSmbSTBW4ad9laWsRL6ww/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.32037037037037036" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526716" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/df73e168-0cf3-4ae6-a3de-5baafe2ebc63/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目主页： https://rbler1234.github.io/MMSI-VIdeo-Bench.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;ArXiv 论文： https://arxiv.org/abs/2512.10863&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face 数据集： https://huggingface.co/datasets/rbler/MMSI-Video-Bench&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 代码库： https://github.com/InternRobotics/MMSI-Video-Bench&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/M4XBlVXjd3Alv5pH3h4tyw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8819386b-0479-410a-a2bc-43f59bb5b386/1767603661870.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;该基准具有以下显著特点：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（1）全面且系统的题型设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MMSI-Video-Bench 首先从视频本身的时空信息理解出发，对模型的基础空间感知能力进行系统考察，主要包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;空间构建（Spatial Construction）&lt;/strong&gt;：聚焦于对全局空间布局的理解，涵盖实体与场景的空间状态属性，以及 相机、实体与场景之间的两两空间位置关系。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;运动理解（Motion Understanding）&lt;/strong&gt;：考察模型对长时运动过程的感知与理解能力，包括实体运动、相机运动，以及多实体之间的交互运动。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此基础上，MMSI-Video-Bench 进一步评测模型基于时空信息进行高层决策的能力，具体包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;基于视频信息进行推理与行动的&lt;strong&gt;规划能力（Planning）&lt;/strong&gt;；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对未来状态进行推断与想象的&lt;strong&gt;预测想象能力（Prediction）&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于真实世界的观测在时间上不一定是连续的，在空间上单一视角的信息不一定是完备的，MMSI-Video-Bench 进一步扩展了任务范畴，以更真实地覆盖现实场景中的复杂情形，考察模型跨视频的推理能力，这包含了跨时间的记忆更新能力（Memory Update）；多视角信息的整合能力（Multi-View Integration）。&lt;/p&gt;&lt;p&gt;通过上述多层次、多维度的题型设计，MMSI-Video-Bench 构建了一个&lt;strong&gt;覆盖感知、推理与决策全过程的空间智能评测体系&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibS5RFP2k3DbicZv9nYBd3S0biaZqtOpttU6nLrFuqc85YeuHciaVonVlxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="1.1175925925925927" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526717" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f29e4049-f1b8-4583-80f9-03016433109d/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; MMSI-Video-Bench 由五大任务类型，13 个子类问题构成&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（2）极具挑战性的问题设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MMSI-Video-Bench 基准的所有问题由 &lt;strong&gt;11 位平均研究年限超过 2.5 年的 3D 视觉研究员亲自把关精细设计&lt;/strong&gt;，严格验收打磨，确保了基准每一个问题清晰准确，具有挑战性。所有模型均表现吃力，即便是最表现最好的 Gemini 3 Pro，也只有 38% 的准确率，相比其它的空间智能基准，具有目前最高的人类&amp;ndash;AI 性能差距 (约 60%)。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; (3) 丰富多样的视频数据来源&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基准的视频数据来源于 25 个公开数据集 以及 1 个自建数据集，包含了机器人操作、从单房间到多层楼宇的室内场景、室外建筑与街景、自然风光、体育活动以及电影片段等多种拍摄类型，全面反映了真实世界中复杂多样、多尺度的空间场景&lt;/p&gt;&lt;p&gt;&lt;strong&gt; (4) 特定领域针对性的能力测评&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;此外，受益于场景类型的丰富以及任务类型的全面性，MMSI-Video-Bench 可以划分出&lt;strong&gt;室内场景感知&lt;/strong&gt; (Indoor Scene Perception)/ &lt;strong&gt;机器人 &lt;/strong&gt;(Robot) / &lt;strong&gt;定位&lt;/strong&gt; (Grounding) 三大子基准，方便针对性测评模型特定能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaib028sPLRgYy7ggWxMdHepE96FLh4umyrH1ibxBI7XZ7jRqnarCPuWl9w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.0916666666666666" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526718" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/511a55c1-0f93-4653-bfb7-d0f8f3db83cd/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; MMSI-Video-Bench 的标注流程 和 比例 / 视频时长 / 词云分布&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;空间智能大考：揭示模型能力边界与瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（1）空间智能大考模型成绩单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队对 25 个主流多模态模型 进行了评测，整体得分普遍偏低。即便是表现最优的 Gemini 3 Pro（38.0），与人类水平 （96.4） 之间仍存在&lt;strong&gt; 接近 60%&lt;/strong&gt; 的显著差距。&lt;/p&gt;&lt;p&gt;与已有空间智能基准的结论一致，实验结果再次暴露了当前模型在空间构建能力上的不足。更为关键的是，得益于 MMSI-Video-Bench 在任务设计上的全面性，研究团队进一步发现：模型在 运动理解、规划、预测以及跨视频推理 等能力上同样存在明显瓶颈。&lt;/p&gt;&lt;p&gt;在所有任务类型中，预测（Prediction） 是最具挑战性的主任务， 相机&amp;ndash;实体之间的空间关系建模 是难度最高的细分类别。此外，研究团队发现，即便是经过专门空间任务微调的模型，其能力也未能有效泛化到 MMSI-Video-Bench。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibttWgWJH5XBebNaEuuriaXyFFrcNKaYc2OGz4fPfXjN1prCbVwdD5Paw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.837037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526720" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/519a593e-aecc-4af9-905c-34ac34111a76/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 不同模型在 MMSI-Video-Bench 上的表现&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（2）错误分析揭示模型瓶颈&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为进一步定位模型性能受限的关键原因，研究团队对模型的推理结果进行了系统化复盘，并将错误归纳为五大类型:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;细致定位错误 (Detailed Grounding Error)&lt;/strong&gt;：模型在精细视觉感知层面出现失效，常见表现包括目标遗漏混淆，或 &amp;ldquo;时间点 - 事件&amp;rdquo; 对应关系感知错误。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ID 匹配错误 (ID Mapping Error)&lt;/strong&gt;：模型在跨帧过程中难以保持一致的实体身份跟踪。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;潜在逻辑推断错误 (Latent Logical Inference Error)&lt;/strong&gt;：模型在需要依赖隐含线索或常识知识的推理任务中失败。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;提示输入对齐错误 (Prompt Alignment Error)&lt;/strong&gt;：模型未能将提示信息（如背景假设、新增条件或辅助图像）与视频信息正确结合进行推理。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;几何推理错误 (Geometric Reasoning Error)&lt;/strong&gt;：模型在空间几何关系理解上存在偏差，对于相对位置或距离关系（如前后左右、远近）出现错误推断。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibx8UtMcbeYRibicfeo1OvpAjb1HyvOp5BtZ8YCNo1oWADNEMou8tqSicOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.49256505576208176" data-s="300,640" data-type="png" data-w="1076" type="block" data-imgfileid="503526721" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a3485c63-54f3-4522-9262-dc396875677e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; MMSI-Video-Bench 的五种错误类型示例&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;研究团队选取 Gemini-2.5-Flash、GPT-4o、O3、QwenVL2.5-72B 四个具有代表性的模型进行了系统的错误分析和统计，结果如图所示。几何推理错误是最为普遍、影响最大的错误类型，而进一步的细分分析表明：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;空间构建任务 的低表现主要源于几何推理能力不足；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;运动理解任务 中，模型难以在 快速、细微或长时间跨度的运动 中保持精确定位；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在 规划与预测任务 中，除几何推理错误外，模型往往无法有效理解提示输入，并将其与视频信息进行联合推理；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;跨视频推理任务 的失败主要源于 多目标跨视频定位的复杂性，以及模型难以利用潜在线索（如持续锁定同一目标）完成推理。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibYn8ZARXYibRu4sY6jAfApRiceGlofo8GYaFWCrQxwKdth8WPnI1fX5pA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5287037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526722" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/22cd3dde-695e-40d7-827c-7089163957fb/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; MMSI-Video-Bench 的五种错误类型分布&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;（3）空间线索与推理提示难以弥补核心能力不足&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队进一步探索了两种提升模型性能的策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;引入&amp;nbsp;&lt;strong&gt;3D 空间线索&lt;/strong&gt;&amp;nbsp;以辅助模型理解，如图所示，通过使用高性能的 3D 重建模型从视频帧重建 3D 场景，并多视角渲染生成 2D 全局图像作为额外输入，给予模型 3D 空间线索辅助模型的理解推理；&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibWXWc8Ju7dbkS6SiamseDCHianysfjnhZhDh4Bg6yZ4Rn7ZemLHEsTHVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.4935185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526723" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/ed02a42f-2745-4bfa-b353-887b46d2b312/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3D 空间线索辅助方法&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;采用 &lt;strong&gt;思维链（Chain-of-Thought）&lt;/strong&gt;技术，提示引导模型进行更规范的推理过程。上述方法均 未能带来显著的性能提升，这些结果进一步揭示了两个关键事实：&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如何设计模型真正 &amp;ldquo;可理解、可利用&amp;rdquo; 的空间线索，仍是一个开放且极具挑战性的问题；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当前模型的失败 并非由于缺乏显式推理步骤，而是受限于 底层推理能力本身仍然不足。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9oUEwHUybwRnJPtnGzVEiaibWqAplShSS2xicibLbJp68PufYrhibsx0PqLce3zBeHK4s6WZibrEL0eo5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.5018518518518519" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526724" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/416e026e-8117-48ea-b208-5589c12ed4d0/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 3D 空间线索辅助与思维链提示下的模型性能变化&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MMSI-Video-Bench 是一个高质量、高挑战性且系统全面的视频空间智能评测基准，系统性地评估了多模态大模型在视频理解中的空间认知、推理与决策能力，评测结果清晰揭示了当前模型在多项核心任务上与人类表现之间仍存在显著差距。基于深入而细致的实验分析，研究进一步明确了现阶段模型的关键能力瓶颈，并为未来空间智能模型的技术演进指明了研究方向。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
