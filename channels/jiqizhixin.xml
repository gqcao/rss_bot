<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>田渊栋2025年终总结：救火Llama4但被裁，现任神秘初创公司联创</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 16:34:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8424773e-17cc-41e0-990d-fa02d4896204/1767514913138.png" style="width: 700%;" class="fr-fic fr-dib"&gt;去年 10 月，Meta 人工智能部门的裁员波及到了一大波人，其中包括了知名华人科学家田渊栋及其团队成员。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526647" data-ratio="0.35833333333333334" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvec6g9tXXabibQOzlY0JvibauE7ejbf0UT0bZyqTBp5YTISsOp6CR5REQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d3dca246-b4cf-400b-bf92-1645e61105ea/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;就在这两天，田渊栋分享了自己的 2025 年终总结。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526634" data-ratio="0.36666666666666664" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvUeu6A4XtFHXTBVpwI9UquicAm3k67UzWOnyOb1zC4XI52GJicDPwG7gQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3e3c2d98-dd58-4e9b-893e-76cd72540370/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526635" data-ratio="0.36203703703703705" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvuD2txxHZzC9uE00XlaqESdqUy2WRB5anPBD6HxCTXaibuc49dJM8hvw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/cd2e5b79-8119-4a83-890f-377fa506a7af/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;他首先透露了自己「救火」Llama 4 项目的经历以及之后被裁、未来的工作规划；接着回顾了 2025 年的主要研究方向，包括大模型推理和打开模型的黑箱；最后探讨了 AI 驱动下的社会变革、生产力重构以及个人价值的存续逻辑。&lt;/p&gt;&lt;p&gt;接下来为田渊栋知乎原文内容。&lt;/p&gt;&lt;h3&gt;2025年终总结（一）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;关于被裁&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 2025 年 1 月底被要求加入 Llama4 救火的时候，作为一直以来做强化学习的人，我事先画了一个 2x2 的回报矩阵（reward matrix），计算了一下以下四种可能（虽然在那时，因为来自上面的巨大压力，不同意是几乎不可能的）：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526636" data-ratio="0.14907407407407408" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvEOFaib9lToC1ibZdQgTsQ6TMEGXVgicFDF5Wgv2LtuLvg15fgK7grjqzA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/092daf5d-2084-440f-9d66-a67617b3596a/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当时想的是我们去帮忙的话，即便最后项目未能成功，也至少尽力而为，问心无愧。不过遗憾的是，最后发生的是没在计算之内的第五种可能，这也让我对这社会的复杂性有了更为深刻的认识。&lt;/p&gt;&lt;p&gt;尽管如此，在这几个月的努力过程中，我们还是在强化学习训练的核心问题上有一些探索，比如说训练稳定性，训推互动，模型架构设计，和预训练 / 中期训练的互动，长思维链的算法，数据生成的方式，后训练框架的设计等等。这个经验本身是很重要的，对我的研究思路也带来了不小的转变。&lt;/p&gt;&lt;p&gt;另外其实我也想过在公司十年多了，总有一天要离开，总不见得老死在公司里吧，但总是因为各种经济上和家庭上的原因还是要待下去。最近一两年的说话和做事方式，都是抱着一种 &amp;ldquo;公司快把我开了吧&amp;rdquo; 的心态，反而越来越放开。2023 年年末我休第一个长假的时候，其实几乎差点要走了，但最后没签字还是选择待在公司继续，所以说真要做出离开的决定也不容易。现在 Meta 帮我做了也挺好。&lt;/p&gt;&lt;p&gt;这次波折和今年一年的起起落落，也为接下来的小说创作提供了非常多的新素材。所谓 &amp;ldquo;仕途不幸诗家幸，赋到沧桑句便工&amp;rdquo;，生活太平淡，人生就不一定有乐趣了。还记得 2021 年年头上的时候，因为在年末工作总结里面写了几句关于&amp;rdquo; 为啥 paper 都没中 &amp;ldquo;的反思，喜提 Meet Most，有一种突然不及格的懵逼感。但想了想与其到处抱怨世道不公，不如就在大家面前装成自己刚刚升职吧，结果半年后果然升了职，而那篇 21 年头上无人问津的工作，在 21 年 7 月份中了 ICML Best paper honorable mention，成为一篇表征学习中还比较有名的文章。&lt;/p&gt;&lt;p&gt;10 月 22 号之后的一段时间，基本上我的各种通信方式都处于挤爆的状态，每天无数的消息和邮件，还有各种远程会议或者见面的邀请，实在是忙不过来了。一直到几周之后才渐渐恢复正常。这两个月非常感谢大家的关心和热情。如果那时有什么消息我没有及时回复，请见谅。&lt;/p&gt;&lt;p&gt;虽然最后有不少 offer，大家能想到的知名公司也都联系过我，但最后还是决定乘自己还年轻，去当一家新初创公司的联合创始人，细节暂时不公开，先安静地忙活一阵吧。根据 Linkedin 信息显示，他已经于去年 12 月在这家公司上任。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526637" data-ratio="0.18518518518518517" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvcupzicP87od5baiapaoriaDUg3lrlnnHrISLqqbTUuSibrwS4EUQfEiatuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c7586d1e-5f84-48d2-880b-795415e766cc/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一些研究的方向&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2025 年的主要方向，一个是大模型推理，另一个是打开模型的黑箱。&lt;/p&gt;&lt;p&gt;自从 2024 年末我们的连续隐空间推理（coconut，COLM&amp;rsquo;25）工作公开之后，25 年在这个研究方向上掀起了一股热潮。大家探索如何在强化学习和预训练中使用这个想法，如何提高它的训练和计算的效率，等等。虽然我们组随后就被拉去 llama 干活，没能再继续花很大力气往下挖，但这个让我觉得非常欣慰。尽管如此，我们还是在上半年发了一篇理论分析（Reasoning by Superposition，NeurIPS&amp;lsquo;25）的文章，展示连续隐空间推理有优势的地方究竟在哪里，获得了不少关注。&lt;/p&gt;&lt;p&gt;另外是如何提高大模型的推理效率。我们的 Token Assorted（ICLR&amp;rsquo;25）的工作，先通过 VQVAE 学出隐空间的离散 token，再将所得的离散 token 和 text token 混在一起进行后训练，减少了推理代价的同时提高了性能。我们的 DeepConf 通过检测每个生成 token 的自信程度，来决定某条推理路径是否要被提前终止，这样推理所用的 token 减少了很多，但在 majority vote 的场景下性能反而更好。ThreadWeaver 则是通过制造并行推理的思维链，并在其上做后训练，来加快推理速度。另外我们也在 dLLM 上用 RL 训练推理模型（Sandwiched Policy Gradient），也有在小模型上学习推理的尝试（MobileLLM-R1）。&lt;/p&gt;&lt;p&gt;在可解释性方面，Grokking（顿悟）这个方向我大概两年前就在关注了。因为之前我做表征学习（representation learning）的分析，虽然能分析出学习的动力学过程，看到模型出现表征塌缩的原因，但究竟学出什么样的表征，它们和输入数据的结构有什么关系，能达到什么样的泛化能力，还是个谜团，而通过分析 Grokking 这个特征涌现的现象，从记忆到泛化的突变过程，正好能解开这个谜团。一开始确实非常难做没有头绪，2024 年先做了一篇 COGS（NeurIPS&amp;lsquo;25，见求道之人，不问寒暑（十）），但只能在特例上进行分析，我不是很满意。在一年多的迷茫之后，在和 GPT5 大量互动之后，最近的这篇 Provable Scaling Laws 的文章应该说有比较大的突破，能分析出之前的线性结构（NTK）看不到的东西，并把特征涌现的训练动力学大概讲清楚了。虽然说分析的样例还是比较特殊，但至少打开了一扇新的窗口。详细解释请看田渊栋的想法。&lt;/p&gt;&lt;p&gt;年末的这篇 The path not taken 我很喜欢，对于 RL 和 SFT 的行为为何会如此不一致，在权重的层面给出了一个初步的答案。SFT 造成过拟合和灾难性遗忘（catastrophic forgetting），其表层原因是训练数据不够 on-policy，而深层原因是权重的主分量直接被外来数据大幅修改，导致 &amp;ldquo;根基&amp;rdquo; 不稳，模型效果大降。而 RL 则因为用 on-policy 的数据进行训练，权重的主分量不变，改变的只是次要分量，反而能避免灾难性遗忘的问题，而改变的权重其分布也会较为稀疏（特别在 bf16 的量化下）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关于可解释性的信念&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;很多人觉得可解释性，或者 &amp;ldquo;AI 如何工作得那么好&amp;rdquo; 这个问题不重要，但我却觉得很重要。试想之后的两种场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;场景一：如果我们仅仅通过 Scaling 就达到了 AGI 乃至 ASI，全体人类的劳动价值都降为零，AI 作为一个巨大的黑盒子帮我们解决了所有问题，那如何让 AI 作为一个超级智能，一直行善，不欺骗不以隐秘的方式作恶，就是当务之急，要解决这个问题就要做可解释性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;场景二：如果 Scaling 这条路最终失效，人类在指数增长的资源需求面前败下阵来，必须得要寻求其它的方案，那我们就不得不去思考 &amp;ldquo;模型为什么有效，什么东西会让它失效&amp;rdquo;，在这样的思考链条之下，我们就必须回归研究，可解释性就是目所能及的另一条路了。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这两种情况下，最终都需要可解释性来救场。就算最终 AI 是个全知全能全善的神，以人类好奇和探索的天性，必然还是会去研究 AI 为什么能做得好。毕竟 &amp;ldquo;黑盒&amp;rdquo; 就意味着猜疑链的诞生，在大模型技术爆炸，开始达到甚至超过人类平均水平的今天，《三体》中 &amp;ldquo;黑暗森林&amp;rdquo; 的规则，也许会以另一种方式呈现出来。&lt;/p&gt;&lt;p&gt;目前打开训练好模型的黑箱，去找到电路（circuit），还是处于比较初步的阶段。可解释性真正的难点，在于从第一性原理，即从模型架构、梯度下降及数据本身的固有结构出发，解释为什么模型会收敛出这些解耦、稀疏、低秩、模块化、可组合的特征与回路，为什么会有大量不同的解释，这些涌现出来的结构和模型训练的哪些超参数相关，如何相关，等等。等到我们能从梯度下降的方程里，直接推导出大模型特征涌现的必然性，可解释性才算真正从生物式的证据收集走向物理式的原理推导，最终反过来指导实践，为下一代人工智能的模型设计开辟道路。对比四百年前的物理学，我们现在有很多 AI 版的第谷（收集数据），一些 AI 版的开普勒（提出假说），但还没有 AI 版的牛顿（发现原理）。&lt;/p&gt;&lt;p&gt;等到那一天来临的时候，我相信，世界一定会天翻地覆。&lt;/p&gt;&lt;h3&gt;2025年终总结（二）&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;未来会是什么样子&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;抛开前公司里每三个月一次的组织架构重组不谈，2025 年一年的变化本身已经很大。25 年年初的 Deepseek-R1 的发布，现在想来几乎已经算是上个世纪的事情了。带思维链的推理模型的巨大成功，让强化学习（RL）又回到了 AI 的主流视野之中，也带动了 AI4Coding 及 AI Agent 的发展，而后两者让大模型有了大规模落地，大幅度提高生产力的切实可能。&lt;/p&gt;&lt;p&gt;以前做项目，招人是很重要的一环，但现在脑中的第一个问题是 &amp;ldquo;还需不需要人？&amp;rdquo; 几个 Codex 进程一开，给它们下各种指令，它们就可以 24 小时不间断干活，速度远超任何人类，而且随便 PUA 永远听话毫无怨言。和 AI 工作，我最担心的是工作量有没有给够，有没有用完每天的剩余 token 数目。这也是为什么各家都在试验让 AI Agent 做几个小时连续不断的工作，看 AI 的能力上界在哪里。因为人的注意力永远是最昂贵的，人要休息，要度假，要允许有走神、睡觉和做其它事情的时间。减少人的介入，让 AI 自己找到答案，干几个小时活之后再回来看看最好。&lt;/p&gt;&lt;p&gt;这每个月交给 OpenAI 的 20 块钱，一定要榨干它的价值啊。&lt;/p&gt;&lt;p&gt;我突然意识到，就因为这区区 20 块钱，我已经成为了 &amp;ldquo;每个毛孔里都滴着血&amp;rdquo; 的肮脏资本家。我能这么想，全世界最聪明和最富有的头脑，也一定会这么想。&lt;/p&gt;&lt;p&gt;所以请大家丢掉幻想，准备战斗吧。&lt;/p&gt;&lt;p&gt;在帮忙赶工 Llama4 期间，我经常在加州时区晚上 12 点接到东部时区的组员消息，在伦敦的朋友们更是永不下线，熬夜折腾到凌晨四五点是寻常事，但大模型越来越强，辛勤劳动最终达到的结果，是看到大模型达到甚至超越我们日常作事的水准。&lt;/p&gt;&lt;p&gt;这应该说是一种陷入囚徒困境之后的无奈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;人类社会的 &amp;ldquo;费米能级&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果以后以 AI 为中心，那还需要人么？&lt;/p&gt;&lt;p&gt;如果考虑劳动力的投入 - 回报模型，传统思维会告诉你，工作经验积累越多，人的能力越强，回报也越大，是个单调上升的曲线。这就是为什么大厂有职级，职级随年限晋升，越老越香。但现在的情况已经不同了。职级已经没有意义，过去的经验也没有意义，人的价值从按照 &amp;ldquo;本人产出的劳动数量及质量&amp;rdquo; 来评估，变成了是否能提高 AI 的能力，人加 AI 要大于 AI 本身的产出，这样才行。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526638" data-ratio="0.5458984375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvb9ibc7SiaZpCfsGhFicrTEY8H8kCQiblaYCGFXl0e2wFJSiagY8hZclqvCA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=6" data-type="jpeg" data-w="1024" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/d7c742da-0c94-4a67-b660-15dd5bf39df2/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这样就让投入 - 回报曲线从一个单调递增曲线变成了一个先是全零，再在一定阈值之后增长的曲线（也即是 soft-thresholding 的曲线）。一开始人的能力是比不过 AI 的，而 AI 的供给只会越来越便宜，所以在很长一段成长期内，人本身是没有价值的。只有在人的能力强到一定程度之后，能够做到辅助 AI 变强，才开始变得有价值起来。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526640" data-ratio="0.5458984375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv5rB3QyfiaGc32cj9PSGsNP7fMssxjN0icPib5Z6AKqrtibH1X073Ehof8Q/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-type="jpeg" data-w="1024" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/f869485b-aac9-4bfd-8c98-8be837f32414/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;并且，在跨越阈值之后，厉害人对 AI 的加成，会高于普通人很多很多，因为普通人只会对 AI 的一两条具体产出花时间修修补补，而厉害的人在看了一些 AI 存在的问题之后，能提出较为系统性和普遍性的解决方案，结合手上的各类资源（GPU 和数据等），可以进一步让 AI 变得更强，而这种效应随着 AI 的广泛部署，会被几何级数地放大。&amp;ldquo;一骑当千&amp;rdquo; 这种小说笔法，将很快变成现实。&lt;/p&gt;&lt;p&gt;在这样一个非常两级分化的投入 - 回报模型之下，如果把人 + 所有个人能获取的 AI 当成一个智能体，整体来看，它的能力分布会和电子能级在材料里的分布很像：低于或达到某个水准线的智能体遍地都是，求着客户给它活干，以证明自己还是有用的；而高于这个水准线的智能体则指数级地变少，获取和使用它非常花钱，还常常排不到。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526642" data-ratio="0.5458984375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvZTNib5hrO7ea0ZRql7zvclvkLLeddUau7EBKdWQD8QEX2DpicDG6IvDA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=8" data-type="jpeg" data-w="1024" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/10fe7ea3-b46b-4a0c-81ca-1566a4c9902c/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这个水准线，就是 AI 洪水的高度，就是人类社会的 &amp;ldquo;费米能级&amp;rdquo;。低于费米能级的职业，可能在一夜之间就被颠覆掉，就像一场洪水或者地震一样，前一天还是岁月静好，后一天整个行业被端掉了。&lt;/p&gt;&lt;p&gt;随着时间变化，这条水准线还会一直往上走。其进展的速度，和它能获取到的，比它更强的数据量成正比。如果大模型的训练过程没有特别大的进展，那和自动驾驶无人车一样，越往上走，有用的数据是越来越少的，进展也会越慢，最顶尖的那部分人，还能在很长时间内保有自己的护城河。如果训练过程有突破，比如说找到新的合成数据手段，乃至新的训练算法，那就不好说了。&lt;/p&gt;&lt;p&gt;当然以上的判断是假设有无限的 GPU 和能源的供给，并没有考虑到各种资源短缺的情况。能源短缺，芯片产能短缺，内存短缺，整个地球能否满足人类日益疯狂增长的 AI 需求还是个未知数，这方面深究下去，或许可以做一篇论文出来。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;遍地神灯时代的独立和主动思考&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;那么，接下来会怎么样呢？&lt;/p&gt;&lt;p&gt;未来的世界，或许不再是传统故事里描绘的那样 &amp;mdash;&amp;mdash; 人们为了争夺稀缺的武功秘籍，或是千辛万苦寻找唯一的阿拉丁神灯、集齐七颗龙珠而展开冒险。相反，这将是一个 &amp;ldquo;遍地神灯&amp;rdquo; 的时代。每一个 AI 智能体都像是一个神灯，它们能力超群，渴望着实现别人的愿望，以此来证明自己的价值。&lt;/p&gt;&lt;p&gt;在这种环境下，真正稀缺的不再是实现愿望的能力，而是 &amp;ldquo;愿望&amp;rdquo; 本身，以及将愿望化为现实的那份坚持。&lt;/p&gt;&lt;p&gt;然而，在这个 AI 能力极其充沛的时代，巨大的便利往往伴随着巨大的陷阱。大模型提供了极其廉价的思考结果，在当前信息交互尚不充分的市场中，这些结果甚至可以直接用来交差并获取经济价值（例如那些一眼就能看出的 &amp;ldquo;AI 味&amp;rdquo; 文案）。这种唾手可得的便利，会让许多人逐渐失去思考的动力，久而久之丧失原创能力，思想被生成式内容和推荐系统所绑架和同化。这就是新时代对 &amp;ldquo;懒人&amp;rdquo; 的定义：不再是因为体力上的懒惰，而是精神上没有空闲去思考，没有能力去构思独特的东西。&lt;/p&gt;&lt;p&gt;最终，变成一具空壳，连许愿的能力都失去了。&lt;/p&gt;&lt;p&gt;那我们该如何保持独立思考？如何不被 AI 同化？战术上来说，我们需要学会不停地审视 AI 的答案，挑它的毛病，并找到它无法解决的新问题。未来的新价值将来源于三个方面：（1）新的数据发现；（2）对问题全新的深入理解；（3）新的路径，包括可行的创新方案及其结果。利用信息不对称来套利只是暂时的。随着模型越来越强，社会对 AI 的认知越来越清晰，这种机会将迅速消失。如果仅仅满足于完成上级交代的任务，陷入 &amp;ldquo;应付完就行&amp;rdquo; 的状态，那么在 AI 泛滥的今天，这种职位极易被取代。&lt;/p&gt;&lt;p&gt;就拿 AI Coding 来说，用多了，我会觉得它虽然可以很快弄出一个可以跑的代码库满足需求，但随着代码越来越长，屎山也越来越高，它贡献的代码也就越来越不如人意，还是需要人来做大的设计规划。如何调教它让它更快达成自己的长远目的，这个会成为人类独有价值的一部分。如果只是盲目地命令它做这个做那个，而不自己去思考如何做才能和它配合做得更好，那就会和大部分人一样停留在应用层面，而无法理解得更深入，就更不用说独一无二了。&lt;/p&gt;&lt;p&gt;战略上来说，无论主动还是被动，每个人都将面临从 &amp;ldquo;员工&amp;rdquo; 角色向 &amp;ldquo;老板&amp;rdquo; 或 &amp;ldquo;创始人&amp;rdquo; 角色的转变。这种转变的核心在于 &amp;ldquo;目标感&amp;rdquo;。如果心中有一个坚定的目标，并愿意动用一切手段（包括将大模型作为核心工具）去达成它，那么主动思考就是自然而然的结果。目标越远大，触发的主动思考就越多，激发的潜力就越大。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526643" data-ratio="0.5458984375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv6ficvOKY8P71oQHhS2DJI7ScOwuZVlTEIxgzqDDPicO6ssibDSTDwrOdQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=9" data-type="jpeg" data-w="1024" type="block" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/9db6b922-93d6-45cf-8c62-a31a1b08034d/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;因此，如果将来的孩子立志要去土卫六开演唱会，或者想在黑洞边缘探险，千万不要打压这样看似荒诞的志向。因为这份宏大的愿望，或许正是他们一辈子充满前进动力，主动思考的根本源泉，也是让他们始终屹立于 &amp;ldquo;费米能级&amp;rdquo; 之上的关键。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;知乎原文链接 1：&lt;a href="https://zhuanlan.zhihu.com/p/1990809161458540818"&gt;https://zhuanlan.zhihu.com/p/1990809161458540818&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;知乎原文链接 2：&lt;a href="https://zhuanlan.zhihu.com/p/1991073922217709984"&gt;https://zhuanlan.zhihu.com/p/1991073922217709984&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>科研人福音！一键生成PPT和科研绘图，北大开源Paper2Any，全流程可编辑</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 16:21:17 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/97cd12ca-73af-4e95-b58f-b71b385d4176/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;你是否经历过这样的至暗时刻： 明明实验数据已经跑通，核心逻辑也已梳理完毕，却在面对空白的 PPT 页面时陷入停滞； 明明脑海里有清晰的系统架构，却要在 Visio 或 Illustrator 里跟一根歪歪扭扭的线条较劲半小时； 好不容易用 AI 生成了一张精美的流程图，却发现上面的文字是乱码，或者为了改一个配色不得不重新生成几十次&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;在内容生产的过程中，&amp;ldquo;写&amp;rdquo; 往往只占了一半，而将文字转化为结构图、流程图，再整理成演示用的 PPT，这个过程繁琐、耗时，且极度考验设计感。为什么我们不能让 AI 像理解文字一样，理解我们的逻辑，并自动帮我们要展示的 &amp;ldquo;视觉物料&amp;rdquo; 准备好？&lt;/p&gt;&lt;p&gt;为了解决这一痛点，北京大学 DCAI 课题组 基于自动化数据治理 Agent 框架 DataFlow-Agent，推出了全新的多模态辅助平台 &amp;mdash;&amp;mdash; Paper2Any。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibekqYWo7JUbFwicgaf1YCxOb28LdzzK8k7B9eqjeLb7op5L5VbQkRiaicqw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.6138888888888889" data-type="png" data-w="1080" data-width="2216" data-height="1361" data-imgfileid="503526157" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3d7fda9a-ad08-4238-a004-39a7c44b906e/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeiaHXY1wcnjklMH07NIcVRRrj9P3XIEibpK7xCsvchCaziaoJHJ2pqjRCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5925925925925926" data-type="png" data-w="1080" data-width="2869" data-height="1699" data-imgfileid="503526158" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c9974b03-f1e5-4e70-a993-934d6797a68d/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;它不再是一个简单的 &amp;ldquo;文生图&amp;rdquo; 工具，而是一整套自动化的内容视觉化 Workflow。从阅读资料、理解逻辑，到生成图像、切割元素，最终输出完全可编辑的 PPT 和 SVG 文件，Paper2Any 正在试图重塑我们准备 Presentation 的方式。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;本地部署方式：&lt;a href="https://github.com/OpenDCAI/Paper2Any?tab=readme-ov-file#-linux-% E5% AE%89% E8% A3%85"&gt;https://github.com/OpenDCAI/Paper2Any?tab=readme-ov-file#-linux-% E5% AE%89% E8% A3%85&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;网页体验地址：&lt;a href="http://dcai-paper2any.nas.cpolar.cn/"&gt;http://dcai-paper2any.nas.cpolar.cn/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;文章多模态工作流 Paper2Any：&lt;a href="https://github.com/OpenDCAI/Paper2Any"&gt;https://github.com/OpenDCAI/Paper2Any&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;一、 核心突破：打破 &amp;ldquo;不可编辑&amp;rdquo; 的魔咒&lt;/h4&gt;&lt;p&gt;目前市面上的 AI 绘图工具虽然效果不错，但在科研与办公等场景下有一个致命缺陷：生成的图片是 &amp;ldquo;死&amp;rdquo; 的。 文字无法修改，模块无法拖拽，风格难以统一。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 820.328px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibek6G1ZNCvJetcmtYmkobtQDsmJiaibqKAAzmdRtloCcO7CWOSVzoqxMWg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5453703703703704" data-type="png" data-w="1080" data-width="1408" data-height="768" data-imgfileid="503526160" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8fed86ca-3701-4ba8-98cc-2f906be2c82f/640.png" alt="图片" data-report-img-idx="7" data-fail="0"&gt;&lt;span class="fr-inner"&gt;&lt;p data-pm-slice="0 0 []"&gt;工作流实现逻辑&lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 820.328px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeTJy3OIML1Mt5ZxFw9nN8AbhONlzDf1NSTwLibmSkSq9CC4Ao38WWa5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7481481481481481" data-type="png" data-w="1080" data-width="1287" data-height="963" data-imgfileid="503526161" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/9c3940bb-95ca-429b-9d53-29b3a931fe3a/640.png" alt="图片" data-report-img-idx="6" data-fail="0"&gt;&lt;span class="fr-inner"&gt;&lt;p data-pm-slice="0 0 []"&gt;生成示例PPT绘图&lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;Paper2Any 的核心差异在于它实现了从逻辑到结构化元素的映射。&lt;/p&gt;&lt;p&gt;系统内置的智能体首先对输入的文章或文本进行语义分析，提取核心贡献与思路。接着，它不仅生成视觉图像，更进一步对草稿图进行图文内容分割 &amp;mdash;&amp;mdash; 自动识别其中的文字、图表、结构模块、图标，并记录每个元素的元数据。&lt;/p&gt;&lt;p&gt;这意味着，你拿到的不再是一张不可直接修改的 PNG，而是一组独立、分层、可操作的图文块。用户可以在 PPT 中自由移动、编辑、替换、重新布局。（Paper2PPT 和 PPTPolish 功能暂时仅支持输出 PDF，可通过 PDF2PPT 功能将其结果转为可编辑 PPTX）。&lt;/p&gt;&lt;h4&gt;二、 功能全景：从草稿到演示的自动化闭环&lt;/h4&gt;&lt;p&gt;Paper2Any 目前支持的功能主要涵盖以下四大核心场景，旨在解决从 &amp;ldquo;输入素材&amp;rdquo; 到 &amp;ldquo;最终汇报&amp;rdquo; 的最后一公里问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Paper2Figure：智能科研绘图，草图变精图&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/qQqIZeYhMiOrzi1qeez93g"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/525c2410-0254-4c12-88cd-c3e4f344f29a/1767514539478.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gW9EricQXXByphb4tN0ha6mibeQy9roDYibpt78xU56za926nhg1yZkDM8Hibuiadr59gsk2UG6ISGfo8qg%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4319948465079271432" data-ratio="1.6127110228401191" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4319948465079271432" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="3248" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4319948465079271432"&gt;&lt;div data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5="" data-v-db5bdc2b=""&gt;&lt;br&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;用户无需从零学习复杂的矢量绘图软件。Paper2Figure 支持多模态输入（PDF、文本、甚至随手画的草图截图），系统便能自动识别你的意图。&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;模型架构图： 上传论文或描述，系统自动梳理模块连接关系，生成清晰的架构图。支持生成 SVG 和 可编辑 PPTX，图里的方框、线条都能动。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;技术路线图： 无论是中文还是英文，系统能根据方法论自动绘制流程与逻辑步骤。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;实验数据图： 扔给它一堆实验数据文本或表格，它能自动转化为可视化的对比柱状图或折线图。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Paper2PPT：文章结构化解析与 PPT 生成&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gW9EricQXXByphb4tN0ha6mibexcEJZEicZGs8EbmLMyNutvLWZBRxWNz5GzPOTichKd3topVwM1pjCs0w%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4319955297046839314" data-ratio="1.5932560590094837" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4319955297046839314" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="3024" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4319955297046839314"&gt;&lt;div data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5="" data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;a href="https://mp.weixin.qq.com/s/qQqIZeYhMiOrzi1qeez93g"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/15321460-455c-44b0-ba90-192c69e1c712/1767514581652.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;这是为 &amp;ldquo;赶进度&amp;rdquo; 的研究者和职场人准备的救星。Paper2PPT 不仅仅是简单的摘要生成，它利用算法对文档结构进行深度语义分析，提取背景、方法论、关键图表。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;三种输入模式： 直接上传 PDF 论文、粘贴长文本、或者仅仅输入一个研究 Topic（系统会自动深度搜索）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;自定义设置： 支持用户自定义幻灯片页数、风格及自由选择中英文语言；支持逐页生成 PPT，用户可自由调整每页 PPT 的大纲。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;超长 PPT 支持：首次支持制作超过 40 页的超长 ppt，无论是综述的演示还是深入研究某个主题都能一次满足！&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;中文适配与呈现： 可解决大模型生成 PPT 字体怪异及表达僵硬问题。输出结果采用标准中文字体与规范的排版，文案逻辑自然流畅，可减少 &amp;ldquo;AI 痕迹&amp;rdquo;，满足正式场合演示需求。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeiaV0PgTzmEjvZRPTtAttRWoc8s1uC5v51IO3TVanzib2wOXO3SZibUMBg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526167" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/53841032-2a3b-4b5b-bd78-a774a1b3fda7/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;PDF2PPT：让静态文档可编辑&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibenF0Wa8acjiasy7lOOyF08OEZOQp9XacOQ4kVZuoZe395a11fibYWuqsQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5666666666666667" data-type="png" data-w="1080" data-width="2217" data-height="1257" data-imgfileid="503526168" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/afe35107-01c2-4e21-a173-d8771df35162/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;你是否遇到过这种情况：手里只有一份 PDF 格式的讲义或报告，却需要对其进行修改和汇报？&lt;/p&gt;&lt;p&gt;PDF2PPT 模块利用 MinerU 与 SAM (Segment Anything Model) 模型，像 &amp;ldquo;拆积木&amp;rdquo; 一样对版面进行高精度解析，将原本锁死的 PDF 页面还原为可编辑的 PPTX。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;黑科技加持： 系统集成了 Gemini Nano 模型进行图像内补（Inpainting）。当系统将文字提取出来后，会自动修复文字覆盖区域的背景，实现 &amp;ldquo;去字留影&amp;rdquo;，最大程度还原原始底图的视觉效果。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;PPTPolish：交互式美化专家&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果你的 PPT 内容已经写好，但排版却有些简陋，PPTPolish 可以接手后续的美化工作。系统会自动分析页面并生成美化提示词，用户可以逐页修改提示词来微调美化方向。&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FKmXPKA19gW9EricQXXByphb4tN0ha6mibeCkAgXJDGJBfLNysAKJ9vqE5LOA4qwwiaoQ4wialRicmBXX1IKrD3FjEdA%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4319954344587476999" data-ratio="1.5932560590094837" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4319954344587476999" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="3024" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4319954344587476999"&gt;&lt;div data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5="" data-v-db5bdc2b=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;div data-v-6b6a00a5=""&gt;&lt;a href="https://mp.weixin.qq.com/s/qQqIZeYhMiOrzi1qeez93g"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9bc1bf83-7a25-4158-b5f4-ebc4771800c4/1767514615797.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/section&gt;&lt;h4&gt;三、 示例高能时刻：从输入到输出的 &amp;ldquo;视觉魔法&amp;rdquo;&lt;/h4&gt;&lt;p&gt;空口无凭，我们来看看 Paper2Any 的实际表现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;科研绘图：拯救手残党&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;模型架构图生成：&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. 论文 PDF &amp;rarr; 符合论文主题的架构图&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeMAsVicBialuE1wL5NhyWdQnGIClQZICHhGsUSUmvyNTerWBDljo5Yxpw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.825925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526181" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/cccafe73-059f-46fe-8f66-e131b3d29883/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;2. 科研配图 / 示意图截图 &amp;rarr; 可编辑 PPTX&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeiaWicRia3icQWaDYbSRgCvEXpU1jLLe1FO90ewXERH7nKJp9F0IPkgCqpA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.3435185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526182" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/c75a9988-42f6-4d50-8298-16eef07816ae/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;3. 论文摘要文本 &amp;rarr; 可编辑架构图&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeBZejS9unQfL4Ky1lv5jtHVicQTH6JiaL4X1bjk3BS0XTszlJkcsa1JNw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5333333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526183" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/999d5657-33c0-4f34-9b90-b92afd2d9880/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;技术路线图智能梳理：&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. 论文 PDF &amp;rarr; 符合论文主题的技术路线图&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe5shmB1lG8StmVz91wN75gLQlnzicFGeb97fGvKqNFTSJiblYeiawqaxJw/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.41203703703703703" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526184" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/87bd1869-dd32-4b31-80e4-dcae48914da7/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;2. 论文摘要文本 &amp;rarr; 符合论文主题的技术路线图&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeRKORp3dh0UXHgLXjVVicdvia0QiatrlRkrY9hbgoCjYeYVw8drzeJKyNQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.3824074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526188" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/f1f91047-7555-4408-a963-2c26180cbbfa/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;实验数据可视化：&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. 论文 PDF &amp;rarr; 自动提取实验数据绘制 PPT&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibedpDjicNHBkhVbiav9ibRwSZdc99vaPQSGLWibDoftygJUtL0rlkhTVJLvQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.46944444444444444" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526189" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/32749ffb-9389-435a-87f3-f8416a212456/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 820.328px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeJ4xGjzmXo1GID9uMj4tE3VUyED8gCOF29rtdY4lyZiaSVgkBQCFnDyg/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.4601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526190" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/a686c2bd-2794-49e6-be20-b882edde47f4/640.png" alt="图片" data-report-img-idx="11" data-fail="0"&gt;&lt;span class="fr-inner"&gt;&lt;p data-pm-slice="0 0 []"&gt;不同类型与不同风格的生成图示例&lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;2. 论文实验表格文本 &amp;rarr; 自动整理实验数据绘制 PPT&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibem5ZexxicLTBibxRdoeOX36xsEf9LBtcP9ds0gb9NJY2AvPicUibaXxiclVw/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.3138888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526191" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/1b9e2c26-402d-4efa-a520-5cb388c46284/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;PPT 智能生成与美化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;从文档到演示，Paper2Any 提供了全链路的解决方案。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Paper2PPT：&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeiclpZYib91jPfhtrhdmapHmT7knBEUtibOToian0MdNAyo6T1ibnWoEvttA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.9416666666666667" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526194" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/60288f0e-0dac-4410-9655-8f88f3558ef2/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibevHe42BSTA3McV42QpewCeAa9V3aKf3LUpsic4AyGt8ojmWbDcmbyISg/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.9027777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526195" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/e791743f-9419-4d65-9540-97d60342c64d/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibethPnmyiaHcWk1uTp6N4w0iapGtOA0ESDJDiayvibnPGLAP3gecjbzsU2icg/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.6768518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526196" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/991b84d5-bcad-4508-a204-6069c4a98d28/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeQln2FsMgs9BvVdwa1gbzCYWqVwUD5CNia0fGF5sxRavCz9picfT0muZg/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.4064814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526197" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/f7ee70ac-afba-4651-abb4-b98389e00494/640.png" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibejqlE3Qtgov3EBB7yygalrEldBs6kdK2tZV5aXGFw92xTO61SXibJObg/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-ratio="0.9675925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526198" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/f8f44938-7af8-4e6c-8df0-1a8470a6597d/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeCZzsh7H6WmzocFrqG6XLNzDFurqznlk6hckjiaNIZ6L3tFmXDgbL0IA/640?wx_fmt=png&amp;from=appmsg#imgIndex=20" data-ratio="0.9842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526225" data-aistatus="1" data-original-style="null" data-index="22" src="https://image.jiqizhixin.com/uploads/editor/b37c9bc1-4171-4c52-b2a3-e5acc56bc71f/640.png" alt="图片" data-report-img-idx="20" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeVXQVStROBfyLJTgcgfQW653Naj2Y4vgLmzI0OBic2lPOYRWwOjgPOmQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=21" data-ratio="0.5638888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526256" data-aistatus="1" data-original-style="null" data-index="23" src="https://image.jiqizhixin.com/uploads/editor/6f1721c0-6dab-4750-81a1-5f4978c27ae3/640.png" alt="图片" data-report-img-idx="21" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeicsmuM2DkWX6Z1nlDmABrZqMLibzOwJJG7HXj37X96jiceBGweS52WW8A/640?wx_fmt=png&amp;from=appmsg#imgIndex=22" data-ratio="0.5055555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526206" data-aistatus="1" data-original-style="null" data-index="24" src="https://image.jiqizhixin.com/uploads/editor/5d59735f-94c0-478e-8dc7-7fb91b9a4087/640.png" alt="图片" data-report-img-idx="22" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeDSrRcCG9zT5khovqVqZ724nDvU16qTaSdKibCMnh5oslIczsia79eib1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=23" data-ratio="0.5055555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526207" data-aistatus="1" data-original-style="null" data-index="25" src="https://image.jiqizhixin.com/uploads/editor/ff587460-3034-44f3-9bfe-c01da602d033/640.png" alt="图片" data-report-img-idx="24" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;与 Gemini 3 Pro、NotebookLM 相比，Paper2Any 生成的 PPT 有以下优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;结构化图表生成能力强&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;中文文字表达与字体呈现效果更自然&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;可读性更好，干货更多，排版布局更具专业感与人工感&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;PDF2PPT：&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe15E244WJgBLZyD4OJlczsPYf2ecEWviagmrA2hDuubkwFOdyRe8CCew/640?wx_fmt=png&amp;from=appmsg#imgIndex=24" data-ratio="0.3453703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526211" data-aistatus="1" data-original-style="null" data-index="26" src="https://image.jiqizhixin.com/uploads/editor/126de627-eeaa-4cbc-a67a-b9bb3faab6a5/640.png" alt="图片" data-report-img-idx="25" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;PPTPolish：&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. PPT 增色美化&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibezUxPd9ALjfdAOgzoiaibsCzkkw6lj24U9BD4G1hA5bBwiaqvzV07BOsTA/640?wx_fmt=png&amp;from=appmsg#imgIndex=25" data-ratio="0.6324074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526213" data-aistatus="1" data-original-style="null" data-index="27" src="https://image.jiqizhixin.com/uploads/editor/db8e31cc-38ab-4d23-8c30-17536cabb490/640.png" alt="图片" data-report-img-idx="23" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;2. PPT 润色拓展&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeiaz8ER9cT0kXb1Qk1z47y07Tic6KOBKgXVicJH2xW00l63YymIWKX3a2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=26" data-ratio="0.6370370370370371" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526214" data-aistatus="1" data-original-style="null" data-index="28" src="https://image.jiqizhixin.com/uploads/editor/f558ae90-6d6e-4627-aa09-67ca8316b254/640.png" alt="图片" data-report-img-idx="26" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;原始 PPT 只是简单的文字罗列；润色后，系统自动添加了科技感背景、可视化图标、以及逻辑图示，瞬间提升汇报档次。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、 如何使用与部署&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Paper2Any 提供两种使用方式：&lt;/p&gt;&lt;p&gt;1. 本地部署（开发者推荐）&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你希望深入研究、二次开发或本地运行，可以基于 Github 仓库进行本地部署。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Github 仓库：&lt;a href="https://github.com/OpenDCAI/Paper2Any"&gt;&amp;nbsp;https://github.com/OpenDCAI/Paper2Any&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;快速开始指引： &lt;a href="https://github.com/OpenDCAI/Paper2Any?tab=readme-ov-file#-linux-% E5% AE%89% E8% A3%85"&gt;https://github.com/OpenDCAI/Paper2Any?tab=readme-ov-file#-linux-% E5% AE%89% E8% A3%85&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;参考 Readme 文档启动 Web 前端即可。&lt;/p&gt;&lt;p&gt;2. 网页版快速体验&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队已推出可视化的 Web 前端，支持拖拽上传与实时进度展示。新用户可免费注册，登录后可查看历史使用记录。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;访问地址： &lt;a href="http://dcai-paper2any.nas.cpolar.cn/"&gt;http://dcai-paper2any.nas.cpolar.cn/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;结语：让配图成为一种「自动获得的附加值」&lt;/p&gt;&lt;p&gt;Paper2Any 的愿景，是希望建立一条新的科研与工作惯例：写文章 + 一键配图 + 一键生成 PPT + 一键展示。&lt;/p&gt;&lt;p&gt;在未来，课题组计划陆续支持 Paper2Rebuttal（论文返修）、Paper2Idea（创新点生成）和 Paper2Poster（文章海报生成）等更多的多模态功能。我们相信，工具的价值在于释放人类的创造力，让你从繁琐的格式调整中解脱出来，将宝贵的时间投入到那些真正闪光的 Idea 之中。&lt;/p&gt;&lt;p&gt;欢迎大家关注使用 DCAI 的开源项目并与我们进行技术交流，如果觉得好用也请在 GitHub 仓库点一个 star ~&lt;/p&gt;&lt;p&gt;Data-centric AI 开源项目：&lt;/p&gt;&lt;p&gt;文章多模态工作流 Paper2Any:&lt;a href="https://github.com/OpenDCAI/Paper2Any"&gt; https://github.com/OpenDCAI/Paper2Any&lt;/a&gt;&lt;/p&gt;&lt;p&gt;自动化数据治理 Agent 框架 DataFlow-Agent: &lt;a href="https://github.com/OpenDCAI/DataFlow-Agent"&gt;https://github.com/OpenDCAI/DataFlow-Agent&lt;/a&gt;&lt;/p&gt;&lt;p&gt;LLM 数据准备系统 DataFlow (1.9k star): &lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;https://github.com/OpenDCAI/DataFlow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;DataFlow 技术报告（&lt;a data-topic="1" href="javascript%3A;"&gt;#1&lt;/a&gt; of the Hugging Face daily paper）: &lt;a href="https://arxiv.org/abs/2512.16676"&gt;https://arxiv.org/abs/2512.16676&lt;/a&gt;&lt;/p&gt;&lt;p&gt;LLM 数据训练系统 DataFlex (基于 LLaMA-Factory): &lt;a href="https://github.com/OpenDCAI/DataFlex"&gt;https://github.com/OpenDCAI/DataFlex&lt;/a&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>从「被动」到「主动」，为什么给耳机装上「眼睛」后AI范式变了？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 14:44:01 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/76faa914-85ab-4639-b140-468b4bef28ab/1767508636102.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;h4&gt;先行一步&lt;/h4&gt;&lt;p&gt;Sam Altman 与 Jony Ive 联手探索的无屏 AI 硬件，正在被逐步揭开。供应链信息显示，这款产品并没有选择屏幕，而更像是一种可穿戴设备：体积接近 iPod Shuffle，可以放入口袋或随身佩戴；内置麦克风与摄像头，持续感知用户所处的真实环境，与之并肩工作，主动给出建议。&lt;/p&gt;&lt;p&gt;在「无屏、主动式 AI」这条路径上，中国公司其实已经先行一步。&lt;/p&gt;&lt;p&gt;12 月底，光帆科技在北京发布了 Lightwear AI 全感穿戴设备。这是一套由 AI 耳机、智能手表以及设计独特的充电盒组成的组合式终端。其中，AI 耳机也是全球首款具备视觉感知能力的主动式 AI 耳机。&lt;/p&gt;&lt;p&gt;三款设备实时协同，扮演一个「始终在场」的 AI 助理 ，与你一同观察世界，并主动参与日常生活与决策。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526619" data-ratio="0.6330935251798561" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvhMxvDpLfFpAEAcCFBCISzgia3Fj2GbUAR8J4ibiaPv0MWe0n4U22RlndA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="556" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1529d019-91c2-4d8f-9374-2f07fafa078e/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;des&gt;Lightwear AI 全感穿戴设备，这是一个由 AI 耳机、智能手表以及设计独特的充电盒组成的套装。&lt;/des&gt;&lt;p&gt;「喂，晓帆。」一名戴着耳机的女孩在超市里购物，拿起一瓶饮料，随口喊了一句。发布会现场，出现了这样一个场景。&lt;/p&gt;&lt;p&gt;「在呢。」 隐身在耳机里的 AI 助理被唤醒。&lt;/p&gt;&lt;p&gt;「这个在网上咋卖？」女孩问。AI 「看」了一眼她手中的商品：识别出商品名称，随即在网上搜索同款价格 &amp;mdash;&amp;mdash;500 毫升 15 瓶，57.9 元，更便宜。&lt;/p&gt;&lt;p&gt;在女孩的确认下，AI 直接完成下单。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvd9lefoRI8wMagiasbJ5X3MXZzexRPWzIicmYyxGBdKucbpjDeiawNIIHQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="0.4981949458483754" data-s="300,640" data-type="jpeg" data-w="831" type="block" data-imgfileid="503526618" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/6fab65ae-bc54-4225-9f82-2902dd8a3cc2/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;des&gt;耳机黑色部分就是 AI 的眼睛，为 AI 提供视觉感知的摄像头。&lt;/des&gt;&lt;p&gt;类似的主动能力，并不只体现在购物场景中。耳机盒内置 GPS，当用户快到家时，晓帆会主动提醒有快递要取。&lt;/p&gt;&lt;p&gt;在另一个更长任务的演示中，用户只用表达需求，AI 主动把事情完成，并告诉你结果，中间沟通个一两次就行。&lt;/p&gt;&lt;p&gt;整个流程从一句「XX 问你什么时候有空和王总吃饭」开始。晓帆自动检查日程冲突，发现约饭时间与一场产品会议重叠后，按用户要求调整了会议安排。&lt;/p&gt;&lt;p&gt;随后，它继续主动询问是否需要一并处理机票和酒店：机票按照「再早一点」的要求重新预订；酒店则直接按「常住的那一家」定了两晚。&lt;/p&gt;&lt;p&gt;这些场景，都映射出光帆科技试图呈现的主动式 AI 雏形。&lt;/p&gt;&lt;p&gt;发布会之后，这家创业公司也迅速受到关注。其创始人董红光是小米早期员工（第 89 号），长期负责操作系统与智能化相关核心工作，几乎贯穿了小米多个关键技术阶段。成立仅一年多时间，光帆科技便吸引了一批颇具分量的投资机构入局，也为这条「无屏、主动式 AI」路径增添了更多现实注脚。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvTHxwSz9as1HXDtTA5dwS1bqdL9iaVoN4PiaoXFiaDXpV2nUQeCUXWa31A/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.6658653846153846" data-s="300,640" data-type="png" data-w="832" type="block" data-imgfileid="503526620" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f9f51a58-3d52-4be6-a015-d4f81846504d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;AI 硬件大爆发，被动式 AI 面临挑战&lt;/h4&gt;&lt;p&gt;在光帆科技压轴登场之前，仅在 2025 年这一年里，全球范围内就已密集涌现出一批 AI 硬件产品。阿里推出夸克 AI 眼镜，字节加码 AI 耳机、AI 手机，同时还有 AI Pin、戒指、项链、手环等更具「脑洞」的新形态。&lt;/p&gt;&lt;p&gt;AI 正在加速脱离屏幕，为自己寻找新的「肉身」。而这场 「物种大爆发」，并非偶然。&lt;/p&gt;&lt;p&gt;一方面，大模型能力持续跃迁，终于能够支撑复杂场景的理解，以及长链路任务的稳定执行（如 AI Agent）；响应速度也被拉进「1 秒俱乐部」，交互体感开始逼近真人对话。&lt;/p&gt;&lt;p&gt;另一方面，推理与部署成本持续下探，再叠加中国在制造与供应链上的系统性优势，让中国玩家在这一轮 AI 硬件竞赛中显得尤为活跃。&lt;/p&gt;&lt;p&gt;但问题，也同样清晰。&lt;/p&gt;&lt;p&gt;大多数 AI 硬件已经足够贴身，却并不「始终在场」；看起来随时可用，却仍在等待一道清晰的命令。这依然是一种被动式智能，存在认知摩擦。&lt;/p&gt;&lt;p&gt;比如，你需要先掏出手机、打开 App，再用近乎「产品经理式」的方式，把真实需求拆解成一段段包含关键词的 Prompt；又或者，只有在你主动提问「这是什么？」时，AI 眼镜才会启动识别并给出反馈。至于耳机，更是高度依赖语音唤醒和明确指令。&lt;/p&gt;&lt;p&gt;主动式智能正试图消除的就是这种负担。它会持续进行云端计算，感知、理解用户所处的情境（「你现在在超市」）+ 记忆（「你记得要买果汁」），在合适的时机（「你路过商店」），在你尚未开口之前主动介入 &amp;mdash;&amp;mdash;「别忘了，顺手买果汁。」&lt;/p&gt;&lt;p&gt;事实上，谷歌的 Project Astra 一直在尝试构建这样一个主动的 AI 助手：拥有眼睛、耳朵和声音，能够与你共处、理解你正在经历的世界。这与光帆科技所追求的、带有「活人感」的 AI 助理 &amp;mdash;&amp;mdash; 全天候、全感知、主动智能 &amp;mdash;&amp;mdash; 在理念上高度一致。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvJuMc47djI1LmLHU1nFJ76utuQo6NKJOxR5a79ID58twJcYljdriciaSg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5755395683453237" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526621" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/4e41af5d-9ebd-4830-9984-2c2f5094b93b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;只不过，Project Astra 尚未脱离手机；而光帆科技的选择，是让 AI 不再依附于手机、建立新的交互范式。但是，这样的 AI 硬件，究竟该如何搭建？&lt;/p&gt;&lt;p&gt;他们先从「AI 需要感知什么、怎么感知」出发，逐步决定是否要做加法、怎么加。&lt;/p&gt;&lt;h4&gt;「看得见」，是主动智能的门票&lt;/h4&gt;&lt;p&gt;在硬件形态上，光帆科技没有选择已有手机做加法，或是更为主流的眼镜，而是对耳机进行「改造」，在上面装上摄像头。看似反直觉的选择背后，隐藏着他们的清晰认知：视觉感知，是主动智能的门票。&lt;/p&gt;&lt;p&gt;而要做到随时看、随时听、随时跟用户说话，手机和眼镜很难满足。&lt;/p&gt;&lt;p&gt;手机，是为触控交互而生，依赖显式唤醒、依赖用户主动将注意力集中到一块屏幕上，从根本上限制了 AI 的「持续观察力」。而且，手机大部分时间都放在口袋里，无法主动感知，用户也无法随时与之交流。&lt;/p&gt;&lt;p&gt;眼镜似乎更为自然，包括 AI 大厂和初创都很看好，但从长期来看，也并非「最优解」。&lt;/p&gt;&lt;p&gt;首先，在用户接受度上就不太友好，尤其是很多非近视人群根本没有戴眼镜的习惯，而且重。技术层面，精密结构下，电池容量、重量、功耗（尤其叠加 AR 后）之后，很难平衡。而一旦进入「持续视觉扫描」状态，摄像头正对路人，隐私与伦理压力几乎不可避免。&lt;/p&gt;&lt;p&gt;耳机就不同了。用户体量大、接受度高、佩戴自然，选择给耳机装上摄像头，并非简单的硬件堆砌，而是一套围绕感知能力的重构 &amp;mdash;&amp;mdash; 在耳机已有听觉感知的基础上，在左右耳塞各置一枚 200 万像素摄像头，实现双目视觉感知，并配合充电盒进行辅助定位。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvgXLzlDCTasHxr4jibb9fJmibpGHHTdNwKc0N3pbS43fdDzGHiaEEve4ng/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6258992805755396" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526622" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3ecfe787-2b3b-4cad-b353-93f23075b30e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这里的摄像头拍摄，不是给人看，是让 AI「看」，用以理解物理世界的空间与物体，支持「阅后即焚」，不必担心隐私问题。&lt;/p&gt;&lt;p&gt;只有 200 万像素，其实是蕴含着一个重要的「低像素哲学」：更强调「语义理解」而非「光学美感」，AI 无需欣赏 4K 画质的电影，只需要能分辨出用户手中拿的是橙汁、咖啡，还是药品，就足够了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvoRkmsicfgkc6mwz0Dd9kcnatR0SxGTTON3FEAm9icVqOkdRFFicdIEzKA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.697841726618705" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526623" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/2f4b7ac4-878c-4835-9986-8951da2a4006/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;真正的关键在于 &amp;mdash;&amp;mdash; 只叠加了一个「视觉感知」，一切都因此而变得不同，因为，视觉是「主动性」的唯一基石。&lt;/p&gt;&lt;p&gt;主动智能的本质，在于主动感知环境、理解上下文并预测行动时机。而这一能力首先依赖对真实世界空间结构、物体关系与动态变化的持续感知，这些关键信息只有视觉能够提供。&lt;/p&gt;&lt;p&gt;而耳机「双目」的视觉高度，恰好与人类视野持平 &amp;mdash;&amp;mdash; 你看到什么，它就看到什么。于是，AI 可以实时理解你所处的情境，建立稳定的世界模型，判断你的关注焦点，形成「共同注意力」。&lt;/p&gt;&lt;p&gt;没有视觉，AI 无法真正理解世界；没有世界模型，就不可能有真正的主动协作。语音、记忆、推理，只有嵌入视觉框架，才会产生质变。&lt;/p&gt;&lt;p&gt;比如，当用户在路过超市时，AI「看到」用户所处的环境，其「记忆」模块才能被激活，主动发出提醒，「该买橙汁了。」&lt;/p&gt;&lt;p&gt;当用户看到心仪餐厅，想要进一步了解，发出「帮我看下这家餐厅怎么样」的提问指令时，AI 只有「看到」餐厅后，才能启动实现个性化口味比对、附近更优餐厅推荐、餐厅位置准确告知等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvPc8lgnqGu3OV5EYpia9kPYUF675JJSB4lgRC72iaKUibibeibDNUNACqib0A/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.5503597122302158" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526624" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d15a90d5-85a6-4809-9d39-5b1b5baf1ccb/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;从单兵作战到多感官协同，主动智能的必经之路&lt;/h4&gt;&lt;p&gt;要实现真正的主动式 AI，只「薅」一个硬件显然不够。&lt;/p&gt;&lt;p&gt;哪怕是最核心的耳机，也会不可避免地面临感知盲区 &amp;mdash;&amp;mdash; 比如身体出现异常，AI 根本无从得知。&lt;/p&gt;&lt;p&gt;更现实的问题是，人在睡觉、洗澡、刚起床等场景下，并不会持续佩戴耳机；一些关键信息，也很难长期依赖记忆来维持。&lt;/p&gt;&lt;p&gt;只有走向多感官协同，主动智能才可能真正成立，并逐步逼近全天候、全感知的状态。基于这一判断，在为耳机补上视觉能力之外，光帆科技还为系统引入了一块手表：耳机负责「听」和「看」，手表负责「显示」和「触控」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvbPYzcEXXqhj8yiaUX61shz8mh7lINxvNga4PuOfD18kdN2NESvIDfiaQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.6942446043165468" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526625" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/3ccd22bd-06ea-48ef-9e98-d8b853b5a714/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;首先，手表补齐了语音交互的短板。&lt;/p&gt;&lt;p&gt;那些并不适合通过声音完成的信息交互 &amp;mdash;&amp;mdash; 例如购物验证码、导航定位、简单提示 &amp;mdash;&amp;mdash; 可以直接在屏幕上呈现，降低打扰，也提升效率。&lt;/p&gt;&lt;p&gt;更关键的是，手表本身是一枚持续工作的身体传感器。&lt;/p&gt;&lt;p&gt;如果 AI 想要更主动、更贴近个体，就必须理解「人」的状态，而不仅仅是环境。通过持续采集心率、血氧、睡眠、压力等数据，AI 才能感知身体变化，并在合适的时刻给出针对性的提醒与建议。例如在运动中心率异常升高时，主动介入。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv6rJ99B3uRWXRQFUO2VTNwy2n1xhxeHcVjPQiasr2ZRsIPUdA2buqmcA/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.6546762589928058" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526626" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/893704d9-c217-4d21-870b-cedb0886aa90/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;与此同时，光帆科技还对耳机充电盒进行了功能重构。&lt;/p&gt;&lt;p&gt;它内置 2020mAh 电池， eSIM 卡与定制化 AI 通信协议，可脱离手机直接联网，还内置高精度 GPS；同时集成算力、独立麦克风和扬声器，即便不佩戴耳机，也可以通过语音与 AI 进行交互。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvEVic4tYPFyMoahibY9nUsGzSW8A07FSxLyKcld9DM2icPMAficTsz5xuhw/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.7913669064748201" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526627" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/16d229c7-7940-42fa-bf99-8e623f9edcbb/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tviatn4vibPVYVRFh6WSomRyP59YXKySLVywyuzId61tItVsibxk0HVqiceQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.60431654676259" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526628" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/6f51e4a5-7428-470c-a7d0-7e3e9e241bc1/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;des&gt;充电盒上的独立麦克风。&lt;/des&gt;&lt;p&gt;因此，在洗澡、起床、阅读等「不想戴耳机」的场景下，用户依然可以与 AI 保持基本互动，例如询问当天的天气或日程安排。&lt;/p&gt;&lt;p&gt;这种分布式协作的思路，并非个案。&lt;/p&gt;&lt;p&gt;在 Meta 的 Orion 项目中，除了眼镜本体，还配套了一个手势追踪腕带，以及一个遥控器大小的计算模块，三者通过无线方式协同工作。其中，腕带用于读取与手势相关的神经信号，帮助 AI 更精准地理解用户意图。&lt;/p&gt;&lt;p&gt;从这个角度看，手表、耳机、眼镜，乃至充电盒，并不是彼此替代的竞争关系，而是在不同位置、不同维度，分别承担 AI 助理的「感官」与「分身」。它们分工协作、彼此补位，最终目标是一件事：让 AI 真正「在场」，并主动融入生活。&lt;/p&gt;&lt;p&gt;再往远处看，设备的边界只会持续模糊。光帆科技对主动智能的判断是：未来一定是多设备联动，由一个统一的 AI 大脑进行调度。基于自研操作系统，他们后续还将接入更多形态的终端 &amp;mdash;&amp;mdash; 例如脖挂、眼镜、项链等。&lt;/p&gt;&lt;h4&gt;无人区的艰难跋涉&lt;/h4&gt;&lt;p&gt;主动智能，不属于某一件硬件，而属于一个协同运作的分布式系统。&lt;/p&gt;&lt;p&gt;而做这样一套分布式 AI 硬件，并不是把耳机、手表、充电盒简单叠加，而是一场关于算力如何分配、设备如何低功耗通信，以及人机工程学如何取舍的极限运动。&lt;/p&gt;&lt;p&gt;其中最核心、最根本的问题是：如何让一个只有几克重的设备，承载起接近大模型的「灵魂」？&lt;/p&gt;&lt;p&gt;光帆科技的解法，是自研一套端云结合的操作系统：Lightware OS，不是把所有能力都塞进单一设备，而是建立一种类似「生物神经系统」的层级分工与调度机制。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvV7zW8rsu438yIUeJPgYSfCGtp37jd19axTJeXFO5mW031CbAGx0Ylw/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.6546762589928058" data-s="300,640" data-type="png" data-w="556" type="block" data-imgfileid="503526629" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/b00dd2d2-2ffd-4e9b-b7ce-99e9217eb595/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;最「聪明」、算力最强的大脑，放在云端，负责调用不同的大模型，完成语音与图像理解、意图识别，以及复杂推理与决策。&lt;/p&gt;&lt;p&gt;比如，结合你的位置、你看到的招牌，以及历史评价等信息，判断这是一家什么类型的餐厅、口碑如何、值不值得走进去 &amp;mdash;&amp;mdash; 这些都交给云端完成。&lt;/p&gt;&lt;p&gt;随身携带的充电盒，同样具备算力，但它并不负责「深度思考」，而是反应足够快、兜底足够稳。&lt;/p&gt;&lt;p&gt;内置 4G eSIM 保证「永不掉线」。它是流量的调度站，在毫秒级内判断请求类型（是查地图还是听歌），瞬间将音视频流推向云端。同时，在网络波动时利用本地算力进行「行为缓冲」，避免 AI 变成「人工智障」。&lt;/p&gt;&lt;p&gt;至于耳机，更像是全天候的「感官末梢」，负责「听」和「看」，只跑最轻量的 AI 任务（如语音唤醒、低像素物体轮廓识别），让这些能力在后台长时间「静默运行」，以极低功耗换取随时在场的体验。&lt;/p&gt;&lt;p&gt;另一个同样棘手的问题，是如何恰如其分地与用户交互。&lt;/p&gt;&lt;p&gt;一个缺乏分寸感的 AI 助手，很快就会从「贴心」变成「打扰」，最终被用户关闭。&lt;/p&gt;&lt;p&gt;因此，在 Lightware OS 中，系统层必须具备对场景的判断能力：用户是否忙碌？当前是否适合打断？这一次介入是否真的有价值？这种对「干扰优先级」的判断，无法只靠给大模型写一段 Prompt 解决，而必须被写进系统的底层逻辑中。&lt;/p&gt;&lt;p&gt;如何让这套分布式硬件长期、可靠地作为一个整体运行，同样是一道工程难题。&lt;/p&gt;&lt;p&gt;哪怕只看端侧，多设备之间的实时通信本身就已经足够复杂；更现实的是，单个设备内部往往也不止一颗芯片，芯片之间如何高效协作，直接决定了系统稳定性。这不是「写好一个程序」就能解决的问题，而是必须在硬件层、驱动层、通信层同时成立。&lt;/p&gt;&lt;p&gt;还有硬件工艺上的「极限平衡」。在耳机这样极度受限的形态中加入摄像头，意味着必须同时权衡体积、重量、续航、散热与佩戴舒适度。&lt;/p&gt;&lt;p&gt;最终，加入摄像头和更大电池后，单只耳机重量被控制在 11g，远低于常见智能眼镜约 40g 的重量，佩戴舒适度和行业头部的耳挂式耳机相当，并无明显不适和异物感。&lt;/p&gt;&lt;p&gt;这几年，CES 一直是「杀手级 AI 硬件」想象力的集中展示场。在众多方向中，个人穿戴与随身设备始终是焦点。而耳机这一高频入口，也正在被重新定义。&lt;/p&gt;&lt;p&gt;2026 年 1 月 6-9 日，光帆科技将携全球首款主动式 AI 耳机亮相 CES。下一代 AI 硬件的方向，或许正藏在这些看似熟悉、却正在被重新塑造的随身设备之中。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvab5d5XwYqic0lzcG0qq497NicAgrIg8S1SRd8ibFiaglwTktGLjszSqAwg/640?wx_fmt=jpeg#imgIndex=13" data-ratio="1.256578947368421" data-s="300,640" data-type="jpeg" data-w="1064" type="block" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvgL6rGq8ZMicqicXzG4G9apbtRUA3iacF2icz9Z2XL0ib2JWeYwHXC8CWwmg/640?wx_fmt=jpeg&amp;from=appmsg" data-cropx2="1064.626334519573" data-cropy2="1337.508896797153" data-imgfileid="503526649" data-aistatus="1" data-original-style="width: 554px;height: 696px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/cc78cd38-3509-4ee8-ad25-7d04606273bf/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>500万人在线围观，Claude Code创建者的13条独家实战秘籍爆火</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 14:35:26 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3b47561e-e621-4bed-a5a7-80f80ca0bdee/1767508320970.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;2026 新年第三天，Claude Code 创建者、负责人 Boris Cherny 开展「线上教学」，亲自示范他自己使用这个 AI 编程工具的工作流。&lt;/p&gt;&lt;p&gt;他表示，自己的配置可能出乎意料地「素」（即简单）！Claude Code 开箱即用非常出色，所以他个人并没有做太多自定义。&lt;/p&gt;&lt;p&gt;使用 Claude Code 没有所谓的「标准答案：在设计时就希望它是可定制、可 Hack 的。用户完全可以按自己的喜好来使用。事实上，Boris 团队里的每个人用它的方式都大不相同。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503526654" data-ratio="0.6037037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvyn9wtvqPlSWeqyZj7kja3xeAgjFZIU085cdK98KrtE3UkASxMryic6w/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/35ed452a-2eed-4b32-be5b-399952ec60f0/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;一、五线并行&lt;/h4&gt;&lt;p&gt;在终端里同时运行 5 个 Claude 窗口，给这些标签页排上 1 到 5 号，并开启系统通知，这样当某个 Claude 需要他输入指令时，便会立刻收到提醒。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvG8Sv1yicOauH1iaDQBelJD3Ttb2NF2IHOvicxmFjaLBfAW3jFbooZXF0g/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6555555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526655" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b2b6bffd-e804-4785-95cc-b71b5a0ccca7/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;二、多端无缝衔接&lt;/h4&gt;&lt;p&gt;除了本地终端，他还会同时在网页端（&lt;a href="http://claude.ai/code"&gt;http://claude.ai/code&lt;/a&gt;）运行 5 到 10 个 Claude 任务。&lt;/p&gt;&lt;p&gt;在终端写代码时，他经常用 &amp;amp; 把本地会话交给后台，或者直接在 Chrome 里启动新会话。有时还会使用 --teleport 命令在两者之间「传送」进度。&lt;/p&gt;&lt;p&gt;每天早上甚至会用手机（iOS 版 Claude App）启动几个会话，之后再回电脑上查看进度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvsd1gpoRWh50Cia9ibINIQdnl2zoIxZVkicUSUibw1fUyaInicPHbjmVMCAw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.0144927536231885" data-s="300,640" data-type="png" data-w="966" type="block" data-imgfileid="503526657" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/df3ac330-9a86-43dc-9bb8-41f039aa0ea7/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;三、全力投入 Opus 4.5&lt;/h4&gt;&lt;p&gt;他会给所有任务开启 Opus 4.5 (带 Thinking 模式)，这是他用过的最强编程模型。&lt;/p&gt;&lt;p&gt;虽然它比 Sonnet 更大、更慢，但同时它更聪明、更擅长调用工具，不需要费心去引导它，所以从结果来看，它通常反而比小模型更快完成任务。&lt;/p&gt;&lt;h4&gt;四、共享知识库：CLAUDE.md&lt;/h4&gt;&lt;p&gt;团队共用一个 CLAUDE.md 文件。他们把它存放在 Git 仓库里，团队成员每周都会多次更新。只要发现 Claude 哪里做错了，他们就把规矩写进 CLAUDE.md，确保它下次不再犯同样的错误。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv3tUEcbMm3zwKntEAzoO8BSxvJQRT15BmNBwH512icTXpyVUUic4HEXpw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7775700934579439" data-s="300,640" data-type="png" data-w="1070" type="block" data-imgfileid="503526659" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/70ff3d4b-5fc0-4d94-98fe-9cab550d3988/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;五、持续复利：代码评审&lt;/h4&gt;&lt;p&gt;在代码评审（PR）时，他经常会 @.claude，让它把同事 PR 中的一些规范沉淀到 CLAUDE.md 中。他们通过 /install-github-action 安装了 Claude Code 的 GitHub Action。这就是他们版本的「复利工程」（Compounding Engineering）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv1LibgFWfqPeEF9uvxN7pSScYFiaGibRcibAqYQicXpG9kPmeRPuqqlecZ7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.8453703703703703" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526660" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/6b028417-91dd-4d44-8af4-6f236389cbae/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;六、谋定而后动：Plan 模式&lt;/h4&gt;&lt;p&gt;大多数任务都从 Plan 模式开始（连按两次 Shift+Tab）。如果目标是写一个 PR，他会先在 Plan 模式下反复和 Claude 沟通，直到认可它的方案。&lt;/p&gt;&lt;p&gt;之后，他会切换到自动接受修改模式（auto-accept edits），Claude 通常能直接「一波带走」。一个好的方案至关重要！&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvric5j52YQwmYpvrrJpxFndeGKoM4NrypAZJRybmia9tvxSHXUDvca67Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.30580357142857145" data-s="300,640" data-type="png" data-w="896" type="block" data-imgfileid="503526661" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/489a1733-5eee-46b0-b5f0-c39bed4a0b11/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;七、打造自己的斜杠命令（Slash Commands）&lt;/h4&gt;&lt;p&gt;他会把每天重复多次的 &amp;ldquo;内环&amp;rdquo; 工作流都封装成斜杠命令。这让他免于重复输入提示词，也让 Claude 能直接调用这些流程。这些命令存放在 .claude/commands/ 下并提交到 Git。&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如，他和 Claude 每天会用几十次 /commit-push-pr。这个命令利用内联 Bash 预先计算 Git 状态等信息，运行极快，避免了反复对话。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tvov7xy0rL7ElPwuXbly8DYibYYnEOiaUIFiaXrIyqcwqvGxcQwtrMKWwSw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.24375" data-s="300,640" data-type="png" data-w="960" type="block" data-imgfileid="503526662" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/c8149617-2640-4650-a4e9-454b4dc6a01c/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;八、善用子智能体（Subagents）&lt;/h4&gt;&lt;p&gt;他经常使用特定的子智能体：比如 code-simplifier 用来在完成后简化代码，verify-app 用来端到端测试。和斜杠命令一样，子智能体本质上是把 PR 中最常见的流程自动化。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvlzQgUuHpe58Daaic4vjZk14TdIfePpwEicfiaiafPiblCDSruic1jzYHnmRQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.6883116883116883" data-s="300,640" data-type="png" data-w="462" type="block" data-imgfileid="503526663" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/5a9a4174-ec60-418c-a203-c76d82209940/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;h4&gt;九、自动代码美化&lt;/h4&gt;&lt;p&gt;他们使用了 PostToolUse 钩子来格式化代码。虽然 Claude 写的代码格式已经很好了，但这个钩子能搞定最后 10% 的细节，避免在 CI（持续集成）阶段报错。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvPz1OdeRurGN7wCyTlFkVr3wAALugW2QwtEo8jjiaAoAGicjia45681ibRw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.47300215982721383" data-s="300,640" data-type="png" data-w="926" type="block" data-imgfileid="503526665" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/a03e8dbf-9a53-4572-974e-722d4c5ad6de/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;h4&gt;十、权限管理&lt;/h4&gt;&lt;p&gt;他不用 --dangerously-skip-permissions（危险跳过权限提示）。相反会用 /permissions 预先授权一些在当前环境下安全的常用 Bash 命令。这些配置保存在 .claude/settings.json 中，团队共享。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tv9V5ln0HfOw2g0C5lyLVufSkzHxqkmPRDpskJIpVAmfzIS8uUrlCjAw/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.832258064516129" data-s="300,640" data-type="png" data-w="930" type="block" data-imgfileid="503526666" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/eefc7a10-235d-4411-a116-77fffbe39315/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;h4&gt;十一、工具全家桶&lt;/h4&gt;&lt;p&gt;Claude Code 会帮他操作所有工具，经常通过 MCP 服务器搜索并发送 Slack 消息，运行 bq 命令行执行 BigQuery 查询，或者从 Sentry 抓取报错日志。Slack 的 MCP 配置保存在 .mcp.json 中供团队使用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9TvM1icqtu1CGIODff8FfF3ZpJW3FOL6BLwmUACUM38cQxkyPqr1Z246Tw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.37445887445887444" data-s="300,640" data-type="png" data-w="924" type="block" data-imgfileid="503526667" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/60f43706-cf48-4c37-9d81-af8324173915/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;h4&gt;十二、长时间任务&lt;/h4&gt;&lt;p&gt;对于耗时较长的任务，他会：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;让 Claude 在完成后启动一个后台智能体进行验证；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;使用 Stop 钩子进行确定性检查；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;使用 ralph-wiggum 插件。 在这种情况下，使用 --permission-mode=dontAsk 或在沙盒环境中使用跳过权限模式，这样 Claude 就能心无旁骛地「输出」，不会被权限弹窗卡住。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic5UPZUVKVAgo73uyMial9Tve1TGUZkicXzmtNevnU1ibiaR5KiawMhffu4WksXxSHIrEOibNDUluOKb3Ew/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.2199248120300752" data-s="300,640" data-type="png" data-w="1064" type="block" data-imgfileid="503526668" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/14be42d2-bb41-47c5-837f-fcb5fc52ddd8/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;最后：构建反馈闭环&lt;/h4&gt;&lt;p&gt;最后一点，也是拿到高质量结果的关键：给 Claude 一个验证自己工作的途径。如果有反馈闭环，结果的质量能提升 2 到 3 倍。 Claude 在更新网页版代码时，会通过 Chrome 插件测试每一个改动：它会自动打开浏览器，测试 UI，不断迭代，直到代码跑通且交互体验丝滑。&lt;/p&gt;&lt;p&gt;验证方式因领域而不同：可能是运行一段 Bash 脚本、跑测试套件，或者在模拟器里运行 App。请务必花精力把「验证流程」做得坚如磐石。&lt;/p&gt;&lt;p&gt;感兴趣的开发者，可以在日常使用 Claude Code 时，以 Boris Cherny 的做法作为一个参考。&lt;/p&gt;&lt;p&gt;原推链接：&lt;a href="https://x.com/bcherny/status/2007179832300581177"&gt;https://x.com/bcherny/status/2007179832300581177&lt;/a&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AAAI 2026 | 小鹏联合北大，专为VLA模型定制视觉token剪枝方法，让端到端自动驾驶更高效</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 14:29:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-ym-citation="1"&gt;&lt;img alt="图片" data-aistatus="1" data-backh="321" data-backw="562" data-ratio="0.5703703703703704" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1#imgIndex=0" data-w="1080" data-original-style="width: 100%;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/a26e0896-a6f0-44e2-9314-ea5b06c87f46/640.png" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-ym-citation="3"&gt;VLA 模型正被越来越多地应用于端到端自动驾驶系统中。然而，VLA 模型中冗长的视觉 token 极大地增加了计算成本。但现有的视觉 token 剪枝方法都不是专为自动驾驶设计的，在自动驾驶场景中都具有局限性。&lt;/p&gt;&lt;p data-ym-citation="5"&gt;小鹏汽车联合北京大学计算机科学学院多媒体信息处理国家重点实验室发表论文《FastDriveVLA》，不仅为自动驾驶 VLA 模型中的高效视觉 token 剪枝建立了新的范式，也为特定任务的剪枝策略提供了有价值的洞察。&lt;/p&gt;&lt;p data-ym-citation="7"&gt;受人类驾驶员主要关注前景区域而非背景区域的启发，研究团队做出假设：对于自动驾驶而言，与前景信息相关的视觉 token 比与背景内容相关的视觉 token 更有价值。为了验证这个假设，研究团队构建了大规模自动驾驶标注数据集 nuScenes-FG（包含来自 6 个摄像头视角的、带有前景区域标注的 24.1 万个图像 - 掩码对），通过 MAE 风格的像素重建策略和新颖的对抗性前景 - 背景重建策略，训练出了一个适用于不同 VLA 模型的、可以即插即用的视觉 token 剪枝器 ReconPruner。&lt;/p&gt;&lt;p data-ym-citation="9"&gt;实验结果显示，在不同剪枝比例下，FastDriveVLA 在 nuScenes 开环规划基准测试中均取得了 SOTA 性能。FastDriveVLA 也非常高效，当视觉 token 数量从 3249 减少至 812 时，FastDriveVLA 的 FLOPs 直降约 7.5 倍；在 CUDA 推理延迟方面，FastDriveVLA 将预填充（prefill）时间减少了 3.7 倍、将解码（decode）时间减少了 1.3 倍，显著提升了推理效率。&lt;/p&gt;&lt;p data-ym-citation="11"&gt;该篇论文被 AAAI 2026 录用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LE7TcWeJl10kLRRsn7Oqnl13BbKHpglU95Bl2jnB3eicZlGQfEQHJjxw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.27575757575757576" data-s="300,640" data-type="png" data-w="990" type="block" data-backw="578" data-backh="159" data-imgfileid="503526433" data-aistatus="1" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6fa23cde-cca6-4352-bd87-b84f502bf3c1/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-ym-citation="14"&gt;论文标题：FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-ym-citation="15"&gt;论文链接：https://arxiv.org/pdf/2507.23318&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h4 data-ym-citation="17"&gt;研究背景与问题&lt;/h4&gt;&lt;p data-ym-citation="19"&gt;端到端自动驾驶最近展现出巨大潜力，有望彻底改变未来的交通系统。与传统的模块化自动驾驶系统不同，端到端方法在一个统一的框架中学习整个驾驶流程，这种设计不仅减少了模块之间信息传递时的误差，还增强了系统的简洁性。&lt;/p&gt;&lt;p data-ym-citation="21"&gt;然而，现有的 VLA 模型通常将视觉输入转换为大量的视觉 token，这种方法导致了巨大的计算开销和推理延迟的增加，对真实场景的车端部署提出了重大挑战，因为计算资源和推理速度都受到严重限制。&lt;/p&gt;&lt;p data-ym-citation="23"&gt;已经有大量研究尝试通过减少视觉 token 来加速 VLM 的推理，但在自动驾驶场景中都具有局限性：引入新设计的多模态投影器需要重新训练整个模型，基于注意力的剪枝策略容易受到无关信息的影响，基于相似性的剪枝策略会错误保留与驾驶无关的信息。&lt;/p&gt;&lt;p data-ym-citation="25"&gt;为了解决这些挑战，我们专为端到端自动驾驶 VLA 模型定制了一个新型的、基于重建的视觉 token 剪枝框架 FastDriveVLA。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lu0d2cqJ7ggdZ8oOKcJtEnVSQiaz5gCqu6gZtFhl9kofxgict7qT41g0A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3685185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="213" data-imgfileid="503526441" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/8aa04b62-b73d-4101-80ca-caf7392eaf73/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-ym-citation="27"&gt;图 1：不同视觉 token 剪枝策略的对比，（c）为基于重建的剪枝策略&lt;/p&gt;&lt;h4 data-ym-citation="29"&gt;方法与创新&lt;/h4&gt;&lt;p data-ym-citation="31"&gt;&lt;strong&gt;nuScenes-FG 数据集&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="33"&gt;受人类驾驶员主要关注前景区域而非背景区域的启发，我们首先对自动驾驶场景中的「前景区域」进行了明确定义。这些区域包括行人、道路、车辆、交通标志（含交通信号灯）以及交通障碍物（如位于车道上或紧邻车道的障碍物）等对驾驶决策具有直接影响的元素。相比之下，建筑物、天空、行道树等背景区域即使被完全遮挡，通常也不会显著影响人类驾驶员的判断。然后，借助 Grounded-SAM 对 nuScenes 场景进行细粒度、语义一致的前景分割，构建了 nuScenes-FG 数据集。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LRVvEAVpDcJVILqCiaTdbcGFEvAdlRVp5MnqHI6Q8d9acHSqa4WI52lA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.010185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="584" data-imgfileid="503526442" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/91053c96-58b0-48d9-81fd-6751bb4c8457/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-ym-citation="35"&gt;图 2：nuScenes-FG 数据集，为 nuScenes 场景提供了 24.1 万个前景分割标注。&lt;/p&gt;&lt;p data-ym-citation="37"&gt;&lt;strong&gt;基于重建的剪枝器 ReconPruner&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="39"&gt;我们提出了一种轻量级的、可即插即用的剪枝器 ReconPruner，主要目标是让 ReconPruner 能够有效识别并选择包含有意义前景信息的视觉 token，因此借鉴 Masked Image Modeling（掩码图像建模）方法设计了 MAE 风格的像素重建策略。在训练过程中，我们选取 ReconPruner 预测的可获得高分的视觉 token 子集，用于掩码前景重建。该子集上的重建误差作为监督信号，鼓励 ReconPruner 为真正对应前景内容的视觉 token 打高分。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LgfN1fJaGrFFq6DsJILHa82Pyz3ic38QdpfkljbJSQrNQS1bpqiagRSOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.8092592592592592" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="468" data-imgfileid="503526443" data-aistatus="1" data-original-style="width: 100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/36fff5cd-254c-4bcd-a5a0-ea38f7401312/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-ym-citation="41"&gt;图 3：FastDriveVLA 框架。在训练阶段，提出了一种新颖的「前景 - 背景对抗重建」策略，以增强 ReconPruner 对前景视觉 token 的感知能力；在推理阶段，ReconPruner 可直接嵌入自动驾驶 VLA 模型，用于 token 剪枝。&lt;/p&gt;&lt;p data-ym-citation="43"&gt;&lt;strong&gt;对抗性前景 - 背景重建策略&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="45"&gt;但若仅仅依赖前景重建，ReconPruner 可能会采取捷径，不加区分地为所有视觉 token 打高分。我们从生成对抗网络（GANs）中汲取灵感，提出了对抗性前景 - 背景重建策略。具体来说，ReconPruner 还需要使用获得低分的视觉 token 来重建背景区域。这种对抗性设置增强了 ReconPruner 区分前景 token 和背景 token 的能力。&lt;/p&gt;&lt;h4 data-ym-citation="47"&gt;实验结果&lt;/h4&gt;&lt;p data-ym-citation="49"&gt;&lt;strong&gt;实验设置&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="51"&gt;我们采用 Impromptu-VLA 作为视觉 token 剪枝的基础模型，在专为城区自动驾驶设计的大规模基准测试数据集 nuScenes 上对不同剪枝方法进行了评估。nuScenes 数据集包含 1000 个驾驶场景、每个场景约持续 20 秒。测试时，我们总计使用了 6019 个测试样本，并通过 L2 轨迹误差、碰撞率、路外率三个指标来评估开环规划的性能。&lt;/p&gt;&lt;p data-ym-citation="53"&gt;我们使用余弦调度器以 2e-5 的学习率训练 FastDriveVLA，总计进行了 10 轮训练，仅在两块 H800 GPU 上运行 3 小时就完成了训练。&lt;/p&gt;&lt;p data-ym-citation="55"&gt;不同剪枝方法在 nuScenes 数据集上的对比&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LVkt3Jl1V3ujCfuNG2IZzEiadU4YHWuspPItcvsseVGu5K9VLWmsNHzA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5925925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="342" data-imgfileid="503526451" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/33832c03-2372-4c17-8982-63b3a1f60a61/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-ym-citation="57"&gt;FastV、SparseVLM 是基于注意力的基线，DivPrune、VisPruner 是基于相似性的基线。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-ym-citation="59"&gt;当剪枝 25% 时，FastDriveVLA 在所有评估指标上均表现最佳，尤其在 L2 轨迹误差和碰撞指标上分别比未剪枝的原始模型低了 0.1% 和 1.0%，这证明了聚焦于与前景相关的视觉 token 是提升自动驾驶性能的关键。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-ym-citation="60"&gt;当剪枝 50% 时，FastDriveVLA 在碰撞指标上的表现优于剪枝 25%。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-ym-citation="61"&gt;当剪枝 75% 时，FastDriveVLA 在路外率指标上的表现优于剪枝 50%。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-ym-citation="63"&gt;总体来看，FastDriveVLA 在各种剪枝比例下均优于现有方法。特别值得注意的是，当剪枝 50% 时，FastDriveVLA 在所有指标上的表现都更加均衡。因此，我们建议，在实际部署自动驾驶系统时采用 50% 这一剪枝比例，以实现性能与效率的最佳平衡。&lt;/p&gt;&lt;p data-ym-citation="65"&gt;&lt;strong&gt;效率分析&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="67"&gt;为了展示 FastDriveVLA 的高效，我们从 FLOPs 与 CUDA 延迟的角度对不同剪枝方法进行了效率分析。当视觉 token 数量从 3249 减少至 812 时，FastDriveVLA 的 FLOPs 直降约 7.5 倍。在 CUDA 推理延迟方面，FastDriveVLA 将预填充提速 3.7 倍、解码提速 1.3 倍，实际推理效率显著提升。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lficapu9c0pgcOT8hmM0BqKcjLpXG0rib5js2LdmdAxyt8uHdxRzCwqjg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.4749455337690632" data-s="300,640" data-type="png" data-w="918" type="block" data-backw="578" data-backh="275" data-imgfileid="503526452" data-aistatus="1" data-original-style="width: 100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/dc4f3966-788d-45a7-9a39-9003f069f925/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-ym-citation="70"&gt;&lt;strong&gt;定性可视化分析&lt;/strong&gt;&lt;/p&gt;&lt;p data-ym-citation="72"&gt;ReconPruner 几乎完整留下了前景 token ，把背景压成极稀疏的色块，重建画面依旧清晰，证明它能在减少 token 冗余的同时保留关键信息，如图 4 所示。&lt;/p&gt;&lt;p data-ym-citation="74"&gt;再把 FastV（基于注意力）、DivPrune（基于相似性）和 FastDriveVLA 放到图 5 中进行对比，可以看到：我们的点密密麻麻落在车道、车道线和车身；FastV 几乎漏掉了车辆；DivPrune 虽然撒点更多，却几乎没往车道线上靠。&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td data-colwidth="287"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LywOqxiasiam6xowFySJa3Xphvz9eBrBw4xEfeHFpVuawDLSe5Cd0LLUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.9239130434782609" data-s="300,640" data-type="png" data-w="920" type="inline" data-imgfileid="503526455" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/b539e93a-6b79-4820-b980-ad7e789df51f/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 350px;"&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="287"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LbrU56ycqgicbb1pUdTvibPicwuLGGHbyEQ0ficjoNN5HaOualdQaP7Jdiag/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.9528907922912205" data-s="300,640" data-type="png" data-w="934" type="inline" data-imgfileid="503526454" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/34f12197-d7c9-4c64-94ab-2af05fc39f32/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 350px;"&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>统一结构与上下文信息的计算平台，德国慕尼黑大学等提出端到端的单细胞扰动分析框架</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Sun, 04 Jan 2026 14:06:02 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1SYeQdiaOD7Tx47rMGNvib0BIp6VibOgibdqcBWbmPx94ezotVnAaDdicrnDUC9V5unpU7BenHPQJ6Zw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5574074074074075" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="322" data-imgfileid="100027048" data-aistatus="1" data-original-style="width: 100%;" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/56c150de-fe6d-47a4-b0dc-12527369dc7a/640.png" data-sec-load-status="2" data-report-img-idx="1" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;编辑丨&amp;amp;&lt;/p&gt;&lt;p&gt;随着单细胞技术的发展，研究者可以在数以万计的细胞中同时测量多个基因或分子标记，并且通过遗传、化学或环境干扰（perturbation）引入实验变量，深入理解细胞反应机制。&lt;/p&gt;&lt;p&gt;这种类型的数据不仅体量巨大，而且结构复杂，不同实验条件、不同细胞类型和干扰策略之间的差异，使得传统的分析工具难以有效覆盖整体流程。现有方法大多只针对单个任务，或者专注于某种类型的环境干扰，而缺乏一个&lt;strong&gt;能够统一管理、分析和解释&lt;/strong&gt;各种单细胞扰动实验的平台。&lt;/p&gt;&lt;p&gt;考虑到现有的生物背景，德国慕尼黑亥姆霍兹中心（Helmholtz Center Munich）与慕尼黑工业大学等（Technical University of Munich）提出了一个基于 Python 的模块化框架&amp;nbsp;pertpy，可用于分析大规模单细胞扰动实验。&lt;/p&gt;&lt;p&gt;相关研究内容以「Pertpy: an end-to-end framework for perturbation analysis」为题，于 2025 年 12 月 31 日发布在《Nature Methods》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1SYeQdiaOD7Tx47rMGNvib0500MH5ObnqgGdCUzEQNkoH7icTCoto3CDTxURjzczxpax3LS9icJmnCg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.4162330905306972" data-type="png" data-w="961" data-width="961" data-height="400" data-imgfileid="100027045" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/9e21bd17-98be-4274-acc9-471805f27c57/640.png" alt="图片" data-before-load-time="1767506733433" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：&lt;em&gt;https://www.nature.com/articles/s41592-025-02909-7&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;端到端的框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;单细胞技术的进步，尤其是 Perturb-seq、CROP-seq&amp;nbsp;等高通量扰动技术的出现，让科学家能够以前所未有的规模进行&amp;ldquo;细胞实验&amp;rdquo;。他们可以同时敲除成千上万个基因，或施加数百种药物，并在单细胞分辨率下观察结果。这为系统性理解基因功能、药物机制和疾病通路提供了革命性的窗口。&lt;/p&gt;&lt;p&gt;但这种实验常被数据的庞大数量级冲垮。现有的工具，如&amp;nbsp;MUSIC、ScMAGeCK 等，只擅长处理特定类型的扰动或解决单一问题。而为了解决扩展性与通用性的框架缺失问题，pertpy 团队给出了自己的看法。&lt;/p&gt;&lt;p&gt;团队的解决方案并非简单地堆积功能，pertpy 的设计哲学是模块化、互操作与可扩展。它包含分析单一和组合扰动的方法，涵盖多种扰动数据类型，包括遗传敲除、药物筛选和疾病状态。该框架设计灵活，提供 100 多个可组合且互作的分析功能，组织成模块，进一步简化后续的解释和可视化。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1SYeQdiaOD7Tx47rMGNvib0ibASHTmLvtBAgh2S1d9iahQPUkYficQb6Ekic5QlLrtwa6icqljRMg5aicMA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.2598540145985402" data-type="png" data-w="685" data-width="685" data-height="863" data-backw="546" data-backh="688" data-imgfileid="100027043" data-aistatus="1" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/2e6e209b-eb28-4c64-8d3b-e03a358aa9ad/640.png" alt="图片" data-before-load-time="1767506733460" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 1：pertpy 框架的模块。&lt;/p&gt;&lt;p&gt;团队表示，尽管设计中 pertpy 主要设计用于探索遗传改造、药物治疗等扰动，但其效用也扩展到多种扰动环境，包括未应用实验扰动的多种疾病状态。所有这些功能通过 JAX 库实现 GPU 加速，其速度相较于原始实现有数量级提升。&lt;/p&gt;&lt;p&gt;首先，框架通过数据转化，将引导 RNA（gRNA）分配给细胞。接下来，它会处理诸如技术变异、其他单细胞特异性质量控制问题等不受欢迎的混杂因素。&lt;/p&gt;&lt;p&gt;经过严格的质量控制后，pertpy 开始对细胞系本体或药物本体进行扰动注释处理，并用来自癌症依赖地图的额外元数据丰富扰动。而为了迎接扰动数量增加带来的挑战，pertpy 提供了多种不同方式来学习生物学上可解释的扰动空间，这些方法不同于细胞的个体主义视角，而是每个扰动生成一个单一嵌入，汇总细胞反应。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;场景验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为验证 pertpy 学习有意义扰动空间的能力，团队分析了最初由 Norman 等人公开发布的&amp;nbsp;CRISPRa&amp;nbsp;筛查数据集。，包含 111,255 个 K562 细胞的单细胞转录组，经历了 287 次单基因和基因对扰动。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1SYeQdiaOD7Tx47rMGNvib0icCENcCHGXqE00wcykTmyXlrZgkf9RLxt892HNJ0rXCx6YW86q6eb0g/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9255474452554745" data-type="png" data-w="685" data-width="685" data-height="634" data-backw="546" data-backh="505" data-imgfileid="100027046" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/8d973935-89c8-4f62-8d16-ec9c0f12a058/640.png" alt="图片" data-before-load-time="1767506733497" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 2：通过 pertpy 的扰动空间流水线，学习组合 CRISPRa 扰动 scRNA-seq 数据中的统一扰动空间。&lt;/p&gt;&lt;p&gt;团队测试了多种针对微扰的处理策略，并利用基于多层感知子（MLP）的判别器分类器，将剩余细胞的归一化基因表达投射到扰动空间中。&lt;/p&gt;&lt;p&gt;结果表示，所有策略产生的微扰空间相似。这表明对于该数据集，不依赖基于微扰特征的单元过滤方法更为可取。&lt;/p&gt;&lt;p&gt;而面对复杂的微扰实验的发现流程， pertpy 同样以极高的效率分析了包含 172 个细胞系和 13 种药物治疗的数据集。这只需要几个步骤：注释、可视化、比较分析。这其中还允许用户将其细胞系的 RNA 谱与已建立的公开数据集进行比较，从而提供快速的质量控制功能。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1SYeQdiaOD7Tx47rMGNvib0liaUNIwoJzVwnFFlELD9HUKiczTX1IZHkZ8C4Dib0uJTHqsBjkPeXMzqQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.1883211678832117" data-type="png" data-w="685" data-width="685" data-height="814" data-backw="546" data-backh="649" data-imgfileid="100027044" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9f97c465-b80a-4fcc-bb25-e1747537f288/640.png" alt="图片" data-before-load-time="1767506733512" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 3：scRNA-seq 药物筛选数据中存活性相关反应特征的解卷积。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;可扩展的单细胞扰动分析&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为一款开源的分析工具，pertpy 将过去分散的单细胞 perturbation 分析方法整合到一个结构化、可重复、易扩展的框架中。它极大地降低了领域门槛，为构建大规模扰动图谱奠定了基础。&lt;/p&gt;&lt;p&gt;Pertpy 不仅为研究者提供了工具链，还为未来算法的开发和集成奠定了基础，是单细胞 perturbation 研究数据层面解决方案的重要一步。它提供的丰富距离度量和分析模块，正是评估这些模型预测是否具有生物学意义的标尺。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>前OpenAI CTO押注的赛道，被中国团队抢先跑通，AI「下半场」入场券人人有份</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 12:58:47 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/aaff880e-a3ff-4059-80a7-9f1b4a0e6a28/1767502380182.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;section&gt;在大公司一路高歌猛进的 AI 浪潮里，小创业者和高校研究者正变得越来越迷茫。就连前段时间谷歌创始人谢尔盖・布林回斯坦福，都要回答「大学该何去何从」「从学术到产业的传统路径是否依然重要」这类问题。&lt;/section&gt;&lt;p&gt;AI，真的只是大公司的游戏吗？被算力掣肘的其他研究者、创业者，机会在哪里？在「强化学习」后训练引领「下半场」的当下，这个问题变得愈发重要。&lt;/p&gt;&lt;p&gt;好在，国内外都有专业团队在关心这个问题，比如前 OpenAI CTO Mira 创办的 Thinking Machines Lab，前段时间就推出了一个叫「Tinker」的产品，专注于解决后训练 Infra 的复杂性。&lt;/p&gt;&lt;p&gt;而在国内，一群由 95 后青年科学家组成的团队做出了足以对标甚至超越 Tinker 的竞品，成为&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;line-height: 1.75em;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;世界第一家能够对标 Thinking Machines Lab 的公司。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这个研究中心叫 Mind Lab，是 Macaron AI 背后的实验室。1 月 1 日，他们发布了亮相以来的第一款产品&amp;mdash;&amp;mdash;Mind Lab Toolkit（MinT）。这是一个用 CPU 的机器就能高效训练万亿参数模型的后训练平台，且成本优化了十倍，一天即可轻松完成一轮训练。此外，它比 Thinking Machines 更早实现了 1T LoRA-RL，是业界在万亿参数模型上进行高效强化学习的第一个成果。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6k7TmyajrYuWWKFGUGB91HagqfeUYIeiaOuhAibpNZib2lMJrQKNvtwaFw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.7796296296296297" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526612" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/7786e930-5dc5-45ca-9dd1-bd0fc1b150d9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;如果你是 Agent 领域创业公司或高校顶尖实验室的成员，并且被算力限制了想象力，那你将是 MinT 的首批受益者。它的应用场景涵盖基础研究到垂直行业的广泛领域，已经在圈内做出了一些成果。&lt;/p&gt;&lt;p&gt;细看一下，Mind Lab 的创始团队也堪称豪华。创始人 Andrew 毕业于 MIT，目前担任深圳清华大学研究院的研发中心主任，代表工作有和姚顺雨合作的 Agent 微调的经典工作之一 FireAct。&lt;/p&gt;&lt;p&gt;首席科学家马骁腾博士则毕业于清华大学自动化系，常年深耕强化学习领域。团队成员来自清华、MIT、CMU等高校，并有OpenAI、DeepMind、Seed 等顶尖实验室的工作经历。&lt;/p&gt;&lt;p&gt;团队累计发表论文超 100 篇，总引用量超 3 万次。&lt;/p&gt;&lt;p&gt;这样一个团队打造的 MinT，正以极致的工程效率，将 AI 下半场的入场券交还到每一位研究者手中。&lt;/p&gt;&lt;h4&gt;预训练时代结束，AI 下半场开启&lt;/h4&gt;&lt;p&gt;过去几年，预训练一直是 AI 领域的主旋律 &amp;mdash;&amp;mdash; 更大的模型、更多的数据、更长的训练周期。&lt;/p&gt;&lt;p&gt;如今，这一阶段已趋于饱和：开源社区已经拥有万亿参数级别的模型，能够编写代码、总结文档、通过标准化考试。&lt;/p&gt;&lt;p&gt;但当这些系统被部署到真实产品中，新的瓶颈开始显现。模型一旦完成训练，参数就被 &amp;#39; 冻住 &amp;#39; 了，不停重复着相同的错误，也无法适应不断变化的用户需求，实际使用效果只能靠抽卡。&lt;/p&gt;&lt;p&gt;强化学习，正是破局的关键。&lt;/p&gt;&lt;p&gt;DeepSeek R1 的发布更是向业界证明，强化学习能够带来惊人的泛化性和样本效率 &amp;mdash;&amp;mdash; 模型不再只是 &amp;ldquo;记住&amp;rdquo; 数据，而是学会了在复杂任务中进行推理。&lt;/p&gt;&lt;p&gt;在 Gemini、DeepSeek V3.2、Kimi K2 等多个前沿模型的技术报告中都反复强调：后训练仍是一片蓝海，强化学习还没看到天花板。&lt;/p&gt;&lt;p&gt;2026 年的主旋律，是后训练。&lt;/p&gt;&lt;h4&gt;后训练时代的基础设施&lt;/h4&gt;&lt;p&gt;强化学习这么重要，为什么没普及？答案是：算法太复杂，训练太不稳定。&lt;/p&gt;&lt;p&gt;为了解决这个问题，前 OpenAI CTO Mira 创立的 Thinking Machines 发布了 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650993756&amp;idx=1&amp;sn=4bd29a2b9fdaf00b29f97f6f2139decb&amp;scene=21#wechat_redirect" target="_blank"&gt;Tinker&lt;/a&gt;，定义了后训练 API 的新范式，迅速获得美国学界和硅谷创业公司的热捧。&lt;/p&gt;&lt;p&gt;在 OpenAI 经历了 Sam Altman 被解雇又回归的内部动荡后，Mira 选择离开，并迅速组建了一支 &amp;ldquo;梦之队&amp;rdquo;&amp;mdash;&amp;mdash; 核心成员包括 OpenAI 前研究副总裁 John Schulman、Lilian Weng 等业界顶尖人才。资本市场对这家公司的追捧堪称疯狂。2025 年 7 月，Thinking Machines 完成了硅谷历史上最大的种子轮融资 &amp;mdash;&amp;mdash;20 亿美元，估值 120 亿美元。&lt;/p&gt;&lt;p&gt;他们押注的，正是后训练赛道。2025 年 10 月，Thinking Machines 发布了首款产品 Tinker，12 月面向所有用户开放。如果说 OpenAI 定义了大模型的推理 API 范式，那么 Tinker 定义的就是模型的训练 API 范式，让所有模型训练共享。&lt;/p&gt;&lt;p&gt;Tinker 已经获得了学术界和工业界的广泛认可，成为了硅谷和美国顶尖高校的训练新范式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LlwU7iawaLO1ofZSo8284JlqOyYuDyJO8ezbf85vqIgwzLsaSq2weSZg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.9625" data-s="300,640" data-type="png" data-w="720" type="block" data-imgfileid="503526461" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b4eea727-56ad-4e66-9e8b-bf974454a2df/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;Mind Lab 与 MinT，国产后训练基础设施的崛起&lt;/h4&gt;&lt;p&gt;Tinker 在海外大火的同时，国内也涌现出了对标甚至超越的力量 &amp;mdash;&amp;mdash;Mind Lab 推出的 MinT（Mind Lab Toolkit）。&lt;/p&gt;&lt;p&gt;Mind Lab 秉持 &amp;ldquo;From Static &amp;#39;Brains&amp;#39; to Adaptive &amp;#39;Minds&amp;#39;&amp;rdquo; 的理念，致力于让 AI 系统能够从真实世界的经验中不断成长。&lt;/p&gt;&lt;p&gt;在他们看来，当前大模型最大的问题是：训练完就 &amp;quot;冻住&amp;quot;，无法从真实交互中持续学习进化。&lt;/p&gt;&lt;p&gt;MinT，正是为解决这个问题而生。&lt;/p&gt;&lt;p&gt;MinT 和 Tinker 是什么关系？可以从两个层面理解：&lt;/p&gt;&lt;p&gt;兼容性上，MinT 做到了模型够大够全、接口完全一致 &amp;mdash;&amp;mdash; 与 Tinker API 完全兼容。这意味着使用 Tinker 的开发者可以几乎零成本地迁移到 MinT，享受国产基础设施带来的便利。&lt;/p&gt;&lt;p&gt;技术领先性上，MinT 不是简单的 &amp;ldquo;国产替代&amp;rdquo;。事实上，早在 2025 年 12 月 1 日，Mind Lab 就比 Thinking Machines 更早实现了 1T LoRA-RL，是业界在万亿参数模型上进行高效强化学习的第一个成果。&lt;/p&gt;&lt;p&gt;相关实现方案已经开源，并获得了 Nvidia 官方转载。&lt;/p&gt;&lt;p&gt;具体方案详见 Mind Lab 的技术报告：https://macaron.im/mindlab/research/building-trillion-parameter-reasoning-rl-with-10-gpus&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6fQjTAArySmqus1SJKvVHTMfDNVliaH0KIzS3FrX4zXrkiapXdlYEG16A/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5768518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526613" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9dcc18d7-aa65-455e-ae03-072a1b3bce33/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;MinT 解决了什么问题？&lt;/h4&gt;&lt;p data-pm-slice="0 0 []"&gt;MinT 的核心价值可以用一句话说清：不论模型是1B还是1T，需要调度多少GPU，你只管数据和算法，基础设施的复杂工程全交给平台。&lt;/p&gt;&lt;p&gt;具体来说：用户只需在本地 CPU 机器上写几行 Python 代码，MinT 就会自动把计算任务分发到大规模 GPU 集群执行。集群调度、资源管理、容错恢复，这些让开发者和研究人员头疼的工程问题，统统由 MinT 搞定。切换不同的模型，只需修改代码中的一个字符串。&lt;/p&gt;&lt;p&gt;技术路线上，MinT 采用 LoRA 技术，使多个训练和推理任务可以共享同一计算资源池，从而显著降低成本。LoRA 在选择最优学习率的情况下，训练进程与全参数微调几乎完全一致，这为大规模高效后训练奠定了理论基础。&lt;/p&gt;&lt;h4&gt;目前，MinT 已支持 Kimi K2 Thinking（万亿参数级别的 MoE 推理模型）、Qwen3-VL 系列视觉语言模型等前沿开源模型，并全面兼容 Tinker API。值得一提的是，MinT 还优先支持了 &amp;pi;0 等具身 VLA 模型，这也体现出了中国公司在具身智能上的领先优势。&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LTibznqTsmeKhpl1LRmV9KWwibEF7ndPM1w4ia9ibSXbClo1NiaoCgglJCUw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.6277777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526463" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/ef80e279-c142-4717-ba05-6d61f04251e4/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;为什么需要 1T LoRA-RL？&lt;/h4&gt;&lt;p&gt;强化学习被视为让大模型从 &amp;ldquo;背题&amp;rdquo; 走向 &amp;ldquo;推理&amp;rdquo; 的关键，但现实里有三大难题：训练不稳，小模型难以收敛，算力成本高。LoRA 提供了一条低成本路径，只训练少量低秩适配器即可显著提升下游任务表现，且在 RL/Agent 训练上几乎不损失性能。&lt;/p&gt;&lt;p&gt;Mind Lab 在 Kimi K2（万亿参数 MoE）上实现了端到端 LoRA 强化学习，带来三点突破：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;成本：仅用常规全参 RL 约 10% 的 GPU 资源，64 块 H800 即可完成训练。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;稳定性：奖励与任务成功率平稳提升，无灾难性发散；在 held-out 基准上既提升特定任务，又保持基座模型通用能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;系统：统一调度张量 / 流水线 / 专家 / 序列并行，针对 MoE 路由不均衡与通信压力做了专项优化。相关技术已贡献至 NVIDIA Megatron-Bridge 与火山引擎 verl 等开源项目。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LicQl5utTv09mc7ZmfDdJPTKGmLpQhScehrZbosymeYRiaZrFWlAaibb2w/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.762962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526464" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/788c9863-a97c-45dd-bbb1-5ac255b6fc1a/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;为什么选择 MinT？&lt;/h4&gt;&lt;p&gt;MinT 的产品设计围绕一个核心目标：把后训练和强化学习的门槛打下来。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;验证成本上：MinT 允许开发者仅用 CPU 机器进行训练验证，告别配置 GPU 驱动和 OOM 的烦恼。这让团队可以在投入大规模 GPU 资源前，先低成本验证算法可行性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;工程效率上：MinT 将采样、训练、回写与发布无缝串联，减少了工程拼装成本。并行策略、权重管理、optimizer state 管理、滚动训练、日志与可复现性等，都按工程标准打通。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;开发体验上：MinT 完全兼容 Tinker API，现有代码可快速适配，切换不同模型只需一行代码。目前已支持 Qwen、Kimi 等先进的开源大模型。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;迭代速度上：采用 LoRA-RL 技术让模型迭代周期从 &amp;ldquo;按周&amp;rdquo; 缩短到 &amp;ldquo;按天&amp;rdquo;，真正服务于快节奏的产品开发需求。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LZG22NWxVibjZYMKv2QwOTF0iakB8mWHibjf78vSIt9TDMNdn1kpXz0Yaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.26851851851851855" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526465" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/f48c1560-3737-41d5-ab51-b0e097428da1/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4&gt;谁是 MinT 最大的受益者？&lt;/h4&gt;&lt;p&gt;第一批使用 MinT 的受益者，一定是 Agent 领域的创业公司和研究模型的高校顶尖实验室。&lt;/p&gt;&lt;p&gt;它们共同的特点是：掌握核心的数据和问题的设定。他们并非不了解前沿算法，而往往是被算力与训练框架难住了。&lt;/p&gt;&lt;p&gt;据 Mind Lab 官网介绍，目前 MinT 已经获得了顶尖高校和多个创业公司的认可，应用场景涵盖基础研究到垂直行业的广泛领域。&lt;/p&gt;&lt;p&gt;在学术机构方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;清华大学人工智能学院黄高副教授&lt;/strong&gt;团队（CVPR best paper 以及 NeruIPS best paper runner up 获得者）利用 MinT 开展了 RL 如何突破 Base model 知识边界的研究。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;上海交通大学副教授、上海创智学院全时导师蔡盼盼&lt;/strong&gt;的 RoPL 实验室使用 MinT 在具身决策大模型和决策世界模型方面展开研究。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在行业应用方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;硅谷创业公司 &lt;strong&gt;Eigen AI&amp;nbsp;&lt;/strong&gt;合作探索运用 MinT 和 Data Agent 合成数据在 1T 模型上进行 agentic RL 训练。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;脑机接口公司&lt;strong&gt;姬械机&lt;/strong&gt;利用 MinT 支持了他们的脑机接口 Agent &lt;strong&gt;BCI-Love&lt;/strong&gt;，可以进行情感交互对话。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;瑞铭医疗&lt;/strong&gt;利用 MinT 对医疗编码模型进行了基于 RL 的后训练，显著提升了医疗编码的准确率，&lt;strong&gt;并落地到数十家三甲医院&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些案例展现了 MinT 的通用性 &amp;mdash;&amp;mdash; 从基础研究到垂直行业，都能用。&lt;/p&gt;&lt;h4&gt;中国团队引领后训练浪潮&lt;/h4&gt;&lt;p&gt;如何让模型真正 &amp;ldquo;理解&amp;rdquo; 而非只是 &amp;ldquo;记住&amp;rdquo;，是众多创业团队与科研工作者共同面对的核心问题。强化学习被视为解决这一问题的关键路径，但其高门槛、高成本与不稳定性，长期限制了它在真实产品和中小团队中的落地。&lt;/p&gt;&lt;p&gt;2025 年，中国团队在开源模型上大放异彩。&lt;/p&gt;&lt;p&gt;2026 年，后训练将是中国 AI 弯道超车的下一个关键战场。&lt;/p&gt;&lt;p&gt;Mind Lab 选择了 LoRA-RL 这一技术路径，在超大规模模型上完成了万亿参数级别的探索与验证，再次证明了中国团队在前沿研究上的工程能力与原创实力。MinT 正是 Mind Lab 希望将这些研究成果系统化、工具化的产物 &amp;mdash;&amp;mdash; 让后训练和强化学习不再只属于少数头部机构，而是成为更多公司与实验室可以日常使用的能力。&lt;/p&gt;&lt;p&gt;这正是 Mind Lab 真正布局的方向：让先进研究转化为可用工具，让中国团队在模型后训练与强化学习这一关键技术浪潮中，实现自主可控。&lt;/p&gt;&lt;section&gt;可以访问以下链接了解更多：&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;section&gt;Mind Lab 官网：&lt;a href="https://macaron.im/mindlab"&gt;https://macaron.im/mindlab &lt;/a&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;相关文档：&lt;a href="https://mint.macaron.im/doc"&gt;https://mint.macaron.im/doc&lt;/a&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ControlNet作者张吕敏最新论文：长视频也能实现超短上下文</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 01:39:09 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3a90df6c-40f3-401f-a574-a08345c9959e/1767461789356.png" style="width: 700%;" class="fr-fic fr-dib"&gt;大部分的高质量视频生成模型，都只能生成上限约15秒的视频。清晰度提高之后，生成的视频时长还会再一次缩短。&lt;/p&gt;&lt;p&gt;这就让尝试AI视频创意的创作者们非常苦恼了。要想实现创意，必须使用分段生成，结合首尾帧，不仅操作起来很麻烦，而且需要来回抽卡来保证画面的一致性。&lt;/p&gt;&lt;p&gt;那么，限制视频生成时长的瓶颈在哪里？&lt;/p&gt;&lt;p&gt;大家可能不知道的是，一段 60 秒、480p、24 帧/秒的视频，在模型内部会被拆解成 &lt;strong&gt;超过 50 万个「潜在 token」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这些 token 就像一条极长的记忆胶带，模型想要保持剧情连贯、画面一致，就必须从头到尾保存上下文记忆。但代价是：算力直接爆炸，普通显卡根本扛不住。&lt;/p&gt;&lt;p&gt;这正是当前自回归视频生成模型的核心矛盾。一边是越长的上下文，画面越连贯；另一边是越长的上下文，计算成本越高。&lt;/p&gt;&lt;p&gt;于是，研究者们不得不做出妥协：要么用滑动窗口切掉大部分历史，换取可运行的算力；要么对视频进行激进压缩，牺牲清晰度和细节。&lt;/p&gt;&lt;p&gt;问题在于，这些压缩方法往往最先丢掉的，正是决定画面真实感与一致性的高频细节。&lt;/p&gt;&lt;p&gt;也正是在这一困境下，&lt;strong&gt;苏州大学校友，斯坦福大学博士，ControlNet 创作者张吕敏团队&lt;/strong&gt;为此投入了研究&lt;strong&gt;，&lt;/strong&gt;提出了一种新的解决思路，给出了&lt;strong&gt;专为长视频设计的记忆压缩系统，在压缩的同时尽可能保留精细视觉信息。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6ntzwTaPZrJSGJWibn6Sh0xFuawwkzzOvn1B0pVevz8OSrHLPPIMKbmQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.21296296296296297" data-type="png" data-w="1080" data-width="1694" data-height="360" data-imgfileid="503526567" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/de2b4df6-f147-4d93-8755-19dd14bdb9eb/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Pretraining Frame Preservation in Autoregressive Video Memory Compression&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2512.23851v1&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;研究团队提出了一种神经网络结构，用于将长视频压缩为短上下文，并设计了一种显式的预训练目标，使模型能够在任意时间位置保留单帧中的高频细节信息。&lt;/p&gt;&lt;p&gt;基线模型可以将一段&lt;strong&gt;&amp;nbsp;20 秒的视频压缩为约 5k 长度的上下文表示&lt;/strong&gt;，同时支持从中随机检索单帧，并在&lt;strong&gt;感知质量上保持良好的外观保真度&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种预训练模型可以直接微调为自回归视频模型的记忆编码器（memory encoder），从而以较低的上下文成本实现长历史记忆建模，并且仅带来相对较小的保真度损失。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6nEyUzOSCvIiaIiaXO1A1wuch9icnJt4CXNj3oA0KJeiclIdDoibQb5q764Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.49166666666666664" data-type="png" data-w="1080" data-width="1142" data-height="562" data-imgfileid="503526570" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/617126f8-783a-41c4-8658-37a0a5138fd1/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;该视频是使用完整历史上下文（不切割任何历史帧）逐秒自回归生成的。20 多秒的历史被压缩为 &amp;sim; 5k 上下文长度，并由 RTX 4070 12GB 处理。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;全新的记忆压缩架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具体而言，研究团队采用&lt;strong&gt;两阶段策略&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;首先，预训练一个&lt;strong&gt;专用的记忆压缩模型&lt;/strong&gt;，其目标是在任意时间位置上尽可能保留高保真帧级细节信息。&lt;/p&gt;&lt;p&gt;该预训练目标通过对从压缩历史中随机采样的帧最小化其特征距离来实现，从而确保模型在整个序列范围内都能稳健地编码细节信息。&lt;/p&gt;&lt;p&gt;在网络结构设计上，提出了一种&lt;strong&gt;轻量级双路径架构&lt;/strong&gt;：模型同时处理低分辨率视频流和高分辨率残差信息流，并通过将高分辨率特征直接注入 Diffusion Transformer 的内部通道，绕过传统 VAE 所带来的信息瓶颈，从而进一步提升细节保真度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;预训练记忆压缩模型&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6VYclIm3XC3ojia6Y7ohfHTfRzmffPiaom5XDGoNqVicbCZXOpvOtsqcUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.9779116465863453" data-type="png" data-w="996" data-width="996" data-height="974" data-imgfileid="503526569" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/55a28b11-a6bf-41e6-852e-e62f2369ca20/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;记忆压缩模型的预训练。记忆压缩模型需要将长视频（例如 20 秒）压缩成短上下文（例如长度为 5k）。预训练的目标是在任意历史时间位置检索具有高频细节的帧。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;该方法的&lt;strong&gt;核心创新在于其预训练目标设计&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;研究团队观察到，衡量视频压缩机制保留上下文细节能力的一个合适的指标是其任意时间位置高质量帧检索的能力。对于高压缩率，完美检索变得不切实际，因此目标变为最大化任意帧的检索质量。&lt;/p&gt;&lt;p&gt;给定一段长视频历史 H，记忆压缩模型 &lt;span data-meta-block-props='{"blockId":"973803be-232b-42a2-92a2-2756fcc10998","blockType":"EQUATION_BLOCK","initData":{},"props":{"data":{"equation":" \\phi(\\cdot)\n"},"displayMode":"inline","viewType":"inline"}}'&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6qEr9AMGBrcRsPehaEvu0E6KQlSpPn1HicTMgSiabGmMxatg9NK6ibHiaOQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9444444444444444" data-s="300,640" data-type="png" data-w="108" type="block" data-imgfileid="503526582" data-aistatus="1" data-original-style="width:32px;height:30px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/f00003e1-8c99-4012-8698-552ed010ab03/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dii" style="width: 3.48%;"&gt;&lt;/span&gt;学习将其压缩为一个紧凑的上下文表示 &lt;span data-meta-block-props='{"blockId":"172ef25e-499d-4cbe-b735-6f3987ebd742","blockType":"EQUATION_BLOCK","initData":{},"props":{"data":{"equation":" \\phi(H)\n"},"displayMode":"inline","viewType":"inline"}}'&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX60y81u8hqicy3e5QHpAzUMyzrs6JG0vkIVia09OzXvsKUpkibicf2qMrL4g/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.7692307692307693" data-s="300,640" data-type="png" data-w="156" type="block" data-imgfileid="503526583" data-aistatus="1" data-original-style="width:38px;height:29px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/d25f0cba-e318-4cee-8651-794a54f60db6/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 4.95%;"&gt;&lt;/span&gt;，同时仍然保持对&lt;strong&gt;任意时间位置帧&lt;/strong&gt;进行重建的能力。&lt;/p&gt;&lt;p&gt;在训练过程中，模型从历史序列中随机选择一组帧索引 &amp;Omega;，并对其余所有帧进行噪声掩蔽处理；模型必须仅依赖压缩后的表示来重建这些被选中的帧。&lt;/p&gt;&lt;p&gt;如上图所示，帧选择 &amp;Omega; &lt;span data-meta-block-props='{"blockId":"ef138d7a-f199-4790-8aa4-a155e3c33c09","blockType":"EQUATION_BLOCK","initData":{},"props":{"data":{"equation":"\\Omega\n\n"},"displayMode":"inline","viewType":"inline"}}'&gt;与检索过程 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX63Td8IxhnG3eibQNMOqH4KuZHYR9f3X7tehZT7qZ7wicHeL8WFm3081HQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.4152542372881356" data-s="300,640" data-type="png" data-w="236" type="block" data-imgfileid="503526584" data-aistatus="1" data-original-style="width:64px;height:27px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e9124a17-0edd-4689-a739-31f69946103f/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 7.7%;"&gt;&lt;/span&gt; 可以被构建为一个&lt;strong&gt;自回归视频扩散框架&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;本文采用以噪声作为掩蔽的方法：为被掩蔽帧加入服从&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6jengnEYaUyPacWRm8EPP4s8EDRAGAPztexgw99icfdvmNeheCqffvkQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.43902439024390244" data-s="300,640" data-type="png" data-w="246" type="block" data-imgfileid="503526585" data-aistatus="1" data-original-style="width:65px;height:29px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/74cd78bf-119a-4e09-a27e-f0f10fe536e1/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dii" style="width: 7.06%;"&gt;的潜在噪声水平。&lt;/p&gt;&lt;p&gt;随后，研究团队将所选的干净帧复制作为扩散模型的目标，使扩散系统能够在任意时间位置重建目标帧。该过程可表示为：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX69NAwLh9R5DuVcF2DdvNxwqwDsE9V8TosxJbOCGl8DZufesWQYACibeQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.15399239543726237" data-type="png" data-w="1052" data-width="1052" data-height="162" data-imgfileid="503526566" data-aistatus="1" data-original-style="width: 397px;height: 61px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/3647232f-0613-4358-bdb4-32bb3993bd38/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;这种随机化选择机制有效防止模型通过仅编码易于访问的帧（例如首帧或末帧）来「投机取巧」，从而迫使模型学习一种能够在整个时间序列范围内持续保留细节信息的表示方式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6mBSA9c9Lz9EZjbtXGhicx4ick1miccIfoMMIyKxA7mozlvmnJpDNoqXWw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.4718875502008032" data-type="png" data-w="996" data-width="996" data-height="470" data-imgfileid="503526568" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/acd7f2ec-afc7-4244-8735-a5349aab1f63/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;内存压缩模型的架构。使用 3D 卷积、SiLU 和注意力机制来构建一个轻量级的神经网络结构，作为基准压缩模型。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;视频扩散模型的微调&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6owEAkic3p28rgL6ZicKpgKunRXKF14yrnbbKdaicBYXGDDiarjJY2Fibiauw/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.3965863453815261" data-type="png" data-w="996" data-width="996" data-height="395" data-imgfileid="503526572" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/714d42ff-c701-4d71-ab06-55af827343b5/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;微调自回归视频模型。展示了最终自回归视频模型的微调和推理过程。记忆压缩模型的预训练在微调之前完成。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;借助预训练完成的记忆压缩模型&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6qEr9AMGBrcRsPehaEvu0E6KQlSpPn1HicTMgSiabGmMxatg9NK6ibHiaOQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.9444444444444444" data-s="300,640" data-type="png" data-w="108" type="block" data-imgfileid="503526582" data-aistatus="1" data-original-style="width:32px;height:30px;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/cf660150-e057-4dcb-82dc-0ebb3bd2933e/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 3.85%;"&gt;&amp;nbsp;，可以通过对视频扩散模型（例如 WAN，并结合 LoRA 微调）以及该压缩模型作为历史记忆编码器进行联合微调，从而构建一个自回归视频生成系统。&lt;/p&gt;&lt;p&gt;由此得到的视频生成模型具备超长历史窗口（例如超过 20 秒）、极短的历史上下文长度（例如约 5k），并且对帧检索质量进行了显式优化。&lt;/p&gt;&lt;p&gt;该扩散过程亦可按照公式表示为：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6tATcP3WDMxpibVjOZzszibWpCf17SAE1xZP3Tm606OeDUSpVNnpqYrMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.1364522417153996" data-type="png" data-w="1026" data-width="1026" data-height="140" data-imgfileid="503526571" data-aistatus="1" data-original-style="width:393px;height:54px;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/60846920-f904-47e0-b40c-e001106fcf54/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在实验中，研究团队使用 8 &amp;times; H100 GPU 集群进行预训练，并使用 1 &amp;times; H100s 或 A100s 进行 LoRAs 微调。所有实验均在 HunyuanVideo和 Wan 系列的基础模型上进行。&lt;/p&gt;&lt;p&gt;数据集由来自多个网站的约 500 万互联网视频组成。其中约一半是竖屏短视频，其余为普通横屏视频。数据经过质量清洗，然后使用 Gemini-2.5-flash VLM 对高质量部分进行字幕标注，剩余部分使用本地 VLM（如 QwenVL）进行处理。测试集包括由 Gemini-2.5-pro 编写的 1000 个故事板提示和 4096 个未在训练数据集中出现过的视频。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;定性与定量评估&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6lX4ug0torwlpIJR98PiaWNSG2PDnJAWhB7uIg8oMrfBYRmPLvEJlvvg/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.42592592592592593" data-type="png" data-w="1080" data-width="1142" data-height="486" data-imgfileid="503526575" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/09264eff-cf7e-43ad-8ba7-7aa032b6f1d2/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;故事板上的定性结果。通过从故事板中流式传输提示来展示结果。故事板是一组提示，其中每个提示涵盖一定数量的帧。故事板可以由外部语言模型编写。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在定性评估方面，如图所示，研究者证明了模型能够处理多种多样的提示和故事板，同时在角色、场景、物体和情节线方面保持一致性。&lt;/p&gt;&lt;p&gt;在定量评估方面，研究者们从 VBench、VBench2等平台引入了多个视频评估指标，并进行了一些修改。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6DgLECiaGe7sl6qJDW6GNaaaToHm73ibeAr9Na8s6Bs16CqLcFviaAl2Dw/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.4351851851851852" data-type="png" data-w="1080" data-width="1506" data-height="656" data-imgfileid="503526574" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/db1ef678-34e7-4e7a-b3d1-c3689e1f3767/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;视频内容一致性的定量评测结果。其中，Qwen 中的 「1p」 表示仅使用 1 张图像 作为图像模型输入。由于部分方法存在严重伪影，因此未将其纳入人工 ELO 评分统计。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;如表所示，本文提出的方法在多个一致性指标上表现出合理的分数。Wan+Qwen 组合在实例分数上似乎具有领先分数，这可能是由于图像模型不会显著改变或移动对象，从而避免了 VLM 问答检测到的伪影。本文的方法在对象一致性方面表现出有竞争力的分数。此外，用户研究和 ELO 分数验证了本文提出的架构，证实它在压缩和质量之间实现了有效的权衡。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;消融实验&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6MleyBBiajl9POicKZD2k8Lsb9ySJFKjTOeGNXl5tqvkesvBTsOHXrvQA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.4824074074074074" data-type="png" data-w="1080" data-width="1232" data-height="594" data-imgfileid="503526573" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/63980fb6-5535-484a-867a-1c1f3682cbaa/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 压缩结构的定量结果。展示了使用不同消融压缩架构的数值测试。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;评测结果如表所示。结果表明，本文方法在 PSNR、SSIM 等指标上取得了相对更优的性能。此外，即便在 4&amp;times;4&amp;times;2 的较高压缩率条件下，该方法仍然能够有效保持原始图像结构。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6mnbEia2xBbwLjA8Gd3qrnJEgdRkD1dSZiaOia1Fv8YfqC7lofODFu2VWw/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.5046296296296297" data-type="png" data-w="1080" data-width="1142" data-height="576" data-imgfileid="503526578" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/6612af3d-77e7-4185-9224-739802d64b5b/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;压缩重建的视觉比较。展示了使用不同可能的神经网络结构和各种压缩设置进行预训练后的重建结果。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6Jb4ocwSOpPOE0abwBuYw2f9JseuBu78WxhJz3b5TpteibHXegoD7uRw/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.3787037037037037" data-type="png" data-w="1080" data-width="1142" data-height="433" data-imgfileid="503526577" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/7e02e277-3ecb-4536-9eca-a8b612251f23/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;记忆压缩模型预训练的影响。展示了使用或未使用记忆压缩模型预训练的结果。输入是相同的 20 秒历史视频，在输出帧中可视化中间帧。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;除此以外，研究团队还在论文中讨论了不同神经网络架构设计之间的权衡取舍。&lt;/p&gt;&lt;p&gt;更多信息，请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>4个月烧掉30亿Token，这位「菜鸟」程序员做出50多个产品，360万人围观</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 01:33:38 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e6620652-ba3e-4b71-842d-7b7d65629045/1767461471804.png" style="width: 700%;" class="fr-fic fr-dib"&gt;长久以来，代码世界的大门似乎只对少数掌握秘术的人敞开。我们被告知：你必须先理解内存、掌握语法、忍受枯燥的文档，才配谈论创造。&lt;/p&gt;&lt;p&gt;现在，随着大模型的发展，编程不再是一场苦修，而是一场大型即时策略游戏。在这个游戏里，很多人学会了与 AI 并肩作战，学会了用一种更纯粹、更直抵本质的方式去构建自己想要的世界。&lt;/p&gt;&lt;p&gt;Ben Tossell（Factory 开发者关系主管）就是其中一员。他是一位不怎么会写代码的人，在过去四个月里，却消耗了 30 亿个 Token。&lt;/p&gt;&lt;p&gt;这意味着每一分、每一秒，他都在通过终端窗口，观察 AI Agent 写下那些他凭一己之力永远无法写出的复杂代码。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="765" data-imgfileid="503526606" data-ratio="0.770392749244713" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6cCmAhoj1l7IB4QYrY5AJ8hCWSHoJY0R3Z2TSa6pZeRFn4iamUQkgQ5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="993" data-width="993" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6a89c0f5-f5a9-43dc-bd1c-4f74601d3093/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;或许有人会将这种方式贬低为所谓的氛围编程（Vibe-coding），但在 Tossell 看来，这个词带有某种傲慢的偏见。它像极了 2019 年人们对无代码（No-code）的刻板印象 &amp;mdash;&amp;mdash; 而正是那一年，他创办了自己的无代码教育公司并最终被 Zapier 收购。这种偏见选择性地忽视了隐藏在交互与调度背后的核心技能。&lt;/p&gt;&lt;p&gt;在 Tossell 看来，编程新范式下，衡量一个人的技术能力不再是看他能否默写语法，而是看他能否驾驭系统。&lt;/p&gt;&lt;p&gt;从无代码时代的先行者，到如今 30 亿 Token 的调度者，Ben Tossell 证明了一件事：在 AI 时代，通向代码世界的最高通行证不是专业背景，而是那种为了探索而探索的欲望。&lt;/p&gt;&lt;p&gt;为了记录这些体验，Tossell 还专门写了一篇文章，其在 X 上的浏览量已经超过了 360 万。接下来我们看看文章内容。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实际交付的产品&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;虽然烧掉了 30 亿 Token，但 Tossell 表示自己也是收获满满，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;个人网站：Tossell 重新设计了个人网站，使其看起来像一个终端 CLI 工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Feed：Tossell 构建了一个简单的社交媒体追踪器，跟踪 subreddit 帖子和 GitHub 问题。它是开源的，得到了 100 多个 stars，很多人也克隆了这个项目。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Factory Wrapped：Tossell 构建了 Factory 产品的第一个版本，展示给团队后他们非常喜欢，并决定将其融入到实际产品中，现在已经上线。Tossell 还添加了新的指南，重新整理了一些内容。虽然这看起来不像传统的编码，但对 Tossell 来说，它依旧是编码，整个过程没有变。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;定制 CLI 工具：创建了一些 CLI 工具，例如 Pylon CLI，团队用它来帮助处理客户支持请求。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;加密追踪器：Tossell 投资了一家能够准确预测金融、天气、健身和蛋白质折叠等动态数据中正面、负面或中性信号的公司。Tossell 基于这些预测构建了一个加密追踪器，能够自动根据预测开设和关闭多空仓位，类似一个迷你对冲基金。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Droidmas：这是一个 12 天的实验或游戏，围绕 X 上人们讨论的不同主题展开，记忆、上下文管理、vibe coding 等。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;AI 指导的视频演示系统：简单来说，给它一个提示，它会创建一个视频。该系统作为自己的导演、制片人和编辑，能实时观看录制过程，并根据情况做出反应。如果遇到问题、bug，或者需要等待响应，它会处理。Tossell 用这个系统制作了一段视频，并由 OpenAI 发布。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外，Tossell 还做了大约 50 个其他项目，其中有些已经被遗弃。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;完全使用 CLI 来工作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Tossell 的战场不在花哨的网页界面，而是在纯粹的 CLI（命令行界面）。他认为终端胜过网页界面，并且还能看到它的工作过程。&lt;/p&gt;&lt;p&gt;每当 Tossell 有一个新的想法，或者遇到一个问题，Tossell 就会在 Droid（Factory 的 CLI）中启动一个新项目。他会与模型交流几次，提供上下文，然后切换到规范（spec）模式，制定构建计划。&lt;/p&gt;&lt;p&gt;在规范模式下，Tossell 会提出很多问题，比如他不理解这个是什么，为什么需要这个而不是那个，难道不能这样做吗？等等。&lt;/p&gt;&lt;p&gt;接着，在运行时，Tossell 让 Opus 4.5 在高自主性模式下运行，查看发生了什么，并在遇到错误时介入，最后进行测试，提供反馈并迭代。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;agents.md 设置&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Tossell 花了很多时间来思考如何设置最佳的 agents.md，因为这基本上就是操作手册。&lt;/p&gt;&lt;p&gt;Tossell 本地有一个 repos 文件夹，所有的编码项目都放在那里。在这个文件夹中，有一个 agents.md 文件，里面明确规定了每个新仓库的设置流程，做什么、不做什么，如何使用 GitHub、如何提交代码之类的内容，还会标明是否使用工作 GitHub 账户或个人 GitHub 账户。&lt;/p&gt;&lt;p&gt;端到端测试是 Tossell 以前没有特别关注的事情，但现在他非常希望在每个项目中都进行端到端测试。&lt;/p&gt;&lt;p&gt;基于他目前的知识和能力，很多时候在构建和测试过程中，总会有一些本应该早早发现的低级 bug，如果一开始就做了测试，可能就能避免这些问题。&lt;/p&gt;&lt;p&gt;Tossell 表示，他也经常查看他人的 agents.md 文件，看看有哪些可以借鉴的地方。Tossell 一直在努力改进自己的文档，从而让每次新的工作会话更加顺畅。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tossell 学到了什么&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Tossell 主要通过 CLI 而非 MCP 进行工作，不过他曾经使用过 MCP，但现在更倾向于使用 CLI 版本，因为它简单且更高效。比如在 Supabase、Vercel 和 GitHub 上，Tossell 总是使用 CLI 而非 MCP。&lt;/p&gt;&lt;p&gt;他还经常为自己的需求创建 CLI 工具。比如，他构建了自己的 Linear CLI，这样就能通过终端查询问题并执行任务，而不需要进入桌面或网页界面。&lt;/p&gt;&lt;p&gt;Bash 命令：Tossell 在处理变更日志的过程中真正理解了 Bash 命令的工作原理。这是一个重复的过程，最终理解了工作流程。Tossell 让 Droid 创建了一个斜杠命令流程，这是 Tossell 第一次正确使用的命令，它运行多个 Bash 命令，并提示模型做一些特定的任务，比如查看 GitHub 差异、检查功能标志的状态，或者将新功能和 bug 修复放到正确的部分。&lt;/p&gt;&lt;p&gt;VPS：Tossell 之前对 VPS 有一个抽象的理解，知道它是一个 24 小时运行的远程计算机。直到 Tossell 真正需要使用 VPS 时，Tossell 才深入了解它的用途。现在，Tossell 使用 VPS 来运行加密追踪器，获取每分钟的数据，同时保持它始终在线。使用 Droid Telegram 机器人时，Tossell 也依赖 VPS，通过 SyncThing 同步本地的仓库到 VPS，这样 Tossell 的仓库总是保持最新状态，能够随时接着上次的状态继续工作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;新的可编程抽象层&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在看到 Andrej Karpathy 的推文时，Tossell 深有感触 &amp;mdash;&amp;mdash; 现在有了一个新的可编程抽象层需要掌握。&lt;/p&gt;&lt;p&gt;在无代码时代，抽象层是像 Webflow、Zapier 和 Airtable 这样的拖拽工具，将它们拼接在一起，让它看起来像真实的软件（直到你遇到限制）。&lt;/p&gt;&lt;p&gt;但现在，我们不再认为自己必须从零开始学习编写代码才能做这些事情，实际上，需要学习的是如何与 AI 合作。如何给它提供合适的提示？如何确保它拥有正确的上下文？如何把各个部分结合起来以及如何随着时间推移不断优化系统等等。&lt;/p&gt;&lt;p&gt;为了更好的掌握 AI 编程技能，Tossell 还会阅读像 Peter Steinberger 这样的程序员的帖子。从他的帖子中，Tossell 看到了他系统的简洁性：他只是和模型互动，让模型做事。这一发现让 Tossell 感到非常有信心，也让 Tossell 明白自己不需要一个复杂的系统。&lt;/p&gt;&lt;p&gt;Tossell 表示，在 X 上，你会看到很多人不断优化，甚至可能是过度优化自己的系统。对于像 Tossell 这样的人来说，这有时会感到很有压力，但他也认为这正是这个系统的魅力所在：它是一个完全可定制的系统，你可以根据自己的需要，让它按照你希望的方式工作。你可以像 Kieran 一样创建一个计划模式，或者像 Peter 一样直接与模型对话。&lt;/p&gt;&lt;p&gt;不过，他有时也会遇到 bug 和问题，但他明白这些问题其实是用户知识的空白，而不是自己当前能力的局限。&lt;/p&gt;&lt;p&gt;Tossell 的任务是识别这些空白，找到它们，并思考：如何确保这种问题永远不会再发生？或者如何确保自己足够理解这部分系统，以便下次发生时能及时发现。&lt;/p&gt;&lt;p&gt;所以，我们需要做什么呢？其实你只需要问模型。模型知道你不知道的所有东西。你可以不断向它提问。它是你永远耐心的、在你肩膀上的专家程序员。 你可以在 agents.md 中写道：Tossell 不是程序员，你需要非常简单地解释给 Tossell 听。你可以根据自己的需求完全定制它&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们学习的方式改变了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;以往，Tossell 尝试过很多次学习编程，每次都是输入这些字符，按下回车，看看是不是显示 hello world。但 Tossell 觉得这和今天的学习方式差别很大。&lt;/p&gt;&lt;p&gt;如果按照传统的方式来学习编程，想要达到现在能构建的程度，可能需要花费数月甚至数年的时间才能有信心自己写代码。&lt;/p&gt;&lt;p&gt;而现在，Tossell 是从理解代码构建项目的系统思维角度来学习的。Tossell 在经营无代码教育公司时，无意中学到了这一点。你依然要理解：Webflow 是前端，Zapier 是 API 路由和连接层，数据流动，而 Airtable 是数据库。所以 Tossell 之前学会了这些系统化的思维，今天可以帮助他理解这些组件。&lt;/p&gt;&lt;p&gt;有太多东西可以学习了，但也是没有任何软件是不可达成的。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;学会提出那些「愚蠢」的问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;人们常会冒出一些看似笨拙的问题，那些资深程序员或许早已习以为常，不再追问。&lt;/p&gt;&lt;p&gt;比如：既然框架是为了简化人类的工作，而现在的 LLM 已经如此强大且智能，为什么我们不干脆抛弃沉重的框架，让它直接写出最纯粹、零依赖的代码？这样不是能大大减少 Bug 和维护成本吗？&lt;/p&gt;&lt;p&gt;后来 Tossell 逐渐意识到，这并非一个傻问题。框架的存在，不仅是工具，更是共识与生态。LLM 的智慧源于海量的训练数据，而这些数据大多根植于现有的成熟框架之中。&lt;/p&gt;&lt;p&gt;这就是 Tossell 构建认知的过程。以前，Tossell 总觉得自己是代码世界的门外汉，甚至觉得这个领域高不可攀；而现在，通过这些直抵本质的提问，Tossell 正一步步拆掉思维的围墙。他不再只是一个使用者，而是正真实地成为这个工程世界的一部分。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关于氛围编程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;虽然现在氛围编程这个词很火，但 Tossell 总觉得它没能触及灵魂。我们所做的，远不止于凭感觉，而是在深度理解系统 &amp;mdash;&amp;mdash; 去拆解它的逻辑，去改进它的构造。作为新时代的技术阶层，我们究竟该如何定义自己？&lt;/p&gt;&lt;p&gt;Tossell 不想称自己为非技术人员，但 Tossell 也不想被框在程序员这个传统标签里。他更像是一个处在某种无名阶层中的探索者。如果说无代码曾是一种误读，那么 Vibe Coding 现在也正带着某种傲慢的偏见。&lt;/p&gt;&lt;p&gt;对 Tossell 来说，编程更像是一场真实存在的宏大游戏。&lt;/p&gt;&lt;p&gt;这种新范式迷人之处在于：每一个创意都能被即刻实践，每一个念头都能深入探索。它不需要从一开始就追求完美，因为在这个过程中，掌握系统真谛才是重要的。就像代码未必都要上传 GitHub，有时候，它们只是通往某个系统深处的路标。&lt;/p&gt;&lt;p&gt;我们不要再为了工具而去生产工具。就像看到别人的 React 抓取工具，Tossell 不再只是感叹，而是会问自己： 我能做一个属于自己的吗？它的原理是什么？这种为了探索而探索的自由，正是新技术赋予我们的最高权限。&lt;/p&gt;&lt;p&gt;以前，学会编程更像是一种重资产投入。Tossell 曾以为，如果我们费尽心力做出了一个想法的简陋原型，结果却无人问津，那我们一定会因为投入了太多情感和成本而无法放手。&lt;/p&gt;&lt;p&gt;但在无代码时代，Tossell 第一次尝到了快节奏的甜头：一两个小时，一个周末，快速成型。如果市场不买单，那就随它去吧。因为投入极低，所以放手极快。&lt;/p&gt;&lt;p&gt;而现在，AI 让这种反馈循环达到了光速。&lt;/p&gt;&lt;p&gt;我们正处于软件大爆炸的前夜。你会看到平庸的作品泛滥，但更会看到无数惊艳的项目井喷。那些资深的程序员正以前所未有的速度发布着令人赞叹的开源工具。这意味着，我们拥有了一个取之不尽的灵感库和零件厂，可以随时克隆、调整、重混。&lt;/p&gt;&lt;p&gt;比起从读写文件这种最底层的语法练起，这种以结果为导向的重组效率高得惊人。反馈是即时的，输出是持续的。你不需要在起跑线上纠结太久，你只需要不断地去尝试，去碰撞。&lt;/p&gt;&lt;p&gt;在这个范式下，每一个创意都不再是沉重的负担，而是可以随时抛出的探针。如果你想，你可以随时随地、随心所欲地去构建一切。&lt;/p&gt;&lt;p&gt;Tossell 坚信，每个渴望进入技术世界的人都能做到这一点。你不需要计算机学位，你只需要一份允许自己去玩的许可。把编程看作一场游戏：去注册一个 CLI 智能体，告诉它你想做一个 RSS 追踪器、健身应用或个人网站。然后，按下启动键。&lt;/p&gt;&lt;p&gt;在这个过程中，你会撞上无数 Bug，但这正是最精彩的部分。你不再被错误困扰，而是开始好奇：为什么会这样？你要知道，即便顶尖专家也难逃 Bug 的围攻，而你拥有 ChatGPT 或 Claude 这样的多维智囊团。你可以从不同模型中获取视角，在无数种方案中做出选择。&lt;/p&gt;&lt;p&gt;在工具的丛林里，准则只有一条：最快、最简、最远。&lt;/p&gt;&lt;p&gt;面对琳琅满目的工具，没必要陷入选择瘫痪。选定一个，深挖下去。如果觉得缺了什么，尝试自己去造。&lt;/p&gt;&lt;p&gt;Tossell 最后总结说：「这整件事对我而言，是一场巨大的、令人享受的学习实验。不断构建，不断向前失败，然后不断把新作品推向世界。」&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：https://x.com/bentossell/status/2006352820140749073&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>LeCun在Meta还有论文：JEPA物理规划的「终极指南」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 01:29:24 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5606dbf9-4c0f-472c-89ad-f665d94b1d77/1767461152128.png" style="width: 700%;" class="fr-fic fr-dib"&gt;长期以来，AI 领域一直怀揣着一个宏大的梦想：创造出能够像人类一样直观理解物理世界，并在从未见过的任务和环境中游刃有余的智能体。&lt;/p&gt;&lt;p&gt;传统的强化学习方法往往比较笨拙，需要通过无数次的试错和海量的样本才能学到一点皮毛，这在奖励信号稀疏的现实环境中简直是灾难。&lt;/p&gt;&lt;p&gt;为了打破这一僵局，研究者们提出了「&lt;strong&gt;世界模型&lt;/strong&gt;」这一概念，即让智能体在脑海中构建一个物理模拟器，通过预测未来状态来进行演练。&lt;/p&gt;&lt;p&gt;近年来，虽然能够生成精美像素画面的生成式模型层出不穷，但对于物理规划而言，沉溺于无关紧要的细节（如背景烟雾的流动）往往是低效的。真正的挑战在于，如何在错综复杂的原始视觉输入中提取抽象精髓。&lt;/p&gt;&lt;p&gt;这便引出了本研究的主角：&lt;strong&gt;JEPA-WM（联合嵌入预测世界模型）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;从名字也能看出来，这个模型与 Yann LeCun 的 &lt;strong&gt;JEPA（联合嵌入预测架构）&lt;/strong&gt;紧密相关。事实上也确实如此，并且 Yann LeCun 本人也是该论文的作者之一。更有意思的是，在这篇论文中，Yann LeCun 的所属机构为 Meta FAIR。不知道这是不是他在 Meta 的最后一篇论文？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX68YxNjugbtXkMjk0dMp0GGdRictyDWnRclVzOc4KPPywtgGo16ed5rEA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.38055555555555554" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526588" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/2403494c-94f8-4be4-aac9-cf23f974f9cb/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2512.24497&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;JEPA-WM 继承了 JEPA 的衣钵，不再纠结于像素级的重建，而是&lt;strong&gt;在高度抽象的表征空间内进行预判&lt;/strong&gt;。在这项研究中，团队试图通过对架构、目标函数和规划算法的全方位扫描，揭示究竟是什么驱动了物理规划的成功，并试图为机器人装上一个更理性的「大脑」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;JEPA-WM 核心方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该团队将 JEPA-WM 的训练与规划流程形式化为一套统一的「终极指南」，重点在于如何在学习到的特征空间中模拟动力学。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 层次化的编码与预测架构&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6m0DCDB1rO2PjSb4bpE7KbhpKTD5Wf39c6LOM3CVCJ6xqibxNP4jcd7w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5027777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526589" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/1cb180f9-e7e4-4829-9fa8-269446845f43/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在训练阶段，模型主要由四部分交织而成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;视觉编码器&lt;/strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6Sh1picVvKKxS6DeJmf4eib3TY4KGHWzNgk3gzJ20jibGnRtX212Y3baWA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.6962962962962963" data-s="300,640" data-type="png" data-w="135" type="block" data-imgfileid="503526587" data-aistatus="1" data-original-style="width: 35px;height: 24px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/26c8bc0a-1fc5-4001-8991-b4fa80bf993c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 5.23%;"&gt;：使用预训练且冻结的 ViT 权重（如 DINOv2 或 DINOv3）来提取空间特征，确保模型具备敏锐的视觉感知力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;本体感受编码器&lt;/strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6XdHVYRHd6ORX4icLNX7Zl23KJA9ibwrNKnIiafmtmsAm4pJ0XrYq1bickg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5272727272727272" data-s="300,640" data-type="png" data-w="165" type="block" data-imgfileid="503526590" data-aistatus="1" data-original-style="width: 43px;height: 23px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7adfe226-d658-4b59-ac10-3b86361b8ca2/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dii" style="width: 6.63%;"&gt;：一个浅层网络，用于捕捉机器人自身的关节角度和位姿，这与视觉信息共同构成了全局状态嵌入。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;动作编码器 &lt;/strong&gt;A_&amp;theta;：将机器人的控制指令转化为同维度的特征向量。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;预测器 &lt;/strong&gt;P_&amp;theta;：这是模型的心脏。它接收过去窗口内的观测序列 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6lsaA7j2br8pUhZTqqxkIJg3QF94pAaTMfMc8ibDUleyibicagNqY0RERA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3090909090909091" data-s="300,640" data-type="png" data-w="110" type="block" data-imgfileid="503526591" data-aistatus="1" data-original-style="width: 76px;height: 23px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/54435c81-cb97-4179-8f1f-e95927d2e1aa/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 10.46%;"&gt; 和动作序列&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6pV75vaHjK0sv1aKOIVc2cRA3Gfy5CicW1JE8oA0DCtGj5AzwROaR2aA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.32075471698113206" data-s="300,640" data-type="png" data-w="106" type="block" data-imgfileid="503526592" data-aistatus="1" data-original-style="width: 65px;height: 21px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/2fc6422e-f46b-41b9-b7a6-fcf91c012816/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dii" style="width: 10.55%;"&gt;&amp;nbsp;，在因果掩码的保护下，并行预测下一时刻的状态嵌入。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. 多步展开与动作调节细节&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了让模型不至于「走一步看一步」，研究者引入了多步展开损失&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX662uNuU6G1v2DWy1n0ia00QWxtu4MlibD2RfbKLVMMbpemAodSF6voVIg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.9565217391304348" data-s="300,640" data-type="png" data-w="46" type="block" data-imgfileid="503526593" data-aistatus="1" data-original-style="width: 26px;height: 25px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d3e2acf4-d952-4da3-9154-896c1cda9c4f/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dii" style="width: 3.21%;"&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6HAgWXTllUgVgpKHWRQCBe72oXs0fCiaO5ic6J1KRKqaP7ZxibwXgzibwbg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.13165829145728644" data-s="300,640" data-type="png" data-w="995" type="block" data-imgfileid="503526594" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/ad619104-4b01-4067-8d3c-492a307a0dbb/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在训练时，模型不仅要预测下一帧，还要学会在没有真实观测反馈的情况下，基于自己的预测结果递归生成后续状态。为了提高效率，采用了截断反向传播（TBPTT），即只针对最后一步的预测误差计算梯度，而切断之前的累积梯度。&lt;/p&gt;&lt;p&gt;在动作信息如何干预预测过程上，该团队对比了三种关键方案：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;特征调节（Feature Conditioning）&lt;/strong&gt;：将动作向量直接拼接到每一个视觉特征向量上，增加了预测器的隐藏层维度。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;序列调节（Sequence Conditioning）&lt;/strong&gt;：将动作作为一个独立的 Token 插入到 ViT 的输入序列中，通过注意力机制进行信息分发。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;自适应层归一化（AdaLN）&lt;/strong&gt;：动作嵌入被投影为缩放和偏移参数，在每一个 Transformer 块中动态调制归一化统计量，这能有效防止动作信号在深层网络中「淡出」。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;3. 规划逻辑：在嵌入空间中寻找最优解&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;规划被建模为一个在动作空间 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6BEtwOLy7qiauayBTIWiafBr4APxqYEibicQhAAIjgxjwu6tPobsP2gBFxA/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.3761467889908257" data-s="300,640" data-type="png" data-w="109" type="block" data-imgfileid="503526595" data-aistatus="1" data-original-style="width: 62px;height: 23px;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/c50ce09b-7f81-4606-9059-2be7da1962bc/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 6.6%;"&gt; 上的优化问题。给定初始观测 o_t 和目标图像 o_g，智能体会在其内部模型中「试运行」N 条候选路径。评价标准是预测终点的嵌入向量与目标嵌入向量之间的距离&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX60b7YzibOjlKSlWvV5aaLMKE82FXSl859DZU8u7WnyGoWbCU3xnmhE4g/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.21794871794871795" data-s="300,640" data-type="png" data-w="234" type="block" data-imgfileid="503526596" data-aistatus="1" data-original-style="width: 123px;height: 27px;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/14104433-63b2-4b8c-9592-f8ec17190b1a/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dii" style="width: 12.47%;"&gt;。通过多轮迭代，优化器会不断收敛动作分布，最终输出最优的第一步或前 m 步动作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验与结果：从模拟器到真实机械臂&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队在 Metaworld（42 个操纵任务）、Push-T（物体推送）、PointMaze（导航）以及 DROID（真实机械臂数据集）上进行了评估。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 规划器之争：梯度 vs 采样&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;实验结果揭示了一个有趣的现象：在像 Metaworld 这种成本曲线相对平滑的任务中，基于梯度的 Adam 或 GD 优化器表现惊人，因为它们能顺着梯度迅速找到目标。但在 2D 导航（Wall, Maze）任务中，梯度法极易卡在局部极小值（例如对着墙猛撞而不懂得绕过门口），此时&lt;strong&gt;基于采样的交叉熵方法（CEM）&lt;/strong&gt;凭借其探索能力完胜。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWictGgomjsDDxhRKpchgqbX66nLeiahjria1mGzUqhrAhuFILarTlRmZ1niaOoAzqfnFDiaDT5qpfXiaxaA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=11" data-ratio="0.44537037037037036" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503526601" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/98c77a15-ec3e-4b20-ad48-b5ed6b21d6c5/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，新引入的&lt;strong&gt; Nevergrad（NG）&lt;/strong&gt;规划器在无需调参的情况下展现了与 CEM 相当的实力，尤其适合跨任务迁移。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 关键因素的「贡献度」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了量化不同设计决策对智能体最终表现的影响，研究团队采用了一种严谨的控制变量法。&lt;/p&gt;&lt;p&gt;他们以一个基础配置（DINO-WM 结合 ViT-S 编码器及 6 层预测器）为基准，独立改变每一个核心组件，从而在复杂的系统工程中剥离出真正驱动性能增长的关键因子。通过在 Metaworld、Push-T 等多种异构环境下进行数以万计的幕（Episode）测试，实验揭示了世界模型在处理物理逻辑时的内在偏好。以下是影响物理规划成败的核心贡献因素：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;本体感受的显著增益&lt;/strong&gt;：引入机器人内部状态信息（如关节角度、末端位姿）能够一致性地提高规划成功率。在 Metaworld 任务中，这能有效减少机械臂在目标点附近震荡的情况，提供更精准的距离感知。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6I4CRzCs7NnLArxjqW5X4OalfGvdz15rx36qRnKCLWTA59AYBpYJE8A/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.4222222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526597" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/431501cc-df38-40c7-8822-32fd030cbc94/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;编码器架构&lt;/strong&gt;：DINO 系列编码器（DINOv2/v3）在所有任务中均表现出对 V-JEPA 等视频编码器的明显优势。这归功于 DINO 强大的细粒度目标分割能力，这对于需要精确感知物体位置的操纵和导航任务至关重要。在视觉复杂度更高的真实数据（DROID）中，DINOv3 的优势进一步扩大。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;动作调节技术的微妙差异&lt;/strong&gt;：实验发现 AdaLN（自适应层归一化）调节技术在平均性能上表现最强，且计算效率更高。它通过在 Transformer 的每一层注入动作信息，有效防止了控制信号在深层网络传递过程中的消失，相比传统的特征拼接（ftcond）或序列拼接（seqcond）更具稳健性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6BJsmkchtauialiaQwDYY0duEUqZ0MPIbtynEfw1SCXXVdLxfbPPxuCVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.4546296296296296" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526598" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/9b7f24c9-4896-4ce1-b9a1-1160ef9ded68/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;训练上下文长度的权衡&lt;/strong&gt;：预测器需要至少 2 帧上下文来推断速度信息，这在 W=1 与 W=2 之间的巨大性能鸿沟中得到了印证。然而，&lt;strong&gt;盲目增加上下文长度（如 W &amp;gt; 5）反而有害&lt;/strong&gt;，因为这会减少训练中看到的独特轨迹数量，并可能引入无用的梯度噪声。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6ASiaZ9gCydTDkc0787DSCbN0NiakkALDMsq2ib6G0SVicfVrmF2cn9Hiahw/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.4564814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526599" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/6ad53ecd-62c4-4f20-afd8-0ca360e0b253/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;模型规模&lt;/strong&gt;：这是一个令人意外的发现：&lt;strong&gt;在简单的模拟环境（如 Maze, Wall）中，增大模型规模（从 ViT-S 到 ViT-L）非但没有帮助，反而可能由于嵌入空间过于复杂而导致规划效率下降&lt;/strong&gt;。但对于复杂的现实数据（DROID），大容量的编码器和更深的预测器则展现出了明确的正相关收益，说明任务的物理复杂度决定了智能体所需的智力上限。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;多步损失的对齐作用&lt;/strong&gt;：在训练中加入 2 步展开损失能显著改善预测器的长时稳定性，使其训练任务与测试时的递归规划任务更加对齐。对于最复杂的 DROID 任务，最佳的展开步数甚至需要达到 6 步。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 提出的最优解&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究最终汇总所有洞察，提出了针对不同任务的最优配置：&lt;strong&gt;在模拟器中使用 ViT-S 配以 AdaLN，而在真实复杂场景中使用 DINOv3 ViT-L 配以 12 层深度的预测器。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWictGgomjsDDxhRKpchgqbX6PgkuHVyd2aD1v9wHU2vtYGa1DOhHk2JmicLhunA96KcctDOtTYoic4Ug/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.22962962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526600" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/06fdad66-87ca-47e8-9099-86c87ea0e617/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在与 DINO-WM 和 V-JEPA-2-AC 的直接较量中，该模型在几乎所有维度上均取得了领先。&lt;/p&gt;&lt;p&gt;更多详情请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>微信炼出扩散语言模型，实现vLLM部署AR模型3倍加速，低熵场景超10倍</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 01:23:27 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/a37674ca-a177-4add-b12f-94e3516a5105/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;腾讯微信 AI 团队提出 WeDLM（WeChat Diffusion Language Model），通过在标准因果注意力下实现扩散式解码，在数学推理等任务上实现相比 vLLM 部署的 AR 模型 3 倍以上加速，低熵场景更可达 10 倍以上，同时保持甚至提升生成质量。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;引言&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;自回归（AR）生成是当前大语言模型的主流解码范式，但其逐 token 生成的特性限制了推理效率。扩散语言模型（Diffusion LLMs）通过并行恢复多个 mask token 提供了一种替代方案，然而在实践中，现有扩散模型往往难以在推理速度上超越经过高度优化的 AR 推理引擎（如 vLLM）。&lt;/p&gt;&lt;p&gt;问题的关键在于：大多数扩散语言模型采用双向注意力机制，这与标准的 KV 缓存机制不兼容，导致并行预测的优势无法转化为实际的速度提升。&lt;/p&gt;&lt;p&gt;近日，腾讯微信 AI 团队提出了 &lt;strong&gt;WeDLM&lt;/strong&gt;（WeChat Diffusion Language Model），这是&lt;strong&gt;首个在工业级推理引擎（vLLM）优化条件下，推理速度超越同等 AR 模型的扩散语言模型&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe0anhK0cBcVt1P9PTmdMiaCCNDTjrU97HrXYiciajZeSxy1PFzOgsKxODg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5176991150442478" data-s="300,640" data-type="png" data-w="904" type="block" data-imgfileid="503526249" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/24aaf4a5-5411-4106-a5b0-b5f3982a6a51/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文作者：刘瑷玮、何明桦、曾少勋、张思钧、张林昊、武楚涵、贾巍、刘源、周霄、周杰（腾讯微信 AI）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://wedlm.github.io&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub：https://github.com/tencent/WeDLM&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;模型权重：https://huggingface.co/collections/tencent/wedlm&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以下是模型效果：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9EricQXXByphb4tN0ha6mibe79dCBcQmEC2tPPkyY3g1kJLtJR0eUsZMowFYdARekNDeIynVWf06sA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=2" data-ratio="0.6265060240963856" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503526250" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c42cb496-b8e8-4bf1-8e24-809266f55ba3/640.gif" data-order="0" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;上图展示了vLLM 部署的 Qwen3-8B-Instruct（左） 与 &amp;nbsp;WeDLM-8B-Instruct（右） 在相同 prompt 下的实时生成对比。可以直观看到，WeDLM 的生成速度明显更快。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心思路：让扩散解码兼容 KV 缓存&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;WeDLM 的核心洞察是：&lt;strong&gt;mask 恢复并不需要双向注意力&lt;/strong&gt;。扩散式解码只需要让每个 mask 位置能够访问所有已观测的 token，这完全可以在标准因果注意力下实现。&lt;/p&gt;&lt;p&gt;研究团队提出了一个关键指标 &amp;mdash;&amp;mdash; &lt;strong&gt;前缀可缓存性（Prefix Cacheability）&lt;/strong&gt;：在 KV 缓存解码中，只有形成连续左到右前缀的 token 才能被缓存复用。因此，真正影响推理效率的不是「每步预测多少 token」，而是「有多少预测能够转化为可缓存的前缀」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeetiaeODtfAUpE9O0ZXqTeclE7DszONyNSEMp6xKZvcsj0YL3BLuDvQA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.45185185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526257" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f528027b-5ecb-4b85-a491-8cf70b4fbedb/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图：WeDLM-8B 在数学推理任务上实现约 3 倍加速，同时在准确率和推理速度上显著超越 LLaDA、Dream 等扩散模型。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术方案&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;拓扑重排序（Topological Reordering）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;WeDLM 通过拓扑重排序在保持因果注意力的同时，让 mask 位置能够访问完整的观测上下文。具体而言，将所有已观测 token 移动到物理序列的前端，同时通过 RoPE 位置编码保留其逻辑位置。这样，在标准因果 mask 下，每个待预测位置都能看到所有已知信息。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeicv2fialD73zw2k3HdgTDTYKYr8vkYDMUItqsxCdNaVvIScLBWOZiazhg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.497787610619469" data-s="300,640" data-type="png" data-w="904" type="block" data-imgfileid="503526258" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/e8acc538-8319-499c-a0f3-80f111161419/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;双流掩码（Dual-Stream Masking）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为缩小训练与推理的分布差异，WeDLM 设计了双流训练策略：构建一个干净的「记忆流」和一个带 mask 的「预测流」，两者共享位置编码。预测流中的每个 block 从记忆流获取干净的历史上下文，而非可能带噪的中间预测结果。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;流式并行解码（Streaming Parallel Decoding）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;推理阶段，WeDLM 采用流式并行解码策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;距离惩罚机制&lt;/strong&gt;：优先解码靠左的位置，促进左到右的前缀增长&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;即时缓存&lt;/strong&gt;：在因果注意力下，已解码 token 立即成为有效缓存&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;动态滑动窗口&lt;/strong&gt;：持续填充新的 mask 位置，避免 block 边界的等待开销&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeh9LBDDv1J9FucEr9AGOKFnRzCf77L8S3VMib70tTF8pkmfB1RJKh4sA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3827433628318584" data-s="300,640" data-type="png" data-w="904" type="block" data-imgfileid="503526259" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/05e437b2-7d5a-4a94-bd3d-8b768c4f9228/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图：传统 block 解码需要等待整个 block 完成才能提交，而 WeDLM 的流式解码可以即时提交已解析的前缀。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;生成质量&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;WeDLM 基于 Qwen2.5-7B 和 Qwen3-8B 进行训练，使用 100B token 进行继续预训练，10B token 进行 SFT。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeNQSWsYTQ3LsdIiaWJSQEPByUXibsXjwDqTBZA005L4TSrclS9K3icNzbA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5796460176991151" data-s="300,640" data-type="png" data-w="904" type="block" data-imgfileid="503526260" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/2acecb41-5efb-4d21-b342-a3017bdf0ce0/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在 base 模型评测中，&lt;strong&gt;WeDLM-8B 平均得分 74.72，超越 Qwen3-8B（72.61）2.1 个点&lt;/strong&gt;。在数学推理任务上提升尤为显著：GSM8K 提升 4.2 个点，MATH 提升 2.8 个点。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibemqRY2ibBVEJUf1ePdSZAuibc3mZ7t66J5XunVvyztIZYiaTIT0tA66hGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.5176991150442478" data-s="300,640" data-type="png" data-w="904" type="block" data-imgfileid="503526261" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/e7516cce-d36b-4ec0-ba61-c7305e6e808d/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在 instruct 模型评测中，&lt;strong&gt;WeDLM-8B-Instruct 平均得分 77.53，超越 Qwen3-8B-Instruct（75.12）2.4 个点&lt;/strong&gt;，也领先于 SDAR-8B-Instruct（74.22）等扩散模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推理速度&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;关键亮点：所有速度对比均基于 vLLM 部署的 AR 模型基线，而非未优化的实现。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibeaS02hxlXDSZBSL8fFHiaQ8MJ3Btjy8F5yMVxVnYcF8eoByJCgiaAxRKg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.27564102564102566" data-s="300,640" data-type="png" data-w="936" type="block" data-imgfileid="503526262" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/7a316bf7-058a-4c8c-a9d0-5aa1c741036d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;研究团队在论文中展示了不同熵值场景下的速度差异：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;低熵场景（如计数任务）：由于输出高度可预测，模型可以大胆并行预测并接受多个 token，实测达到 1673.3 tokens/s&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;中熵场景（如数学推导）：结构化的推理步骤仍然具有较好的可预测性，实测 745.2 tokens/s&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;高熵场景（如开放问答）：语义多样性高，并行接受率下降，实测 197.8 tokens/s&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;快速上手&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;安装方式非常简单，只需通过 pip 从 GitHub 安装即可。安装完成后，可使用 Python API 快速调用模型进行推理。详细的使用文档和示例代码请参见项目 GitHub 主页。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;WeDLM 的贡献可以归纳为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;因果扩散框架：在标准因果注意力下实现 mask 恢复，天然兼容 KV 缓存和现有推理基础设施（FlashAttention、PagedAttention、CUDA Graphs 等）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;流式并行解码：通过距离惩罚和动态滑动窗口，最大化前缀提交率&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;首次在速度上超越工业级推理引擎部署的 AR 模型：在 vLLM 优化条件下的公平对比中，数学推理实现 3 倍以上加速，低熵场景超过 10 倍&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;研究团队指出，这项工作表明「前缀可缓存性」应当作为并行文本生成的一等设计目标。未来的扩散语言模型应更多地被视为高效的多 token 预测机制 &amp;mdash;&amp;mdash; 并行生成 token 的价值，取决于这些 token 能多快地转化为可缓存的前缀。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>陶哲轩：AI让数学进入「工业化」时代，数学家也可以是「包工头」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sun, 04 Jan 2026 01:19:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-04</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-04</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/941dfc0f-d694-481c-a585-dedc852f0e3f/1767460311831.png" style="width: 700%;" class="fr-fic fr-dib"&gt;很多人提到数学研究，脑子里浮现的还是那个画面：一个人，一块白板，来回踱步，等灵感突然降临。&lt;/p&gt;&lt;p&gt;但当今世界最伟大的数学家之一、菲尔兹奖得主陶哲轩却告诉我们：这种「手工业时代」的数学研究模式正处于崩溃边缘，一场由 AI 和形式化证明语言（如 Lean）引领的「工业革命」已经悄然开启。&lt;/p&gt;&lt;p&gt;这一洞察来自陶哲轩最近的一次访谈：&lt;a href="https://mp.weixin.qq.com/s/wVTRbVtMZX3-WISdtrjbNg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0d5c4674-19d4-402a-a7fd-70fbb68fe850/1767460324061.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;视频标题：Terry Tao on the future of mathematics&lt;/p&gt;&lt;p&gt;视频链接：https://www.youtube.com/watch?v=4ykbHwZQ8iU&lt;/p&gt;&lt;p&gt;在访谈中，陶哲轩指出，数学研究中存在大量的重复性劳动，如查阅文献、调整他人论文中的参数以及繁琐的计算。通过 LLM 辅助的自动形式化（Auto-formalization），这些琐碎的工作正逐渐变得轻松。&lt;/p&gt;&lt;p&gt;与此同时，Lean 等形式化证明语言与 AI 的深度融合正在改变数学协作的本质。形式化并不只是「把证明写得更严格」，而是&lt;strong&gt;把数学拆成了可以独立验证的原子步骤&lt;/strong&gt;。这种原子化让分布式科研第一次变得可行。&lt;/p&gt;&lt;p&gt;陶哲轩预见到，数学界将出现类似软件工程的分工模式。未来的数学家可能扮演「架构师」或项目经理的角色，领导大型协作项目。这种模块化的研究方式可能允许「公民数学家」（非专业领域专家但具备某些技能的人）参与到前沿研究中，降低进入门槛。如此一来，数学研究的进展或显著加速。&lt;/p&gt;&lt;p&gt;参与访谈的另外两位数学家分别是前 OpenAI 研究科学家、Morph Labs 创始人 Jesse Han，以及斯坦福大学助理教授 Jared Duker Lichtman。&lt;/p&gt;&lt;p&gt;以下是机器之心整理的访谈记录。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从几十年到 18 个月 &amp;nbsp;数学研究正被加速&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;： 说实话，在我整个学术生涯中，我一直觉得我们做数学的方式少了点什么。我们在研究一个数学问题时，总想找到那个能打开问题大门的精妙想法。但在那之前，有大量枯燥的苦力活。比如文献综述，比如你在别人论文里看到一个技巧想用到自己的问题上，但所有的输入条件都有点不一样，你就得手动调整所有的论证。还有那些计算 &amp;mdash;&amp;mdash; 它们确实有用，能帮你建立直觉，但很多时候就是硬磨，不停地算啊算。我以前也试过写一些小程序来加速某些计算，但那时候技术还不成熟。&lt;/p&gt;&lt;p&gt;大概两年前，就在 IPAM（纯粹与应用数学研究所）这里，我们办了一个机器辅助证明的会议，我是组织者之一。在那次会议上，我们接触到了各种各样的尝试 &amp;mdash;&amp;mdash;SAT 求解器、计算机辅助软件包、大语言模型。ChatGPT 刚问世，还有 Lean。那是一个令人兴奋的世界，你突然发现很多事情变得可能了，而且正在发生。比如 Peter Scholze 刚完成了一个长达 18 个月的项目，把他的一个重要定理形式化了 &amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：液态张量实验。&lt;/p&gt;&lt;p&gt;陶哲轩： 对，液态张量实验。这是个大工程，一个定理花了 18 个月。但这已经被认为是巨大的突破了，因为 &lt;strong&gt;20 世纪的那些形式化项目，动辄要花几十年才能完成。所以这本身就是一个巨大的提速&lt;/strong&gt;，部分原因是我们已经学会了如何使用软件工程的那些工具，比如 GitHub，以及更智能地组织这些项目。从那以后，我对 AI 和形式化都产生了浓厚的兴趣 &amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;： 就是因为那次会议。&lt;/p&gt;&lt;p&gt;陶哲轩： 对，没错。我开始相信这就是数学的未来，也开始接受一些采访谈这个话题。但到了某个时候，你不能光说不练，得真正动手。所以我就去学了 Lean，花了大概一个月，但其实挺好玩的。这让我想起了写本科分析教材的经历 &amp;mdash;&amp;mdash; 真的是从基础开始，把每一步都做到完全严格。感觉就像在玩电子游戏。我记得 Kevin Buzzard 说过，Lean 是世界上最好玩的电子游戏，大概是这个意思。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;： 让人完全上瘾。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;： 对某类人来说确实非常上瘾。而在过去一年里，&lt;strong&gt;大语言模型追上来了，它们现在可以自动形式化单个证明步骤，真正开始减轻形式化过程中的苦力活，甚至到了可以实时完成的程度。这打开了无数的可能性&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;形式化正在改变数学思维 &amp;nbsp;把含混经验转化为可检验的结构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;： 我第一次接触 Kevin Buzzard，是 2017 年他在 MSRI（美国数学科学研究所）教自守形式那门课的时候。几年后我跟他聊天，他说他当时根本没在关注那门课的内容，因为那个夏天他正在自学 Lean&amp;mdash;&amp;mdash; 在 Tom Hales 在第一届大型证明会议上告诉大家 Lean 将是未来之后。&lt;/p&gt;&lt;p&gt;我自己在第一次学习形式化证明的时候，有一个体会是：我慢慢意识到，其实我从来没有真正学会清晰地思考数学论证。高等数学的证明里有一种普遍的，或者说文化性的混乱感。我很好奇，当你越来越深入地去预判如何形式化证明时，你对自己数学思维的认知有什么变化？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;： 确实有一些变化，改变了我写论文的方式。我现在能看到那些「隐形假设」&amp;mdash;&amp;mdash; 那些我们习惯性地默认成立的东西。你会更认真地思考：怎样才是最干净的定义方式？因为在 Lean 里，当你定义一个概念并想使用它时，你必须先建立一堆琐碎的引理，就是所谓的 API，围绕着每个概念。这些东西在论文里往往是「显然这个概念是单调的」「显然它在某种运算下封闭」，但你其实应该证明它们。而且你会发现，如果定义得不够好，形式化这些「琐碎」命题要花两倍甚至五倍的时间。所以这让我学会了如何精简自己的写作。有时候我会对合作者有点不耐烦，因为有些人没有这个视角，还在用老式的非形式化风格写东西。&lt;/p&gt;&lt;p&gt;Heather Macbeth 写过一篇文章，讲形式化和自动化如何催生了一种新的证明写作风格。传统的证明通常是线性的，从 A 到 B，一步一步推，比如一串等式。但有了自动化工具，你可以说：这里有 10 个相关的事实，用一个标准工具来找出这 10 个事实的正确组合就能完成证明。而这个组合往往很无聊，没什么意思 &amp;mdash;&amp;mdash; 你知道某种线性代数之类的东西能从这些事实得出结论。这是一种不同的证明写作风格，某种意义上反而更容易读懂。对人类来说更难验证，但你能更清楚地看到一个证明的输入和输出，而传统写法往往把这些藏起来了。&lt;/p&gt;&lt;p&gt;Jared Duker Lichtman：Peter Scholze 的情况也是这样，他说过，在形式化过程中获得反馈，实际上让他对某个关键引理的细节思考得更清楚了，他觉得这是一个非常有价值的过程。你有一个很棒的框架 &amp;mdash;&amp;mdash; 前严谨阶段、严谨阶段、后严谨阶段。这个框架怎么融入我们现在讨论的话题？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：对，我写过一篇传播很广的文章，讲学习数学的三个阶段。第一个是前严谨阶段，你并不真正知道什么是证明，但对什么行得通、什么行不通有一些模糊的直觉。这通常是小学阶段对数学的理解方式。有时候你的直觉是对的，有时候是错的，但你没有办法分辨哪个是哪个。&lt;/p&gt;&lt;p&gt;然后是严谨阶段，你被迫完全按照规矩来，每一步都要做得准确无误。但在这个阶段，你往往会失去直觉，因为你全部的注意力都在确保每一步都正确。不过这有助于清除你所有错误的直觉，因为你能看到精确的反例，知道论证在哪里失败了。而所有好的直觉 &amp;mdash;&amp;mdash; 那些与严谨推理一致的 &amp;mdash;&amp;mdash; 都会保留下来。&lt;/p&gt;&lt;p&gt;然后是后严谨阶段，你可以在两种模式之间自由切换。你可以非形式化地论证，但现在是安全的，因为你已经清除了所有错误的直觉。你知道如果需要的话，可以把它转换回严谨的形式。反过来，你也可以读一个严谨的论证，然后把它转换成直觉性的语言。&lt;/p&gt;&lt;p&gt;Lean 确实帮我清理了一些思维中低效或错误的习惯。一个很常见的低效问题是：当你在教科书里陈述一个定理时，往往会加入太多假设。你有点过于保守，想确保证明是对的，就加了一堆额外条件 &amp;mdash;&amp;mdash; 这个非空、那个连续、这个为正之类的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：你会想去对这些假设进行压力测试。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：对。但其实还有自动化的 linter 工具，当你在 Lean 里形式化某个东西，证明结束后它会说：「顺便提一下，你从来没用过这个假设。」然后你就会想：「哦，确实，我其实根本不需要正性条件。」文献里确实有过这样的真正突破：人们心里有个思维定式，觉得某个工具只能用在比如正数的情况下，但其实证明在没有正性条件的情况下照样成立，只是没人注意到。形式化能让你自动发现每个工具的自然适用范围。这已经非常有用了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：这个说法很精辟。我们花了很多时间思考一个问题：来自软件工程和计算机科学的深度洞见，如何影响人们对数学认知和数学研究的思考方式。你刚才说的形式化如何让我们更清楚地理解每个定理的假设和输出，这其实就是良好的软件工程实践。Dijkstra 就专门讲过，人们应该更多地去推理前置条件和后置条件。同样的道理，数学家习惯在定理里堆一堆可能用不上的假设，这在软件工程里是典型的反模式 &amp;mdash;&amp;mdash; 一种公认的坏习惯。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;两个顿悟时刻 &amp;nbsp;形式化正在改变数学领域协作方式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：我特别想问你的是：你在形式化过程中的「顿悟时刻」是什么？显然一开始有很高的启动门槛，你得学习所有这些关于这门小众学术编程语言的晦涩知识。但是，在哪个时刻你意识到，把数学变成软件这个过程，不仅仅是翻译，还能加速你的理解，加速数学发现的过程？&lt;/p&gt;&lt;p&gt;对我来说，是在形式化连续统假设的独立性时。有一个时刻我完全迷失了，所有的参考资料都是错的，但我发现可以打开或关闭某些关键假设，然后很快就获得了比任何教科书都深得多的理解。我很好奇你有没有类似的经历。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：有，我有两个印象特别深刻的时刻。&lt;/p&gt;&lt;p&gt;第一个是我在形式化一个和合作者一起证明的定理，叫 PFR 猜想 &amp;mdash;&amp;mdash; 多项式 Freiman-Ruzsa 猜想。结论里有一个指数常数，我们当时证明的是：存在一个常数，使得某个性质成立，而这个常数最后算出来是 12。原因并不神秘，只是把证明中所有零零碎碎的小常数一路累积下来，最后自然就变成了 12。&lt;/p&gt;&lt;p&gt;我们花了大概三周时间，把这个「C 等于 12」的结论完整形式化成 Lean 代码。那是一个完全没有 AI 的年代，整整 20 个人，全靠手工，是一次非常浩大的工程。&lt;/p&gt;&lt;p&gt;后来，有人往 arXiv 上放了一个很短的预印本，说如果你回到原始论文，只要做五个小改动，就可以把这个 12 降到 11。于是大家就开始讨论：那我们要不要把 C 等于 11 也形式化一遍？问题在于，C 等于 12 已经花了我们三周时间，那再来一遍岂不是又是三周？&lt;/p&gt;&lt;p&gt;实际情况并不完全是这样，但直觉上你几乎只是把最终定理里的 12 改成 11。然后你会发现，大概有五行代码变红了，也就是证明不再成立了。但你去看那篇新的预印本，就会发现，哦，这五行我知道该怎么改。结果一改，这五行是好了，又有另外十行变红了。于是你再回去改那十行。就这样来回几次，我们在一天之内就把整个证明更新成了 C 等于 11。&lt;/p&gt;&lt;p&gt;所以，&lt;strong&gt;形式化确实很繁琐，尤其是第一次把一个结果完整写出来的时候。但一旦你想修改一个已有的证明，它就比传统数学方式好得多。这是我第一个非常深刻的体会&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;第二个经历来自一个名为 Equation of Theories 的项目，然后对一项研究进行形式化时，有一次很深的体会。当时有人在把另一位作者写的证明形式化，结果卡在了某一步。我当时也并不了解整个证明的全貌，甚至可以说完全不理解整体结构，但我盯着那一行代码看了一会儿，发现我其实能理解这一行在做什么。&lt;/p&gt;&lt;p&gt;我能够理解足够多的上下文，从而指出：你这里其实只需要复制并稍微修改这一行，让它在类型上匹配，这样就能调用这个工具了。&lt;/p&gt;&lt;p&gt;也就是说，&lt;strong&gt;我只通过检查一千多行代码中的三行，就给出了一个非常原子级（atomic）的诊断，精确地指出了这个证明该如何修复&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;我认为这正是 Lean，乃至形式化验证软件的一大特点：它具有一种&lt;strong&gt;高度模块化的结构&lt;/strong&gt;，这是很多其他软件甚至传统数学中并不具备的。你可以围绕某一行、某一个非常具体的局部问题展开极其精细的讨论，而完全不需要理解系统的其余部分。&lt;/p&gt;&lt;p&gt;而在传统数学中，只有在与你长期合作、彼此已经在思维方式上高度对齐的情况下，才能做到这一点。那种状态下，你们几乎可以在极其细微的层面上互相理解，甚至补全对方的句子。&lt;/p&gt;&lt;p&gt;通常情况下，当你和一个尚未在思维方式上充分同步的人讨论数学问题时，是很难进行这种粒度如此之细的交流的。&lt;/p&gt;&lt;p&gt;所以你确实可以进入那种高度专注、默契协作的状态，那种感觉非常好。但现实是，能让我进入这种状态的合作者其实只有少数。更多时候，合作中充满了翻译成本：你需要反复澄清定义、解释背景，也不可避免地会出现各种误解。&lt;/p&gt;&lt;p&gt;而在 Lean 中，这些问题在很大程度上都会消失。因为你面对的是一个对问题和修复方式都有着精确定义的类型描述。问题是什么、哪里不匹配、该如何修复，都被明确地写进了系统里。Lean 以一种此前从未有过的方式，把数学原子化了 &amp;mdash;&amp;mdash; 这是其他做数学的方法所不具备的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数学进入「工业化」时代 &amp;nbsp;数学家也可以是架构师&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Jared Duker Lichtman：顺着这个话题再往前想，其实也很有意思：我们正在用一种全新的方式来使用数学。你经历过互联网的兴起，也算是较早参与并推动了类似 Polymath （博学者项目）这种协作式研究项目的人之一。也许你可以谈谈，你对协作的直觉是如何形成的？在过去大约二十年的时间里，这种协作方式是如何演化的？&lt;/p&gt;&lt;p&gt;以及展望未来，在一种高度模块化的交互模式下，有时甚至是匿名的协作中，数学研究可能会呈现出怎样的新形态？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：我想再补充一点。你在几年前发表于《Notices of the American Mathematical Society》的一篇文章里，提到过一个非常有意思的观点：你如何看待数学家角色的演变。&lt;/p&gt;&lt;p&gt;我也很想听你进一步展开这一点，因为这和我们刚才讨论的内容高度相关，比如，当你开始主导、协调这些形式化项目时，你是否也感受到自己角色的变化？以及你在组织 Polymath 项目过程中积累的经验，又是如何与这种变化发生交汇、相互影响的？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：我一直都有一种很强烈的感觉：&lt;strong&gt;我想做的数学，远远超过了一个人所能完成的量。因此，我始终觉得合作极其高效、也极其重要。&lt;/strong&gt;我从合著者身上学到了很多，同样也从互联网上一些看似偶然的交流中学到了很多。&lt;/p&gt;&lt;p&gt;举个例子，我最早开始写博客，其实源于一次非常偶然的经历。有一次，我在自己的网页上随手贴了一个数学问题，并没有期待会有人回应。但当时已经有不少人会浏览我的页面，结果在短短三天之内，就有人给了我一个非常完整的参考说明，直接指出这个问题最早的来源。放在今天，这可能只需要一次简单的 ChatGPT 查询就能得到答案，但在当时，这对我来说是一种颠覆性的体验。&lt;/p&gt;&lt;p&gt;后来，英国数学家 Timothy Gowers 提出了 Polymath 项目，希望通过众包的方式来做数学研究，而我也非常享受参与其中。这种想法和我的直觉高度契合：数学中存在着大量潜在的联系，&lt;strong&gt;参与的人越多，就越有可能产生那些偶然的连接，这些连接往往是任何单一专家、无论多么资深，都很难凭一己之力发现的&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;但与此同时，这种协作方式始终存在一个明显的瓶颈。&lt;/p&gt;&lt;p&gt;在 Polymath 项目中，当同时有十几、二十个人参与贡献时，总需要有人来逐条检查这些想法，确保逻辑上一致，并把零散的讨论整理成一个连贯、可读的整体。这个工作通常由我、Timothy Gowers，或者其他少数人来承担，而这件事实际上是非常耗费精力的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman：原本看似去中心化的群体协作，最终还是回到了一个核心人物 + 众多贡献者的老模式。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：对，这种模式虽然很有潜力，但并没有真正实现规模化。不过，它确实促成了一些非常宏大的研究项目：来自数学中完全不同方向的人，会因为偶然的灵感，贡献出大量有价值的线索。很多时候，项目的组织者事先根本不知道这些人彼此之间存在任何关联，但他们提供的想法却是相关且有用的。&lt;/p&gt;&lt;p&gt;问题在于，当时我们并没有完善的组织与验证基础设施。而且那时我们主要是通过博客和 Wiki 来运作项目，而不是像今天这样使用 GitHub 这类更成熟的协作平台。&lt;/p&gt;&lt;p&gt;也正是在这里，形式化工具和 AI 展现出了另一项关键能力：&lt;strong&gt;它们真正实现了不同技能背景人群之间的无缝协作&lt;/strong&gt;。在一个形式化项目中，并不是每个人都需要懂 Lean，也不是每个人都需要精通数学，更不是每个人都要熟悉 GitHub。你只需要一个技能集合彼此有重叠的群体：每个关键环节都有一部分人能够胜任，整体就能顺利推进。&lt;/p&gt;&lt;p&gt;这也使得数学研究第一次真正具备了分工协作的可能性。&lt;/p&gt;&lt;p&gt;在传统数学研究中，无论是单人还是合作，参与者几乎都需要什么都懂：既要理解全部数学内容，又要会写 LaTeX、检查推导、整理论文，每个人都要覆盖所有环节。而在真正意义上的分工体系中，就像工业化生产一样，会有人负责项目管理，有人负责质量验证，有人专注于具体技术细节。&lt;/p&gt;&lt;p&gt;软件工程其实早就完成了这种转变。早期的软件开发也是一个人包办一切，但这种方式无法扩展；一旦进入企业级开发，就必须依赖高度专业化的角色分工。&lt;/p&gt;&lt;p&gt;因此，&lt;strong&gt;我确实预见到一种趋势：在规模化、工业化的条件下生产数学成果，并且伴随着清晰的专业分工&lt;/strong&gt;。当然，传统的、手工式的数学研究依然会存在，也依然会被高度珍视；只是未来会出现一种与之互补的、全新的数学生产方式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：那么，这是否意味着你预见到，大多数职业数学家的角色将会演变为这些工业化数学体系的架构师？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：我认为，数学家的定义本身会被拓宽。未来会出现一类人，他们擅长运作和管理大型项目，就像大型工程中的项目负责人一样。这些大型项目的管理者会掌握足够多的数学和 Lean 知识，能够在宏观层面理解项目在做什么，但他们未必擅长定位和修复某一条具体的形式化问题。尽管如此，他们能够协调复杂项目的推进，而这本身就是一种非常重要的能力。&lt;/p&gt;&lt;p&gt;同时，也会有一些人，他们可能并不是某个数学领域的专家，但非常擅长形式化工作，或者非常善于使用新的 AI 工具。这些能力本身同样有价值。&lt;/p&gt;&lt;p&gt;在这样的体系中，人们可以更自由地加入或离开项目，协作将变得更加流动。当然，也仍然会存在更传统的研究方式：由一个规模较小的团队组成，所有人都深度参与项目的每一个环节。这种方式依然非常重要，也不会消失。关键在于，我们终于拥有了多种选择。&lt;/p&gt;&lt;p&gt;在当前体系下，许多真正热爱数学的人被挡在数学研究之外，只是因为门槛太高了。如果你想参与前沿研究，就必须掌握博士阶段水平的数学；你还得会用 LaTeX；得知道如何写作、如何避免任何细节错误&amp;hellip;&amp;hellip; 这些要求叠加在一起，对很多人来说极具威慑性，进入门槛过高。&lt;/p&gt;&lt;p&gt;即便成功进入这一体系的人，也常常因为自身技能结构不完整而被忽视或边缘化。但未来并不必然如此，随着工具、形式化和协作方式的变化，这种状况有可能被根本性地改变。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：在门槛被工具和协作机制降低之后，数学研究不再只属于少数职业数学家，而可以像公民科学一样，吸纳大量具有兴趣和部分技能的普通参与者。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：是的，我们其实已经在看到这种趋势了。比如我自己就深度参与过一个数学问题网站。它逐渐发展成了一个社区，聚集了几十位数学背景和受教育程度各不相同的参与者，大家各自贡献一些小而具体的内容。&lt;/p&gt;&lt;p&gt;我们学会了把一个问题模块化拆解：也许你没法完整地解决这个问题，但你可以帮忙查找相关参考文献；或者把问题和某个整数序列联系起来；或者评论、改进他人的证明；又或者做一些数值实验和计算。&lt;/p&gt;&lt;p&gt;正是通过这种方式，很多人都能在自己能力范围内参与进来。&lt;/p&gt;&lt;p&gt;而现实中，确实存在着一个非常庞大的群体，他们渴望参与研究级别的数学工作，只是过去缺乏合适的入口和工具。我希望，也相信，&lt;strong&gt;这些新的工具和协作方式，能够真正释放出这股力量。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 应该先帮数学家「干脏活」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：到目前为止，我们已经谈了很多内容：一方面是你在形式化数学前沿工作的经验，另一方面是你在协调大规模协作项目、加速数学研究方面的实践。而我觉得，正好在这两者的交汇点上，是一个非常合适的时机，来谈谈你目前特别投入、也非常兴奋推动的一个项目，解析数论中数学界限（Bounds）的形式化证明。&lt;/p&gt;&lt;p&gt;或许我们可以从一个简要的介绍开始：面向非专业读者，能否先解释一下 &amp;mdash;&amp;mdash; 为什么这个问题本身如此重要？以及它在某种程度上，如何成为我们刚才讨论过的那些问题（协作、形式化、规模化研究）的一个缩影或体现？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：我想先从一个更宏观的角度来讲。我一直认为，自动化本质上是对人类思维的补充工具。&lt;/p&gt;&lt;p&gt;最直观的一种思路是：把人类最想解决、也最困难的数学问题 &amp;mdash;&amp;mdash; 比如像黎曼猜想这样的重大猜想，直接交给计算机，让它们来尝试解决。计算机在这些问题上确实可能取得一定进展，但我认为，在可预见的未来，它们更有可能在另一类完全不同的任务上发挥巨大优势。&lt;/p&gt;&lt;p&gt;这些任务往往与人类真正擅长、或乐于从事的工作是正交的，尤其是那些需要进行大量枯燥的数值计算、枚举海量可能性、反复筛选组合情况的工作。这类任务人类通常并不享受，甚至极易出错，但对 AI 和计算机来说却并不构成障碍。&lt;/p&gt;&lt;p&gt;以我所从事的领域之一解析数论为例，这里就存在一个非常典型的困难：其中有大量极其繁琐、细碎的组合性计算工作，长期以来几乎只能由人类亲自完成，而这正是自动化和 AI 最有潜力介入、并发挥巨大价值的地方。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：对我个人来说，在思考一个解析数论问题时，至少有 70% 的时间，都花在这种繁琐、机械性的工作上。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：是的，我认为我们其实已经掌握了很多非常精巧的思想和工具，可以把关于数字的一类陈述，或者关于和的展开、各种算术函数等内容，转化为我们真正关心的另一类陈述。解析数论中正是依靠这些工具在不同表述之间来回转换。&lt;/p&gt;&lt;p&gt;但问题在于，这些工具都有各自的输入和输出条件，而真正做研究时，你需要把它们一环一环地串联起来。相关的工具和结果分散在不同的论文中，每篇论文使用的记号体系都不一样，假设条件也往往和你手头的问题并不完全匹配。于是你不得不重新拆解原有证明，根据自己的需求重写一套版本。&lt;/p&gt;&lt;p&gt;在这个过程中，就会产生大量的重复劳动：反复调整参数、对齐条件、重建推导链条，而且非常容易出错。&lt;/p&gt;&lt;p&gt;为了让事情稍微不那么痛苦，我们发展出了一些权宜之计。其中一个最常见的做法是：不去关心具体常数。比如这里原本是 27，那里是 38，我们干脆都记成一个统一的常数 C，只说明存在某个常数，而不去计算它的具体数值。这样可以显著减少计算量，也能在一定程度上避免错误，即便你在常数上算错了，只要结论仍然成立，通常也不会造成严重后果。&lt;/p&gt;&lt;p&gt;但这种做法是有代价的。它导致解析数论中的很多结果都是非显式的。比如你可能证明了：所有足够大的奇数都可以表示为三个素数之和，但足够大究竟是多大？这个常数 C 到底是多少？我们并没有算出来，说白了，是懒得算。&lt;/p&gt;&lt;p&gt;因此，真正去显式计算所有常数的解析数论研究，只占整个领域中非常小的一部分。这类工作极其繁琐、计算量巨大，做的人很少，论文也往往不太好理解。这并不是作者水平的问题，而是因为研究内容本身就充斥着大量细碎、明确的计算过程，几乎没有那种直观的结构美感可言。&lt;/p&gt;&lt;p&gt;说实话，这种研究并不好理解。但我认为，这恰恰是自动化最理想的应用场景之一。如果我们能够搭建一条流水线，把这些显式型的论文纳入进来，其中的思想和工具本身其实已经相当成熟，真正困难的只是把大量彼此略微不兼容的工具拼接在一起，并把所有参数对齐，那么，用现有的方法就完全有可能在规模化条件下完成这些形式化工作。&lt;/p&gt;&lt;p&gt;在此基础上，我们甚至可以引入 AI 或机器学习，去探索这些工具链的最优组合方式。这将为整个领域打开许多全新的观察视角。&lt;/p&gt;&lt;p&gt;举个具体的例子：如果有人在某个算术函数上证明了一个新的界，我们希望能把这个结果直接丢进一个已经形式化好的、包含上百条定理的系统中，然后像操作 Excel 表格一样自动更新，改动一格，所有依赖它的结果都会自动刷新。&lt;/p&gt;&lt;p&gt;这样一来，我们就可以拥有一个持续演化、动态更新的领域最前沿状态，而不再是那些写死了指数和常数的论文。现在的做法是：每当某个关键结果被改进，研究者往往需要重写整篇论文，重新推导所有相关界限，才能弄清楚最新的最好结果是什么。而这类更新，通常十年才发生一次；但如果工具链足够成熟，这些工作完全可以在几分钟内完成。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：所以你的意思是，这本质上是一个软件问题，对吗？就像早期编程时代，人们看待汇编语言时那样，它非常繁琐，到处都是子程序，逻辑隐藏在代码细节里，既不直观，也谈不上可读性。但一旦能够在更高层次上对这些内容进行抽象和推理，情况就会完全不同。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：可以这么理解。而且在现代软件工程中，原则上一切都是可以互操作的。你可以调用别人的子程序，不同工具之间有标准化的接口和格式，它们能够彼此通信，从而构建起极其复杂、庞大的软件生态系统。&lt;/p&gt;&lt;p&gt;当然，这样的系统也会带来一个问题：正是因为系统复杂、组件众多，软件中不可避免会出现各种错误。&lt;/p&gt;&lt;p&gt;但在数学形式化这件事上，像 Lean 这样的工具，至少在理论上，让我们有机会构建一种尽可能无 bug 的协作体系。通过形式化验证，你可以希望、甚至确信这些由大量研究者共同构建的成果是相互兼容、逻辑一致的。而这正是我们目前在数学研究中所缺失的东西：一种真正可靠、可互操作、可规模化扩展的基础设施。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;当新工具出现 &amp;nbsp;数学的研究路径会整体改写吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：那么你是否愿意做一个大致的判断或推测：在数论，乃至其他数学领域中，有多大比例的工作其实是由这些相对枯燥、机械性的劳动构成的？如果这种工作负担的比例发生改变，是否可能由此催生一种截然不同的研究工作流程？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：我想在这个问题上再补充一句。事实上，在数学史上，应该已经出现过不少并非基于形式化验证、也不依赖计算机的例子：某些更好的数学技术或方法被发明出来之后，使数学家得以摆脱以往的一些繁琐劳动，从而能够把精力投入到全新的问题和思考方式中。&lt;/p&gt;&lt;p&gt;我也很好奇，在解析数论的发展过程中，是否存在过这样的重要例子？比如，是否有某些关键方法的出现，真正改变了人们理解和研究这一领域的方式？&lt;/p&gt;&lt;p&gt;如果是这样的话，那么我们是否也可以把如今的形式化工具（如 Lean）以及自动形式化技术，视为历史上这一类技术演进的又一个实例，一次新的数学技术革命？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：我认为数论其实是最早采用实验性方法的数学分支之一。例如，数论中的一个核心问题，关于素数分布的规律，最早就是由高斯提出的猜想。&lt;/p&gt;&lt;p&gt;高斯当年通过一种极其艰苦的方式来获得直觉：他手工计算了前几十万、甚至上百万个素数，并从这些数据中观察到了某些模式，由此提出了后来影响深远的素数分布猜想。&lt;/p&gt;&lt;p&gt;从今天的角度看，这几乎就是一种早期的计算实验数学：通过大量具体数据的积累，来引导理论判断和猜想的形成。这在当时是非常开创性的做法，也深刻影响了数论此后的发展方向。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：而且当时所依赖的，其实只是规模很小的数据。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：是的。高斯展现出了一种非凡的能力：他能够从规模非常小的数据集中，概括出极其深刻、普遍的规律，这正是高斯天才的体现，也正因为如此，后来很多工具都会以他的名字命名。&lt;/p&gt;&lt;p&gt;而随着计算技术的发展，我们才真正能够系统性地展开这种探索。后来也陆续出现了不少类似的例子：一些重要的猜想最初正是通过数值实验和计算探索被发现的；而在更近的时代，还有一些结果是借助大规模枚举，甚至结合机器学习方法，才逐渐显现出其结构和规律的。&lt;/p&gt;&lt;p&gt;这些进展都说明了一点：新的技术手段不断扩展着数学家可探索的空间，也在持续改变人们理解和研究数论的方式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：我想，甚至连图灵当年也在做类似的事情，亲自去计算函数的零点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：像某些算术函数的研究，其实早期就大量依赖数值计算。比如黎曼猜想，在很长一段时间里，正是通过大量数值实验获得了强有力的支持。&lt;/p&gt;&lt;p&gt;因此，历史上早就存在这样的先例：&lt;strong&gt;计算机的引入，催生了一种新的数学研究方式，不再只是依赖纯粹的抽象思考，而是结合数据和实验来推动理论的发展&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;当然，我们现在讨论的这种形式化工作，并不完全等同于数据驱动的数学，但它无疑是一种计算机辅助的研究模式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：那么，撇开机器学习领域里那一小部分人，或者少数主动尝试新工具的研究者不谈，对于一位普通的数学家来说，无论是在数论还是其他领域，在日常研究工作中，有多大比例其实是被这种繁琐、机械性的苦工所拖慢、所构成瓶颈的？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：这个问题其实很难给出一个精确的百分比，但我觉得关键并不在于直接统计时间比例，而在于一种间接影响。&lt;/p&gt;&lt;p&gt;正是因为这些繁琐劳动的存在，我们往往会有意识地改变做数学的方式，尽量减少自己要面对的苦工。比如，当我们意识到某一步组合推导开始变得非常凌乱、计算量巨大时，往往会选择刻意绕开，改用另一条思路继续推进。&lt;/p&gt;&lt;p&gt;因此，如果你只看最终论文里呈现出来的内容，会觉得我们似乎做的都是高判断力的工作，真正的苦工并不多。但那是因为我们在研究过程中，已经下意识地避开了道路上的一个个坑，用一个比喻来说，我们是在不断绕开崎岖路段，而不是去填平它们。&lt;/p&gt;&lt;p&gt;而一旦这些工具真正到位，情况可能会发生根本变化。那时，我们会改变做事方式：如果前方出现一个巨大而繁重的计算任务，我们不再选择绕路，而是直接碾过去，动用所有可用的技术手段，借助计算、形式化工具，甚至直接交给计算机，说清楚从这里到那里该怎么走，然后继续前进。&lt;/p&gt;&lt;p&gt;这样一来，我们就可以穿越那些现在几乎是下意识回避的障碍。所以，&lt;strong&gt;从表面上看，当前数学研究中苦工的比例似乎并不高；但如果把那些被我们主动规避掉的工作也算进去，那这个比例其实远比看上去要大得多&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：之前你提到过，一个非常重要的瓶颈在于：寻找合适的合作者本身就很困难，更不用说还要在工作方式、思路层面与他们建立足够的默契。&lt;/p&gt;&lt;p&gt;我想具体问的是：在这种情况下，你觉得在研究过程中，有多大比例的时间，其实是被人与人之间沟通、对齐思路、传递和同步这些界限结果所消耗的？也就是说，为了在人类专家之间完成某种分布式计算，我们究竟付出了多大的沟通成本？&lt;/p&gt;&lt;p&gt;以及，如果你所设想的这一愿景真的实现了形式化、自动化工具能够承担起这些传递与整合工作，你认为这一领域的数学研究整体上有可能被加速多少倍？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：我觉得确实如此。首先，这是一个信任问题。在这类计算密集的研究中，只要某一步出了错，整个推导就可能全部失效。因此，你&lt;strong&gt;必须清楚哪些作者是可靠的、哪些结果是可以放心使用的，而这些信息往往是隐性的，并不会明确写在论文里&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;现实中，我们不会公开列出哪些工作存在严重问题，于是你只能依赖对学术共同体的熟悉程度：你得知道这个圈子，知道该去问谁。很多时候，如果某个结果还没有正式发表，但你认识相关领域的专家，就可以直接去问他：这个地方是不是只需要稍微改一下就行？对方可能就会给你一个可靠的判断。&lt;/p&gt;&lt;p&gt;这就形成了一个明显的瓶颈：你必须身处这个关系网络之中，认识足够多对的人，才能高效地在这个领域工作。&lt;/p&gt;&lt;p&gt;而一旦我们能够通过形式化工具（比如 Lean）提供这种可验证的信任保证，情况就会发生根本改变。那时，&lt;strong&gt;你可以放心使用来自陌生研究者的结果，即便你从未见过他们，因为所有证明都已经由系统严格验证过&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;正是在这一点上，我认为&lt;strong&gt;形式化将会极大地解锁生产力，消除大量由于信任与沟通成本造成的阻塞，从而释放出此前被压抑的大量研究潜力&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：是的，我明白你的意思。你刚才提到信任这个概念，其实在数学研究中，信任往往是通过长期积累的学术记录建立起来的。一个研究者在某个领域持续工作、不断产出成果，随着时间推移，其他人自然会越来越信任他的结论。&lt;/p&gt;&lt;p&gt;而真正让我开始对形式化和数学基础问题产生强烈兴趣的一个重要故事，正是关于一位数学家的经历。他曾经建立起极高的学术声誉，证明过许多非常了不起的结果，因此在学界拥有极强的可信度。&lt;/p&gt;&lt;p&gt;但在 20 世纪 90 年代末，他写过一篇论文，后来大约在十年之后，他才意识到其中存在一个关键性的错误。回过头来看，他自己也反思到：当时很多人之所以接受那篇结论，很大程度上是因为大家在相信他这个人，而不是因为证明本身被彻底、逐行地验证过。&lt;/p&gt;&lt;p&gt;而这正揭示了一个核心问题：个人声誉和过往记录，并不等同于真理的保证。这类经历也正是形式化证明与基础工具如此重要的原因之一，它们提供的不是基于人的信任，而是基于可验证结构的信任。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：当然，这种做法在深度上是有极限的。我们能够推动数学前进的程度，终究会受到限制。当前在分析学中，这个问题相对没那么严重，是因为这里逐渐形成了一张不断加密的信任之网，而且我们的工作方式往往更接近从第一性原理出发，比其他一些领域更少依赖远距离的结果。&lt;/p&gt;&lt;p&gt;但即便如此，这种基于信任的结构依然是数学发展的一个限制因素。从长远来看，这是一个无法回避的问题，也是形式化和基础工具之所以重要的又一个原因。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jared Duker Lichtman&lt;/strong&gt;：我想再追问一个相关的问题。随着我们开始系统性地回溯并形式化一些经典论文，以及从 20 世纪 60 年代以来的大量文献，你会如何看待这样一个问题：&lt;/p&gt;&lt;p&gt;第一，在现有的数学文献中，可能还存在多少尚未被发现的错误？&lt;/p&gt;&lt;p&gt;第二，这些错误中，有多少只是可以通过小修小补解决的技术性问题？换句话说，整个数学体系作为一个整体，对这类错误究竟有多强的鲁棒性？&lt;/p&gt;&lt;p&gt;也就是说，即便我们真的通过形式化手段暴露出大量隐藏的问题，它们是否大多不会动摇理论的核心结构，而只是需要局部修正？还是说，其中也可能存在少量但影响深远的根本性漏洞？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;陶哲轩&lt;/strong&gt;：说实话，我也很想知道实际的错误率到底是多少。也许结果会让我们惊喜，也可能会让我们不太愉快。等六个月之后再来问我吧。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jesse Han&lt;/strong&gt;：今天这次交流真的非常愉快，真希望能再多聊一会儿。那就希望六个月之后，我们还能再进行一次这样的对话。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
