<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>支付宝携手千问App、淘宝闪购等发布中国首个AI商业协议ACT</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Fri, 16 Jan 2026 11:15:08 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-16-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-16-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1月16日，支付宝联合千问App、淘宝闪购、Rokid、大麦、阿里云百炼等伙伴，正式发布ACT协议（Agentic Commerce Trust Protocol，智能体商业信任协议）。这是中国首个面向 Agent 商业需求设计的开放技术协议框架，为 AI 与电商、外卖等服务平台的协同打造一套 &amp;ldquo;通用语言&amp;rdquo;，让跨终端、跨系统、跨平台的 AI 任务执行，变得更便捷、更高效。&lt;img src="https://image.jiqizhixin.com/uploads/editor/d3a457f1-d771-461f-88af-95c56c05c4ed/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;以千问App为例，依托 ACT协议 ，千问App成功打通淘宝闪购与支付宝 AI 付：用户只需向千问发出指令 &amp;ldquo;帮我点杯珍珠奶茶&amp;rdquo;，千问基于用户地理位置，智能推荐附近符合需求的商品，同步完成比价与优惠券自动核销。用户仅需点击 &amp;ldquo;选它&amp;rdquo;，确认支付宝付款，即可一键完成结账。整个购物流程以对话式、自动化、不跳端的方式推进，千问化身专属 &amp;ldquo;购物助手&amp;rdquo;，包办繁琐操作。&lt;/p&gt;&lt;p&gt;当 AI 的能力边界不断拓展，从&amp;ldquo;聊天对话&amp;rdquo;延伸至购物付款等&amp;ldquo;办事时代&amp;rdquo;，新的问题也随之浮现：AI 操作是否获得用户明确授权？资金交易过程是否足够安全？更换设备或应用后，服务体验能否保持连贯？&lt;/p&gt;&lt;p&gt;ACT 协议的诞生正是为破解这些问题而来。支付宝为其搭建了 &amp;ldquo;委托授权域&amp;rdquo;&amp;ldquo;商业交互域&amp;rdquo;&amp;ldquo;支付服务域&amp;rdquo;&amp;ldquo;信任服务域&amp;rdquo; 四大核心基础设施标准，实现 AI 操作全流程可追溯、可验证，让人更放心；支持自动化交易流程，减少不必要的人工干预，提升服务效率；统一多平台服务标准，避免体验的割裂。&lt;img src="https://image.jiqizhixin.com/uploads/editor/03bad643-d16e-484c-9044-2e39c5b03076/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;与传统付款模式不同，在 ACT协议的规则框架下，AI 仅承担下单操作的执行角色，付款环节始终由用户主导或自主授权。在保障资金安全的前提下，为用户大幅节省时间成本。而对商家而言，未来接入 AI 原生应用时，只需按照协议标准配置统一接口，即可对接全渠道入口，无需单独进行复杂的 API 开发，大幅降低对接成本。&lt;/p&gt;&lt;p&gt;目前，ACT 协议可使用在AI 代买、企业自动化采购等多元场景，并提供两种付款模式：一是即时付款，用户与 AI 实时对话，基于推荐列表自主决策，确认后完成付款授权与身份验证，适用于 AI 点外卖、日常购物等高频场景；二是委托授权，用户可提前设定时间窗口、金额上限、商家范围等条件，即便离线无指令，AI 也能自动监测商品动态并完成下单结算，适用于机票、酒店预订等场景。&lt;/p&gt;&lt;p&gt;该协议最大限度遵循兼容性、隐私性、开放性三大原则，全面适配现有商业与支付系统，并将伴随 AI 行业技术发展持续优化。支付宝同时表示，正积极推动更多支付服务商、商家与平台、AI 开发者、智能终端生态厂商加入，共同完善协议内容，共建 AI 商业信任新生态。&lt;/p&gt;&lt;p&gt;随着 AI 原生应用能力的持续升级，&amp;ldquo;AI 代办&amp;rdquo; 服务日渐普及，支付作为其中特殊且关键的环节，正成为全球科技企业的布局焦点。此前，OpenAI 联合 Stripe 推出协议以支持 ChatGPT 结账功能；近期，谷歌也发布 AI 购物全流程通用商务协议（Universal Commerce Protocol，简称 UCP），将实现用户在 Gemini 内直接下单。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，Geoffrey Hinton成为第二位引用量破百万的科学家</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 16 Jan 2026 10:32:37 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-16-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-16-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/da65f910-b4cb-4a55-b5d3-2ab1ec2ded1e/1768530536279.png" style="width: 700%;" class="fr-fic fr-dib"&gt;刚刚，Geoffrey Hinton 正式成为历史上第二位 Google Scholar 引用量突破 100 万大关的计算机科学家。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD7a3jciaMpwmMUB2VPS8Y9VZnO7RuRt3WIM3OIbTDjDkOyicJUeW0WXqQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.5814814814814815" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503528480" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d8b5fcfd-47a4-4f5a-90bd-ad01d61db6e1/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在他之前，只有他的老搭档、另一位「深度学习教父」&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650997783&amp;idx=1&amp;sn=0f796e9cbaf70b81378e4cb06ba7ea90&amp;scene=21#wechat_redirect" target="_blank"&gt;Yoshua Bengio&lt;/a&gt; 达成了这一成就。目前，Hinton 的引用量仍在以惊人的速度增长，每一次引用都代表着他对人工智能领域不可磨灭的贡献。从反向传播算法的推广到 AlexNet 的惊艳问世，从获得图灵奖到斩获 2024 年诺贝尔物理学奖，Hinton 的职业生涯几乎就是一部现代 AI 的发展史。&lt;/p&gt;&lt;p&gt;这一数字不仅是学术影响力的量化，更是对这位 78 岁长者一生执着探索的最高致敬。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Geoffrey Hinton：来自学术世家的「教父」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;童年&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Geoffrey Everest Hinton，1947 年 12 月 6 日出生于英国伦敦的一个学术世家。他的中间名「Everest」来自他的叔祖父，也就是以其名字命名珠穆朗玛峰英文名的 George Everest。他的家族星光熠熠，曾祖父是布尔逻辑的创始人 George Boole，表姑是参与曼哈顿计划的核物理学家 Joan Hinton（寒春）。&lt;/p&gt;&lt;p&gt;生在这样的家庭，压力与荣耀并存。Hinton 的母亲曾给他下过一道温和却严厉的「最后通牒」：「要么做个学者，要么就是个失败者（Be an academic or be a failure）」。这种高期待或许解释了他日后对学术的极致追求。&lt;/p&gt;&lt;p&gt;他的童年充满了像电影《天才一族》般古怪而硬核的色彩。家里养过猫鼬，车库的坑里甚至养着毒蛇。8 岁那年，Hinton 曾挥舞着手帕逗弄坑里的毒蛇，结果一条蛇猛地扑向他的手，仅差一英寸就咬中了他，差点让他丧命。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PHEe3x1PyBmnqibQbQNCuRUu87ekPehShtIaatlqoQJfts4HUb3ukJTQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="1.4222222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503523890" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/61f3390a-4ba3-4e20-b44d-61c45a1411d5/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 8 岁的 Hinton 搂着一条蟒蛇&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;家族的轶事甚至还涉及到了加拿大政坛。1961 年，他的父亲访华时带回了一打中国乌龟。在旅途中，老 Hinton 与未来的加拿大总理皮埃尔・特鲁多（Pierre Trudeau）住同一间酒店房间。据说老 Hinton 把乌龟都养在了浴缸里，导致特鲁多根本没法洗澡。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;求学之路&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;然而，这位天才的学术之路并非一片坦途，但他对世界本质的好奇心早在 4 岁时就已萌芽。&lt;/p&gt;&lt;p&gt;那时，他在一辆乡村巴士上发现了一个奇怪的现象：当巴士急刹车时，座位上的硬币并没有顺着惯性向前滑，而是反直觉地向后移动。这个违反物理常识的现象困扰了他整整十年，直到后来他才明白这是座位绒毛角度与振动共同作用的结果。对此，他曾说道：「有些人可以接受自己不理解的事物，但我不行。我无法接受有什么东西违反了我对世界的认知模型。」&lt;/p&gt;&lt;p&gt;这种对「理解世界运作方式」的执念贯穿了他的求学生涯。在剑桥大学国王学院期间，他曾在物理学、哲学和心理学之间反复横跳。毕业后，在迷茫中他甚至曾短暂地做过一段时间的木匠。在攻读博士学位期间，由于神经网络在当时不被看好，他一度陷入抑郁和自我怀疑。&lt;/p&gt;&lt;p&gt;在一个类似心理治疗的研讨会上，当其他人都在大喊「我想要被爱」来释放情感时，Hinton 憋了半天，最终吼出了心底最深层的渴望：「我真正想要的是一个博士学位！（What I really want is a PhD!）」。带着这股执拗，他在爱丁堡大学获得了人工智能博士学位，正式开启了他在神经网络荒原上的长征。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6Pzib4NjmVLwTH6jMurebiafQovIpVXiafzHYlwEoua5OAvUwWuoZJVNc2w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.7111111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503523892" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/1af9f84d-96bb-400f-814d-186482e88862/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 31 岁的 Hinton 与他的博士后同学 Chris Riesbeck&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;北上加拿大&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 70 年代和 80 年代，当 AI 领域被符号主义主导时，Hinton 就像一个孤独的异类。由于对罗纳德・里根时代美国国防部主导的军事资助感到失望，他做出了一个改变人生轨迹的决定：离开美国，北上加拿大。&lt;/p&gt;&lt;p&gt;除了政治原因，这背后还有一个鲜为人知的温情理由：当时他和妻子计划收养一对来自南美洲的儿女。他不希望在一个当时正暴力干涉拉美事务的国家抚养这些孩子。于是，他在多伦多大学扎根，在那里数十年如一日地在神经网络的「荒原」上耕耘，这也为后来加拿大成为全球 AI 重镇埋下了伏笔。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;学术成就&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Geoffrey Hinton 最著名的成就之一是与 David Rumelhart 和 Ronald Williams 共同发表了关于&lt;strong&gt;反向传播（Backpropagation）&lt;/strong&gt;的论文，解决了多层神经网络的训练难题，为后来深度学习的爆发埋下了伏笔。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PALqLcHkpiab79b7gQzzHDrcb3qvRJTmfJ4T5kic1Rl359KBf5N0LxWLA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7936962750716332" data-s="300,640" data-type="png" data-w="1047" type="block" data-imgfileid="503523888" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/94d0cf8d-b626-4df4-bd29-4b7583edf5bb/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;但他的贡献远不止于此：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;玻尔兹曼机&lt;/strong&gt;（Boltzmann Machine）与&lt;strong&gt;受限玻尔兹曼机&lt;/strong&gt;（RBM）：为无监督学习和特征表示学习奠定了基础，可用于生成模型和预训练神经网络。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;深度信念网络&lt;/strong&gt;（DBN）：在 2006 年提出，通过逐层贪心训练方法有效训练深度神经网络，点燃了深度学习复兴的火种。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dropout&lt;/strong&gt;：一种简单而高效的正则化技术，通过随机「丢弃」神经元防止过拟合，成为大型神经网络训练的标准做法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;t-SNE&lt;/strong&gt;：一种高维数据可视化技术，用于将复杂数据嵌入低维空间，广泛用于理解深度学习特征表示。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;分布式表示&lt;/strong&gt;（Distributed Representations）：强调分布式特征编码在学习系统中的重要性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;胶囊网络&lt;/strong&gt;（Capsule Networks）：提出对卷积神经网络中空间关系处理不足的问题的一种改进，通过「胶囊」表示和动态路由机制增强特征层次感知。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;混合专家模型&lt;/strong&gt;（MoE）：通过多个子网络（专家）协同工作并由路由器选择性激活，提高模型容量与计算效率，成为大规模模型的重要设计思路。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;知识蒸馏&lt;/strong&gt;（Knowledge Distillation）：提出将大型复杂模型（教师模型）的知识迁移到小型模型（学生模型），在保证性能的同时降低计算成本。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;层归一化&lt;/strong&gt;（Layer Normalization）：改进深度网络训练稳定性和收敛速度的技术，对自然语言处理模型尤其重要。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;深度生成模型与概率图模型&lt;/strong&gt;：在生成模型领域提出了多种创新方法，为后续的变分自编码器（VAE）和生成对抗网络（GAN）奠定了理论基础。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;AlexNet 与 ImageNet 变革&lt;/strong&gt;： 他与学生 Alex Krizhevsky、Ilya Sutskever 共同推出了 AlexNet，在 ImageNet 竞赛中以绝对优势夺冠。这被公认为深度学习时代的「大爆炸」时刻，证明了深层卷积神经网络在海量数据和 GPU 算力下的统治力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Forward-Forward Algorithm&lt;/strong&gt;（前向 - 前向算法，2022）： 这是他在职业生涯后期对反向传播生物学合理性的反思与挑战，提出了一种更接近人脑运作机制的学习替代方案。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2018 年，他与 Yoshua Bengio 和 Yann LeCun 共同获得了计算机领域的最高荣誉：&lt;strong&gt;图灵奖&lt;/strong&gt;。这三人也常被称为「深度学习三巨头」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6Pnn1gWc8dU2wFqDDLjI9vKyhwDe0gapndm7wdNpm8QMicz9QAIvj889g/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.54375" data-s="300,640" data-type="png" data-w="800" type="block" data-imgfileid="503523889" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/9209ee35-71c3-4a04-91b4-7fdc3129c0b7/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;值得注意的是，这三位图灵奖得主也是 Hinton 引用量第二高的论文《&lt;strong&gt;Deep learning&lt;/strong&gt;》的共同作者。该论文于 2015 年 5 月发表于 Nature，十年时间已经收获了超过 10 万引用量。其中系统总结了深度学习的发展历程、基本原理、关键算法（例如多层表征学习、反向传播、卷积神经网络和循环神经网络）以及其在语音识别、视觉识别、目标检测、基因组学等领域的广泛应用，标志着深度学习从学术探索迈向应用驱动的成熟阶段，被公认为推动该领域走向主流的里程碑性工作。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6P4EbLtUBsj1icjWjYdrZiam6RfqsgLTbCRQTliaWXefwoQQIezmaVgm6LA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.37222222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503523891" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/d76d0579-b8e6-4dbe-a390-7f0077f341d7/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;2024 年，Hinton 与 John Hopfield 共同获得了诺贝尔物理学奖，以表彰他们「实现了利用人工神经网络进行机器学习的奠基性发现和发明」。参阅报道《&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650937189&amp;idx=1&amp;sn=0dd0e673b6ecbe994bed94c30b0a0035&amp;scene=21#wechat_redirect" target="_blank"&gt;刚刚，2024 诺贝尔物理学奖授予 Geoffrey Hinton、John Hopfield&lt;/a&gt;》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PvucWg8AFPFtq9Gp1g6vrmRuhJIJQdkSKUNWuFz4gLyicwLQeqFtCADQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.9790697674418605" data-s="300,640" data-type="png" data-w="860" type="block" data-imgfileid="503523894" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/9945f443-870f-4f6e-9b15-a900a3e76bab/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;冷静的警示者&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;然而，这位「AI 教父」在晚年却不仅是一位技术布道者，更成为了一位冷静的警示者。&lt;/p&gt;&lt;p&gt;2023 年 5 月，他从工作了十年的谷歌离职，只为能「自由地谈论 AI 的风险」。他曾表示：「我想我现在对自己毕生的工作有一部分感到后悔。」他担忧数字智能可能会演变成一种比人类更优越的智能形式，并可能因缺乏控制而对人类构成生存威胁。他警告说：「如果你想知道不再是处于食物链顶端的智慧生物是什么感觉，去问问鸡就知道了。」&lt;img src="https://image.jiqizhixin.com/uploads/editor/d3d401b8-d9a5-48ce-bec9-197e06c0490e/1768530659519.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Alex Krizhevsky 与 Ilya Sutskever&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 Hinton 浩如烟海的著作中，引用量最高的一篇无疑是 2012 年发表在 NeurIPS 上的奠基之作：《ImageNet classification with deep convolutional neural networks》。这篇论文目前的引用量已超过 18 万次（可能仅次于引用量近 30 万的 ResNet 论文和引用量超过 20 万的 Transformer 论文），它不仅标志着深度学习时代的正式开启，也让两位共同作者的名字响彻云霄：&lt;strong&gt;Alex Krizhevsky&lt;/strong&gt; 和 &lt;strong&gt;Ilya Sutskever&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;作为 Hinton 的两名得意门生，他们在那间多伦多大学的实验室里，共同推开了 AI 新世界的大门。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PTtYT7rjb3foxvVsjR1P9SOVT7WsJHMazwZHia2RcxUzfQSsyicqia6rMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.6602941176470588" data-s="300,640" data-type="png" data-w="680" type="block" data-imgfileid="503523893" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/457ba80b-4a1a-4702-8a69-51dfc5e342e4/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; Alex Krizhevsky 与 Ilya Sutskever 是 Geoffrey Hinton 引用量最高的论文的第一和第二作者。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Alex Krizhevsky：低调的隐士天才&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为那篇传奇论文的第一作者，Alex Krizhevsky 是 AlexNet 的主要构建者。正是他编写了关键的 CUDA 代码，让神经网络得以在两块 GeForce GPU 上高效训练，从而在 2012 年的 ImageNet 挑战赛上以惊人的 10.8% 优势碾压第二名，一举震惊世界。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PtPicpia3UwvlAIWMNAjf4ic4NDzmoszTmic3vO39MfVv4YwgMXyoROsS9Q/640?wx_fmt=webp&amp;from=appmsg#imgIndex=9" data-ratio="0.5615696887686062" data-s="300,640" data-type="webp" data-w="739" type="block" data-imgfileid="503523898" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/f9e0959f-abe1-4f36-a6d0-a024f262c7d3/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;然而，与他在学术界的赫赫声名形成鲜明对比的是他极度低调的性格。Alex 出生于乌克兰，成长于加拿大。他被很多同行描述为一位「纯粹的工程师」，拥有极深的技术洞察力。在谷歌工作了数年后，他于 2017 年离职，理由是「对工作失去了兴趣」。&lt;/p&gt;&lt;p&gt;此后，他加入了初创公司 Dessa，随后又逐渐淡出公众视野。据悉，他目前可能已处于半退休状态，享受着徒步旅行的乐趣。在科技圈追逐名利的热潮中，Alex Krizhevsky 就像一位事了拂衣去的隐士。尽管 AlexNet 如今在技术上已被更新的模型取代，但正如一位评论者所言：「没有他，就没有今天的 ChatGPT，没有便捷的 3A 大作，也没有先进的医学影像分析。」&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Ilya Sutskever：执着的 AI 愿景者&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说 Alex 是低调的技术天才，那么该论文的第二作者 Ilya Sutskever 则是充满使命感的 AI 领袖。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibpJ08gskIzfop6hz3Fal6P7LePveljFibC5RwGpqtnibHW9EX7VicWHAdWgquogOCZsCAmBWhXdb2sA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=10" data-ratio="0.5626822157434402" data-s="300,640" data-type="jpeg" data-w="686" type="block" data-imgfileid="503523899" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/bca6a09e-9f65-445f-a53d-95f1c52894ca/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Ilya 同样出生于前苏联（俄罗斯），并在以色列和加拿大长大。在多伦多大学期间，他与 Hinton 和 Alex 共同缔造了 AlexNet 的辉煌。随后，他在 Google Brain 参与了序列到序列（Seq2Seq）学习算法和 TensorFlow 的开发，并是 AlphaGo 论文的众多作者之一。&lt;/p&gt;&lt;p&gt;2015 年，Ilya 离开谷歌，作为联合创始人兼首席科学家创办了 OpenAI。他是 ChatGPT 和 GPT-4 诞生的关键人物，被誉为能够「通过直觉看到深度学习未来」的人。然而，他对 AI 安全的关注也日益加深。2023 年，他曾主导了 OpenAI 董事会罢免 Sam Altman 的风波，理由是「沟通不坦诚」，尽管后来 Altman 复职，Ilya 对 AI 对齐（Alignment）和安全超级智能（SSI）的执着从未改变。&lt;/p&gt;&lt;p&gt;2024 年，Ilya 成立了新公司 Safe Superintelligence Inc. (SSI)，并为其筹集了 10 亿美元资金。与商业化气息浓厚的硅谷公司不同，SSI 宣称其「第一个产品将是安全的超级智能，在此之前不会做任何其他事情」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Geoffrey Hinton 引用量突破百万，不仅是他个人学术生涯的高光时刻，也是 Alex Krizhevsky 和 Ilya Sutskever 等一代 AI 杰出人才共同奋斗的缩影。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibpJ08gskIzfop6hz3Fal6PhROFKPPyeT2Tv1hiaibaZON9xok21Hpw7JfD5tS5dQ0n0EFfyLiaF59kQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=11" data-ratio="0.6564814814814814" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503523895" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/022a446f-6c3a-4bd3-98b2-8bb8d6847885/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;从 Alex 编写的那行 CUDA 代码，到 Ilya 对通用人工智能（AGI）的深邃构想，再到 Hinton 对神经网络半个世纪的坚守与晚年的忧思，这一里程碑背后，是人类探索智能本质的波澜壮阔的历史。&lt;/p&gt;&lt;p&gt;今天，我们致敬 Hinton，也致敬所有为这一刻铺路的研究者。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://scholar.google.com/citations?user=JicYPdAAAAAJ&amp;amp;hl=en&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.youtube.com/watch?v=giT0ytynSqg&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.britannica.com/biography/Geoffrey-Hinton&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.nobelprize.org/prizes/physics/2024/hinton/podcast/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://torontolife.com/life/ai-superstars-google-facebook-apple-studied-guy/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://yiqinfu.github.io/posts/hinton-intellectual-dynasty/&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>腾讯AngelSlim升级，首个集LLM、VLM及语音多模态为一体的投机采样训练框架，推理速度飙升1.8倍</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 16 Jan 2026 10:24:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-16-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-16-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/fdb3895e-e15b-4dbd-9468-5d71ad7fa81a/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;随着大模型步入规模化应用深水区，日益高昂的推理成本与延迟已成为掣肘产业落地的核心瓶颈。在 &amp;ldquo;降本增效&amp;rdquo; 的行业共识下，从量化、剪枝到模型蒸馏，各类压缩技术竞相涌现，但往往难以兼顾性能损耗与通用性。&lt;/p&gt;&lt;p&gt;在此背景下，&lt;strong&gt;投机采样&lt;/strong&gt;作为一种 &amp;ldquo;另辟蹊径&amp;rdquo; 的推理加速范式，正凭借其近乎无损的加速效果成为业界新宠。腾讯混元近日升级的 &lt;strong&gt;AngelSlim 训练框架&lt;/strong&gt;，首次将这一技术的潜力拓展至 LLM、VLM 及语音的全模态场景，实现了从 &amp;ldquo;可加速&amp;rdquo; 到 &amp;ldquo;善加速&amp;rdquo; 的关键跃迁。其核心在于独创的 &lt;strong&gt;Eagle3 训练架构&lt;/strong&gt;，通过让小模型学会 &amp;ldquo;前瞻性&amp;rdquo; 地为大模型起草多步候选 token，再由大模型并行验证，一举将大模型解码阶段的算力冗余转化为提速动能，实测最高可带来 &lt;strong&gt;1.9 倍的推理速度飙升&lt;/strong&gt;。这不仅是一次技术升级，更是对下一代高效推理基础设施的重要定义，为多模态 AI 应用的实时化、普惠化铺平了道路。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、AngelSlim + 投机采样&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;投机采样是一种通过&lt;strong&gt;小模型多步预测 + 大模型一步验证&lt;/strong&gt;的推理加速技术，其核心思想是：使用一个轻量级的草稿模型生成多个候选 token，由目标模型对候选结果进行并行验证是否接受，以此来并行解码加速，在有效利用大模型解码阶段的算力冗余，提升推理吞吐并降低单请求延迟。&lt;/p&gt;&lt;p&gt;AngelSlim 是一款集成了包括量化、投机采样等压缩算法，面向全模态的大模型压缩算法工具包。此次对投机采样训练进行了重磅升级，支持了大语言、多模态理解、语音等不同模态大模型投机采样草稿模型训练能力。&lt;/p&gt;&lt;p&gt;AngelSlim 以 &amp;ldquo;&lt;strong&gt;Eagle3 训练即部署&lt;/strong&gt;&amp;rdquo; 为设计核心，提供从数据处理、模型封装到投机采样算法训练的完整链路，帮助开发在不侵入现有模型结构的前提下，显著降低推理时延与计算成本，各模态、各类大模型加速可达 1.4-1.9 倍。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicFZCBCia2pic36dtxra88bsadVQBnZ1l3jNnmIpyaycSsfWzOAiaZILtlZ8OiastP5UkxHUYGKpAFDUQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.562037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528170" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/79ab0608-0ef6-4a81-8661-20a743402ee9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Github 开源地址：https://github.com/Tencent/AngelSlim&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、核心亮点&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 覆盖从文生文、多模态理解到语音的全模态投机采样训练&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AngelSlim 是一个从设计之初就支持全模态的投机采样训练框架，通过统一的训练接口，不同模态之间共享核心算法与工程能力，避免重复造轮子。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 面向部署&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AngelSlim 并不止步于 &amp;ldquo;能训&amp;rdquo;，而是强调训出来就能用。AngelSlim 训练产出的模型可以无缝用于 vLLM/Sglang 等框架进行部署。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、核心训练组件解析 &lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8Lwoiab3MsrMeRiawMnoTxkAvPGN0Qq82V8jGSeNlNKh5GIw4Ll2RWA3DBySly9j5sNwx5faABDQmQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.2601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528020" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/8704ff60-c4f7-4f48-b690-ca84584d17a9/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;1. 数据处理模块&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicFZCBCia2pic36dtxra88bsaibibRfrda8FurLlibnKt4TVhh2NedCwDiaKJFqMOVshFOPpLPAA58YpOhA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.4027777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528158" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ac37f7ab-656d-4e8c-8c20-7fea0ac01b4a/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;数据处理模块为投机采样训练多个模态提供稳定、可复用的数据基础，主要包括：&lt;/p&gt;&lt;p&gt;a. 数据重采样：针对分布外数据集重新采样，生成分布内数据集用以训练。&lt;/p&gt;&lt;p&gt;b. 数据预处理：&lt;/p&gt;&lt;p&gt;i. 统一不同模态的数据格式，将文本、图像、音频等输入标准化处理成 token ids 和 loss mask。&lt;/p&gt;&lt;p&gt;ii. 草稿模型裁剪词表的映射。&lt;/p&gt;&lt;p&gt;c. 隐藏特征提取：根据处理好的 token ids 获取对应的隐藏特征。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicFZCBCia2pic36dtxra88bsax2X7SsjNjwJAicbX6zTzJM1Zj2whwcvgB7YapLmkiaLwPPEfrQtChyPw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.8527777777777777" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528171" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a301352c-5d1c-4ba5-a57e-709c7d9b9d9f/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 模型模块&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;模型模块是 AngelSlim 实现高度扩展性的关键。&lt;/p&gt;&lt;p&gt;a. 统一的 TargetModel 接口&lt;/p&gt;&lt;p&gt;i.AngelSlim 提供统一的 TargetModel 接口，包括模型加载与权重管理、前向计算、中间层 / 隐状态特征提取等抽象方法；&lt;/p&gt;&lt;p&gt;b. 低成本扩展新的模型后端&lt;/p&gt;&lt;p&gt;ii. 对于新的模型架构或后端，用户只需实现 TargetModel 中定义的抽象方法即可完成模型注册并接入训练流程，无需修改训练器或核心算法代码。这一设计极大降低了对新模型、新模态的适配成本。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8Lwoiab3MsrMeRiawMnoTxkAIS1SpUTy6jlfcaur8ianvaIzPtC9v8AiaNpOCLD1OwS8tS0Mu4hia1R7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.48055555555555557" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528028" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f4f5cae1-59cb-4d25-ac7f-711ada7decae/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3. 训练器模块&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;a. 训练器针对 Eagle3 算法特点设计了两种训练模式：在线训练和离线训练。在线与离线训练的区别在于是否预先生成并存好全量数据的 hidden states。在线训练适合小尺寸模型或显存足够的场景，离线训练适合大尺寸模型、低显存高磁盘空间机器。&lt;/p&gt;&lt;p&gt;b. 训练器实现封装了 Eagle3 等投机采样算法训练的关键逻辑：&lt;/p&gt;&lt;p&gt;i. 训练时测试（training-time-test）：训练时模拟 Eagle3 模型多步生成过程，让 Eagle3 模型看到并学习使用自己的预测。&lt;/p&gt;&lt;p&gt;c. 训练器原生支持断点续训能力，完整保存并恢复：&lt;/p&gt;&lt;p&gt;i. 草稿模型参数&lt;/p&gt;&lt;p&gt;ii.Optimizer/ LR Scheduler 状态以及训练进度&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、实践与部署&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 快速开始&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当安装好 AngelSlim 后，进入 AngelSlim 根目录按照如下命令可以快速开始 Eagle3 的训练：&lt;/p&gt;&lt;section&gt;&lt;pre data-lang="bash"&gt;&lt;code&gt;# 启动vLLM 服务&lt;/code&gt;
&lt;code&gt;bash scripts/speculative/run_vllm_server.sh&lt;/code&gt;
&lt;code&gt;# 生成训练数据&lt;/code&gt;
&lt;code&gt;bash scripts/speculative/generate_data_for_target_model.sh&lt;/code&gt;
&lt;code&gt;# 开始在线训练&lt;/code&gt;
&lt;code&gt;bash scripts/speculative/train_eagle3_online.sh&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;其中前两条命令是准备数据，对训练数据进行重采样，生成目标模型分布内的数据。这一步是可选项，如果训练数据已经是来自目标模型的 SFT 数据或自身生成的数据，这一步可跳过。对 Eagle3 模型进行训练直接执行最后一条命令即可，更多进阶的使用指南可以参见我们的文档。&lt;/p&gt;&lt;p&gt;我们提供了全面的多模态模型 Eagle3 训练与部署指南，支持 LLM / VLM / Audio (ASR &amp;amp; TTS) 模型。&lt;/p&gt;&lt;p&gt;详见：https://angelslim.readthedocs.io/zh-cn/latest/features/speculative_decoding/eagle/eagle.html&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.AngelSlim 训练模型的加速表现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们使用 vLLM 在代码、数学、指令跟随、文本生成、多模态理解等任务上评测了 AngelSlim 所训练的 Eagle3 模型，设置 num_speculative_tokens=2 or 4 下我们所训的模型接收长度可达 1.8-3.5，最高加速可达 1.4-1.9 倍。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicFZCBCia2pic36dtxra88bsaXdAQ10m2nGj1gYJcodovzsH8jGibLgduQwymYHZ7YawrGkOqiauz92vg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5953703703703703" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528159" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/5939823f-d258-4a6f-83e2-a3e27521fcfb/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3. 代码和模型链接&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;AngelSlim 代码 Github 开源仓库：https://github.com/Tencent/AngelSlim&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging-Face Eagle3 模型与权重：https://huggingface.co/collections/AngelSlim/eagle3&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;五、未来计划&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在未来规划中，我们将从工具与算法两个层面持续推进投机采样能力演进：工具方面，计划支持基于 vLLM 的离线 hidden states 生成，以进一步降低数据构建与训练成本，并通过系统性的训练加速优化提升整体训练效率；算法创新方面，将探索多模态理解与语音输入信息在 Eagle3 模型中的深度融合，统一建模文本、视觉与语音特征，拓展投机采样在全模态场景下的适用性与加速潜力。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>DeepSeek连发两篇论文背后，原来藏着一场学术接力</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 16 Jan 2026 10:19:55 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-16-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-16-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/4dc25181-b5e0-4d5f-a42e-139036252ddd/1768529804267.png" style="width: 700%;" class="fr-fic fr-dib"&gt;2026 年 1 月过半，我们依然没有等来 DeepSeek V4，但它的模样已经愈发清晰。&lt;/p&gt;&lt;p&gt;最近，DeepSeek 连发了两篇论文，一篇解决信息如何稳定流动，另一篇聚焦知识如何高效检索。&lt;/p&gt;&lt;p&gt;第一篇论文（&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651010187&amp;idx=1&amp;sn=cc9ae88f676873468dcfc98d54e98aa9&amp;scene=21#wechat_redirect" target="_blank"&gt;mHC&lt;/a&gt;）出来的时候，打开论文的人都表示很懵，直呼看不懂，让 AI 助手用各种方式讲给自己听。我们也翻了翻网友的讨论，发现理解起来比较透彻的办法其实还是要回到研究脉络，看看这些年研究者们是怎么接力的。要理解第二篇论文（&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651011743&amp;idx=1&amp;sn=34e666de69c7ad8166d2d46a4f8938fc&amp;scene=21#wechat_redirect" target="_blank"&gt;Conditional Memory&lt;/a&gt;）也是如此。&lt;/p&gt;&lt;p&gt;于是，我们就去翻各路研究者的分析。这个时候，我们发现了一个有意思的现象：DeepSeek 和字节 Seed 团队的很多工作其实是存在「接力」的 &amp;mdash;&amp;mdash;&lt;strong&gt;mHC 在字节 Seed 团队 HC（Hyper-Connections）的基础上进行了重大改进；Conditional Memory 则引用了字节 Seed 的 OverEncoding、UltraMem 等多项工作&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;如果把这些工作之间的关系搞清楚，相信我们不仅可以加深对 DeepSeek 论文的理解，还能看清大模型架构创新正在往哪些方向突破。&lt;/p&gt;&lt;p&gt;在这篇文章中，我们结合自己的观察和学界专家的点评，尝试为大家梳理了一下。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;残差连接的十年接力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要理解 mHC，得先回到 2015 年。&lt;/p&gt;&lt;p&gt;那一年，AI 大牛何恺明等人提出了 ResNet，用残差连接解决了深度神经网络训练中的老大难问题：网络层数一多，信息从前往后传递时会逐渐失真，到最后几层几乎学不到东西。残差连接的思路很简单，每一层不光接收上一层处理过的结果，还同时保留一份原始输入，两者加在一起再往下传。&lt;/p&gt;&lt;p&gt;这个设计堪称深度学习的基石，十年来几乎所有主流深度网络架构都以残差连接为默认配置。从视觉领域的各类 CNN，到自然语言处理领域的 Transformer，再到如今的大语言模型，无一例外。&lt;/p&gt;&lt;p&gt;期间，研究者们大多在注意力机制、归一化方法、激活函数等方面做了大量改进，但残差连接的基本形式几乎没有根本性变化。&lt;/p&gt;&lt;p&gt;直到 2024 年 9 月，字节 Seed 提出了 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650941988&amp;idx=2&amp;sn=1b35f2af9982c529a1c9217bfab24a02&amp;scene=21#wechat_redirect" target="_blank"&gt;HC&lt;/a&gt;，论文后来被 ICLR 2025 接收。&lt;/p&gt;&lt;p&gt;HC 的核心创新在于显著提升了网络的拓扑复杂度，同时不改变单个计算单元的 FLOPs 开销。这意味着在相同的计算预算下，模型可以探索更丰富的特征组合方式。&lt;/p&gt;&lt;p&gt;中国人民大学长聘副教授、博士生导师刘勇认为：&lt;strong&gt;HC 打破了由 ResNet 统治的恒等映射残差连接传统，提出了多路并发连接的新范式。&lt;/strong&gt;它通过引入宽度动态性和跨层特征聚合，证明了通过增加残差路径的特征维（Expansion）和引入可学习的 Dynamic Hyper Connections 可以有效缓解 Representation Collapse 的问题并提升大语言模型的预训练效率，提供了一个超越传统残差网络的全新架构底座，即不再局限于单路径的特征叠加，而是通过超连接构建一个更高维、更灵活的特征流动空间。&lt;/p&gt;&lt;p&gt;DeepSeek 在 mHC 论文中表示：近年来，以 Hyper-Connections（HC）（Zhu et al., 2024） 为代表的研究，为残差连接引入了一个新的维度，并在实验上验证了其显著的性能潜力。HC 的单层结构如图 1 (b) 所示。通过扩展残差流的宽度并提升连接结构的复杂性，HC 在不改变单个计算单元 FLOPs 开销的前提下，显著提升了网络的拓扑复杂度。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDPYzibK7O47ktlVpGcV2SM8HpJeWJtx92Kk2JX0XgthUVgicgpRj9BYCQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.687962962962963" data-type="png" data-w="1080" data-width="1160" data-height="798" data-imgfileid="503528535" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/2a29df81-262a-42a4-936e-bd3e5e7781d3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;可以看出：字节 Seed 提出的「扩展残差流宽度 + 可学习连接矩阵」这一新的架构范式，构成了其后续方法设计的重要基础，相关工作正是在这一范式框架内进一步展开的。&lt;/p&gt;&lt;p&gt;但 HC 在走向大规模训练的过程中遇到了瓶颈，导致训练不稳定和受限的可扩展性。尽管如此，但其为后续研究指明了方向。刘勇认为，HC 论文为 mHC 研究提供了三个核心思路：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;首先是&lt;strong&gt;宽度扩展（Stream Expansion）&lt;/strong&gt;，即通过将残差流维度扩大（如扩大至 4 倍或更多），能够显著增强模型的容量和学习能力；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;其次是&lt;strong&gt;多尺度连接的权重化&lt;/strong&gt;，通过引入可学习矩阵来分配不同层级特征的贡献，启示了连接权重管理（mHC 中的&amp;nbsp;Sinkhorn-Knopp&amp;nbsp;算法）的重要性；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;最后是&lt;strong&gt;动态拓扑的潜力&lt;/strong&gt;，论文展示了模型可以根据深度动态调整特征流向，这种软拓扑结构为解决深层网络训练难点提供了新视角。这些探索让 mHC 意识到，虽然拓扑结构的复杂化能带来增益，但也必须解决随之而来的训练稳定性与工程效率问题。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;正是基于这些探索，DeepSeek 团队得以明确 mHC 的研究方向：在继承 HC 架构优势的同时，针对性地解决其规模化瓶颈。&lt;/p&gt;&lt;p&gt;刘勇指出：mHC 针对 HC 在大规模部署时暴露的稳定性风险和内存访问开销进行了针对性改进。在&lt;strong&gt;研究思路上&lt;/strong&gt;，mHC 延续了 HC 的宽度扩展与多路径聚合，并进一步通过 Sinkhorn-Knopp 等技术手段，施加流形约束，将 HC 的广义空间投影回特定流形，从而在保留 HC 性能优势的同时，重新找回了残差网络至关重要的恒等映射特性，解决了 HC 在超大规模训练时的不稳定性。在&lt;strong&gt;工程层面&lt;/strong&gt;，mHC 中提出了更高效的内核优化（Infrastructure Optimization），使该范式从理论实验走向了万亿级参数规模的工业级应用。&lt;/p&gt;&lt;p&gt;基于这些改进，mHC 不仅解决了稳定性问题，且在大规模训练中（如 27B 模型）表现出卓越的可扩展性。&lt;/p&gt;&lt;p&gt;我们不难发现，mHC 解决了 HC 在大规模训练中的工程瓶颈。通过引入流形约束，mHC 在保留 HC 架构优势的同时恢复了训练稳定性，使得这一新范式真正具备了在主流大模型训练中应用的条件。&lt;/p&gt;&lt;p&gt;有网友认为：DeepSeek 提出的 mHC 是对字节 Seed HC 训练架构技巧的一次颇具说服力的推进。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDUr1PuE81VotNDvvRpib8yc0Z2KfWXAg0c84LWY5Dzmvbjco3Iibu7CJA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6844221105527638" data-type="png" data-w="995" data-width="995" data-height="681" data-imgfileid="503528536" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f470e310-03a6-47e3-a4c3-19143c3e795d/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;从 2015 年残差连接问世，到 2024 年字节 Seed 提出 HC，再到 2026 年 DeepSeek 提出 mHC，我们清楚地看到残差连接在算法上的演进，是不同机构、研究者持续接力和优化的结果。&lt;/p&gt;&lt;p&gt;而在 DeepSeek 发布的另一篇论文中，我们看到了几乎相同的模式再次上演。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;都用 N-gram，字节 Seed、DeepSeek 接连导出新结论&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;和 mHC 论文的「抽象」感不同，「Conditional Memory」论文解决的问题比较好理解：大模型被问到的很多问题是可以直接查表解决的，比如「法国的首都是哪里」，但由于标准 Transformer 缺乏原生的知识查找原语，即使这样简单的问题，模型也得去计算，就像你上了考场还要自己推导公式，这无疑是一种浪费。&lt;/p&gt;&lt;p&gt;对此，「Conditional Memory」论文提出的解决方案是给模型装一个「小抄本」（Engram），常见的词组直接查表，省下来的算力用来做更复杂的推理。&lt;/p&gt;&lt;p&gt;具体来说，Engram 的做法是：给模型配一个巨大的「词组词典」，当模型读到某个词（比如「Great」时，就把它前面几个词拼成 N-gram（比如「the Great」或「Alexander the Great」），然后用哈希函数把这个 N-gram 变成一个数字，直接去词典里查对应的向量。&lt;/p&gt;&lt;p&gt;这个「N-gram 哈希查表」的做法，字节 Seed 之前也用过。在提出&amp;nbsp;&lt;strong&gt;OverEncoding 方法&lt;/strong&gt;的论文（题为「Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling」）中，他们发现：给模型配一个巨大的 N-gram 词典，几乎是「白捡」的性能提升。为什么说白捡？刘勇分析说，因为&lt;strong&gt;这&lt;/strong&gt;&lt;strong&gt;些海量的嵌入参数是稀疏激活的，每次推理只查其中极少数，所以既不怎么吃显存，也不怎么费算力。&lt;/strong&gt;更重要的是，论文发现词典越大、性能越好，而且提升幅度是可预测的。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDG1Gw1ohKRqQdHDBHKp8bY7ltngtf1ChrEN0qZtp8KcfTAiaQKuEGE9g/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.7305555555555555" data-type="png" data-w="1080" data-width="1776" data-height="1298" data-imgfileid="503528537" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/c3545c1a-01a6-488c-a31d-69e5462d76f9/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文地址：https://arxiv.org/pdf/2501.16975&lt;/p&gt;&lt;p&gt;如果说字节 Seed 论文用实验告诉我们「把输入词表加大就能涨分」，DeepSeek 论文则另开一条赛道：把 N-gram 做成外挂存储 Engram，与 MoE 分工，正式提出「条件存储」这条新轴线，并告诉我们该怎么分参数才最划算。&lt;/p&gt;&lt;p&gt;还是回到考场的比喻：字节 Seed 发现给学生发公式手册成绩会提高，于是得出结论 &amp;mdash;&amp;mdash;「大词表是更好的输入表示」。DeepSeek 则进一步追问：这种做法还能以什么方式提高成绩？他们通过 LogitLens 等工具进行机制分析，发现这种 lookup 机制能将模型从繁重的局部静态模式重建中解放出来，使早期层直接获得高阶语义，从而增加了模型的有效推理深度。&lt;/p&gt;&lt;p&gt;基于这个洞察，DeepSeek 不再仅仅将 N-gram 视为简单的词表扩展，而是将这一实验性结论升华为「条件存储」（Conditional Memory），这是一条与条件计算（MoE）并列的 scaling law 新轴线。在此基础上，他们提出了「稀疏分配」（Sparsity Allocation）问题：在固定参数预算下，如何在 MoE 专家与静态存储模块之间分配参数？实验揭示了一条 U 型缩放规律 &amp;mdash;&amp;mdash; 全押 MoE 并非最优解，将约 20%-25% 的参数分配给 Engram 反而效果更好。&lt;/p&gt;&lt;p&gt;刘勇表示，在工程实现上，DeepSeek 也进行了系统性的技术改良。架构层面，它改进了前作仅在输入层（Layer 0）注入信息的局限，将 Engram 模块注入到模型的中间层，使存储访问与深度计算实现并行与融合。交互机制上，它放弃了简单的嵌入加和，引入了「上下文感知门控」，利用隐状态动态调节检索结果。系统优化上，它通过分词器压缩提高存储效率，并利用硬件层面的预取技术（Prefetching）解决海量参数导致的延迟问题，使该技术真正具备了大规模工业落地的能力。&lt;/p&gt;&lt;p&gt;在论文的 3.2 章节，我们发现，DeepSeek 把自己的 Engram 与字节 Seed 的 OverEncoding 方法进行了对比，指出虽然两者都能从更大的嵌入表中获益，但在相同的参数预算下，Engram 的缩放效率明显更高。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDRgicuxvDYLLG5rcicO3pMLVfNicWw5ZicNKH01E0zgFKZibb2GOUSAP4XTg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5212962962962963" data-type="png" data-w="1080" data-width="1568" data-height="818" data-imgfileid="503528541" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/c2e6d714-3ed5-4ecd-81dd-8622ab2df3b3/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一起上分、互相启发 &amp;nbsp;研究发表的意义具象化了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每次 DeepSeek 一发论文，推特上都能引发不小的轰动，有位博主甚至提到他搭乘的飞机上有 30% 的人都在看 DeepSeek 刚发的论文。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDuiaA53s7ehhXFKf4ZYLgcsHziaILAZnDV6ayb3jElWXhRbNHDoIcrEBg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0944444444444446" data-type="png" data-w="1080" data-width="1186" data-height="1298" data-imgfileid="503528539" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f940bdf1-5b1d-4bf8-a22a-96279f4e7c43/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;归根结底，这反映出一个问题 &amp;mdash;&amp;mdash; 目前还愿意公开自己研究成果、带着大家一起「上分」的头部大模型厂商已经越来越少了。DeepSeek 和字节 Seed 在研究上的接力让我们看到了公开研究成果的价值。&lt;/p&gt;&lt;p&gt;同时，DeepSeek 对于社区内优秀成果的挖掘也给了我们一些启发，类似字节 Seed 这样的国内头部大模型团队其实有很多想法值得继续探索。&lt;/p&gt;&lt;p&gt;比如，在架构层面，除了前面提到的 OverEncoding，DeepSeek 论文中还提到了几篇字节 Seed 的相关研究，包括稀疏模型架构 UltraMem 和它的新版本 Ultramemv2。这个全新的模型架构通过分布式多层级联内存结构、Tucker 分解检索与隐式参数扩展优化，有效解决了传统 MoE 架构在推理阶段的高额访存问题，同时验证了其优于传统架构的 Scaling Law 扩展特性。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD4DoXCPEO9rd4KDf0BgGQLLcaX5WUwEQ5DBgsibvJLG26yUeAib5EEYQQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.2361111111111111" data-type="png" data-w="1080" data-width="1346" data-height="318" data-imgfileid="503528540" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/c1069c3a-9cd9-41d3-aae7-6555b851b1d1/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，字节 Seed 在基础研究上还发表过很多大胆探索全新范式的尝试，比如&lt;strong&gt; Seed Diffusion Preview&lt;/strong&gt;，系统性地验证离散扩散技术路线作为下一代语言模型基础框架的可行性；&lt;strong&gt;SuperClass&lt;/strong&gt;，首次舍弃了文本编码器，直接用原始文本的分词作为多分类标签，在视觉任务上效果优于传统的 CLIP 方法；甚至提出了新型神经网络架构 &lt;strong&gt;FAN&lt;/strong&gt;，通过引入傅里叶原理思想，弥补了 Transformer 等主流模型在周期性建模方面的缺陷。&lt;/p&gt;&lt;p&gt;这些底层技术的研究，虽然在短期内无法用于商业模型的训练，但是科技行业的进步，正是在无数研究者对未知领域的探索中发生的。&lt;/p&gt;&lt;p&gt;毕竟，真正推动技术进步的，从来不是单一的突破，而是持续的积累与相互启发。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>仅需一个混频器的无线射频机器学习推理，登上Science Advances！</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 16 Jan 2026 10:15:03 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-16</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-16</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/252567e8-dbe4-482f-b630-8808af1866e3/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="2" data-pm-slice="0 0 []"&gt;&lt;strong&gt;本文作者包括来自杜克大学的高智辉、陈廷钧教授和 MIT 的 Dirk Englund 教授团队。&lt;/strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-path-to-node":"26","style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;高智辉，杜克大学电子与计算机工程系博士生。本科毕业于复旦大学电子工程系。研究兴趣于下一代网络系统，包括信息物理系统、机器学习加速等。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8aNpw3cnGzHxwGIt0DZ2iaicZgK0FZiaiapjhMWLHYdoBX0aoezPry02VS3TNYibEQ2ibzWR4K3dJOjKXg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.2111111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528311" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3af36de2-b64a-4ae0-80bc-3547ef1783f7/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Disaggregated machine learning via in-physics computing at radio frequency&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://www.science.org/doi/10.1126/sciadv.adz0817&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["list",{"type":"ul","style":"list-style-type: disc;margin-left: 8px;margin-right: 8px;","class":"list-paddingleft-1","start":null},"listitem",{"style":""},"para",{"tagName":"p","attributes":{"style":"text-align: left;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;开源代码：https://github.com/functions-lab/WISE&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;strong&gt;模型-数据的分解式计算&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="5"&gt;机器学习部署在边端设备的时候，模型总是存储在云端服务器上（5G 基站），而模型输入输出总是在边端设备上（例如用照相机拍摄照片然后识别其中的目标）。在这种场景下，传统有以下两种方案完成机器学习的推理：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;方案一：上传模型输入到云端。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种方案需要每个用户分别把自己的模型输入上传到云端，然后在云端完成推理，最后把模型输出下载到各个用户。&lt;/p&gt;&lt;p&gt;这种方案需要消耗大量的带宽资源，尤其是在大用户规模的情形下；其次，这种上传用户模型输出的方案会涉及用户隐私泄露的问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;方案二：广播模型下载到边端。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种方案要求是云端服务器把模型广播给所有的用户，每个用户各自存储模型，并且在边缘端进行计算。&lt;/p&gt;&lt;p&gt;这种方案极大挑战了边缘用户的算力，并且在模型存储的过程中还有边端存储读写的开销。&lt;/p&gt;&lt;p&gt;在我们的工作里，我们提出了第三种分离式计算（disaggregated computing）的方案：&lt;strong&gt;广播模型并在射频上完成计算&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD9mVdhV4Vu1bphrp9woiceSia8enELUIOQEuKBxhiaPwXc9icU4LSQTJPDA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.32685185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528371" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b87ad115-d623-4dc1-a5be-a28ac29700dd/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;在这种方案里，模型存储在云端并且在射频上广播，用户也把模型输入调制到射频上。所有的计算都在边缘端的混频器（frequency mixer）的模拟计算中完成，混频器输出直接就是模型的输出。&lt;/p&gt;&lt;p&gt;这种方案成功解决了上述两种方案的问题：模型不需要存储在边缘端，所以没有存储读写的开销；混频器是所有带网络连接功能的边缘设备的必备元件，并且是无源的，所以功耗极低。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8aNpw3cnGzHxwGIt0DZ2iaicWaicoQuHpsLKCPuWuoYmicjzdUguva7iaRiacExeJWCKEhibYde3gPBDE7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3138888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528298" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/d30475e3-6501-44ee-b106-8d117387c92a/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="9"&gt;&lt;b data-index-in-node="0" data-path-to-node="9"&gt;利用混频器进行矩阵向量乘&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="10"&gt;混频器的本质是一个时域上乘法器。它把收到的射频信号和本地震荡器产生的信号相乘，输出就是解调后的基带信号。在我们的工作中，我们把射频信号换成了广播的模型，本地震荡器的信号换成了模型输入，于是混频器的输出就成了模型的输出。&lt;/p&gt;&lt;p&gt;在数字信号处理中，时域上的乘法就是频域上的卷积。当我们把模型推理过程抽象成矩阵向量乘 y = Wx 的时候，我们就可以用卷积来完成这个矩阵向量乘。&lt;/p&gt;&lt;p&gt;另外，我们还需要提前在云端测量无线信道 H。在发送的时候就预调制一个无线信道的逆变成 V，这样通过无线信道后在边缘端接收到的信号就变成了我们希望得到的 W。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDvorJfNiazjOutFLU4MtmCHvict0kWicHfrlmTTJgQcCzwUJXydicic5Kib5A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.42685185185185187" data-type="png" data-w="1080" data-width="5591" data-height="2387" data-imgfileid="503528376" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/6d241582-6729-4738-88d5-335e69a48067/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="12"&gt;&lt;b data-index-in-node="0" data-path-to-node="12"&gt;在测试平台上的实验&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们实现了一台云端服务器广播给三个边缘设备的机器学习推理。我们在软件定义测试平台（software-defined radio testbed）上进行实验验证，其中我们使用 USRP X310 作为主要的无线收发机，外接 ZEM-4300+ 作为主要的混频器。&lt;/p&gt;&lt;p&gt;我们使用了 915 MHz 的频率和 25 MHz 的带宽来无线广播模型。&lt;/p&gt;&lt;p&gt;我们先考虑了通用复数域的 4096 点的向量内积进行计算精度的测试，实验上得到的最高计算精度能达到 5.5 bit，对于大部分机器学习推理已经足够。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8aNpw3cnGzHxwGIt0DZ2iaicUd5KxPSeWHcaMA3p8j2A078x7BLTUMgI0EFOcOPibiawGuWAmaQzVL5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3675925925925926" data-type="png" data-w="1080" data-width="6045" data-height="2222" data-imgfileid="503528312" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a5bb65c8-17e6-47cc-8a03-7020497c98a2/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="14"&gt;&lt;b data-index-in-node="0" data-path-to-node="14"&gt;计算能耗分析&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="15"&gt;考虑一个输入维度是 N，输出维度是 M 的矩阵向量乘，我们的模拟计算架构能耗来源于三个部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="16,0,0"&gt;数模转换器（DAC）：用于产生模型输入的信号 x，复杂度是 O(kMN)。这里的 k 取决于整个系统的能量效率（例如混频器的插入损失、接收器的噪声系数等），且远小于 1。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="16,1,0"&gt;模数转换器（ADC）：用于采样模型输出的信号 y，复杂度是 O(N)。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="16,2,0"&gt;解码器：使用 FFT 把输出的信号 y转到频域，并且提取出最终的模型输出 y，复杂度是 O(N)。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;综上所述，整个系统的能量消耗是 O(M + kMN) 对整个矩阵向量乘；均摊到一个乘累加（MAC）上就是 O(1/N + k)。也就是说，计算的矩阵向量乘规模越大，单个乘累加的能耗越低。&lt;/p&gt;&lt;p&gt;在我们的实验平台上，我们实现了最高到 32768 点的向量内积，能耗可以达到飞焦级，比传统的数字计算（皮焦级）低了 2~3 个数量级。&lt;a href="https://mp.weixin.qq.com/s/II5PcGZG5fk-NumDiD4Spg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/cc3b44b5-f7aa-44dc-892c-0ae745057b3a/1768529582938.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDF9YYkx0c6ko5rcpvPMUUqicm1muEpwOIEQz4mkM19EICiadiaTHauLunA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.2833333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528368" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/cc28bf41-85f9-4673-b818-8419279722f0/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="18"&gt;&lt;b data-index-in-node="0" data-path-to-node="18"&gt;机器学习推理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 MNIST 数据集上，我们训练了一个单全连接层的机器学习模型（等价于逻辑回归），我们展示了一个视频样例。&lt;/p&gt;&lt;p&gt;此外，我们也考虑了三个全连接层的模型，传统的数字计算可以达到 98.1% 的精确度。在用我们的框架时，精确度可以达到 95.7%，但是能耗仅需 6.03 fJ/MAC，也就是一次推理共计6.42 fJ。&lt;/p&gt;&lt;p&gt;我们也考虑了其他机器学习任务，例如 AudioMNIST 数据集上的语音识别，精确度达到了 97.2%，而能耗下降到了 2.8 fJ/MAC。&lt;/p&gt;&lt;p data-path-to-node="20"&gt;&lt;b data-index-in-node="0" data-path-to-node="20"&gt;论文总结&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="21"&gt;我们的核心创新包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;模型无线广播，多终端同时推理&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;神经网络模型被编码为无线射频信号，由中心无线节点统一广播，覆盖范围内的任意数量边缘设备都可同步完成推理，实现真正的「计算即广播」的多终端 AI 推理范式。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;无需改硬件，把「算力」直接搬进无线射频&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;利用边缘设备中本就存在的射频频率混频器，该方法无需任何专用 AI 芯片或电路改动，就能在射频信号域完成乘加运算，实现真正「零额外能耗」的模拟计算。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;单个射频器件即可支持规模化维度的神经网络计算&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通过频域编码，一个频率混频器即可完成高达 32,768 维的内积运算，突破了传统模拟计算在规模上的限制，能够支撑现代深度学习模型的推理需求。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>国内首个可复现！萝博派对公开人形机器人 “从 0 到跑” 全开源方案</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Thu, 15 Jan 2026 21:37:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-13</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-13</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2026 年 1 月 15 日，萝博派对（Roboparty）在官方 GitHub 仓库正式完整开源双足人形机器人 &amp;ldquo;萝博头原型机（Roboto_Original）&amp;rdquo;，并同步启动全球开发者共创计划。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5cd0eb52-5cf5-4e3d-936c-f2dedf0aa6a3/1768483880892.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;这款搭载&lt;strong&gt;拟人步态的 AMP 运控算法&lt;/strong&gt;、跑步速度达 3m/s 的原型机，凭借全栈透明的技术开放模式，成为目前全球范围内技术成熟度领先的全开源人形机器人。&lt;/p&gt;&lt;p&gt;不同于&amp;ldquo;只开源代码或只开源结构图&amp;rdquo;的碎片式开放，本次开源以&amp;ldquo;可复现、可二开、可验证&amp;rdquo;为目标，覆盖参考硬件、控制/训练栈、工程化调试与验证方法，以及长期维护的行业 Know-how 共创知识库。&lt;/p&gt;&lt;p&gt;萝博派对希望把&amp;ldquo;从 0 到跑&amp;rdquo;做成行业共享的具身 Infra 底座：把路径标准化、把经验工具化、把验证流程公开化，推动行业把时间用在真正的场景与能力突破上。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;全栈开源，直击人形机器人开发痛点&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;人形机器人真正的门槛，往往不在某一个算法点，而在&amp;ldquo;从设计&amp;mdash;装配&amp;mdash;标定&amp;mdash;训练&amp;mdash;验证&amp;mdash;迭代&amp;rdquo;的系统工程。基于此，萝博派对针对行业长期存在的三大核心痛点&amp;mdash;&amp;mdash;闭源导致开发壁垒高、设计规范缺失、架构标准不统一&amp;mdash;&amp;mdash;以&amp;ldquo;可复现、可二开、可验证&amp;rdquo;为目标，正式发布双足人形机器人&amp;ldquo;萝博头原型机&amp;rdquo;的全栈开源方案，并同步推出&amp;ldquo;动手学人形机器人问题清单&amp;rdquo;Know-how 共创文档，推动行业经验从&amp;ldquo;各自积累&amp;rdquo;走向&amp;ldquo;公开共享&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e065a1fa-a367-425d-a7d1-59487d761c42/1768483905745.jpeg" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在硬件层面，萝博头原型机公开 1.2m 身高、30kg 重量级本体的全套结构图纸，覆盖关节排布、线束收束方案以及金属结构件选型标准等关键设计细节。同时，项目同步开放关节模组核心参数、选型指南与拆机报告，并提供国内优质供应商清单，配套完整 EBOM 物料清单与 SOP 组装流程，从采购、装配到复现路径形成闭环，显著降低硬件研发与复刻门槛。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/505751f8-3523-4602-81c3-23ab790688ca/1768483942763.gif" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1a5bd777-4415-4e02-8596-f868b0c8fb80/%E5%8E%9F%E5%9E%8B%E6%9C%BA%E8%B7%91%E6%AD%A502_406x232.gif" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在软件与控制层面，项目开放底层控制全量代码，涵盖模仿运动、感知运动与导航运动三大核心模块，并支持 SMPL-X 人体模型适配，使开发者能够直接复用海量人体动捕数据，减少新任务开发中的微调成本，提升能力迁移效率，缓解传统控制方案在泛化性与工程落地上的不足。同时，萝博头原型机同步开源拟人步态的 AMP 运控算法代码，为步态自然度与运动稳定性的进一步迭代提供可直接复用的技术基础。&lt;/p&gt;&lt;p&gt;在工程化落地层面，萝博派对将研发过程中形成的 sim2real gap 弥补方案、样机测试矩阵与调试经验总结系统化公开，并同步沉淀关键避坑要点与流程规范，帮助开发者与合作团队减少重复试错、提升调试效率，让&amp;ldquo;跑起来&amp;rdquo;不再依赖隐性经验，而是可以被复现、被验证、被持续迭代的工程流程。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/583269a9-b11b-4a57-b0ef-669d87976e4d/1768484102694.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;与此同时，萝博派对长期建设并持续维护&amp;ldquo;动手学人形机器人问题清单&amp;rdquo;共创知识库，覆盖行业发展、硬件研发、软件研发与生产制造等关键环节，旨在将行业讨论从&amp;ldquo;表演型炫技&amp;rdquo;拉回&amp;ldquo;实用落地&amp;rdquo;。该知识库主张人形机器人优先解决行走稳定性、抗摔性等基础能力，并围绕尺寸、重量、散热、成本等量产关键问题展开共建，以&amp;ldquo;全员编辑、按紧急度排序&amp;rdquo;的开放机制，将单一团队的经验沉淀升级为&amp;ldquo;全行业共建的落地指南&amp;rdquo;，推动行业从&amp;ldquo;各自试错&amp;rdquo;走向&amp;ldquo;协同突破&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心突破：性能与步态双达标&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/86e01880-18d4-452f-a057-5817dadbf79d/1768483952840.gif" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;萝博头原型机的关键优势，在于&amp;ldquo;硬件性能&amp;rdquo;与&amp;ldquo;控制体验&amp;rdquo;的同步提升。&lt;/p&gt;&lt;p&gt;在运动能力上，原型机跑步速度达到 3m/s 级别，跻身全球全开源人形机器人第一梯队，回应了行业长期存在的&amp;ldquo;开源性能滞后于闭源&amp;rdquo;的刻板印象。为支撑高速与稳定运行，硬件端采用类车规级本体结构与高刚性金属材料，提升力传递效率与整体结构稳定性；同时通过模块化关节模组实现更高的扭矩密度与更快的动态响应，为跑步与复杂动作提供可靠的执行基础。&lt;/p&gt;&lt;p&gt;在控制体验上，萝博头原型机搭载拟人步态的 AMP 运控算法，作为其核心控制能力底座。该算法基于数据驱动范式，并深度适配 Behavior Foundation Model（BFM）预训练框架，通过学习人体动捕数据，使机器人的行走与跑步更贴近人类生物力学特征，在提升动作自然度的同时兼顾稳定性表现，能够在复杂路况中保持更可靠的姿态控制。同时，这一范式显著降低新步态与新任务的微调成本，使步态扩展从&amp;ldquo;重研发&amp;rdquo;转向&amp;ldquo;可迁移、可复用&amp;rdquo;的工程流程。&lt;/p&gt;&lt;p&gt;对开发者而言，这意味着在不额外承担高昂研发投入的前提下，即可获得兼具高性能与自然步态的人形机器人参考方案，并在此基础上更高效地进行二次开发与场景适配，加速具身能力向真实应用落地。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;生态共建：以开源推动协同创新&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a0150afe-b8b1-4ac5-a9b6-cc94ffcdb3dc/1768484141606.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;此次开源是萝博派对推进人形机器人行业协同生态建设的关键一步。在开发者生态层面，团队已搭建面向行业的技术交流与共创网络，吸引上市公司技术负责人、高校科研人员及创业公司核心成员等专业群体加入，形成更高效率的技术交流与资源共享平台，持续推动经验沉淀与问题协作解决。&lt;/p&gt;&lt;p&gt;在商业与产业层面，该项目已获得经纬创投、小米战投、光源资本等机构的千万美元种子轮融资。萝博派对认为，这不仅是对团队技术路线与工程能力的认可，更是对&amp;ldquo;具身智能 Infra 化&amp;rdquo;路径的验证：通过开源与标准化，把开发所需的关键链路沉淀为可复用的基础设施，让行业将更多精力投入到真实场景与能力创新之中。&lt;/p&gt;&lt;p&gt;&amp;ldquo;我们的目标是让具身智能的开发成本降低 80%。&amp;rdquo;萝博派对团队表示，当硬件不再成为门槛、算法不再是黑盒，具身智能才能真正进入&amp;ldquo;千行百业&amp;rdquo;的应用阶段，形成规模化的产业价值。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/b138010f-b7e5-4aa2-9daf-2ff98eb9b4bb/1768484153705.png" style="width: 50%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;除开源共创外，萝博派对也为产业伙伴提供 JDM（联合定义制造）设计与联合开发，加速从参考样机到工程化交付的全流程，覆盖结构/电气/控制集成、BOM 与供应链、试产与测试矩阵等关键工作。&lt;/p&gt;&lt;p&gt;目前，全球开发者可通过官方渠道获取核心资源与参与共创：&lt;/p&gt;&lt;p&gt;萝博头原型机开源仓库已在 GitHub 上线，作为从硬件到软件的汇总入口，保持持续更新。&lt;/p&gt;&lt;p&gt;萝博派对 Github ：&lt;a href="https://github.com/Roboparty/roboto_origin"&gt;https://github.com/Roboparty/roboto_origin&lt;/a&gt;&lt;/p&gt;&lt;p&gt;同时，团队长期维护&amp;ldquo;动手学人形机器人问题清单&amp;rdquo;Know-how 文档，鼓励开发者通过社区参与编辑、提交行业痛点与复现经验，共同建设可持续迭代的落地知识库。&lt;/p&gt;&lt;p&gt;&amp;ldquo;动手学人形机器人问题清单&amp;rdquo; Know-How 文档：roboparty.com/roboto_origin/doc&lt;/p&gt;&lt;p&gt;萝博派对将持续基于社区反馈优化技术方案，推动行业从&amp;ldquo;各自为战&amp;rdquo;走向&amp;ldquo;协同共赢&amp;rdquo;，并欢迎全球开发者加入共创，探索人形机器人技术在真实场景中的实用化落地路径。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>联发科天玑9500s、8500发布：GPU、光追拉满，红米Turbo 5Max将搭载</title>
      <description>&lt;![CDATA[支持硬件级光线追踪技术。]]&gt;</description>
      <author>李泽南</author>
      <pubDate>Thu, 15 Jan 2026 18:48:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1 月 15 日，联发科（MediaTek）正式发布了天玑 9500s 和天玑 8500 移动芯片。&lt;/p&gt;&lt;p&gt;作为天玑家族的新成员，两款新品承袭了天玑旗舰芯片的诸多先进技术，在性能、能效、AI、影像、游戏和无线连接等方面表现强大，为旗舰细分市场注入了新动力。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/646b7712-6fe8-489a-aa6e-41f290d2154f/QQ20260115-181348.png" style="width: 59.67%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;天玑 9500s 采用 3nm 制程工艺和全大核架构（拥有超过 290 亿晶体管），八核 CPU 包含 1 个主频 3.73GHz 的 Cortex-X925 超大核、3 个 Cortex-X4 超大核和 4 个 Cortex-A720 大核，配备同档次出众的大容量高速缓存（二级缓存、三级缓存+系统缓存共 29m），结合第二代天玑调度引擎，可为手机等终端带来强大性能和能效表现。&lt;img src="https://image.jiqizhixin.com/uploads/editor/d2e2574e-f4c9-4894-9923-d9a57039e491/QQ20260115-181722.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;GPU 方面，天玑 9500s 搭载 Immortalis-G925 GPU ，提供重载硬核手游满帧、沉浸式的畅游体验，能够满足游戏发烧友、电竞选手对性能的期待。天玑 9500s 支持先进的光线追踪技术，天玑 OMM 追光引擎能够高效渲染图形，大幅提升游戏画面的真实感和精细度，带来主机级的环境光照和反射效果。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2ee4cf4f-04c6-445c-99bc-bfc0d0173632/QQ20260115-181925.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;此外，借助天玑星速引擎的自适应技术 3.0（MAGT 3.0）和天玑倍帧技术 3.0（MFRC 3.0），天玑 9500s 可显著提高主流游戏的能效表现，延长终端的续航时间。该芯片还支持 165 超高帧游戏，助力玩家体验快人一步。&lt;/p&gt;&lt;p&gt;在 AI 性能方面，天玑 9500s 集成了旗舰级 NPU，拥有强大的端侧 AI 推理能力，面向生成式推理、多模态模型进行了优化，以构建强大的旗舰端侧影像、内容生成等多元能力。该芯片支持端侧 AI 实况照片美化、AI 照片编辑（扩图、抠图、消除），AI 内容摘要（通话、会议和文件）等日常高频功能，能够助力终端厂商打造用户的个人随身 AI 设备，满足日益增长的社交、生产力场景需求。&lt;/p&gt;&lt;p&gt;联发科表示，其正在持续与大量应用端厂商合作，致力于打造更多新形态的 AI 体验。&lt;/p&gt;&lt;p&gt;影像方面，天玑 9500s 搭载先进的 MediaTek Imagiq 影像处理器，支持实时 30 帧运动追焦和 8K 全焦段杜比视界 HDR 视频录制，视频创作者可轻松捕捉清晰、生动的视频画面。此外，该芯片还支持杰出的抓拍和降噪技术，带来既快又清晰的旗舰拍照体验。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6a7e02ad-98bf-4006-8d04-10882c83eac4/QQ20260115-182809.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;今天推出的另一款芯片天玑 8500 采用台积电 4nm 制程打造，全大核架构 CPU 包含 8 个主频至高可达 3.4GHz 的 Cortex-A725 大核，带来性能和能效的进一步提升。天玑 8500 支持精准的调度技术，支持传输速率更高的 LPDDR5X 9600Mbps 内存，用户在日常应用、游戏和多任务处理等使用场景中能够享受丝滑流畅、持久续航。&lt;/p&gt;&lt;p&gt;天玑 8500 搭载性能更强的八核 Mali-G720 GPU，峰值性能相较上一代提升 25%，功耗相较上一代峰值性能下降低 20%。得益于全面升规的计算核心、天玑调度和星速双引擎，天玑 8500 可为玩家带来兼具游戏满帧稳帧、疾速加载和冰峰高能效的劲爽体验。据介绍，在流行开放世界手游的高画质设置上，搭载该芯片的手机可以保持 60 帧满帧的效果。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5c118ad0-80d7-448c-8274-26f667762031/QQ20260115-183705.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;此外，天玑 8500 还将光线追踪技术落地于主流移动游戏，提供更加逼真的画质效果，显著提升玩家的沉浸感。&lt;/p&gt;&lt;p&gt;联发科表示，基于天玑新一代次旗舰芯片，通过与腾讯语音团队的合作，目前双方已在王者荣耀游戏中落地了 AI 语音转文字的功能。&lt;/p&gt;&lt;p&gt;至于搭载新一代芯片的手机，联发科介绍了与小米（REDMI）vivo，OPPO 等厂商的深度合作。&lt;/p&gt;&lt;p&gt;小米集团手机部副总裁李俊也来到了发布会现场，介绍了即将搭载联发科新一代芯片的手机。他表示，即将在本月发布的红米 Turbo 5 Max（天玑 9500s 版）的安兔兔跑分达到了 361 万分，在 2500 元价位上实现了前所未有的性能。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8bc2160d-3391-46d7-b63b-43ab0b276fdb/QQ20260115-183809.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;除了天玑 9500s 与 8500 芯片，红米 Turbo 5 Max 预计还将拥有大屏幕及超大容量电池。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>通用级PixVerse P1的技术突破，揣着进入平行世界的密码</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 15 Jan 2026 17:26:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-path-to-node="4" data-pm-slice="0 0 []"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/de5cc1f9-b163-420b-a4eb-108d509b18a0/1768468906800.png" style="width: 700%;" class="fr-fic fr-dib"&gt;原来，视频生成卷到极致，就是突破大脑和视觉的边界，让想象力进入 AI 构建的虚拟空间。&lt;/p&gt;&lt;p data-path-to-node="5"&gt;昨天，&lt;strong&gt;PixVerse R1&amp;nbsp;&lt;/strong&gt;突然上线。一开始我们以为这只是一次普通的版本更新，但那种「即时响应、即看即创」的全新交互体验，却是前所未有的。读完技术报告我们发现，这不仅仅是一次卷到极致的性能提升，更是量变带来的质变。&lt;/p&gt;&lt;p data-path-to-node="7"&gt;回顾过去，23 年推出第一版模型，随后 Web 端、移动端全面铺开，爱诗科技在 DiT 路线上一路狂奔：从 24 年底的 10 秒生成，到 25 年 2 月实现 5 秒生成社交级视频，再到 11 月将 1080P 视频生成压缩至 30 秒。在自研模型技术和工程化落地的思想下，PixVerse 确实将「传统视频生成」的速度推向了极限。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;与此同时，行业加速从未停歇。就在上个月，生数科技宣布其与清华大学团队研发的 TurboDiffusion 框架，也让视频生成正式迈入「秒级」门槛。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;&lt;strong&gt;但时间上的「卷」就是视频生成的全部吗？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="10"&gt;显然，再快的速度，如果不能生产出符合制作需求的画质和一致性，依然无法成为&lt;strong&gt;通用&lt;/strong&gt;的标准。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;PixVerse 曾是业界第一个把 5 秒视频生成做到 5 秒之内的团队，而当一切看似达到极限时，在 2026 年开年，PixVerse R1 模型与产品同步横空出世。&lt;/p&gt;&lt;p data-path-to-node="12"&gt;通过将计算效率提升&lt;strong&gt;数百倍&lt;/strong&gt;，它不再局限于「秒级」，而是做到了人类肉眼感知范围内的「实时」生成。发布即实装，这是一款真正的「通用」实时世界模型。这已不仅是单点的技术突破，而是一步到位、直接实现应用层级质变的代际跨越。&lt;/p&gt;&lt;p data-path-to-node="13"&gt;以下，我们将通过技术报告，为您详细解析 R1 的这次突破。&lt;a href="https://mp.weixin.qq.com/s/LybgC6RD9cu0kJyGbTJlog"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/471b59d3-d128-4863-a75f-74b88ee28675/1768468946429.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="15"&gt;看了这个视频，大家或许理解了什么是「无限内容」的视频生成。&lt;/p&gt;&lt;p data-path-to-node="16"&gt;在这个模型创造的世界里，「汉语竟是上古禁咒」，你只要说出「春」即刻绿草如茵，说出「鸟」即刻飞鸟成群。一切都是如此连续，直白，世界实时响应你的呼唤，时间和空间都在你的掌控之中。或许，PixVerse R1 已经彻底掌握了「无限流」的真谛。&lt;/p&gt;&lt;p data-path-to-node="17"&gt;&lt;strong&gt;简单来说，PixVerse R1 是全球首个支持最高 1080P 分辨率通用实时世界模型。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="18"&gt;这也是第一次，AI 可以基于用户的意图实时生成一个持续演化、物理上合理的世界，标志着视频生成正式从「静态输出」迈入「实时交互」的全新阶段。&lt;/p&gt;&lt;p data-path-to-node="19"&gt;回顾视频生成技术的发展路径，行业始终受困于速度、质量与成本的不可能三角：高画质往往意味着高延迟（如传统扩散模型），而追求速度又不得不牺牲物理一致性。PixVerse R1 没有盲目追求参数军备竞赛，而是找到了一条通往「通用」的平衡之路：&lt;/p&gt;&lt;p data-path-to-node="20"&gt;&lt;strong&gt;当一个模型首先做到了打破物理极限的实时响应（IRE），并以此为基础结合了通用全模态（Omni）与长时序世界模拟（自回归），它就已经超越了传统意义上的视频生成工具。&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-backh="198" data-backw="578" data-height="534" data-imgfileid="503528507" data-ratio="0.3425925925925926" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDOGSH2gVorSTm3wLaibtRUJPLjHAxt5iaB0sak4Y0ibD0v9NkwVw9Djk4Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-width="1558" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/094780b4-4100-4632-80b2-3f46e76e776c/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="21"&gt;&lt;b data-index-in-node="0" data-path-to-node="21"&gt;技术博客链接：&lt;/b&gt;https://pixverse.ai/en/blog/pixverse-r1-next-generation-real-time-world-model&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="22"&gt;&lt;strong&gt;交互的物理极限：瞬时响应引擎（IRE）&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="23"&gt;在通往通用世界模型的路径上，「实时性」始终是阻碍技术从实验室走向大规模应用的核心工程障碍。&lt;/p&gt;&lt;p data-path-to-node="24"&gt;传统扩散模型的生成逻辑本质上是一种精细的迭代去噪过程，通常需要 50 步甚至更多的采样步骤，才能将高斯噪声转化为清晰的视觉内容。这种机制虽然在一定程度上保证了生成质量，但其带来的秒级甚至分钟级的高延迟，使得 AI 视频生成长期停留在「离线制作、预录制回放」的阶段，无法满足即时交互的严苛需求。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;不过，生成速度始终是 PixVerse 的强项，其在响应时间上一骑绝尘。早在 PixVerse V4.5 的时候我们就实测过，即使我们将各项生成指标拉满，平台输出结果的时间也&lt;strong&gt;没有超过 1 分钟&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="26"&gt;但是，为了更进一步，实现彻底的「实时响应」，PixVerse 在 R1 上决心彻底重构底层推理架构，提出了&lt;strong&gt;瞬时响应引擎（Instantaneous Response Engine，IRE）&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="27"&gt;这是一套针对采样过程的系统级加速方案，通过三大关键技术，在保持 1080P 高分辨率生成的前提下，将推理时间压缩到极致。&lt;/p&gt;&lt;p data-path-to-node="28"&gt;&lt;b data-index-in-node="0" data-path-to-node="28"&gt;时间轨迹折叠&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="29"&gt;不同于传统方法在加噪去噪过程上进行漫长的逐步逼近，该技术引入「直接传输映射」作为结构先验，建立噪声到数据的直线通路，能够直接预测干净数据的分布路径。&lt;/p&gt;&lt;p data-path-to-node="30"&gt;这种方法在数学上有效地「折叠」了原本冗长的时间维度，将传统扩散模型所需的 50+ 采样步数暴力压缩至仅需 &lt;strong&gt;1-4 步&lt;/strong&gt;。这一数量级的步数缩减，直接从源头上解决了计算量过大的问题，实现了推理速度的质变。&lt;/p&gt;&lt;p data-path-to-node="31"&gt;&lt;b data-index-in-node="0" data-path-to-node="31"&gt;引导校正&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="32"&gt;针对为了保证生成质量通常采用的无分类器引导策略（Classifier-Free Guidance，CFG）所带来的双倍计算开销问题，PixVerse R1 团队通过将条件梯度直接融合进模型内部，使得系统在推理阶段无需再进行正负样本的双重计算。&lt;/p&gt;&lt;p data-path-to-node="33"&gt;这一优化成功绕过了传统 CFG 的计算瓶颈，在不牺牲指令遵循能力的情况下，进一步降低了计算复杂度。&lt;/p&gt;&lt;p data-path-to-node="34"&gt;&lt;b data-index-in-node="0" data-path-to-node="34"&gt;自适应稀疏注意力&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="35"&gt;为了应对高分辨率视频生成带来的巨大显存与计算压力，IRE 采用了自适应稀疏注意力机制。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;该机制能够动态分析视频生成过程中的上下文依赖，智能识别并剪除长程依赖中的冗余计算，从而显著压缩了计算图，大幅提升了整体推理效率。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-backh="224" data-backw="512" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDiaMqMbPXuSGI6vNxibmjibgmm3AmalWas0x47nYp9STBK31jw4iaJa5Slw/0?wx_fmt=png&amp;from=appmsg" data-cropx1="73.25259515570934" data-cropx2="1366.5397923875432" data-cropy1="250.06920415224914" data-cropy2="815.8823529411765" data-height="962" data-imgfileid="503528508" data-ratio="0.43796296296296294" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDfTWTmsrlHvlYibrRcpHGMz9beVhylIugVh2HFDClI9X1L1FN5dqrn0Q/640?wx_fmt=jpeg#imgIndex=2" data-type="png" data-w="1080" data-width="1460" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/a4736c88-788c-4c6f-ae03-5edd54e39491/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="37,0"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;即时响应引擎由三个模块组成：时间轨迹折叠、引导修正和自适应稀疏注意力学习。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="38"&gt;&lt;strong&gt;通用的认知底座：Omni 原生多模态基础模型&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="39"&gt;如果说「瞬时响应引擎」解决了传输的速度问题，那么一个强大的底座模型，则决定了传输内容的质量与上限。&lt;/p&gt;&lt;p data-path-to-node="40"&gt;底座模型是一切新功能新特性的基础。构建通用实时世界模型的第一步，在于打破单一模态的感知壁垒，&lt;strong&gt;只有设计一个完全端到端的原生多模态基础模型，才能彻底超越传统生成流程的局限&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="41"&gt;在当前的视频生成技术栈中，多为非端到端的生成方式。往往需要生成一种模态之后通过级联的方式生成另一种模态，这种方式下需要反复的铺路搭桥，尤其是在处理复杂的跨模态交互上，自然显著影响了生成的效率，也限制了模型的通用性。&lt;/p&gt;&lt;p data-path-to-node="42"&gt;为了实现无限的通用性，模型必须强调：&lt;strong&gt;原生，原生，还是原生&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="43"&gt;PixVerse R1 提出的 Omni &lt;strong&gt;原生端到端&lt;/strong&gt;多模态基础模型，正是通过底层架构的重构，实现了「&lt;strong&gt;因原生而通用&lt;/strong&gt;」。&lt;/p&gt;&lt;p data-path-to-node="44"&gt;&lt;b data-index-in-node="0" data-path-to-node="44"&gt;原生统一表示&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="45"&gt;Transformer 架构给了生成模型无穷的想象和可能性。&lt;/p&gt;&lt;p data-path-to-node="46"&gt;Omni 模型引入了&lt;strong&gt;统一 Token 流架构&lt;/strong&gt;。该架构基于 Transformer，摒弃了异构模型拼接的传统路径，将文本、图像、音频与视频等不同模态的数据，统一编码为单一的生成序列。&lt;/p&gt;&lt;p data-path-to-node="47"&gt;在这一框架下，模型不再是将文本「翻译」为视觉信号，而是在原生层面上实现了对多模态数据的联合处理与理解。这种全模态的「通感」能力，使得模型能够精准捕捉文本指令与视听内容之间的深层关联，从而支撑起游戏、影视等多领域的通用化应用。&lt;/p&gt;&lt;p data-path-to-node="48"&gt;&lt;b data-index-in-node="0" data-path-to-node="48"&gt;原生分辨率&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="49"&gt;除了多模态数据的原生处理，第二个原生，是实现高分辨率视频生成的核心特性：&lt;strong&gt;原生分辨率&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="50"&gt;Omni 模型引入这一机制，旨在解决传统视频生成模型中因数据预处理而导致的画面构图破坏与几何失真问题。&lt;/p&gt;&lt;p data-path-to-node="51"&gt;为了适配固定的模型输入结构，传统方案往往采取「强制裁剪」或「缩放拉伸」的策略。这种「削足适履」的方式，会导致画面关键信息被裁切丢失，或使物体形态发生非物理的扭曲变形（如被压扁或拉长）。&lt;/p&gt;&lt;p data-path-to-node="52"&gt;相比之下，Omni 模型坚持在&lt;strong&gt;原生分辨率和原始比例&lt;/strong&gt;下进行端到端的学习。这一架构使其能够自适应处理任意长宽比的素材，&lt;strong&gt;从根源上消除了因裁切或缩放带来的视觉偏差，确保了生成内容在构图完整性与物理几何上的真实感&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD2CMr7jx87AODMcgvO9KYUaibyqRbwwQorjwbSbK9BRDPqtKP0aEmLRw/640?wx_fmt=jpeg#imgIndex=3" data-ratio="0.3574074074074074" data-type="png" data-w="1080" data-width="1358" data-height="860" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDMdmEVN1GqpBWUiasPAZh5TsYia8MLPTg4HHAgUm4whozGhHHuNkFS3Rg/0?wx_fmt=png&amp;from=appmsg" data-cropx2="1358" data-cropy1="197.35640138408306" data-cropy2="683.6989619377164" data-backw="578" data-backh="207" data-imgfileid="503528510" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ab19db3d-53f3-408f-85f1-89541a903e95/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="53,0"&gt;&lt;sup&gt;Omni 原生多模态基础模型的端到端架构，统一设计使 Omni 模型能够接受任意多模态输入并同时生成音频和视频。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="54"&gt;值得一提的是，模型通过原生学习大量真实世界视频数据，来确保真实世界的内在物理定律和动态的真实性。因此，Omni 模型的功能，似乎不仅限于生成引擎，&lt;strong&gt;更具备构建世界模型的潜力&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="55" skip="true"&gt;&lt;strong&gt;世界的连续演化：自回归流式生成机制&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="56"&gt;构建「世界模型」的挑战之一，在于如何从生成分段的「切片」，跨越到模拟连续的「过程」。在这一维度上，PixVerse R1 重点解决的是长视频生成中普遍存在的「长时序一致性」难题，以及伴随而来的显存成本瓶颈。&lt;/p&gt;&lt;p data-path-to-node="57"&gt;在传统的视频生成流程中，模型通常受限于固定时长的生成窗口。当试图延长视频长度时，往往面临「&lt;strong&gt;时间误差累积&lt;/strong&gt;」的问题：随着生成帧数的增加，微小的预测偏差会不断叠加，导致画面内容逐渐偏离初始设定，例如角色的外貌特征发生漂移，或物理环境逻辑出现崩坏。&lt;/p&gt;&lt;p data-path-to-node="58"&gt;此外，为了维持上下文的一致性，传统架构需要保存海量的历史状态，导致显存消耗呈指数级上升，使得长视频生成在计算成本上变得不可控。尤其是在 PixVerse R1 追求的「无限内容」的生成模式下，以上问题如果没有妥善处理，会出现严重的问题。&lt;/p&gt;&lt;p data-path-to-node="59"&gt;针对上述痛点，PixVerse R1 摒弃了传统的全局预测模式，构建了&lt;strong&gt;自回归流式生成机制&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="60"&gt;&lt;b data-index-in-node="0" data-path-to-node="60"&gt;无限流式生成&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="61"&gt;在生成范式上，R1 采用了&lt;strong&gt;自回归建模&lt;/strong&gt;。系统将视频合成任务重构为逐帧预测的&lt;strong&gt;流式过程&lt;/strong&gt;，而非一次性生成固定片段。&lt;/p&gt;&lt;p data-path-to-node="62"&gt;这种架构从根本上解除了时长的硬性约束，实现了理论上的「无限流式生成」。视频不再是受限的帧组合，而成为可以根据即时输入，无限向前延展的时间流。&lt;/p&gt;&lt;p data-path-to-node="63"&gt;&lt;b data-index-in-node="0" data-path-to-node="63"&gt;时间一致性&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="64"&gt;为了在无限延展中保持逻辑自洽，传统方法下基于帧上下文的特征记忆，大多有着数十秒的时间限制，显然是不够用的。&lt;/p&gt;&lt;p data-path-to-node="65"&gt;为此，R1 引入了&lt;strong&gt;记忆增强注意力模块&lt;/strong&gt;。该模块能够显式地提取并锁定视频中的关键特征（如角色的身份特征、场景的空间布局等），将其转化为紧凑的记忆单元。&lt;/p&gt;&lt;p data-path-to-node="66"&gt;在生成后续内容时，模型无需回头重算所有历史数据的全量注意力，而是直接调用「记忆」。这一设计在维持长程依赖的同时，极大地优化了计算效率，避免了显存资源的爆炸式增长。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDMOR6twhO2tDpibr9njDJ8gaDXGfMf7u9MfIkTnjtNnxGkwOVSXnibJrQ/640?wx_fmt=jpeg#imgIndex=4" data-ratio="0.512962962962963" data-type="png" data-w="1080" data-width="1564" data-height="914" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD4MYNE18FrcYeZnPOZeXynV6wcTQvrd4CO6AyexT1dOxPMLfXN5N5Kg/0?wx_fmt=png&amp;from=appmsg" data-cropx1="62.235294117647065" data-cropx2="1453.058823529412" data-cropy1="89.29411764705883" data-cropy2="803.6470588235295" data-backw="514" data-backh="264" data-imgfileid="503528512" data-aistatus="1" data-original-style="width: 100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b0348070-f9d3-45fc-9e47-503590130bab/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="67,0"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 集成自回归建模与全能基础模型。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="68"&gt;从技术逻辑上看，这一机制赋予了 AI 模型「长期记忆」的能力，打破了传统帧间上下文的限制，确保了 PixVerse R1 生成的内容不再是孤立、破碎的视觉片段，而是一个具备持续演化能力的「平行时空」。&lt;/p&gt;&lt;p data-path-to-node="69"&gt;无论生成时长如何延伸，核心主体的统一性与环境逻辑的连贯性始终保持稳定，这种物理与逻辑的持久性，&lt;strong&gt;正是「通用实时世界模型」成立的关键基石&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="70"&gt;&lt;strong&gt;结语：正在发生的现在&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="71"&gt;正如爱诗科技 CEO 王长虎所言：传统视频是被记录的历史，而 PixVerse R1 开创了「正在发生的现在」。&lt;/p&gt;&lt;p data-path-to-node="72"&gt;PixVerse R1 开启的是 AI 原生游戏、互动电影、实时仿真等全新媒介形态的大门，是未来「可交互的数字世界」的计算基础设施。&lt;/p&gt;&lt;p data-path-to-node="73"&gt;视频内容的消费边界正在消融。&lt;/p&gt;&lt;p data-path-to-node="74"&gt;媒体形态将不再局限于预先渲染的固定画面，而是转向由用户意图驱动的即时生成流。&lt;/p&gt;&lt;p data-path-to-node="75"&gt;PixVerse R1 以「通用实时世界模型」的形态，为这一未来提供了可落地的技术样本，也让视听媒介真正从「回放过去」迈向了「未来创作」。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Mira公司内乱？CTO被开除，带团队回OpenAI，翁荔上推发言</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 15 Jan 2026 17:19:37 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/b9c1a8f6-b98e-4a36-bbac-12bc5c58ce02/1768468525334.png" style="width: 700%;" class="fr-fic fr-dib"&gt;今天对于 &lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;Thinking Machines Lab&lt;/span&gt; 和 &lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;OpenAI &lt;/span&gt;来说都是不同寻常的一天。&lt;/p&gt;&lt;p data-pm-slice="2 1 []"&gt;Thinking Machines Lab 创始人兼 CEO Mira Murati 官宣了&lt;strong&gt;与&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;联合创始人兼 CTO&amp;nbsp;&lt;/span&gt;Barret Zoph 的分道扬镳&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;同时，她也宣布了&lt;strong&gt;新任 CTO 的人选 &amp;mdash;&amp;mdash;Pytorch 之父 Soumith Chintala&lt;/strong&gt;。这位在现代 AI 基础设施领域颇具影响力的研究者在去年 11 月初离开了 Meta，并选择加入 Thinking Machines Lab。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528489" data-ratio="0.7027777777777777" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD3phKYnaBF2rZh69V2O2bwFeIp27m0jncaX80xicibKOkgScZDD4abwQg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/089feeed-48d1-4421-8e4b-e480cb7dcb99/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;大约 1 个小时后，OpenAI 应用 CEO Fidji Simo 宣布，&lt;strong&gt;Barret Zoph 将重返 OpenAI&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;连同他一起回归 OpenAI 的还有&lt;strong&gt;另一位 Thinking Machines Lab 联合创始人 Luke Metz&lt;/strong&gt; 以及&lt;strong&gt;创始团队成员 Sam Schoenholz&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528488" data-ratio="0.6342592592592593" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDib1djzble50QZozs0KKEQIZmJnW2aPpic3suricia0HianJ0n8QQa4ofsVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f11ea676-f0dc-4a7e-8330-a6b4929292db/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;两位联合创始人同时从 Thinking Machines Lab「出走」，这一消息在圈内造成了不小的冲击。&lt;/p&gt;&lt;p&gt;根据有人获悉的内部消息，&lt;strong&gt;此次是由于 Barret Zoph 个人的不道德行为，Thinking Machines Lab 才解雇了他&lt;/strong&gt;。Mira Murati 甚至是在全体员工大会上宣布了这一消息。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528490" data-ratio="0.42407407407407405" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDKOG7ZyBkaUsulMACA0N0Y0JLtHADlSP4DAJjT8sOXzfRqoeE9uDKLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/6ae524a4-e601-4b4f-95c3-5f4779ef46a1/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;作为一家明星创企，Thinking Machines Lab 自 2025 年 2 月正式成立以来便备受关注，其核心团队成员主要由来自 OpenAI、Google DeepMind 等顶级实验室的前核心成员组成。&lt;/p&gt;&lt;p&gt;公司在成立五个月后获得了约 20 亿美元种子轮融资，投后估值达到了 120 亿美元，成为硅谷历史上规模最大的种子轮融资之一，投资方包括了英伟达、AMD、Cisco 等。&lt;/p&gt;&lt;p&gt;此次，两位核心联合创始人的离开，会不会对「势头正盛」的 Thinking Machines Lab 造成一些冲击呢？我们尚未可知。&lt;/p&gt;&lt;p&gt;随着事件发酵，&lt;strong&gt;另一位 Thinking Machines Lab 联合创始人、前 OpenAI 安全研究副总裁翁荔（&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;Lilian Weng）&lt;/span&gt;&lt;/strong&gt;也发了一段意味深长的话。&lt;/p&gt;&lt;p&gt;她表示，「今天我跟好几个人都表达过这种看法：能和一群真正在乎产品、追求工匠精神的伙伴共事，真的是一种享受。当工作不仅仅是为了谋生，而是有机会投身于自己热爱的事业，这不仅仅是幸运，更是一种荣幸。我对此倍加珍惜，绝不认为这是理所应得的。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDbkhDC5I8zEocwiamE9iaamEicSicur4sDiciazXUBMWoG9ibYR1pDh6AuKudw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.36203703703703705" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528492" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/0357a587-d7c9-4ddc-b187-379e2e8de168/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;对于如今人工智能圈的人员流动现状，有人感到好奇，「现在的 AI Labs，即使挂着创始人或者创始团队成员的名头，也跟普通员工一样来来去去。」&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528493" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDDuzb7zrrZ4oHSGN7uGQKiatms0sq1zSU2v3c7KgzkHZGF24fS7vGiapQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a0ddb5e6-c8c8-4f50-a7a1-8e5b6c954965/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;几位「当事人」简介&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Barret Zoph&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528494" data-ratio="0.8166666666666667" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDqKibYk4dNajYhDIcJOaZbuaicru90Ozsicjrm0Eo3y0tLMtXePmLsdSrA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/aafba9cd-1af4-4a45-9274-532d9504d971/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;个人主页：https://barretzoph.github.io/&lt;/p&gt;&lt;p&gt;在加入并担任 Thinking Machines Lab CTO 之前，Barret Zoph 曾是 OpenAI 的一位技术主管，领导过 OpenAI 的后训练团队，涉及的研究方向包括对齐、工具使用、评估、ChatGPT、搜索、多模态等等。同时他还是一位专注投资 AI 公司的天使投资人。&lt;/p&gt;&lt;p&gt;再之前，他还曾在谷歌与信息科学学院担任过研究科学家，参与训练了大型稀疏语言模型并将其应用于各种应用的研究工作。&lt;/p&gt;&lt;p&gt;他是两篇重要论文《Learning transferable architectures for scalable image recognition》和《Neural architecture search with reinforcement learning》的第一作者。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Luke Metz&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528495" data-ratio="1.0324074074074074" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDMwtrhZbndicfYPU0QibsO8ib8dshnrlwDO3KSpwdFrt39fW0xeQASXgbA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/5d6373a4-adf5-4a72-8e84-c3dceb31fbd7/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;个人主页：http://lukemetz.com/about/&lt;/p&gt;&lt;p&gt;在加入 Thinking Machines Lab 之前，Luke Metz 曾是 OpenAI 创始团队成员，他与 John Schulman（现为 Thinking Machines Lab 联合创始人兼首席科学家）、Barret Zoph、Liam Fedus（现为 Periodic Labs 联合创始人）等人在内部共同开发了「low-key research preview」，这是 ChatGPT 的雏形。&lt;/p&gt;&lt;p&gt;同时，他也是 GPT-4、GPT-4o、o1 等重量级模型的贡献者之一。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528496" data-ratio="0.5333333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDmG65NGI3ibO8icrL0unict6Cnib7TmoMgs56D5yM66chVX4iagPQhUSY8CA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/56a874a6-0418-459a-b47e-5ea645b8de0a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Sam Schoenholz&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDFLnK5gyIdZlic1rOc4ocS9yXjj7icaqufX75qrxE6ESxcwTe5YN56cVg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.8863198458574181" data-s="300,640" data-type="png" data-w="1038" type="block" data-imgfileid="503528497" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/7a466678-cc16-423a-b5b9-4f3d4b49f7b7/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在 2025 年 1 月加入 Thinking Machines Lab 之前，Sam Schoenholz 曾领导 OpenAI 的可信赖扩展团队和 GPT-4o 优化。&lt;/p&gt;&lt;p&gt;再之前，他曾在 Google Brain 从事统计物理学与机器学习的交叉研究。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDYg0iaJwfamCicAS4VzSJgiaLTXtomicC3FaFOyribSfwHasOdCOxtHR2M1g/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.4064814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503528498" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/eaf98ca5-9512-4a2d-8ecd-19a080bf0c55/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Soumith Chintala&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528499" data-ratio="1" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDRyryRNnXIDwrg7aCTLCJXgEVmKchVg7qfoN74cYR5ySFPrUCdWBfkw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-type="png" data-w="605" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/d826c9c2-e9b9-4cd2-af24-3340c123e05b/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;对于 Soumith Chintala，想必圈内的人应该很熟悉了。他在 2014 年 8 月加入 FAIR，在此期间与团队一起创造出了 PyTorch，并最终于 2025 年 11 月离职，选择加入 Thinking Machines Lab。如今，他又成为了这家初创公司的首席技术官。&lt;/p&gt;&lt;p&gt;关于他一路走来的经历请参考文章：&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651001820&amp;idx=1&amp;sn=d19e3f5a72465ebf663482294271ba59&amp;scene=21#wechat_redirect" target="_blank"&gt;他「二本」出身，数学很差：最终成了 PyTorch 之父、Meta 副总裁&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/fidjissimo/status/2011592010881446116&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/miramurati/status/2011577319295692801&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/lilianweng/status/2011650397635776898&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Nature丨清华等团队揭示AI科研双重效应：个人效率亦或是科学边界</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Thu, 15 Jan 2026 14:02:38 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1UnRGw6gen5reBq2JpALNIANKJ2VGB5Gkx2ndnV3O4rypxOUxzOQqY3YJBD0UBkiaWiaXlzXIibPLA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5778894472361809" data-s="300,640" data-type="png" data-w="995" type="block" data-backw="578" data-backh="334" data-imgfileid="100027161" data-aistatus="1" data-original-style="width: 100%;" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/6947648e-ca36-4274-b7be-d06fe32f6dfe/640.png" data-sec-load-status="2" data-report-img-idx="0" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;编辑丨&amp;amp;&lt;/p&gt;&lt;p&gt;在过去十年里，人工智能几乎渗透进所有自然科学领域：从蛋白结构预测，到材料筛选，再到自动化实验与论文写作。AI 被反复证明能「加速发现」，但一个更深层的问题长期被忽略&amp;mdash;&amp;mdash;&lt;strong&gt;当越来越多科学家依赖 AI，科学整体究竟发生了什么变化？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了突破这一局限，来自清华大学等的徐丰力、李勇教授团队最终推出了「全流程、跨学科的科研智能体系统」&amp;mdash;OmniScientist。他们通过对跨越 45 年、覆盖 4100 余万篇科研论文的分析，首次全景式揭示了 AI 工具融入科学研究后所带来的复杂图景。&lt;/p&gt;&lt;p&gt;相关研究内容以「&lt;em&gt;Artificial intelligence tools expand scientists&amp;rsquo; impact but contract science&amp;rsquo;s focus&lt;/em&gt;」为题，于2026 年 1 月 14 日发布在《&lt;em&gt;Nature&lt;/em&gt;》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1UnRGw6gen5reBq2JpALNpqnMOeJ8osOsurjnkGzniaWPSDzVzia0NibuhEBfWo0pkOCiaUlZkqY08A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3402854006586169" data-type="png" data-w="911" data-width="911" data-height="310" data-imgfileid="100027157" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/68ad18d2-e9d2-4e49-ba20-4801739a81e5/640.png" alt="图片" data-before-load-time="1768456914451" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：&lt;em&gt;https://www.nature.com/articles/s41586-025-09922-y&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;个人扩张与集体收缩&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队构建了一个基于&amp;nbsp;&lt;strong&gt;BERT 的语言模型&lt;/strong&gt;，用于识别「AI 增强型科研论文」。不同于关键词匹配，他们直接让模型学习论文标题与摘要中的语义特征，判断研究是否在方法层面实质性使用了 AI。&lt;/p&gt;&lt;p&gt;最终研究覆盖了&amp;nbsp;&lt;strong&gt;1980&amp;ndash;2025 年间的 41,298,433 篇论文&lt;/strong&gt;，横跨生物、医学、化学、物理、材料与地质六大自然科学领域，并按 AI 发展阶段划分为：&lt;strong&gt;传统机器学习 &amp;rarr; 深度学习 &amp;rarr; 生成式 AI&lt;/strong&gt; 三个时代。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1UnRGw6gen5reBq2JpALNicp20lxYJxzxu4MWvw4lPEulVO1RMpjpP7yK0icwBXoWNVugPq3QpxRg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5703125" data-type="png" data-w="1024" data-width="1024" data-height="584" data-imgfileid="100027160" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e4524c82-bc4a-4435-a7f7-9ac4a68407bc/640.png" alt="图片" data-before-load-time="1768456914942" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 1：AI 在科学领域应用普及率的提升。&lt;/p&gt;&lt;p&gt;在此谨将研究结果分为个人与学术界两个层级进行解读。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对于研究者个人而言：&lt;/strong&gt;与未使用 AI 的同行相比，采用 AI 的研究者年均发表论文数量高出 3.02 倍，获得的引用量高出 4.84 倍。他们的职业发展也明显提速，从「初级研究者」晋升为「资深研究者」的平均时间缩短 1.37 年。AI论文本身也更具影响力，年均引用量高出 98.70%，且更多发表于高影响力期刊。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;但对于学术界可能不算什么好事：&lt;/strong&gt;尽管个体论文影响力增强，但 AI 驱动的科学研究，其集体关注的科学主体空间收缩了 4.63%。这意味着 AI 研究倾向于更集中地围绕已有热门主题展开，而非开拓新的知识疆域。在超过 70% 的细分研究领域中，都观察到了这种知识范围的收缩现象。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1UnRGw6gen5reBq2JpALNmEIUMqZ23CboxWz25f715HxPuB5arz3GoqQWPic1OaDJkic0YubXTa5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7282608695652174" data-type="png" data-w="920" data-width="920" data-height="670" data-imgfileid="100027159" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/ae0f9973-8f04-45a2-b730-61dd63e2bc91/640.png" alt="图片" data-before-load-time="1768456915308" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 2：AI 的采用与自然科学领域知识广度的收缩相关。&lt;/p&gt;&lt;p&gt;究其原因，并非是 AI 不善于创新，而是它更容易&lt;strong&gt;在数据充足、问题定义清晰、评价标准明确的领域发挥优势。&lt;/strong&gt;这使得研究资源、注意力与后续工作，持续向「已有数据密集区」聚集&amp;mdash;&amp;mdash;例如成熟学科、热门问题、已有大规模数据集的方向，而冷门问题、新领域、缺乏标准数据的问题则进一步边缘化。&lt;/p&gt;&lt;p&gt;除此之外，AI 研究催生的后续科学互动模式也发生了变化。单篇 AI 论文能启发较广的知识衍生范围，但后续引用该原始工作的论文之间，彼此相互引用的「后续互动」程度降低了 22%。&lt;/p&gt;&lt;p&gt;这种「孤星」结构，与 AI 领域的学术认可分配不均现象，加剧了科学研究的选择偏颇。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;效率与探索之间的张力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这项研究触及了科学发展的一个根本性张力：在追求研究效率、产出速度和个体成功的激励下，AI 工具正将科研资源引向那些最容易通过数据驱动模型取得快速进展的领域。这固然能加速解决现有范式内的核心问题，提升效率，但可能同时削弱了对数据匮乏、高风险、高不确定性的原创性、颠覆性问题的探索动力。&lt;/p&gt;&lt;p&gt;研究团队指出，这种趋势可能导致科学界困于现有认知的「局部最优解」，而减少了在更广阔、更多元的未知领域进行「分散搜索」的机会。长此以往，科学发现的内涵可能从「提出新问题」向「优化旧方案的答案」倾斜。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLl1UnRGw6gen5reBq2JpALNPLC8vNA2KmI5lIC9bmiaezy4qLlOuh5BibYDdjNdGQ9vHULQNAwib2b6A/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6666666666666666" data-type="png" data-w="915" data-width="915" data-height="610" data-imgfileid="100027158" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/488bb1b6-da1a-4a44-ba50-aa74d6167ebd/640.png" alt="图片" data-before-load-time="1768456915534" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 3：AI 领域后续参与度降低且重叠研究更多。&lt;/p&gt;&lt;p&gt;该研究实质上是在呼吁一种更全面、更平衡的 AI 赋能科学愿景。研究者建议，未来的 AI 系统不应仅仅作为认知能力的放大器，更应发展为感知与实验能力的拓展器。&lt;/p&gt;&lt;p&gt;这意味着 AI 需要帮助科学家去探索、选择并收集来自此前难以触及领域的新型数据，例如设计新型实验、操控机器人实验室、或模拟极端条件，从而主动创造知识探索的新前沿，而非仅仅在现有数据上精耕细作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;笔者小结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为科技前沿的参与者与旁观者，笔者在日常中也常能看到 AI 闪烁的发光点。遍历内容，却发现这些发光点大都依托在已有的实验基础之上，算是站在了巨人的肩膀上更进一层。&lt;/p&gt;&lt;p&gt;AI 带来的学术突破似乎已经将重点从学术转变为了 AI，各类大模型、智能框架的产生，都是为了方便研究者快速完成实验、达到理想中的结果。这并非不是 AI 所带来的时代红利，但人类总要保持对探索未知的热忱。&lt;/p&gt;&lt;p&gt;论文中表示，分析的局限，包括识别方法可能遗漏未明确提及的 AI 使用、主要聚焦自然科学而未涵盖人文社科等。生成式 AI 的影响也尚数据来充分评估。但在&amp;nbsp;AlphaFold、自动化实验室和大模型辅助写作不断加速科研的当下，这种张力，可能正是未来科学必须正视的问题。&lt;/p&gt;&lt;p&gt;相关报道：&lt;em&gt;https://www.science.org/content/article/ai-has-supercharged-scientists-may-have-shrunk-science&lt;/em&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，喝到了千问APP给我点的奶茶</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 15 Jan 2026 13:23:44 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/403bc2e2-1837-4c41-99d5-9b540bfad168/1768454199805.png" style="width: 700%;" class="fr-fic fr-dib"&gt;2026 一开年，智能体的发展立马进入狂奔状态。&lt;/p&gt;&lt;p&gt;本周二，Anthropic 发布 Cowork 掀起了打工人的革命。它不再像 Claude Code 一样专门面向程序员，而是把大模型与智能体能力推进到电脑桌面上，可以解决大部分人的工作问题。&lt;/p&gt;&lt;p&gt;同一时间，谷歌联合 Walmart 等零售商推出了一项专为智能体购物场景设计的开放标准 &amp;mdash;&amp;mdash; 通用商务协议（UCP）。此举旨在推动智能体购物全流程的标准化，实现从商品推荐、购买决策到支付结算的无缝衔接。&lt;/p&gt;&lt;p&gt;1 月 15 日上午，千问又前进了一大步，已经准备让智能体全面接管我们的日常生活了。&lt;/p&gt;&lt;p&gt;这一次，&lt;strong&gt;千问 App 上线了全新 AI Agent 能力「任务助理」，同时全面打通阿里生态，一次开启了 400 多项新功能&lt;/strong&gt;，邀请测试与灰度上线已经同步开启，全都是免费可用的。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528421" data-ratio="0.5564814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDpTYZvbIZoLJGkNGjYxeE7zibEicKqk4ItZAViaJ9ibGTdbwdhPQxibxxiazA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/66817256-da70-4ccb-8c82-d441010c7335/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 千问 C 端事业群总裁吴嘉&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;国内最强的 AI 模型，与最全的应用生态，现在合而为一了。&lt;/p&gt;&lt;p&gt;现在，你只需要对 AI 说「我要两杯奶茶」，千问就可自动找到相应的店铺，选好你的地址、选好商品、下好订单，你只需要点击最终的支付即可。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528423" data-ratio="2.062222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDbORxXCibZXrQOqJazsARXlqTyTVib1DVSmtySEHDhBBtBuCcAlZYBoHQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=2" data-type="gif" data-w="450" type="block" data-original-style="width:404px;height:833px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/449fd024-d31d-4afd-b95d-d034c0d90a90/640.gif" data-order="0" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;如果你想买点什么东西拿不定主意，也可以和千问「任务助理」商量一下，它不仅可以讨论出个符合需求的结果，而且可以直通商店的付款链接。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDgnSNwY2fIZaEkgxe4M68LyWWrFd3pvg0hlR6FChzDYMjHn0xfWcfuQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-ratio="2.0625" data-s="300,640" data-type="gif" data-w="400" type="block" data-imgfileid="503528424" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a7b1ad6b-b72c-47db-a9f8-5496a3c5e07f/640.gif" data-order="1" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;可以看到，千问能够接入的应用包括淘宝、闪购、飞猪、高德地图和支付宝。如果你有需要，千问还能帮你打电话。在发布会现场，千问就展示了 AI 帮人订餐，看起来餐厅老板没有认出与他交谈的是千问。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528425" data-ratio="0.3527777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDYfzLEV6hibXQKPHEabsLpjqKqRSh7ZXNMiceXPicE0siaWt1JO0o5aIU7A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a888f7df-8072-4808-b391-3149cd7b1d61/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;除了各种有意思的功能，我们也初步体验到了 AI 智能体带来的交互革命 &amp;mdash;&amp;mdash; 强大的千问模型，正在把阿里独有的生态优势全部并联起来。不论生活还是工作，以后通行的方法，或许都会被 AI 重新整理一遍。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528428" data-ratio="0.6666666666666666" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD1aQ5QwIjsqMJ8zicTdyjyNIiaTKMNLiahHjpWXdggk1dxUBV1l8AQhlCQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-type="jpeg" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/0760db6c-ab57-4215-93a6-7d1f8fdbed8b/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一手实测 &amp;nbsp;触角已经碰到了物理世界&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;刚过去的 2025 年被普遍视为智能体元年，智能体在人工智能领域的热度一直没有断过。从 Manus、ChatGPT Agent 到更多国产 Agent 大模型与应用，几乎每一次发布都会引起轰动。&lt;/p&gt;&lt;p&gt;智能体的出现，让大模型从拥有智能「大脑」进化出灵活的「手」和「脚」，对复杂任务的自动分析、拆解、执行能力与日俱增。有了智能体的参与，人们可以从繁冗的流程性工作中解放出来，大大节省了工作量与时间成本。&lt;/p&gt;&lt;p&gt;在全面接入一众阿里生态业务之后，千问 App 上的这个智能体新面孔能带来哪些不一样的东西呢？带着这个疑问，我们在拿到内测资格之后，马上对它来了一次摸底测验。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;多品牌团购不在话下&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在上文，我们已经见识到千问「任务助理」点奶茶的便利。接下来的实测中，我们给它上上难度，看能不能搞定多品牌、跨店铺的团购任务。&lt;/p&gt;&lt;p&gt;团购不同牌子的奶茶通常需要我们进入购物 App 并一一查找、浏览对应牌子的奶茶店，还要确认店中有没有自己想要的口味，这会浪费不少时间。在将类似的任务交给千问「任务助理」后，一切的麻烦都没有了。&lt;/p&gt;&lt;p&gt;我们输入指令「帮我点 3 杯霸王茶姬，5 杯瑞幸，8 杯茶百道」，它在确认你的收货地址之后会首先询问你的口味需求。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528435" data-ratio="1.776470588235294" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDVsULfyDOe1ia18IlUDCSka7gcPtq7hUiaufGq9FtZUTocDkibskT4tctg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-type="gif" data-w="340" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e930a75d-168f-4b72-beb6-2cecf4ba2512/640.gif" data-order="2" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;在确认你有无特殊的口味需求之后，它便开始马不停地自动跑完接下来的所有流程，包括&lt;strong&gt;分析用户点单需求、核对点单数量、以及搜索并获取购物平台（这里是淘宝闪购）商品信息&lt;/strong&gt;。随着一系列内外部信息被它吸收消化，紧接着会进入到制定最佳点单方案的环节。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528433" data-ratio="1.776470588235294" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD5OERstLwydwoj6ibjaTnHXtFk3dTjiaQJUHcpS1rEmzqEGibicPzS9k5TA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=7" data-type="gif" data-w="340" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/f7d769c0-246c-4ed4-a917-9d61b454e0d3/640.gif" data-order="3" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;在制定点单方案时，它会根据距离的远近等因素自动为你匹配合适的商家，并初步完成满足你需求的商品筛选与推荐。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528436" data-ratio="1.0212962962962964" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoD46Wawc1oqia4baP8Z47ke1AkCP7n92lj060GvUrTPQKKE85nGwpRCXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/c71f9e2d-7af9-43a8-8ce7-ee01a73c14d3/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;一套丝滑连招下来，它为我们推荐了&lt;strong&gt;三种差异化的方案&lt;/strong&gt;，或想更快收到货、或是选择评分高销量高的门店、或想要订单中包含更多样的饮品种类。这些潜在的用户意图被它精准地捕捉并考虑进来，转化为对应的优先级推荐方案。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528434" data-ratio="1.7377049180327868" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDxlelEqQJ1xt38aaEhJo6qBbypa2ZUJ7QlyiclibLrib2LaGwe76gdBfLw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=9" data-type="gif" data-w="366" type="block" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e7f66bba-15ce-4b73-9662-2e99948340f9/640.gif" data-order="4" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;最后一步，凭自己的喜好下单付款即可。整个操作过程中，除了在有特殊口味需求时需要你的手动介入，其他时候全权交给千问「任务助理」就行了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;定制旅游计划一气呵成&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;前几天，「威海暴雪」的新闻登上了微博热搜，让这座滨海城市闯入了人们的视线。提起山东，大家可能更多地想到青岛。相比之下，威海的名气没有那么大。但从网上的口碑来看，威海以「小而美」著称。&lt;/p&gt;&lt;p&gt;带着对这座城市的好奇，我们让千问「任务助理」制定一份 1 月 16 日（这周五）北京出发的威海两日游计划。&lt;/p&gt;&lt;p&gt;在接收到任务之后，它便自动进入到了任务规划以及逐步的任务执行流程。首先会对我们的需求进行一个整体分析，将威海的景点、美食、住宿等因素统统考虑进来，并启动&lt;strong&gt;搜索子任务&lt;/strong&gt;，即调用搜索工具查询相关的背景知识。&lt;/p&gt;&lt;p&gt;通过不间断地搜索、查询多类型网络来源（包括门户网站新闻、旅行社区热帖等）的威海旅游攻略，尽可能地确保信息准确可靠。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528430" data-ratio="1.7868852459016393" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDmhkiaXoAibooZkvicNx0cw5btgGLmhsudEA0ypich8nfEgYEKXG2IVMsaA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=10" data-type="gif" data-w="366" type="block" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/706050e0-5f0c-43e8-933a-ed9deb7fc905/640.gif" data-order="5" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;紧接着，根据筛选后的优质搜索结果，它为我们规划详细的两日游行程，这里全程对&lt;strong&gt;高德&lt;/strong&gt;和&lt;strong&gt;飞猪&lt;/strong&gt;进行了调用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDicIRyRMJfdiaB0g8A28oYw7IPfJicgCrhTbk5XxvIQwzSkw6uk2aeqFxQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=11" data-ratio="0.5944444444444444" data-type="gif" data-w="1080" type="block" data-imgfileid="503528431" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/52019515-e9e5-42c9-a1d7-231215af99ae/640.gif" data-order="6" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;最终在整合所有行程信息之后，它在高德地图上呈现出了两条交互式路线图。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDBVt8lYxNRn2G3aHYPS875VEWc2IcUzpbibcbrDWq8wxEkWr72YDVObw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=12" data-ratio="0.4444444444444444" data-type="gif" data-w="1080" type="block" data-imgfileid="503528427" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/afdfc4b7-0880-4eb0-b717-4074fd6d9c05/640.gif" data-order="7" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;完整的威海两日游计划出来之后，我们发现，不仅囊括了威海热门景点，还兼顾自然风光与历史文化，并综合考虑了预算成本与游玩体验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDic6TkNZ0Ez4d5116aNqleK2b8A4AKwneL2h8AIuwo42ib7jpeXOmKpPQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=13" data-ratio="0.6370370370370371" data-type="gif" data-w="1080" type="block" data-imgfileid="503528432" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/7682f914-ac05-49fa-badb-a073c8646ddb/640.gif" data-order="8" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在生成的行程计划中，你既可以一键跳转高德来导航去某处景点的路线并一键打车，也能跳转飞猪去订景区门票和酒店。&lt;/p&gt;&lt;p&gt;在日常购物、旅游规划之外，千问「任务助理」擅长的事情还有很多，比如&lt;strong&gt;政务场景&lt;/strong&gt;，在接入支付宝政务服务之后，只需用户一句话就能快速完成政策解读、材料清单梳理等步骤，覆盖办签证、查社保等等场景，并直达办理入口，效率高得惊人。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDzm7qdasXjU8iag2qzxdq1pmgpNZzduq3SnkrLPkrficlPZW7qiaO2XjrA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=14" data-ratio="1.9095477386934674" data-type="gif" data-w="398" type="block" data-imgfileid="503528429" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/7af25a5d-5bb2-4a38-bc8b-9700352dfc7f/640.gif" data-order="9" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;简单的几个任务测下来，我们感受颇深的一点是：在交互方式上，以前我们是与大模型「对话」，现在是给智能体「派单」。只需要给出任务，然后等待结果即可。该说不说，这才是智能体真正的定义。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;千问 AI 助手 &amp;nbsp;有一套「拟人化」思考架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;千问不仅是能点外卖这么简单，千问「任务助理」已经完成了一套基于通用 Agent 体系的底层重构。&lt;/p&gt;&lt;p&gt;首先，&lt;strong&gt;千问 App 采用了一套全新的通用 Agent 体系&lt;/strong&gt;。它基于 MCP 和 A2A 协议，在其中，主 Agent 作为指挥者，它基于千问最强模型拆解和规划任务；子 Agent 作为执行者，它们是多个具有反思能力的智能体，在其领域具有完全决策执行的权限，可以根据任务情况动态纠偏。&lt;/p&gt;&lt;p&gt;这套范式实现了高效的分层规划，在特定任务领域上也可以保证正确的决策，大幅提升了跨领域、长链路的复杂任务执行效率和准确率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;千问深度重构了 Agent 的原生能力栈&lt;/strong&gt;。不同于目前流行的基于视觉识别（GUI）的 Agent 路线，千问选择了更加直接的协议打通，提升了 Agent 在执行任务时的精度和效率，在隐私安全上也更有保障。为了进一步提升效率，千问还专门为 AI 进行了工具栈的重构。&lt;/p&gt;&lt;p&gt;比如在搜索时，Agent 能够自主选择不同的搜索方式，或是进行并发搜索；操作浏览器的 Agent 经过了专门训练，结合阿里自研浏览器内核，具备毫秒级响应和极高的交互精度；在处理可视化、写小程序或复杂表格时，智能体会检索、对齐经过验证的成熟代码范式，确保产出结果具备「工程级」稳定性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「任务助理」多层 Agent 的系统，深度集成了阿里自家生态的各种应用、工具&lt;/strong&gt;，大量的应用会被拆解成原子化的指令级，确保了工具调用的准确。在跨场景任务上，系统能够正确地感知实时的位置、价格等时效信息，减少了大模型常见的幻觉问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;千问的 Agent 系统还具备可以持续演进的能力。&lt;/strong&gt;在完成任务之后，Agent 并不是就结束工作了，而是会像人一样进行「反思」并沉淀经验，让 Agent 可以持续进化。实践的经验会被转化为结构化经验库，作为先验知识在后续任务中动态加载。&lt;/p&gt;&lt;p&gt;这样，AI Agent 就可以逐渐具备人类的工作直觉。&lt;/p&gt;&lt;p&gt;最后，&lt;strong&gt;通过 AI Coding 的能力，千问现在可以在执行任务时发动 AI 生成代码能力现写工具&lt;/strong&gt;。前面说到在大量任务上，Agent 可以实现精准的识别与操作。而在比较少见的任务上，千问的 Agent 可以启动 Agentic Learning 机制，自主编写、测试并封装新的原子工具。随着人们的使用，千问「任务助理」的能力会持续增强。&lt;/p&gt;&lt;p&gt;前天 Anthropic 发布的 Cowork，据说是十天之内用 AI 生成代码能力写出来的。看起来现在千问把类似的能力已经给你集成在智能体上了。千问表示，目前在数百个常用工具中，有超过一半是由 AI Coding 编程自主生成的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智能体的 AI 革命 &amp;nbsp;已经开始了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;今年，AI 领域正在进入产品爆发的阶段。&lt;/p&gt;&lt;p&gt;仅在 1 月份，业界就出现了 Anthropic 的 Cowork，OpenAI 的 ChatGPT Health 等一系列新产品。各家科技公司正在快速兑现 OpenAI 总裁 Greg Brockman 对于智能体在企业、专业领域落地的预言。&lt;/p&gt;&lt;p&gt;刚刚千问的新发布，更是把智能体拉近到了我们身边：它能用快速精准的方式连接最常用的 App，让 Agent 进入到你生活的每一步。在国内，能做到覆盖如此全面的生活场景的公司，还真的只有阿里，其生态囊括了购物、出行、支付、办公等方方面面。&lt;/p&gt;&lt;p&gt;我们能够看出，目前这些 Agent 能力还显得比较简单 &amp;mdash;&amp;mdash; 正如第一代 iPhone 功能的简单并没有掩盖其划时代的意义一样，千问 APP 今日的推出，也许就像是智能体的 iPhone 时刻。从鼠标点击到手指触控，再到自然语言对话的交互方式升级，从这场发布开始打响了第一枪，人与机器的关系也进入到了第三次革命的关口。&lt;/p&gt;&lt;p&gt;当 AI 开始帮你整理发票、规划行程、甚至下单买咖啡时，它不再是云端那个高冷的「先知」，而变成了身边能干活的「助理」，这是 AI 从「言」到「行」的分水岭。&lt;/p&gt;&lt;p&gt;千问，会像淘宝开启移动互联网时代那样，开启一个全新的 AI 时代吗？我们拭目以待。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>人脸机器人登上Science Robotics封面：用AI教会仿生人脸机器人「开口说话」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 15 Jan 2026 13:13:46 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-15-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-15-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/3db839cc-d9a6-4a2c-844b-6c2b943df2b3/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;胡宇航（网名 &amp;ldquo;U 航&amp;rdquo;），毕业于美国哥伦比亚大学，博士学位，首形科技创始人。长期专注于机器人自主学习的研究工作。研究成果发表于《Nature Machine Intelligence》，《Science Robotics》等国际顶级期刊。致力于赋予机器人 &amp;ldquo;自我模型&amp;rdquo; 能力，即构建对自身物理结构与运动的内部表征，使机器人能够更好地理解自身，并适应多变的形态、环境与任务。在仿生人机交互方向，他提出融合语音、视觉与动作的情绪理解与表达一体化系统，为机器人提供更加自然的交互能力。通过自监督学习机制，他的方法使机器人在无需人工干预的情况下不断提升人机互动质量，朝着具备终身学习能力的智能体不断迈进。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528356" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDnxq8JDb1txO71452AibDTYrw4upDGtiaQb7FcuyuCJkFXBAL8nB7LGxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/960fdfdc-fae0-4db7-887e-403f7e494a29/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文地址：https://www.science.org/doi/10.1126/scirobotics.adx3017&lt;/p&gt;&lt;p&gt;曾发表论文：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Hu, Yuhang, et al. &amp;quot;Human-robot facial coexpression.&amp;quot; Science Robotics 9.88 (2024): eadi4724.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hu, Yuhang, Jiong Lin, and Hod Lipson. &amp;quot;Teaching robots to build simulations of themselves.&amp;quot; Nature Machine Intelligence (2025): 1-11.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;https://mp.weixin.qq.com/s/HdnbBweZseTjMedyWHDLSg&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2026 年 1 月 15 日，一项来自美国哥伦比亚大学工程学院的突破性研究正式发表于《Science Robotics》，并登上期刊封面。该研究展示了一项全新的机器人技术：一台具备仿生面部结构的人形机器人，通过深度学习实现与语音和歌曲同步的真实唇部运动。它能跟着人类的语言精准张合嘴唇，甚至，能跟着音乐唱歌。标志着人形机器人在人类最丰富的交流通道之一&lt;strong&gt;唇部表达&lt;/strong&gt;上，迈出了突破性一步。&lt;a href="https://mp.weixin.qq.com/s/BySG2N_jsBf8XsB7v4-SOg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e6fd90e9-df68-431b-b850-b7ec4041c5fd/1768453840414.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;为什么 &amp;ldquo;嘴唇&amp;rdquo; 如此重要？&lt;/p&gt;&lt;p&gt;研究显示，在面对面的交流中，人类将近一半的注意力集中在唇部运动上。我们或许能容忍机器人走路笨拙、手部动作僵硬，但&lt;strong&gt;哪怕极其轻微的不自然面部表情，都会立刻引发本能的不适&lt;/strong&gt;。这正是著名的 &amp;ldquo;恐怖谷&amp;rdquo;。&lt;/p&gt;&lt;p&gt;长期以来，即便是最先进的人形机器人，在 &amp;ldquo;说话&amp;rdquo; 时也只能做出类似木偶的张合动作 &amp;mdash;&amp;mdash; 如果它们有脸的话。但这一次，情况正在发生改变。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一个会自主学习表情的机器人&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这项研究中，研究团队打造了一张高度仿生的机器人面孔：&lt;/p&gt;&lt;p&gt;在一层柔性硅胶皮肤之下，隐藏着&lt;strong&gt;&amp;nbsp;20 余个微型电机&lt;/strong&gt;，能够快速、安静且协同地驱动唇部形变。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528352" data-ratio="0.6361111111111111" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDsCy3zFQRicRlLxHZS47dkecA9f9KCk2qXeuJmGYpiaAOZIumc9JSZQZg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/38af39e9-41cf-4096-afb4-4866494297f3/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 2. 机器人唇形硬件结构。（A）面部机器人设计概览，重点展示了人机交互关键组件：包括扬声器、麦克风、高清摄像模块，以及用于固定柔软硅胶面皮的磁吸式快拆连接器。该连接器能实现面皮的精准定位，并通过推拉双向运动驱动硅胶面皮，完成说话时所需的复杂唇部动作。（B）搭载柔软硅胶面皮的人形机器人外观展示。其底座内部集成有边缘计算设备。（C）唇部驱动系统特写，展示上唇、下唇与唇角连接器分别对应固定于相应唇部支架。柔软可替换的面皮通过磁吸连接器固定，可便捷拆卸以进行维护或个性化调整。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;随后，机器人被 &amp;ldquo;带到镜子前&amp;rdquo;&amp;hellip;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;就像一个第一次对着镜子学做表情的孩子，机器人通过观察自己面部在不同电机驱动下的变化，构建 Facial Action Transformer (FAT) 模型，逐渐学会如何控制自己的脸（机器人自我建模 Robotic Self-modeling)。研究团队将这一过程称为一种 &lt;strong&gt;&amp;ldquo;视觉 &amp;mdash; 动作&amp;rdquo; 的自监督学习&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528353" data-ratio="0.45740740740740743" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDvxzRJ7S84CvoUy65ibZ9lyBIhicQlqh3CiaoLN1tm1xMUjSRgXG1Zc75w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/1baf7b37-25b4-4960-a1f1-94ec0738725a/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 3. 机器人能实现的口型及其对应音标展示。该机器人展示了再现关键英语音标的能力，例如爆破音（/p/ 和 /b/）、双唇音（/m/）以及圆唇元音（/u/ 和 /o/）。通过独立控制上唇、下唇及嘴角，每帧图像均捕捉到其实现的典型唇部运动效果。这些数据为机器人在说话时实现正确的唇形匹配奠定了基础。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;依靠纯声音驱动嘴形动作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;接着，机器人通过观看合成的机器人视频（通过 Wav2Lip）在不同语音语料（由 TTS 和 ChatGPT 生成）的真实唇部变化，进一步学习&lt;strong&gt;声音与唇部运动之间的对应关系&lt;/strong&gt;。最终，这两种能力被整合在一起 &amp;mdash;&amp;mdash; 机器人得以将收到的声音信号，直接转化为连续、自然的唇部运动。无需理解语义，机器人已经能 &amp;ldquo;对得上口型&amp;rdquo;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528354" data-ratio="0.4601851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDgmZPRIx2vjsQwRiagZgIeehaz5XsfiaDDQMNtlVyJTJl8D2JibZkDOkaQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/bea0b7dd-4325-44ac-bb20-def2f2f4f14b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 4. 机器人唇形同步的自监督学习框架。 (A) 数据收集阶段：机器人通过与语音相关的随机指令自主生成数据集，利用 RGB 摄像头捕捉广泛的唇部运动，以获取 3D 唇形数据。(B) 部署过程：始于来自 ChatGPT 的文本输入，文本被转换为音频，随后利用 Wav2Lip 技术合成机器人视频。利用真实机器人视频及其对应指令，训练由编码器和解码器（VAE）组成的机器人逆向变换器，以生成平滑、准确、可供真实机器人执行的电机指令。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;多语言能力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队测试了机器人在多种语言、不同语音环境甚至歌曲中的表现。结果显示，即使在复杂的语音节奏下，机器人也能完成连贯的唇部同步，甚至演唱来自其 AI 生成的曲目。&lt;a href="https://mp.weixin.qq.com/s/BySG2N_jsBf8XsB7v4-SOg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9d830db5-e21b-4894-b2f2-4c5fa7efcbdc/1768453913455.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 机器人多语言口型对齐能力&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503528355" data-ratio="0.375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicZPMSA6j9VpibI41cpxbGoDiaATHafTyLYwmdkhRdwWM7uibKKev13w8AE0LBlcj1JrcbH6feTH8gOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/77feb593-4b24-4d72-afa3-6d52fbc72995/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 5. 多语言唇语同步性能量化表现。x 轴标签下方标注的样本量 n 对应每种语言的测试句子视频帧数。结果表明，所有非英语语言的同步误差均保持在英语误差范围内，显示出稳健的跨语言泛化能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;当然，这还不是终点。研究者坦言，像 &amp;ldquo;B&amp;rdquo; 这类需要完全闭唇的音，以及 &amp;ldquo;W&amp;rdquo; 这类涉及明显撮唇的发音，仍然存在挑战。但关键在于 &amp;mdash;&amp;mdash; &lt;strong&gt;这是一种可以随着学习持续进化的能力，而不是写死的规则。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;跨越恐怖谷的 &amp;ldquo;缺失环节&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在研究者看来，面部表情 &amp;mdash;&amp;mdash; 尤其是唇部的自然运动，正是长期以来机器人能力中的 &amp;ldquo;缺失环节&amp;rdquo;。&amp;ldquo;当前的人形机器人更多关注行走和抓取，但凡是需要与人面对面交流的场景，面部表达同样关键。&amp;rdquo;&lt;/p&gt;&lt;p&gt;随着人形机器人逐渐进入娱乐、教育、医疗、陪护等高度依赖情感沟通的领域，一张温暖、自然、可信的&amp;lsquo;脸&amp;rsquo;将不再是加分项，而是入场券。经济学家预测，未来十年全球或将制造超过十亿台人形机器人进入人们的生活场景。而几乎可以确定的是 &amp;mdash;&amp;mdash; 它们不可能都没有脸。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从实验室走向现实&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这项封面研究，不仅是一次学术突破，也展示了中国学者在国际人形机器人领域具备独特的创新能力。&lt;/p&gt;&lt;p&gt;第一作者胡宇航博士表示，当唇部同步能力与对话型大模型结合时，机器人与人类之间的连接将发生质变。&amp;ldquo;我们交流中有大量情感信息并不在语言本身，而在面部和身体语言中。机器人正在开始触碰这条通道。&amp;rdquo;&lt;/p&gt;&lt;p&gt;当机器人真正学会像人一样 &amp;ldquo;说话&amp;rdquo; 和 &amp;ldquo;表达&amp;rdquo;，&lt;/p&gt;&lt;p&gt;恐怖谷，正在被一步步填平。&lt;/p&gt;&lt;p&gt;人类与机器人的信任和情感，将会迎来新的篇章。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
