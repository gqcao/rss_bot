<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>ICLR 2026 放榜了！28%接收率，欢迎投稿机器之心</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 17:51:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;昨晚深夜，ICLR 2026 官方正式向投稿者发送了今年的论文接收结果通知。&lt;/p&gt;&lt;p&gt;作为机器学习领域的顶级会议， ICLR 2026 将于 2026 年 4 月 23 日至 27 日在巴西里约热内卢举行。官方今年收到了有效投稿约 19000 篇，总录取率约为 28%，该录取率涵盖了所有经过同行评审的完整论文投稿，无论其是否撤稿。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;网友晒出成绩单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;录用通知一出来，网友们也坐不住了。社交平台上，很快被各种成绩单刷屏：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbTPO4JzQfXeUjT4INuZcibiaXwVfTOjmpMzW7CkO9KPkx4z0ciazTUaK0w/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.549074074074074" data-type="png" data-w="1080" data-width="1323" data-height="726" data-imgfileid="503530260" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6979b064-2ac2-47fc-a99e-443be791a3d9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;有的研究者不止一篇被录取：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbDWz4vLZNf0fPpfrPNgKPLWG86aicFCTrqNC4iabXVYb1bjcgL2w7EOvA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.37650602409638556" data-type="png" data-w="996" data-width="996" data-height="375" data-imgfileid="503530261" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d5587e2f-c1ff-4f3c-9b4b-b918506b6fd1/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;甚至还有实验室在这一届一口气拿下 8 篇论文。截图一放出来，评论区立刻炸开了锅，清一色的都是羡慕与感叹。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbrflTpdZQmCuCxnrOU5VwbJaHyicq7880W89udgZXkl1icDq529wrwp6Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.0138888888888888" data-type="png" data-w="1080" data-width="1107" data-height="1122" data-imgfileid="503530262" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/99e55c67-6031-45c8-81e2-84c2c784bbbe/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;被拒稿，可能不是论文的问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;今年的 ICLR 可以说是「史上最乱」的一届，先是第三方机构对审稿意见的系统性统计发现，其中有 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651002013&amp;idx=1&amp;sn=c4588ce6e29f6464cda6e84dfded6af5&amp;scene=21#wechat_redirect" target="_blank"&gt;21% 完全由 AI 生成&lt;/a&gt;；后有&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651005475&amp;idx=1&amp;sn=7a252f9f45f9a23abcc799a8f966d9db&amp;scene=21#wechat_redirect" target="_blank"&gt;&amp;nbsp;OpenReview 评审大开盒&lt;/a&gt;，波及到了 ICLR 2026 超过 10000 篇投稿；接着 AI 生成内容检测平台 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651006067&amp;idx=1&amp;sn=ef47cb61cfe7ac51167fc08776076c0f&amp;scene=21#wechat_redirect" target="_blank"&gt;GPTZero 扫描了 300 篇投稿论文&lt;/a&gt;，发现其中有 50 篇在论文引用上至少包含一处明显的幻觉内容。&lt;/p&gt;&lt;p&gt;然而，这场闹剧并未结束。&lt;/p&gt;&lt;p&gt;研究者 Eldar Kurtić 公开展示了一段离谱的审稿意见：一名审稿人在反馈意见中表示该论文「缺少与 FlexPrune 的具体比较」。不过 Kurtić 调查发现，似乎并不存在名为 FlexPrune 的主流基准方法。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb4rXYjkzIS5RKpwct3zPmq9aAo1gDKN4OVZe6uH9tVKQxtTSRFibAL8w/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9296296296296296" data-type="png" data-w="1080" data-width="1320" data-height="1227" data-imgfileid="503530263" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/05bc27e2-7e1f-4846-9d5e-abd8b111c76e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;更离谱的是，领域主席在随后的裁定中，直接采信了这一错误意见，并将其定性为论文的「致命缺陷」，最终以此为由做出拒稿决定。&lt;/p&gt;&lt;p&gt;该贴发布后迅速走红，目前已获得数万次浏览及大量研究者的共鸣，矛头直指 LLM 在同行评审中的滥用。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb9IXyu2Vib7gkBRGUpHfxjwYqMrIBTBQORsibw2ZYYHdibPVF7RdLHTYcw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5092592592592593" data-type="png" data-w="1080" data-width="1107" data-height="564" data-imgfileid="503530264" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/bfcd068b-3e74-4bb9-b5e8-6ea433ca9a29/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb7ReJM0c3icibzZxludSLHRVNPtIqC6xRJv0ms7wYsk34v9X4xWrP1EbQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.6722222222222223" data-type="png" data-w="1080" data-width="1110" data-height="746" data-imgfileid="503530265" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/089f20c5-fc2e-4456-8d8a-b030fc883e76/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不少学者质疑，该审稿意见极有可能是通过 GPT 或 Grok 等 AI 工具自动生成的。由于 AI 存在「幻觉」特性，容易编造看似专业实则虚假的方法名。而 Meta-Reviewer 的疏忽，导致这种错误未能被纠正，反而成为了拒稿的定论。&lt;/p&gt;&lt;p&gt;这位网友则表示，在评分分别为 8 / 6 / 6 / 6、且评审意见整体偏正面的情况下，论文仍被拒稿。最让人难以接受的并不是拒稿本身，而是 Meta-Review 给出的理由。AC 无视了所有评审的一致支持，额外提出了两个新的质疑（而且这些质疑本身还存在事实性错误），并声称：所有评审意见都较为表面（尽管勉强高于评审的最低要求门槛）。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb6nCibEdJ4qc4OY1NBR5owJZM8okian4oI26Wh4auL0oSM0TwQibF4cicAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.8419540229885057" data-type="png" data-w="1044" data-width="1044" data-height="879" data-imgfileid="503530266" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/acaaef3d-8c2d-42c9-9d07-7e9e5d42ec2d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不管怎样，拒稿并不等于否定你的研究价值，很多经典论文也曾遭遇过拒稿。&lt;/p&gt;&lt;p&gt;最后，也欢迎被录取的作者投稿机器之心，让更多人看到你们的研究。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>英伟达Earth 2开源全新天气预报模型，能生成15天全球预报</title>
      <description>&lt;![CDATA[每个模型都基于新的架构构建。]]&gt;</description>
      <author>李泽南</author>
      <pubDate>Tue, 27 Jan 2026 17:46:53 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;本周二，英伟达在得州休斯顿举行的美国气象学会年会上发布了全新的 NVIDIA Earth-2 系列开放模型、库和框架，用于天气和气候人工智能任务，提供了全球第一个完全开放、加速的天气 AI 软件栈。&lt;/p&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a6c7c8f2-7f11-48e6-a3f2-e4d2f205910e/ezgif-66f96790baa09853.gif"&gt;&lt;span class="fr-inner"&gt;NVIDIA Earth-2 Nowcasting 模型利用基于卫星和雷达数据训练的生成式 AI 来预测真实云层和降雨系统的演变，学习预测风暴的发展和组织方式。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;英伟达表示，这些开放技术（包括预训练模型、框架、定制方案和推理库）可以加速所有预测阶段，范围覆盖从处理初始观测数据到生成 15 天全球预测，或局部风暴的预测。&lt;/p&gt;&lt;p&gt;当前，高精度天气预报一直依赖于运行基于物理模型的超级计算机。AI 驱动的天气预报有望节省大量计算时间和成本，使更多国家、气象机构和企业能够运行针对特定应用场景的预报系统。&lt;/p&gt;&lt;p&gt;在这一领域，NVIDIA Earth-2 是首个开放的、加速的模型和工具集，它使生产就绪的天气 AI 完全可供组织在其自己的基础设施上运行、微调和部署，使开发人员能够将不同的天气和气候 AI 功能整合在一起。 &amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 656.01px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3d66e685-e700-45a2-be0e-76af72df9d25/ezgif-46cd4e8d8176ffa5.gif"&gt;&lt;span class="fr-inner"&gt;使用 NVIDIA Earth-2 中期预报对不同天气变量（总柱水汽、风速和比湿）进行集合预报，并与 ERA5 再分析数据进行比较。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这是一项开创性的工作，旨在加快天气预报速度，提高预报准确性，促进合作，并增进科学家对地球大气状况的整体了解。&lt;/p&gt;&lt;p&gt;当前，各行各业的开发者都在利用 Earth-2 预测天气并从中获取可操作的洞察。这其中包括 AI 天气工具提供商 Brightband；以色列气象局、The Weather Company 和美国国家气象局 (NWS) 等天气预报机构；与日立合作的能源预测和电网运营公司 TotalEnergies、Eni、GCL 和 Southwest Powerpool；能源交易解决方案提供商 Jua 和 Metdesk；以及金融风险和情报公司 AXA、JBA Risk Management 和标普全球能源。&lt;/p&gt;&lt;p&gt;今天公布的全新 Earth-2 开放天气模型包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Earth-2 中期预报模型，其采用名为 Atlas 的全新模型架构，能够对包括温度、气压、风和湿度在内的 70 多个气象变量进行高精度的中期天气预报，预报时效可达 15 天。在标准基准测试中，该模型在业内最常用的预报变量上均优于领先的开源模型。&lt;/li&gt;&lt;li&gt;Earth-2 Nowcasting，采用名为 StormScope 的全新模型架构，利用生成式人工智能技术，在几分钟内即可将国家尺度的预报转化为公里级分辨率、0 至 6 小时的局部风暴和危险天气预报。Earth-2 Nowcasting 是首个通过直接模拟风暴动力学，在短期降水预报方面超越传统物理天气预报模型的平台。它利用 AI 技术直接预测卫星和雷达图像。&lt;/li&gt;&lt;li&gt;Earth-2 全球数据同化系统，采用名为 HealDA 的全新模型架构，能够生成天气预报的初始条件 &amp;mdash;&amp;mdash; 即全球数千个地点当前大气状况的快照，包括温度、风速、湿度和气压。Earth-2 全球数据同化系统可在 GPU 上仅需数秒即可生成初始条件，而超级计算机则需要数小时。结合 Earth-2 中期预报系统，该系统能够生成最精准的天气预报，且整个过程均由开放的、完全基于人工智能的流程完成。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e8c009f8-b8fc-47f4-8fa8-af6359a17ff2/ezgif-64b4f93c484e6e37.gif"&gt;&lt;span class="fr-inner"&gt;Earth-2 临近预报示例，展示了真实的云和降雨系统，说明了风暴是如何发展和组织的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这些模型将加入 NVIDIA Earth-2 堆栈中现有的开放式天气模型：&amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Earth-2 CorrDiff 使用名为 CorrDiff 的生成式 AI 架构，将粗分辨率的大陆尺度预测降尺度到高分辨率的区域尺度天气场，从而提供局部预测所需的精细分辨率，速度比传统方法快 500 倍。&amp;nbsp;&lt;/li&gt;&lt;li&gt;Earth-2 FourCastNet3 对各种天气变量（如风、温度和湿度）的预测精度很高，超越了领先的传统集合模型，可与顶级的基于扩散的方法相媲美，同时预测速度比这些方法快 60 倍。&lt;/li&gt;&lt;li&gt;Earth-2 还集成了来自欧洲中期天气预报中心 (ECMWF)、微软、谷歌等机构的开放模型。此外，Earth-2 模型还可以使用 NVIDIA PhysicsNeMo 进行训练和微调，NVIDIA PhysicsNeMo 是一个用于大规模开发 AI 物理模型的开源 Python 框架。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9111ac79-7793-46d2-99b6-eab384941235/ezgif-3384e23563bc3df0.gif"&gt;&lt;span class="fr-inner"&gt;NVIDIA Earth-2 全球数据同化技术展示了来自卫星、气象气球和气象站的地球观测数据的复杂模式，人工智能模型将这些模式转换为大气状态的平滑、连续估计值，并以此为基础进行预测。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;准确的天气预报有助于拯救生命和保护环境，是农业、能源、公共卫生和其他行业决策的基石。研究人员、气象机构、气候技术创新者和企业已经在运行、微调和构建这些最先进的模型，利用他们自己的本地的 AI 基础设施来取得科学突破。&lt;/p&gt;&lt;p&gt;英伟达介绍了一系列目前 Earth-2 的相关使用案例。其中，中国最大的太阳能材料生产商之一、全球综合能源运营商协鑫（GCL）正在其光伏发电预测系统中运行 NVIDIA Earth-2 模型。与传统的数值天气预报相比，Earth-2 能够以更低的成本提供更精确的预测数据，显著提升协鑫光伏发电预测的准确性。&lt;/p&gt;&lt;p&gt;标普全球能源正在利用 NVIDIA Earth-2 CorrDiff 将气候数据转化为本地化洞察，用于风险评估。全球保险集团安盛正在使用 FourCastNet 生成数千个假设的飓风情景，作为其研发项目的一部分，该项目旨在进行模型评估、方法开发以及现有技术的基准测试。 &amp;nbsp;&lt;/p&gt;&lt;p&gt;参考内容：&lt;/p&gt;&lt;p&gt;https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，DeepSeek又探索新架构了，开源OCR 2</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 14:12:05 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;嘿！刚刚，DeepSeek 又更新了！&lt;/p&gt;&lt;p&gt;这次是更新了十月份推出的 DeepSeek-OCR 模型（参见：&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650996733&amp;idx=1&amp;sn=1bfd3ae23da07de55eda3ed0d51c9b23&amp;scene=21#wechat_redirect" target="_blank"&gt;太强了！DeepSeek 刚刚开源新模型，用视觉方式压缩一切&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;当时 DeepSeek-OCR 的出世，引起了大家对视觉压缩的关注与讨论，而这一次，DeepSeek 对视觉编码下手了。&lt;/p&gt;&lt;p&gt;可以说，刚刚发布的 DeepSeek-OCR 2 通过引入 DeepEncoder V2 架构，实现了视觉编码从「固定扫描」向「语义推理」的范式转变！&lt;/p&gt;&lt;p&gt;当然，和 DeepSeek 几乎每次发布一样，这一次同样也是模型和技术报告齐开源。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbqNFSUT1icue2712kRwib6MhJXcY3GUv4047g1nUQTGE4BKUfDgia9rF2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3824074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530224" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/a8819c24-8f2d-4c4b-b3ef-c94058c476f5/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目地址：https://github.com/deepseek-ai/DeepSeek-OCR-2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;模型地址：https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这项研究的三位作者分别是魏浩然、孙耀峰、李宇琨。&lt;/p&gt;&lt;p&gt;具体来说，&lt;strong&gt;该研究的核心创新在于将原本基于 CLIP 的编码器替换为轻量级语言模型（Qwen2-500M），并引入了具有因果注意力机制的「因果流查询」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种设计打破了传统模型必须按从左到右、从上到下的栅格顺序处理图像的限制，赋予了编码器根据图像语义动态重排视觉 Token 的能力。通过这种两级级联的 1D 因果推理结构（编码器重排与译码器解析），模型能够更精准地还原复杂文档（如带表格、公式和多栏布局）的自然阅读逻辑。&lt;/p&gt;&lt;p&gt;这就像是为机器装上了「人类的阅读逻辑」，让 AI 不再只是生搬硬套地扫描图像。对比之下，传统的 AI 就像一个死板的复印机，不管页面内容多复杂，都只能从左上角到右下角按行扫描。&lt;/p&gt;&lt;p&gt;在维持极高数据压缩效率的同时，DeepSeek-OCR 2 在多项基准测试和生产指标上均取得了显著突破。模型仅需 256 到 1120 个视觉 Token 即可覆盖复杂的文档页面，这在同类模型中处于极低水平，显著降低了下游 LLM 的计算开销。&lt;/p&gt;&lt;p&gt;在 OmniDocBench v1.5 评测中，其综合得分达到 91.09%，较前代提升了 3.73%，特别是在阅读顺序识别方面表现出了更强的逻辑性。&lt;/p&gt;&lt;p&gt;此外，在实际生产环境中，该模型显著降低了 OCR 识别结果的重复率，并为未来构建统一的 omni-modal（全模态）编码器提供了可行路径。是的，未来同一个 AI「大脑」或许能用同样的方法去处理声音、视频等所有模态的数据，真正实现多模态的深度统一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DeepSeek-OCR 2 架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如图 3 所示，DeepSeek-OCR 2 延续了 DeepSeek-OCR 的整体架构，由编码器（encoder） 和解码器（decoder） 组成。编码器负责将图像离散化为视觉 token，而解码器则在这些视觉 token 与文本提示（text prompts）的条件约束下生成输出。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbQ2WQb28XxriaTXxT52ZKuAag9BIa0iamFMOVT9SwpeTOJTGX60IFiaOSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.4898148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530226" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d9f509cb-face-4086-baf0-25dfde3248e5/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;两者的关键区别在于编码器部分：&lt;strong&gt;DeepSeek 将原有的 DeepEncoder 升级为 DeepEncoder V2。在完整保留前代能力的基础上，DeepEncoder V2 通过一种全新的架构设计，引入了因果推理能力（causal reasoning）。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DeepEncoder V2&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DeepEncoder V2 的第一个组成部分是视觉分词器（vision tokenizer）。延续了 DeepEncoder 的设计，DeepSeek 采用了一种由参数规模为 8000 万的 SAM-base 与两层卷积层组成的架构。相比 DeepEncoder，DeepSeek 将最终卷积层的输出维度从 1024 降至 896，以与后续处理流程保持一致。&lt;/p&gt;&lt;p&gt;在 DeepEncoder 中，视觉分词器之后接入的是一个 CLIP ViT，用于进一步压缩和建模视觉语义。DeepEncoder V2 对这一组件进行了重新设计，将其改造为一种类 LLM 的架构，并引入了双流注意力机制（dual-stream attention）。&lt;/p&gt;&lt;p&gt;其中，视觉 token 采用双向注意力，以保留 CLIP 所具备的全局建模能力；而新引入的因果流查询（causal flow queries） 则使用因果注意力。这些可学习的查询 token 被作为后缀追加在视觉 token 之后，每个查询都可以关注所有视觉 token 以及其之前的查询 token。通过保持查询 token 与视觉 token 数量一致，该设计在不改变 token 总数的前提下，对视觉特征施加语义上的排序与蒸馏约束。最终，只有因果查询 token 的输出会被送入 LLM 解码器。&lt;/p&gt;&lt;p&gt;从整体上看，该架构实际上构建了一种两阶段级联的因果推理机制：首先，编码器通过可学习查询对视觉 token 进行语义重排；随后，LLM 解码器在这一有序序列之上执行自回归推理。与依赖位置编码施加刚性空间顺序的传统编码器不同，这种因果排序查询能够更自然地贴合连续的视觉语义，并与 LLM 的单向注意力模式高度一致。该设计有望在二维空间结构与一维因果语言建模之间搭建起一座桥梁。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb2BibuODUKvU42icZOmSr3arPiax16ShoiaTiaNYziafBFFPrzeDfng6EhZ1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.44166666666666665" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530228" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/49cc4411-c69e-4424-9058-784b1b5691f5/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;为更直观地展示 DeepEncoder V2 的注意力机制，图 5 对其注意力掩码进行了可视化。该注意力掩码由两个相互区分的区域组成。&lt;/p&gt;&lt;p&gt;左侧区域对原始视觉 token 采用双向注意力机制（类似于 ViT），使任意 token 都可以与其他所有 token 建立可见性，从而实现完整的全局建模；右侧区域则针对因果流 token 使用因果注意力（三角形掩码，与纯解码器 LLM 完全一致），其中每个 token 只能关注其之前的 token。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbiabYo5jmhO7BR7KFfxQN8wYzVWusL6WMYKS7S6xn785dXnicA3dS0T2Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4101851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530230" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/77f090e4-325a-4477-bc13-70fa65cdacbf/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;DeepSeek-MoE Decoder&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;由于 DeepSeek-OCR 2 的改进重点主要集中在编码器 上，并未对解码器部分进行升级。遵循这一设计原则，模型继续沿用 DeepSeek-OCR 的解码器 &amp;mdash;&amp;mdash; 一个参数规模为 30 亿的 MoE 结构，其中约 5 亿参数在推理时处于激活状态。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练数据与训练流程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在数据层面，DeepSeek-OCR 2 沿用了与 DeepSeek-OCR 相同的数据源，由 OCR 1.0、OCR 2.0 以及通用视觉数据组成，其中 OCR 数据占混合训练数据的 80%。同时引入了以下两项改进：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;针对 OCR 1.0 数据采用了更均衡的采样策略，并按内容类型（正文、公式和表格）以 3:1:1 的比例对页面进行划分；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;通过合并语义相似的类别（例如统一「插图说明」和「插图标题」）来优化布局检测的标签。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在训练阶段，DeepSeek-OCR 2 主要分为三个阶段来完成：&lt;strong&gt;（1）编码器预训练；（2）查询增强；（3）解码器专门化&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其中第一阶段使视觉分词器（tokenizer）和 LLM 风格的编码器获得特征提取、token 压缩和 token 重排的基础能力。第二阶段进一步加强编码器的 token 重排能力，同时增强了视觉知识的压缩。第三阶段冻结编码器参数，仅优化解码器，从而在相同的 FLOPs 下实现更高的数据吞吐量。&lt;/p&gt;&lt;p&gt;接着来看细节。&lt;/p&gt;&lt;p&gt;首先是&lt;strong&gt;训练 DeepEncoder V2&lt;/strong&gt;。遵循 DeepSeek-OCR 和 Vary 的方法，使用语言建模目标来训练 DeepEncoder V2，将编码器与轻量级解码器耦合，通过预测下一个 token 进行联合优化。采用了 768&amp;times;768 和 1024&amp;times;1024 两种分辨率的数据加载器。视觉分词器初始化自 DeepEncoder，LLM 风格的编码器则初始化自 Qwen2-0.5B-base。预训练完成后，仅保留编码器参数用于后续阶段。&lt;/p&gt;&lt;p&gt;本阶段使用 AdamW 优化器，学习率采用余弦退火，从 1e-4 降至 1e-6，在 160 台 A100 GPU（20 个节点 &amp;times; 8 台 GPU）上以 640 的批大小训练 40k 次迭代（采用长度为 8K 的序列打包，约包含 1 亿个图文对样本）。&lt;/p&gt;&lt;p&gt;其次是&lt;strong&gt;查询增强&lt;/strong&gt;。在 DeepEncoder V2 预训练之后，将其与 DeepSeek-3B-A500M 整合为最终的流水线。冻结视觉分词器（SAM-conv 结构），并联合优化 LLM 编码器和 LLM 解码器以增强查询表示。本阶段通过多裁剪策略将两种分辨率统一到单个数据加载器中。此外采用 4 阶段流水线并行：视觉分词器（PP0）、LLM 风格编码器（PP1）以及 DeepSeek-LLM 层（PP2-3 每阶段 6 层）。&lt;/p&gt;&lt;p&gt;本阶段利用 160 台 GPU（每台 40GB 显存），配置了 40 个数据并行副本（每个副本 4 台 GPU），过程中使用相同的优化器，以 1280 的全局批大小进行训练，学习率在 15k 次迭代中从 5e-5 退火至 1e-6。&lt;/p&gt;&lt;p&gt;最后是&lt;strong&gt; LLM 持续训练&lt;/strong&gt;。为了快速消耗训练数据，本阶段冻结 DeepEncoder V2 的所有参数，仅更新 DeepSeek-LLM 的参数。本阶段加速了训练（在相同全局批大小下，训练速度提升了一倍多），同时有助于 LLM 更好地理解 DeepEncoder V2 重排后的视觉 token。&lt;/p&gt;&lt;p&gt;承接第二阶段，本阶段进行了另一次学习率退火，从 1e-6 降至 5e-8，共训练 20k 次迭代。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;评估结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队选用 OmniDocBench v1.5 作为主要评测基准，该基准包含 1355 页文档，覆盖中英文两种语言的 9 大主要类别，包括杂志、学术论文、研究报告等。凭借其多样化的测试样本与严格的评测标准，OmniDocBench 为验证 DeepSeek-OCR 2 的整体性能，尤其是 DeepEncoder V2 的有效性，提供了一个可靠有效的平台。&lt;/p&gt;&lt;p&gt;如表 1 所示，在使用最小视觉 token 上限（&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbQVAPX5gOsRRNkSM7rltExFovYqm7ibbWJ7EdfwUEiaxh127jLL5w8SNQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.26481481481481484" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530231" data-aistatus="1" data-original-style="width:77px;height:20px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/1e8eece1-1066-46ad-9b0f-bfca0e93db58/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 11.28%;"&gt;）的情况下，DeepSeek-OCR 2 仍取得了 91.09% 的领先性能。与 DeepSeek-OCR 基线模型相比，在采用相似训练数据来源的前提下，其性能提升了 3.73%，验证了新设计架构的有效性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbjSDBaeoicBictibvETORbYEjx28Zc6VghIowNWOChsjnpibc8YaaUPSsAQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.8916666666666667" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530233" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/fc025f92-ea71-4f83-af08-213a92591967/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，除了整体性能提升，阅读顺序（R-order）指标上的编辑距离（Edit Distance，ED）也显著下降，从 0.085 降至 0.057。这表明，新的 DeepEncoder V2 能够根据图像信息更有效地选择并排列初始视觉 token。&lt;/p&gt;&lt;p&gt;进一步如表 2 所示，在相同的视觉 token 预算（1120）条件下，DeepSeek-OCR 2 在文档解析任务中的 ED（0.100）低于 Gemini-3 Pro（0.115）。这进一步证明了新模型在保持高视觉 token 压缩率的同时，仍能确保更优的性能，并展现出极高的潜力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbJkdbTuvh1MrM8IH9n4vu93C79GlPmZGQtCvnynYAWdyZibDyHPmLhxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.27037037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530234" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/929acccf-54c6-40e1-93d7-3af904b610d9/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;改进空间&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队在 9 种文档类型上，对 DeepSeek-OCR 与 DeepSeek-OCR 2 进行了细致的性能对比，结果表明：DeepSeek-OCR 2 仍具有较大的提升空间，如表 3 所示。在文本识别的编辑距离（ED）指标上，DeepSeek-OCR 2 在大多数场景中优于 DeepSeek-OCR，但在某些类型上仍存在明显不足，例如报纸类文档，其 ED 超过 0.13。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb0gBHFTv7f2iavaXm5ZE6gyY6mh1V0SriaN2iaUfibBghyGcYNFoRbdsVEQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.30092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530236" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/24637db9-40a5-4a64-87cb-12d9c90bee76/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;团队认为主要原因有两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;视觉 token 上限较低，可能影响了文本极为密集的报纸类文档识别效果，这一问题可在未来通过增加局部裁剪（local crops）的数量来缓解；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;报纸类数据不足 &amp;mdash;&amp;mdash; 当前训练集中仅包含约 25 万条相关样本，这对于训练 DeepEncoder V2 来说仍然不够充分。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，在阅读顺序（R-order）这一指标上，DeepSeek-OCR 2 在所有类别中始终优于 DeepSeek-OCR，这进一步验证了所提出的「视觉因果流」编码器设计的有效性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实际应用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 主要面向两类生产场景：一是为 DeepSeek-LLM 提供图像 / 文档读取能力的在线 OCR 服务，二是用于批量 PDF 处理的预训练数据流水线。在比较了 DeepSeek-OCR 2 与 DeepSeek-OCR 在真实生产环境中的表现后发现，由于生产环境中无法获得标准答案，因此团队主要采用「重复率」作为核心质量指标。&lt;/p&gt;&lt;p&gt;如表 4 所示，相比前代模型，DeepSeek-OCR 2 在实际可用性方面有了显著提升：在在线用户日志图像中，重复率从 6.25% 降至 4.17%；在 PDF 数据生产场景中，重复率从 3.69% 降至 2.88%。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFwSLiaGGJPufDHDJadOImqlRLycvEM8JKmN4ZRaHfXm2IVGoOaB2KLw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.25555555555555554" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530238" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/99056c29-b085-42e6-991b-1e3065fd900f/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这些结果进一步验证了 DeepSeek-OCR 2 架构的有效性，尤其体现了其在逻辑性视觉理解方面的优势。&lt;/p&gt;&lt;p&gt;更多详情信息，可阅读原文获取！&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>高效智能体的「幕后推手」是谁？一篇综述带你从记忆×工具学习×规划看透</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 14:07:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/2c04cf4d-fb98-4a8d-9950-ddf2aab313e2/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;随着大模型能力的跃迁，业界关注点正在从 &amp;ldquo;模型能不能做&amp;rdquo; 快速转向 &amp;ldquo;智能体能不能落地&amp;rdquo;。过去一年可以看到大量工作在提升智能体的有效性（effectiveness）：如何让它更聪明、更稳、更会用工具、更能完成复杂任务。&lt;/p&gt;&lt;p&gt;但在真实应用里，另一个更 &amp;ldquo;硬&amp;rdquo; 的问题常常决定能否上线：高效性（efficiency）。智能体即便表现很好，如果每次都要消耗大量算力、时间与调用成本，也很难在生产环境大规模部署。&lt;/p&gt;&lt;p&gt;基于这一视角，论文整理并撰写了一篇面向 &amp;ldquo;高效智能体&amp;rdquo; 的综述，系统梳理当前主要方法，并从三个最关键的机制出发组织全文框架：&lt;strong&gt;记忆 &amp;mdash; 工具学习 &amp;mdash; 规划&lt;/strong&gt;。论文从设计范式出发对代表性方法进行归纳总结，聚焦那些以效率为目标或能够提升效率的核心设计与实现路径，从而更清晰地呈现智能体在真实落地场景中的成本 &amp;mdash; 性能权衡。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529880" data-ratio="0.45" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDPbVGKvncv8ibiaNI3NlxCE7FmQuFwd9lkbrH5HemhQRPwGbibC4h6ksBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f544bd96-dcde-42fa-a6c9-c958608b1679/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.14192&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 地址：https://github.com/yxf203/Awesome-Efficient-Agents&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDBb2kzG46OwkgsvEn8swy1kBNegFZxh6MEWg1eQkjhNe5HG30a5ZtbQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529882" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b454d21b-c76b-4894-83b1-a390dec5af7b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一、智能体记忆：让 &amp;ldquo;会记&amp;rdquo; 更省、更准、更可扩展&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDu5BvcBvmk8PVAtlrtHiab9PiaPaPibcUC1YARYfwcGKicvOzWFuL8CfTVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529892" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f9268945-4705-4777-866e-57497c3c6fec/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;智能体要做长任务，离不开记忆。但把历史一股脑塞进提示词，会带来 token 暴涨和智能体处理长上下文能力下降。因此，高效记忆系统的关键在于把 &amp;ldquo;长历史&amp;rdquo; 加工成 &amp;ldquo;可用、可检索、可复用&amp;rdquo; 的信息资产。&lt;/p&gt;&lt;p&gt;论文按记忆生命周期梳理三步：构建 &amp;mdash; 管理 &amp;mdash; 访问。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆构建&lt;/strong&gt;：通过概括、压缩与结构化把 &amp;ldquo;长对话&amp;rdquo; 转成 &amp;ldquo;可用记忆&amp;rdquo;。一类是留在推理链路的工作记忆，文本式直观但吃上下文，隐式式更像缓存，可减少重复编码；另一类是外置为可检索系统的外部记忆，先将信息压成小单元再按需召回，包括条目式、图式与分层式。此外论文也提到要警惕过度压缩带来的信息损失，即需要考虑如何在降成本与保真之间取得平衡。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆管理&lt;/strong&gt;：防止 &amp;ldquo;存爆炸&amp;rdquo;，也避免 &amp;ldquo;取太慢&amp;rdquo;。规则式快但可能误删重要内容，大模型式更聪明但更贵，混合式则按层级或场景组合两者策略，在效果与成本之间取得折中。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆访问&lt;/strong&gt;：选什么 + 怎么用。访问分记忆选择与记忆整合，通过检索或训练等方式挑选记忆，再用压缩过滤或隐式注入减少 token 与重复编码。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另外，多智能体记忆也成为新趋势。相较于只靠通信，近年更多工作开始引入 &amp;ldquo;记忆&amp;rdquo; 这一概念来支撑规模化协作，论文将其概括为：共享记忆 / 本地记忆 / 混合记忆三类。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、工具学习：让 &amp;ldquo;会用工具&amp;rdquo; 更少调用、更少等待、更少走弯路&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD8fbTenP3yZSG2JMtH5iaoxozzxPdlwd4ylKBg2CcFSsvUYvHuogbXwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4203703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529883" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/bd89d7fd-af27-430e-9efa-6600178dbe45/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;工具让智能体从 &amp;ldquo;会说&amp;rdquo; 变成 &amp;ldquo;能做&amp;rdquo;，但成本也最容易在工具链路里失控。论文按三条主线梳理提效思路：工具选择 &amp;mdash; 工具调用 &amp;mdash; 工具融合推理。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具选择&lt;/strong&gt;：目标是 &amp;ldquo;更快选对、少塞进 prompt&amp;rdquo;。相关方法包括外部检索器、多标签分类，以及将工具映射为特殊 token 等思路，核心都是在大量工具中更快、更准地选出最需要的那几个。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具调用&lt;/strong&gt;：核心是 &amp;ldquo;少等、少调、少走弯路&amp;rdquo;。典型路线包括边生成边调用、并行化调用，以及利用成本感知调用与测试时高效扩展来削减冗余调用；进一步还可通过面向效率的后训练把 &amp;ldquo;短轨迹、少调用&amp;rdquo; 写进策略本身。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具融合推理&lt;/strong&gt;：让模型学会 &amp;ldquo;该不该用、何时用、用几次&amp;rdquo;。代表性方向包括选择性调用，引导智能体只在必要时才发起工具调用；以及成本约束策略优化，在保证效果的同时对冗余交互与过长轨迹施加惩罚，从而学到更短、更省的工具使用策略。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;三、智能体规划：在 &amp;ldquo;深度&amp;rdquo; 与 &amp;ldquo;宽度&amp;rdquo; 上同时省下来&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDMb81A1Giaog2roygZGb37uOaakgQ2bxISWPcTKReTx1UKoeicZ6icuJBA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.45925925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529893" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/75045766-dc82-473a-968b-9407fc03a9cb/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;规划决定智能体如何在多步决策空间里行动。效率问题要么来自单体推理 &amp;ldquo;想太深、搜太贵&amp;rdquo;，要么来自多体协作 &amp;ldquo;聊太多、通信太重&amp;rdquo;。因此论文从两条线展开：单智能体规划与多智能体协作规划。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;单智能体&lt;/strong&gt;：少算但不掉效果。主要思路包括自适应预算与控制的 &amp;ldquo;选择性思考&amp;rdquo;、结构化搜索的剪枝与代价感知、任务分解的先规划后执行；以及通过策略优化与记忆 / 技能获取把高效规划 &amp;ldquo;内化或复用&amp;rdquo;，越用越省。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多智能体&lt;/strong&gt;：少通信但尽可能不丢信息。方向主要有三类：拓扑稀疏化减少全连接带来的&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDwG1eBjkVcibhkvdroWqGsmqDL1g0UCoILic6rFy2tqLCUwLdLwzPoMcg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5573770491803278" data-s="300,640" data-type="png" data-w="122" type="block" data-imgfileid="503529894" data-aistatus="1" data-original-style="width:47px;height:26px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/84e72352-d8c7-41db-92f3-caebc303da67/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 6.87%;"&gt;的消息传递开销；协议与上下文优化压缩则关注 &amp;ldquo;传什么 / 怎么传&amp;rdquo;；蒸馏方法通过将多智能体协作能力蒸馏回单体，来减少运行时多智能体之间协调的成本。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;四、基准与评测（Benchmark）：没有 &amp;ldquo;可比的尺&amp;rdquo;，就谈不上 &amp;ldquo;可落地的效率&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在谈记忆、工具学习与规划的提效方案之前，先要把 &amp;ldquo;尺子&amp;rdquo; 定清楚：高效到底怎么量？&lt;/p&gt;&lt;p&gt;论文强调，效率必须建立在有效性之上。省了资源却显著掉性能，不算高效。因此论文采用的定义是：在给定预算下取得更好的效果，或在相近效果下消耗更少资源。&lt;/p&gt;&lt;p&gt;基于这一视角，论文先梳理了以有效性为主的 benchmark，并进一步汇总了与效率相关的评测内容：一方面，整理了在 benchmark 中显式纳入效率信号（成本、延迟、调用次数等）的评测设置；另一方面，总结了智能体方法中常用的效率指标，用于刻画 &amp;ldquo;省在哪儿、省多少&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、挑战与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;论文同时也提出了目前的一些挑战与展望：&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 1）统一评测框架&lt;/strong&gt;：指标口径统一，模块开销边界清楚，才能真正让各个智能体方法可比可复现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 2）智能体的隐式推理（Latent Reasoning）&lt;/strong&gt;：大模型侧的隐式推理正在升温，但面向智能体的研究仍相对稀缺。由于智能体链路更长、更复杂，还要处理工具调用、规划与记忆等环节，如何把中间推理 &amp;ldquo;做在隐式空间里&amp;rdquo;、在不掉效果的前提下降低成本，既是挑战，也是机会。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 3）面向部署设计&lt;/strong&gt;：在多智能体场景下，需要把部署成本纳入考量，核心问题是投入产出比。也就是说，增加智能体带来的收益，是否足以覆盖新增的开销。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 4）多模态效率&lt;/strong&gt;：多模态智能体发展很快，但效率研究仍相对欠缺。文本智能体的一些提效思路可以借鉴，但是直接迁移却并不容易，因为多模态智能体的感知输入、行为空间与任务结构更复杂、交互成本更高。因此，如何在多模态场景下系统地兼顾效果与成本，仍是亟待解决的关键问题。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>即刻体验国内最强推理模型Qwen3-Max-Thinking，千问PC和网页端已接入</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 27 Jan 2026 13:34:43 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1月26日，阿里正式发布旗舰推理模型Qwen3-Max-Thinking，AI助手千问同步在PC端和网页端（qianwen.com）接入这一国内最强&amp;ldquo;AI大脑&amp;rdquo;，千问App也即将接入。用户只需在模型选择栏中一键切换，即可体验更为强大的推理能力。&lt;/p&gt;&lt;p&gt;Qwen3-Max-Thinking是目前阿里规模最大、能力最强的推理模型，总参数量超万亿（1T），预训练数据量高达36T Tokens。经过大规模强化学习训练，该模型在涵盖事实知识、复杂推理、指令遵循、人类偏好对齐等19个公认的大模型基准测试中，刷新多项最佳表现纪录，整体性能可媲美 GPT-5.2-Thinking-xhigh、Claude Opus 4.5 和 Gemini 3 Pro。&lt;/p&gt;&lt;p&gt;千问切换至这一模型后，不仅更主动、更智能，还能进行深度逻辑推演与自我校验：&lt;/p&gt;&lt;p&gt;更强的事实记忆与世界知识：无论是冷门科学、历史典故还是文化问题，都能提供更准确、权威的回答，同时显著提升上下文连贯性，更好记住用户偏好，大幅减少&amp;ldquo;失忆&amp;rdquo;现象；&lt;/p&gt;&lt;p&gt;专家级复杂推理能力：在高难度科学、数学与逻辑问题上表现卓越，可以为科研人员与职场人士提供多维度分析与结构化推理，辅助相关决策；&lt;/p&gt;&lt;p&gt;自迭代推理机制：面对复杂问题，会先草拟思路、验证假设、优化路径，再输出结论，显著提升推理质量与响应速度；&lt;/p&gt;&lt;p&gt;更契合人类价值观：指令遵循更精准，输出内容更安全、可靠，符合伦理规范与社会共识。&lt;/p&gt;&lt;p&gt;据悉，目前千问C端月活跃用户数已突破1亿，在学生和白领人群中增长迅猛。此前，千问App全面接入淘宝、支付宝、淘宝闪购等阿里生态业务，同时邀测体验&amp;ldquo;任务助理&amp;rdquo;，已成为全球首个能完成真实生活复杂任务的AI助手。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>蚂蚁具身研究首次亮相！就解决了机器人「看」透明玻璃这些难题，还开源了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:32:40 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜冷猫&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;众所周知，「具身智能」是连接数字世界和现实世界的桥梁。&lt;/section&gt;&lt;p&gt;真正的「具身智能」，是全面自主决策自主行动的通用机器人，需要建立在对物理世界完全理解的基础上。&lt;/p&gt;&lt;p&gt;空间视觉感知是自动驾驶、机器人操作等真实世界应用的底层能力，核心目标只有一个：&lt;strong&gt;让机器能够理解并参与三维环境中的交互。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这类机器人大多都以 RGB-D 相机获取真实世界视觉和深度信息，这是行业内综合了成本，精度，以及实用性后普遍的选择。&lt;/p&gt;&lt;p&gt;但物理世界是极为复杂的，要想让这些自主执行任务的机器人卡壳，只需要简单的一块玻璃。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503530189" data-ratio="0.7805555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFUiaQVMzUvicT1pZoW4u6aqOy098EhA1lHK95qyMyDLbCHgO5WOSibqibA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=1" data-type="gif" data-w="720" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/b2810da7-aed7-4ae6-8009-4dd46ec14234/640.gif" data-order="0" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 家务机器人撞玻璃的翻车场面&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;对机器来说，玻璃几乎是世界里的幻影。人类会下意识地把反射、折射进行判断，但机器人并没有这种生活经验。玻璃这类又透明又反光的物体，恰好屏蔽了 RGB-D 相机获取的全部特征，深度和像素点都很难准确识别。&lt;/p&gt;&lt;p&gt;随着自动驾驶和智能机器人离我们的生活越来越近，这个现象已经逐渐成为一个亟需解决的痛点。&lt;/p&gt;&lt;p&gt;令人欣喜的是，我们发现刚刚开源的&lt;strong&gt;全新具身智能感知模型 LingBot-Depth&lt;/strong&gt; ，非常针对性的解决了机器人识别真实世界的「玻璃问题」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;LingBot-Depth 是蚂蚁灵波科技开源的高精度空间感知模型&lt;/strong&gt;，可在不更换硬件的前提下显著提升透明、反光等复杂材质场景的深度输出质量，给机器人一双看清三维空间的眼睛。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbwFqKibYxYzGJS6Te7PMgUIX1KYMTIJEr6wj2bOiaofdlxYgEGID845Jw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=2" data-ratio="0.58125" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530190" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/446278ab-5f67-47c4-86c2-db4d94d4dc9b/640.gif" data-order="1" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;LingBot-Depth 在传统深度传感器易失效的复杂场景中，仍可输出具备真实尺度的高精度深度结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;令人振奋的是，从技术报告来看，这一模型在深度精度和像素覆盖率方面均优于业界顶级的深度相机。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbrvIIMq67zMjJwUVUib15YFmDz64VeaEL2U0y9sZaX4ctm0kI7ibsNPeA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3787037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530191" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/72ec25fd-37b6-4c93-8e30-c06bf3df289f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目链接: https://technology.robbyant.com/lingbot-depth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;HuggingFace 链接：https://huggingface.co/robbyant/lingbot-depth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;技术报告链接：https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;超海量真实场景与崭新的训练范式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;简单来讲，RGB-D 相机在复杂、多变的物理环境中，却频频暴露出难以回避的短板。&lt;/p&gt;&lt;p&gt;尤其是在面对透明或高反光材质，例如玻璃、镜面、不锈钢表面等等，深度相机发射的主动光信号往往无法形成稳定、可靠的回波，导致深度测量值缺失或异常，最终在深度图中表现为大面积空洞、噪声密集以及物体边缘的严重断裂。&lt;/p&gt;&lt;p&gt;即便是最先进的商用传感器，在一些挑战性场景中也难以满足获取稠密、像素级对齐的几何信息的需求。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakb0aOOLlfHuiaDMceJKIVVDAShkmM7Q0M5ibrXDtlbF1ib6ntHRyDEWxOug/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="0.3812615955473098" data-s="300,640" data-type="gif" data-w="1078" type="block" data-imgfileid="503530192" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/1abd64c0-e49e-42d1-b239-6a4befadd988/640.gif" data-order="2" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;LingBot-Depth 能将含噪且不完整的传感器深度优化为干净、稠密且具备真实尺度的三维测量结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;除了透明与反光场景外，在强逆光、极暗光或明暗对比极端的场景下，RGB 图像与深度信息之间的对齐关系更容易失效，深度图的稳定性和一致性显著下降。&lt;/p&gt;&lt;p&gt;更关键的是，感知层面的不可靠会被层层放大，直接影响后续的规划与控制：不完整的深度图会导致机器人误判空间，边缘破碎会影响抓取位姿的计算，而噪声与空洞则可能引发对障碍物距离的系统性偏差。这些问题最终体现为抓取失败、动作犹豫、路径规划异常，甚至是不可接受的碰撞风险，成为制约机器人从「能演示」走向「可长期落地」的关键瓶颈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;双线并行的数据集&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去解决方案很难达到令人满意的效果，核心原因就是数据。巧妇难为无米之炊，RGB-D 数据比仅包含 RGB 的数据要少得多。&lt;/p&gt;&lt;p&gt;更致命的是，现有的大多数 RGB-D 数据集，在设计之初就刻意回避了真实世界中最棘手的成像条件，这类数据过于干净，要么选择纹理丰富、反射少的理想场景；要么干脆依赖渲染引擎，生成近乎完美的深度图。它们几乎不包含真实传感过程中自然出现的深度空洞、回波缺失和异常噪声，彻底回避了真实世界感知的痛点问题。&lt;/p&gt;&lt;p&gt;为了解决这一根本性瓶颈，LingBot-Depth 从数据分布本身入手，&lt;strong&gt;系统性地重构了 RGB-D 训练数据的来源与生成方式&lt;/strong&gt;。其核心思路只有一个：尽可能保留真实世界传感过程自然产生的深度缺失模式。&lt;/p&gt;&lt;p&gt;具体来看，蚂蚁灵波 构建了一套双路径并行的数据筛选与生成流程。一条路径基于自建高质量 3D 资产，走合成仿真路线；另一条路径则来自真实世界，通过可扩展的 RGB-D 采集系统，使用奥比中光等工业级深度相机直接采集现实场景数据。&lt;/p&gt;&lt;p&gt;由此，模型训练数据被明确划分为两类子集：来自&lt;strong&gt;合成路径的 LingBot Depth-S&lt;/strong&gt;，以及来自&lt;strong&gt;真实采集路径的 LingBot Depth-R&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;与现有方法直接输出完美深度图不同，LingBot-Depth 在合成流水线中刻意模拟了真实主动式 RGB-D 相机的成像过程。研究团队在 Blender 中同时渲染 RGB 图像、精确深度图以及带有斑点结构的灰度立体图像对，并使用半全局匹配（SGM）算法生成深度结果，从而引入与真实传感器高度相似的采集伪影。立体基线、焦距等关键参数均通过随机采样生成，以覆盖多样化的成像几何条件。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb7Do9jnwYWLEhbQkGEE3eribMicjbuiaDReGNibOB2tv8DJVbZOnHBQCzMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530193" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/27b28ad1-1171-4e6c-ab36-dacd79184142/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;数据生成管线的一条合成数据样本。每个样本包含一幅 RGB 图像、一幅渲染得到的理想深度图、一对带有散斑图案的立体图像、对应的真实视差图，以及通过半全局匹配（SGM）计算得到的模拟传感器深度图，用以逼近真实世界主动式深度相机所产生的成像伪影。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;此前一些尝试模拟不完美深度测量的工作，数据规模普遍偏小；而部分依赖机器人仿真器的数据集，则在视觉保真度上仍与真实世界存在明显差距。相比之下，LingBot-Depth 的数据构建方式，更接近真实传感器在复杂物理环境中的「所见即所得」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFAaER60jQq9wITiatcjiaE6zfbY5ic6zEprm2yNm07P9icGicPH2pDNqoibw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.45" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530194" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/607a2309-c0cd-4b64-8c78-73405ed05554/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 蚂蚁灵波团队在真实世界 RGB-D 采集数据在不同场景类别下的分布情况。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在此基础上，除了自行构建的 &lt;strong&gt;320 万条&lt;/strong&gt;数据外，蚂蚁灵波还使用了一些开源数据集作为训练数据，最终共构建了 &lt;strong&gt;1000 万条&lt;/strong&gt;用于掩码深度建模的训练样本，覆盖了从理想条件到复杂现实环境的多种深度缺失模式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbV8oqK21l2aMGnmJaJt0bzLam8GmkhJLOQK9TFCAWBygOfstYrdOhjQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.6537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530195" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/43ebd558-0390-4623-aa9c-42d2cb8bd019/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;数据管道整理的 MDM 数据概览。展示了共计 210 万真实采集样本及模拟采集样本，同时展示了 RGB-D 输入和对应的 GT 深度图。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;具身智能感知能力的上限，很大程度上不取决于模型结构，而取决于是否敢于直面真实世界的「不完美」。 LingBot-Depth，正是从数据这一最底层的环节，补上了数据集中被忽略的一块短板。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;崭新的思路 &amp;mdash;&amp;mdash; 掩码深度建模&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传感器输出的像素与深度信息的不完整是能够进行优化的，将不一致的数据作为噪声剔除，随后通过算法修补，在计算机视觉和深度学习领域已经是历史悠久的研究方向。&lt;/p&gt;&lt;p&gt;而 LingBot-Depth 创新性地提出了一个全新的思路：&lt;strong&gt;与其将这些传感器故障视为需要丢弃的噪声，不如将其作为有益的学习信号加以利用。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这个思路的指引下，掩码深度建模方法（Masked Depth Modeling, MDM）应运而生，构建了 LingBot-Depth 的根基，通过算法对传感器输出进行增强，使机器人获得更完整、更稳定、更可用的深度图。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb1b1waTXwVrbIf1Yezt5icQuu2pfW5WFIR5f9c0ctLHT7XPNicDKDUoVA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.36018518518518516" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530196" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b1c05992-910a-4678-86f3-6b4bdc0c6ed4/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;MDM 预训练方法利用 RGB-D 传感器中自然缺失的深度测量值作为掩码，以学习度量尺度下完整且精确的深度表示。由此产生的 LingBot-Depth 模型可作为强大的空间感知先验，用于下游应用，包括 3D 点追踪和灵巧抓取。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;整体框架仍然沿用了近年来视觉领域中行之有效的编码器&amp;ndash;解码器范式，但学习目标是：在 RGB-D 输入条件下，预测稠密、像素级对齐的场景深度。&lt;/p&gt;&lt;p&gt;与传统 MAE 方法最大的不同在于，MDM 并不依赖人为构造的随机掩码。相反，它直接利用 RGB-D 相机在真实世界中天然产生的深度缺失区域 &amp;mdash;&amp;mdash; 也就是那些由透明、反光、弱纹理等复杂成像条件引发的「孔洞」，作为训练时的掩码信号。&lt;/p&gt;&lt;p&gt;这一转变看似简单，却极具挑战性。因为这些自然掩码并非随机分布，而是高度集中在视觉和几何最模糊的位置，其重建难度远高于随机丢弃的 patch。换句话说，模型必须真正理解 RGB 外观与几何结构之间的关系。&lt;/p&gt;&lt;p&gt;为此，MDM 在架构上明确引入了一个关键约束：RGB 信息始终完整可见，深度信息则存在真实缺失。模型被迫在「完整的视觉上下文」和「残缺的几何观测」之间建立联合推理能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbg75nNz4wuia9ic7J22z8w320npeDKkkjCSQ2wLSkcmc9g26MvVmqTmKw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530197" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e2a58477-d2f3-4231-9c80-ca2020678e2a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;MDM 框架示意图。对应于传感器缺失测量的深度标记会被掩码，ViT 编码器基于上下文标记（即 RGB 图像）以及剩余未被掩码的深度标记，学习联合 Embedding 表示。在解码阶段，潜在的深度标记被丢弃，解码器仅依赖潜在的上下文标记重建完整的深度图。右下角展示了一幅未被掩码的深度图，作为参考。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;面向 RGB-D 的 ViT 设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在具体实现上，LingBot-Depth 采用了标准的 ViT-Large 作为编码器主干，但对输入建模方式进行了针对 RGB-D 场景的定制。&lt;/p&gt;&lt;p&gt;RGB 图像与深度图通过两套独立的补丁嵌入层进行处理，分别生成在同一空间网格上对齐的 RGB token 和深度 token。这种设计使得 Transformer 的自注意力机制能够在同一空间位置上，同时建模外观语义与几何线索的交互关系。&lt;/p&gt;&lt;p&gt;此外，为避免不同模态在注意力计算中「混淆身份」，模型还显式引入了模态嵌入，与二维空间位置编码共同构成每个 token 的位置信息。这种处理方式，使得 ViT 能够在统一的序列中区分这是「 RGB 信息」还是「深度信息」，同时保留空间一致性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;利用深度缺陷，而不是回避它&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在掩码策略上，MDM 并未简单地对所有缺失深度一刀切。考虑到真实 RGB-D 数据中，完全没有深度缺失的样本同样具有重要价值，模型采用了一种基于补丁统计的自适应掩码策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对深度值完全缺失的 patch，必然作为掩码；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对同时包含有效与无效测量的 patch，提高其被掩码的概率；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;若仍未达到目标掩码比例，再从完全有效的深度 patch 中进行随机补充。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一策略的核心目标，是在保证训练难度的同时，尽可能保留「不完美但有信息量」的深度观测，让模型学会在真实、不干净的数据分布下进行推理。&lt;/p&gt;&lt;p&gt;这也正是 LingBot-Depth 在方法层面最具启发性的地方，它开创性地尝试让模型理解噪声背后的物理与视觉规律。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;领先的精度，落地的性能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;LingBot-Depth 让模型在预训练阶段就直面不完整、带噪声的深度世界，会显著增强它对真实三维结构的理解能力，并在多个下游任务中持续受益。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;专业对口：深度补全&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MDM 的核心思想，是在深度存在大量缺失和噪声的情况下，让模型学会利用 RGB 上下文和残余深度信息去「脑补」完整的几何结构。因此，第一个被检验的任务，自然是深度补全（Depth Completion）。&lt;/p&gt;&lt;p&gt;研究团队将基于 MDM 预训练得到的模型 LingBot-Depth，与多种当前主流方法（如 OMNI-DC、PromptDA、PriorDA）进行了正面对比，并设计了两种极具现实意义的评测协议。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;区块级深度缺失：模拟深度相机的「翻车现场」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在第一种协议中，研究团队通过随机抹掉真实深度图中的成块区域，来模拟真实传感器中常见的深度丢失现象；同时，还人为加入高斯噪声和类似 Kinect 的散粒噪声，以还原量化误差、光子噪声等传感器伪影。&lt;/p&gt;&lt;p&gt;结果非常直接：在所有数据集、所有难度级别下，LingBot-Depth 均稳定超越全部对比方法。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbkOAatlRUzRqIOn4vOvQItmibkrlA9FlwfROBzwRsGgltzpREOVqPhXg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530198" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ff6e21d9-032b-4b4c-99d8-d0efce5e872c/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;深度补全结果。（a）在 iBims、NYUv2 和 DIODE 数据集上，采用四个难度级别的区块级深度掩码进行评估。（b）在 ETH3D 数据集上，使用稀疏 SfM 深度输入进行评估。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbVqic5FgDsEGxrYtApwmXSO8LzSaBbB2YtqUH18iaWo9ZcPW9Ilp6uZpQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.35648148148148145" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530199" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/fdc7de7b-3b92-4832-ada9-aec1dde9984d/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;尤其是在「极端」条件下，其 RMSE 相比此前表现最好的 PromptDA 仍有显著下降，说明模型并不是靠「记住干净数据」，而是真正学会了在结构严重缺失、测量高度不可靠的情况下恢复合理的三维形状。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;稀疏 SfM 深度：更复杂的现实问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;第二种协议进一步拉高了难度：输入不再是密集但有缺失的深度图，而是极度稀疏的 SfM / SLAM 点云。在很多真实应用中，当深度相机不可用时，这是获取几何信息的唯一途径。从定性结果来看，它生成的深度边界更加清晰，结构连续性更强，尤其在遮挡严重或观测稀疏的区域，优势尤为明显。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不止补全：单目深度估计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;深度补全之外，蚂蚁灵波研究团队进一步追问一个更本质的问题：如果模型在预训练阶段学会了 RGB 与深度之间的对应关系，这种能力是否能迁移到「只有一张 RGB 图像」的单目深度估计任务中？&lt;/p&gt;&lt;p&gt;为此，他们将 LingBot-Depth 的 RGB 编码器作为预训练主干，替代目前广泛使用的 DINOv2，用于初始化 MoGe 模型。&lt;/p&gt;&lt;p&gt;需要注意的是，在这一设置下，模型在推理阶段完全不再接触深度输入 &amp;mdash;&amp;mdash; 深度分支和解码器被全部移除，考察的是「几何理解是否已内化进编码器」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbPPcgGGyYeP4GXTtN9HVQBvwH3lruwHzAdyCwIcLLadxhU2icw1NKTtQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="1.239814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530200" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/a22de475-a6f9-4b29-8b0e-e9927901ea7e/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;在四个基准数据集上的深度补全方法定性对比。对于每个数据集，依次展示了 RGB 输入、稀疏 / 被掩码的深度输入，以及 OMNI-DC、PromptDA、PriorDA 和 LingBot-Depth 方法的预测结果。可以看到，LingBot-Depth 在深度边界上更加清晰、结构更加完整，尤其是在存在严重遮挡或观测极为稀疏的区域，优势尤为明显。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb3ia6wwkqefThga6NB5gdfTFjMB6VrLZqc4icqdH42w9U74T9XCpK9f2g/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.3861111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530201" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/bd4811b7-ebb2-46bb-a0ce-092e52a2f7ae/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;使用不同预训练主干网络（DINOv2 与 MDM 方法）的 MoGe 单目深度估计结果。在 10 个多样化的基准数据集上，从仿射不变、尺度不变以及视差不变三类评价指标出发，系统评估了深度预测和点云映射的精度表现。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;实验结果给出了肯定答案：&lt;/p&gt;&lt;p&gt;在多个测试数据集上，基于 MDM 预训练的编码器稳定地优于 DINOv2 初始化的模型，并且表现出更好的泛化能力。&lt;/p&gt;&lt;p&gt;这说明，这一方法的确是一种能够&lt;strong&gt;将三维几何知识压缩进视觉表示中的预训练机制&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;总体而言，LingBot-Depth 依托真实复杂场景数据进行训练，使模型能够覆盖更多透明、反光和极端光照等长尾情况，因而具备更稳定的泛化能力；同时，其对深度空洞与噪声的有效修复，显著提升了深度图的完整性与边界质量。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;落地，已就绪&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;性能再强，我们当然也不希望它只停留在实验室里。毕竟，「跑分」从来不是终点 &amp;mdash;&amp;mdash; 只有那些真正走进真实场景、能够稳定支撑工业生产和机器人应用的模型，才是行业值得拥抱的模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三维世界的稳定追踪&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了检验模型实际落地的能力，LingBot-Depth 被进一步接入到在线三维追踪任务 &lt;strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"class":"p1"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"b","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;SpatialTrackerV2&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;中，来看其是否真的能够支撑更复杂、更长链路的几何应用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbwnNQjUuQF35mLNrdrEa2AngsQa9xDr7xia4dibBnAPAbyc2MsNgGjwxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.30277777777777776" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530219" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/aa025f87-f467-49f7-a302-55dce866b8a1/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;相机追踪与场景重建结果。从左到右依次为：RGB 输入图像、原始传感器深度图、模型生成的精细深度图、估计得到的相机轨迹，以及最终重建的场景几何结构。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果首先体现在&lt;strong&gt;相机追踪&lt;/strong&gt;上。在包含大量玻璃与反光表面的室内场景中，替换为 LingBot-Depth 补全后的深度图后，输出的相机轨迹明显更加平滑、连续且稳定。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbSmU8v95mnF7ly1GoP1Hx21oeMajc1ibEs1VkI1QOvvoAXJEUfxWOUnA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5398148148148149" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530220" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/e354a471-44cb-4d68-937f-5aea6170d08b/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;动态三维点追踪结果。上：目标物体上的查询点；中：被持续追踪的三维轨迹（按时间以彩虹色编码）；下：对应的深度图结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;更进一步，&lt;strong&gt;动态三维追踪&lt;/strong&gt;具有十足的可靠性。基于 LingBot-Depth 输出的深度，SpatialTrackerV2 能够恢复出连贯一致的三维运动路径，彩色轨迹在空间中呈现出清晰的结构与稳定的时序关系。&lt;/p&gt;&lt;p&gt;从应用角度来看，LingBot-Depth 已经具备作为基础感知能力直接嵌入现有三维系统的成熟度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;真实灵巧手的实战验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要验证 LingBot-Depth 是否真正具备真实世界可用性，最直接、也最有说服力的方式，便是将其&lt;strong&gt;直接接入真实的灵巧抓取系统进行实机验证&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;验证系统由 Rokae XMate-SR5 机械臂 + X Hand-1 灵巧手 和 Orbbec RGB-D 相机组成，深度图先被转为点云，再用于预测抓取姿态。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbDJsfCCO4iaWnQmV3mIM5NrJ8UGiaPhkQ1DXvkjUsjvoXd44CGvVWMnqw/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.29907407407407405" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530202" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/40bf626a-341c-4add-9d4d-4f1b5ffc38c0/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;抓取实验的定性结果。左图：包含机械臂、灵巧手和深度相机的硬件系统示意。右图：四个目标物体的 RGB 图像、原始传感器深度图，以及 LingBot-Depth 方法生成的精细深度图。对于反光物体（钢杯）和透明物体（玻璃杯、收纳盒），原始深度图严重缺失，而 LingBot-Depth 的方法能够生成完整且几何上准确的深度图。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbYAgBbodfESxXF6uVtMShyBd8dKC8uzbEf3jKfD2XQSF0e4yL1kw2hw/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.4740740740740741" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530203" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/82d37dbb-4128-486e-9d6c-1609c6e49dd0/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;抓取姿态生成与真实世界执行。上图：将预测的抓取姿态以灵巧手形式叠加在由精细深度重建的点云上进行渲染。下图：机器人系统在每个目标物体上成功执行抓取的场景。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果非常直观：在包含透明、反光物体的真实场景中，使用原始传感器深度时，部分物体（如透明收纳盒）因深度大面积缺失完全无法抓取；而使用 LingBot-Depth 补全后的深度，系统能够恢复合理几何结构，&lt;strong&gt;抓取成功率显著提升&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakb1sueVzvPAIUANjINKxbzmmibvFBSQjsnx70WjP9a3TLqcCJ8sPhQ1DQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=18" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530204" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/68863782-c70f-4b93-bba6-10e6dd4ea6ee/640.gif" data-order="3" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 灵巧手抓取反光不锈钢杯&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbHGIqAWMDq0r8Pibpj6F7mAvmXKbb3CE5Yua6gR4SyDBic1qctgib8x1Cw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=19" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530207" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/4e4a8105-6c0c-4e48-a014-d90b38472c84/640.gif" data-order="4" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 灵巧手抓取透明玻璃杯&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在真正的灵巧手交互中，借助 LingBot-Depth 的能力抓起反光的不锈钢杯和完全透明的玻璃杯完全不在话下，轻而易举。&lt;/p&gt;&lt;p&gt;此外， LingBot-Depth 在蚂蚁灵波团队的努力下，&lt;strong&gt;已经完成了模型的轻量化，并完全做好了落地的应用准备&lt;/strong&gt;。&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，LingBot-Depth 模型的&lt;strong&gt;部署非常灵活&lt;/strong&gt;：它无需更换现有的 RGB-D 或 3D 相机硬件，就能作为算法增强模块直接嵌入现有系统，大幅降低升级成本和工程门槛。&lt;/p&gt;&lt;p&gt;同时，模型&lt;strong&gt;完全开源、可复现&lt;/strong&gt;，便于研究者和产业团队快速进行验证、二次训练和工程化集成，加速从实验室到真实场景的落地应用。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身智能具备识别复杂光照环境，甚至精准捕捉透明，反光材质物体的能力，就已标志着通用级具身智能落地的一大障碍的突破。&lt;/p&gt;&lt;p&gt;归根结底，具身智能的发展不仅是算法迭代的赛跑，更是对行业认知和落地能力的考验。&lt;/p&gt;&lt;p&gt;LingBot-Depth 展示了一种思路的升级：面对真实世界的复杂性，在硬件受限的情况下，如何运用算法与数据、模型与物理认知的深度融合，来提升对真实世界的感知能力，是未来通用具身智能的核心方向。&lt;/p&gt;&lt;p&gt;蚂蚁灵波将 LingBot-Depth&lt;strong&gt; 完全开源&lt;/strong&gt;，用户可以通过开源仓库获取模型权重、推理代码、评测脚本与使用文档，快速上手实验与验证；如需面向具体相机型号或机器人平台进行工程集成和性能调优，也可以对接官方的合作与技术支持渠道。&lt;/p&gt;&lt;p&gt;开放与可落地的策略，将深刻影响人工智能向现实价值转化的速度和格局。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>性能比肩Gemini 3 Pro！昨晚，阿里千问最强模型来了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:23:08 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1 月 26 日深夜，阿里千问旗舰推理模型 Qwen3-Max-Thinking 正式上线。&lt;/p&gt;&lt;p&gt;该模型在科学知识（GPQA Diamond）、数学推理（IMO-AnswerBench）、代码编程（LiveCodeBench）等多项权威基准测试中刷新纪录，其综合性能已可对标 GPT-5.2 与 Gemini 3 Pro，成为目前最接近国际顶尖水平的国产大模型之一。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="8294" data-imgfileid="503530211" data-ratio="0.47962962962962963" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbsblzhhT5icU0h5dMjkicM59iafJ9deZnt2vz3ibRj5J2uhZYE1uglj4icUQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-width="17277" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d741854d-6dc9-4feb-9515-92b568791c85/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;下表为更全面的评估分数：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="1051" data-imgfileid="503530212" data-ratio="0.9731481481481481" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbJYqRVhBVWbRpTicLjWMxJo8sGjwdia8qOU3UBmDbprB0SeVGbVJW1ibxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" data-width="1080" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/dbfaac36-017c-472f-b8d9-e49eae01ba44/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;据了解，Qwen3-Max-Thinking 总参数量超万亿（1T），预训练数据量高达 36T Tokens，是阿里目前规模最大、能力最强的推理模型。&lt;/p&gt;&lt;p&gt;此前，预览版 Qwen3-Max-Thinking 已展现出不俗实力。基于这一基础，通义团队进一步扩大了强化学习后训练规模，对模型进行了系统性优化，使正式版在多项核心能力上实现整体跃升。&lt;/p&gt;&lt;p&gt;在覆盖事实知识、复杂推理、指令遵循、人类偏好对齐以及 Agent 能力等 19 项主流评测基准中，Qwen3-Max-Thinking 取得多项领先成绩，刷新了多项最佳纪录，其综合表现已进入与 GPT-5.2-Thinking-xhigh、Claude Opus 4.5、Gemini 3 Pro 同一竞争梯队。&lt;/p&gt;&lt;p&gt;真实表现如何，我们上手体验了一下。&lt;/p&gt;&lt;p&gt;我们输入提示：帮我做一个技能五子棋的游戏网页，要求是在普通的五子棋规则上，玩家可以使用技能。直接给我个 html 文件。&lt;/p&gt;&lt;p&gt;一会儿工夫，Qwen3-Max-Thinking 就嗖嗖甩出 1000 多行代码，把一个可交互、能上手就玩的五子棋直接写完整了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbxqias7eOPytS1bePicyR4DCA7LeC6Jj6dzAFHyibhqXBQM05nltHUsh9A/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-ratio="1.0064874884151993" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503530213" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4dac559d-e342-4ce1-a2ee-f0e0f8546764/640.gif" data-order="0" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;下一项测试，我们让 Qwen3-Max-Thinking 生成一个跳一跳游戏。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbShoQzn3UMbUuq6zUtIQ8ebsFjadaRdSezicNqx84g3GkAUBrY7OWr3w/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="1.284274193548387" data-s="300,640" data-type="gif" data-w="496" type="block" data-imgfileid="503530214" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/32e17d6e-5e72-4582-86ea-7bac2ade6df5/640.gif" data-order="1" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;请用纯 HTML + CSS + 原生 JavaScript 写一个可在浏览器直接打开的《跳一跳》小游戏（不要依赖任何外部库）。要求：画面：简洁 2D 即可（canvas 或 DOM 都行）；操作：按住蓄力、松开起跳（按住时间决定跳跃距离）；规则：从一个平台跳到下一个平台，落空则结束；生成：平台位置随机，但保证可达（不要生成必死局）；计分：落在平台上加分，连跳加成可选；体验：有起跳动画、落地判定、失败提示、重新开始按钮；代码：完整可运行，放在一个 HTML 文件里，注释清晰。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这个游戏最难的地方，就在于按住鼠标的时间既是操作，也是赌注：短了跳不过去，长了直接飞过头，容错窗口小到离谱。第一跳很容易失误，然后就 Game Over。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbK6EhLEG6tzrag50UcsicOYANxicA0nibic7ZInr7yibcyvjicew65ibwLMQDg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-ratio="1.3165618448637317" data-s="300,640" data-type="gif" data-w="477" type="block" data-imgfileid="503530215" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3368d91b-1a3a-40f2-ba5f-f6b1c16fcab4/640.gif" data-order="2" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;体验地址：https://chat.qwen.ai/&lt;/p&gt;&lt;p&gt;&lt;strong&gt;测试时扩展的重新定义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;推动 Qwen3-Max-Thinking 的核心创新在于其对传统推理方式的突破。与大多数模型按线性方式逐 token 生成不同，Qwen3 引入了一种由测试时扩展（Test-time scaling）驱动的 Heavy Mode（重推理模式）。&lt;/p&gt;&lt;p&gt;通俗来说，这一技术让模型能够用更多算力换取更高智能水平。但它并非简单的 best-of-N 采样方式，例如一次生成 100 个答案再从中选出最优结果 &amp;mdash;&amp;mdash; 而是采用了一种经验累积的多轮推理策略。&lt;/p&gt;&lt;p&gt;这种方法更接近人类的解题过程。当模型面对复杂问题时，它不会直接给出一次性猜测，而是进入反复自我反思与迭代推理。通过一种专有的 take-experience 机制，模型能够从此前的推理步骤中提炼有效经验，从而实现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;识别死胡同：在无需完整走完错误推理路径的情况下，判断某条推理思路正在失效；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;聚焦算力：将计算资源重新分配到尚未解决的不确定点，而不是反复推导已经得到的结论。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种机制带来了实实在在的效率提升。通过避免冗余推理，模型可以在同样的上下文窗口中整合更丰富的历史信息。千问团队表示，该方法在不显著增加 token 成本的前提下，实现了性能的大幅跃升：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GPQA（博士级科学问题）：得分从 90.3 提升至 92.8；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LiveCodeBench v6：成绩从 88.0 提升至 91.4。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;自适应工具调用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说推理能力决定了模型会不会想，那么工具调用能力决定的，是模型能不能真正把事做成。在 Qwen3-Max-Thinking 中，通义团队不再将推理与工具使用视为两个割裂的阶段，而是将工具能力内生进思考过程本身，构建起一种边思考、边行动的原生 Agent 式模型框架，让大模型从静态的文本推理，迈向可执行、可验证的复杂任务处理。&lt;/p&gt;&lt;p&gt;在完成基础的工具使用微调后，通义团队进一步在大量多样化任务上，引入基于规则奖励与模型奖励的联合强化学习训练，使模型学会何时调用工具、如何结合工具展开推理，而不是机械执行指令。由此，Qwen3-Max-Thinking 获得了更具策略性的工具协同思考能力。&lt;/p&gt;&lt;p&gt;这一自适应工具调用能力已在 QwenChat 中完整落地：模型可自主调度搜索、个性化记忆与代码解释器等核心 Agent 工具，在一次交互中完成信息获取、计算推演与结论生成，回答更贴近专业人士的工作方式，也显著降低了模型幻觉，为解决真实世界中的复杂问题奠定基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;截至 2026 年 1 月，阿里通义千问（Qwen）系列模型在 Hugging Face 平台上的累计下载量超过了 10 亿次，这一数据使得 Qwen 成为了 Hugging Face 上最受欢迎、下载量最高的开源 AI 模型系列之一。&lt;/p&gt;&lt;p&gt;Qwen3-Max-Thinking 的推出代表着 2026 年人工智能市场的成熟。它将讨论的焦点从谁拥有最智能的聊天机器人转移到谁拥有功能最强大的智能体。通过将高效率推理能力与自适应、自主的工具调用机制相结合，Qwen 已经牢牢确立了自己在企业级 AI 竞争格局中的领先地位。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：https://venturebeat.com/technology/qwen3-max-thinking-beats-gemini-3-pro-and-gpt-5-2-on-humanitys-last-exam&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>跨境电商版Sora发布：全球首个AI原生电商视频Multi-Agent来了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:17:46 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Youli&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;section&gt;你的下一个视频团队，不一定非得是人。&lt;/section&gt;&lt;/blockquote&gt;&lt;p&gt;做电商的朋友，一定对这样的时刻不陌生：前期找团队、磨脚本、&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;拍视频&lt;/span&gt;，筹备了半个月，好不容易在亚马逊或 TikTok 后台上新一款潜力爆款。谁想到，半夜在 TikTok 刷到竞品的一条爆火视频，作为行家，你一眼就看出这是泼天流量，你也想接住。&lt;/p&gt;&lt;p&gt;可粗略一算： 找模特、找摄影师、约场地、等剪辑，整套流程走完费用不低，且制作周期没半个月下不来&amp;hellip;&amp;hellip; 等把视频做出来，流量窗口早就关了，爆款也成了库存。&lt;/p&gt;&lt;p&gt;这时候你一定幻想过：&lt;strong&gt;如果有一个工具，能跳过所有拍摄流程，直接生成一条能出单的视频就好了。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;你可能想到了 Sora。 还记得 Sora 刚发布时，全行业都在狂欢，以为这一时刻终于来了。但后来却被现实狠狠「打脸」：Sora 懂物理世界，懂光影，能生成惊艳画面，可它不懂生意，不知道什么是「点击率」，更不知道什么是「卖点」。而且动辄几美元一秒的成本，让量产成了奢望。&lt;/p&gt;&lt;p&gt;但现在都 2026 年了，技术狂奔的当下，Sora 没能做到的事情，&lt;strong&gt;一个由营赛 AI 发布的名为 inSai Hilight 的中国产品做到了&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;不需要任何拍摄素材，不需要复杂提示词，只「扔」进去一个商品链接或上传一张商品图，系统就会进入类似 DeepSeek 的「慢思考」模式， 20-30 分钟后，一条逻辑严密、商品 100% 还原、且符合 TikTok 爆款节奏的营销视频诞生了。&lt;/p&gt;&lt;p&gt;这就是 Hilight 历时 9 个月打造的&lt;strong&gt;全球首个 AI 原生电商视频 Multi-Agent&amp;mdash;&amp;mdash;Hilight AI，也可以说是「念过营销学的电商版 Sora」&lt;/strong&gt;。传统视频 AI 虽然生成素材时长短，但素材人物和产品都很难完全一致，人工优化的时间可能比自己剪的时间还要长，而 HIlight 是真正实现了商用级的稳定素材产出，是 AI 营销领域一次新的革命。它将 Sora 级别的视觉能力和 4A 公司般的营销大脑完美结合，直接重新定义电商营销视频的生产方式， 营销视频不再是拼手速的「快餐」，而是拼逻辑的「推理」。&lt;/p&gt;&lt;p&gt;即便是刚刚入行的新人，也能以&lt;strong&gt;传统拍摄 1/20 的成本、10 倍的效率，指挥一支 AI 团队，生产出全球通用的爆款视频&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;营销视频的「DeepSeek 时刻」，已然到来。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「万亿市场」被生产方式拖后腿？电商营销视频「叙事」该变了&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;其实，前面「你」的感受，并不是个例，而是整个行业的集体阵痛。&lt;/p&gt;&lt;p&gt;据国家统计局最新数据显示，2025 年全国实物商品网上零售额增长 5.2%，达 13.09 万亿元。整体来看，电商行业延续了稳健增长，并在结构优化、技术创新和开放合作方面取得了显著进展。而在跨境电商上，2024 年，仅中国跨境电商的出口额就达到 2.15 万亿元，连续九年增长，2025 年以来，中国卖家在亚马逊欧美市场的销售额增长超过 15%，在新兴市场的增长速度甚至更快，超过 30%。&lt;/p&gt;&lt;p&gt;电商市场规模在扩大的同时，竞争的逻辑也已经发生变化。一个共识是，&lt;strong&gt;流量的重心正在从图文转向视频&lt;/strong&gt;。有数据显示，视频内容的转化率比图片高出 5 到 10 个百分点，相对提升接近 80%~100%。对平台而言，这是既定方向，对卖家而言，这却是一道越来越高的门槛 &amp;mdash;&amp;mdash; 视频的重要性被反复强调，但制作方式还停留在「原始时代」，简单来说就是慢、贵、不可规模化。&lt;/p&gt;&lt;p&gt;慢：制作周期长，写脚本、找模特、拍摄、剪辑，周期短则几天，长则几周甚至几个月，错过流量窗口；如果是跨境电商，还存在本地化难，花时间翻译语言等难题，且即便语言能翻译，但创意、文化、审美等难以恰当匹配，易翻车。&lt;/p&gt;&lt;p&gt;贵：成本高，模特、布景、拍摄、剪辑等各环节都需要高昂成本，单条视频成本从几百到几千美元不等。&lt;/p&gt;&lt;p&gt;不可规模化：素材少，只能反复混剪，内容同质化严重，且也无法规模化，账号多、平台多、内容更新频率高，传统制作方式不可持续&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;结果就是，&lt;strong&gt;产品已就位，内容却永远「慢半拍」&lt;/strong&gt;。社交平台上，常常看到国内外电商卖家因为营销视频跟不上的吐槽声或求助贴，对于他们来说，即便认识到行业发展趋势，却也只能无奈困在这种「清醒的焦虑」中。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwttSVTuIXxT52WaooibUyt0hokfPcicjK0XuT7MFN6xWibJhUwIFoOFHMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5527777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530026" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/5401a5cd-bafb-47f8-9d19-27f907b08d96/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwricwTvfQkpsv6FpibrI8lbL8K8T5bZfLWtDxymMzx6RNibySl8iaqunxtA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6425925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530027" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c131dbd2-449d-4006-927b-97d1af40026f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwg7EY4o4gEZrx9hMlKsusicfT2eO1eE35GbPbdbSygOdiczQ0P2nELWWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5203703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530029" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/3a8ec2c3-eb48-4636-bed1-40d968c02512/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;诚然，市面上的 AI 视频生成工具并不稀缺，很多工具已经能够帮助卖家实现提升效率的目的，但这些工具也面临着同质化、初级、需要二次加工等多种问题，效果也忽高忽低。要么是「低智的套壳 AI」，不仅生成的视频一眼假，甚至因为大量使用通用素材库，导致做出来的视频在 TikTok 上和几百个竞品撞车&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;而对于卖家来说，真正的需求从来都不是「能不能生成视频」，而是能不能稳定、低成本、规模化地产出「可投放的营销视频」。所以，「再来一个 AI 视频工具」也解决不了问题。&lt;/p&gt;&lt;p&gt;Hilight 早已洞察到这一点，并选择对准一个更底层问题：&lt;strong&gt;在电商场景里，营销视频短缺的根源不在于生成速度，而在于生产方式&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;因此，Hilight 推出的全球首个 AI 原生电商视频 Multi-Agent，它不是简单的混剪合成工具，而是一个从脚本到画面、从素材到成片的纯 AI 原创营销内容多智能平台，让卖家不再被迫走向营销视频的低质量混剪，可以让每一款商品都拥有量身定制的原创营销视频。而且生成视频价格低至 3 元起，最高也不过十几元，相较传统方式，成本大幅降低。&lt;/p&gt;&lt;p&gt;而这也正是 Hilight AI 不同于 Sora、Keling 等视频生成工具的最大不同，&lt;strong&gt;它不是在简单构建一个工具，而是在重新定义下一代营销视频解决方案&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;那么，问题来了，凭什么是 Hilight 首先让电商营销视频进入「Sora 时刻」？它的底气是什么？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;敢信？每一个爆款视频都是十几个 Agent「吵」出来的&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/55d76d3d-4c45-4465-83f4-347c58432f50/1769490693898.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;这是 Hight AI 在视频生成过程中的流程展示，可以看到，在一个视频生成的背后是多个智能体之间相互配合与协同的结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;前面提到，当下市面上的 AI 视频工具都在「快」上卷生卷死，仿佛谁能「秒级出片」谁就赢了，但电商视频的叙事逻辑并非如此，问题的关键不在各类更快的「快餐式」素材片段，而是「如何持续产出」高质量的即用式营销视频。&lt;/p&gt;&lt;p&gt;基于此，Hilight 走了一条「反直觉」的道路 &amp;mdash;&amp;mdash; &lt;strong&gt;决定对现有电商视频制作流程进行革命性颠覆，底层逻辑是引入与 DeepSeek 异曲同工的「慢思考」逻辑，打造全球首个 Multi-Agent（多智能体） 协同营销模型。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;简单来说，Hilight AI 不再是传统意义上由单模型组成的「视频生成工具」，而是一支由 10 多个 Agent 组成的「视频制作团队」。在视频生成过程中，这些 Agent 之间互相配合，通过模拟真人导演的策划与监制逻辑，来生成远超同类竞品的高质量视频 。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwZ0aqAJPv4KNCicWcymKmARiatHPceticiazH3icA1BcgdRFclVa5z3ygXWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.1022727272727273" data-s="300,640" data-type="png" data-w="792" type="block" data-imgfileid="503530067" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a6769a4b-34a4-4155-9913-3675459dccb2/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;换句话说，&lt;strong&gt;每一个生成视频都是这十几个 Agent「吵」出来的结果。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;怎么理解？&lt;/p&gt;&lt;p&gt;具体来看，Hilight 的这套多智能体架构共分为三个层级：&lt;strong&gt;理解与洞察层、创意与结构层、执行与成片层&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在视频生成过程中，理解与洞察层的 4 个 Agent 接收到需求后，负责理解用户输入的需求和素材，就像资深策划一样拆解需求、洞察「卖点」和受众，从而将模糊需求变成精准的营销指令。&lt;/p&gt;&lt;p&gt;接着，创意与结构层的 4 个 Agent 则负责「努力干活」，创意 Agent 写脚本，剧本策划 Agent 会拆成详细带有时间轴的分镜，紧接着素材匹配 Agent 进行素材匹配，以及定向优化 / 修复，目的就是将「好想法」变成能被执行的视频结构，将其交付给下一层级执行。&lt;/p&gt;&lt;p&gt;之后在执行与成片层，剪辑执行 Agent 和成片生成 Agent 接手，负责把所有的素材进行按轨道、时间排列成片。最后，质检复盘 Agent 还会对成片质量进行最终检验，以保证最终输出一个完整的、可投放的视频资产。&lt;/p&gt;&lt;p&gt;Hilight AI 研发团队介绍，这样的架构设计，一是保证每一个 Agent 节点都具备独立判断能力，以实现有效决策。&lt;/p&gt;&lt;p&gt;二是 Hilight 定义了一套严苛的评测协商机制，下游的 Agent 对上游 Agent 的输出结果有自己的判断标准，如果不符合则「打回重做」，以保证每一个节点不合格是可以局部重算，而不必整个流程重新推翻。&lt;/p&gt;&lt;p&gt;比如当剧本策划 Agent 察觉到脚本偏离卖点，会让创意 Agent 再重新做进行创作，剪辑执行 Agent 可以根据实际的成片效果，重新安排整个视频的节奏&amp;hellip;&amp;hellip; 这种内部的「对抗与协作」，有点像真实视频团队中策划、导演、剪辑师彼此之间的极限拉扯，为的是保证输出视频的逻辑严密性、节奏对味。&lt;/p&gt;&lt;p&gt;三是自我进化能力，通过爆款数据的回流以及创意范式的自动更新，系统能够快速适配新的平台规则来帮助用户实现长期营销增长和整体视频成片效果的增强。&lt;/p&gt;&lt;p&gt;所以，在用户一键成片的背后，是这 10 多个 Agent 组成的「团队」在通力协作，保证生成的视频能够直接拿去投放，但还不止于此。&lt;/p&gt;&lt;p&gt;相较于其他 AI 视频生成工具，&lt;strong&gt;Hilight AI 生成的视频具有非常高的跨帧一致性，而这也正是其最大的「杀手锏」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;对于电商营销视频来说，「一致性」往往是生死线，因为常用的 AI 视频工具最大的问题就是不一致，比如露营灯在第一秒还是圆的，第三秒就变成了方的；数字人口型对不上，或者手持产品时手指穿模&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;为了攻克这一难题，Hilight 可是下了大功夫的。一方面，Hilight AI 接入的是目前行业最顶尖的基座模型。基于此，Hilight 还针对电商场景构建自研能力，重点突破跨帧一致性、口唇同步，以及商品 / 服装上身（Try-On）等模型技术。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwiconJ6RDUPAh61SFLh92SFIFicJo7lDVWNrr3oo8QJMUl2DRiaV8JCwRg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.5518518518518518" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503530171" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/49c308a3-199f-426b-8d7a-5ba6fe0c13a5/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;首先，&lt;strong&gt;剧本规划启动前，Hilight 就开始对商品进行「深度解码」&lt;/strong&gt;，不是单纯的识别商品名称，而是依托先进的商品知识图谱，对商品属性进行「抽丝剥茧」，不管是西装的亚麻材质，还是羽绒服的版型长度，都能被精准捕捉，并进行颗粒度极细的结构化拆解与梳理，继而构建成全维度的信息基座。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这就像为后续视频创作配备了「细节质检员」&lt;/strong&gt;，无论镜头如何切换、场景怎样跳跃，都能主动校验商品特征，及时修正偏差，从源头避免因信息缺失导致的细节混乱，为跨帧一致性打下坚实基础。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwfaGves4vPhBFQM4Ez2AMN20HjdyB7r47ZPBEEQ2PoHCc4PzPjPluDA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5583333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530069" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/3842c0f4-f4cb-4e24-98eb-b5a57f8df7ac/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;而在视频生成过程中，&lt;strong&gt;全程基于「图生视频」进行，先生成一张高质量首帧图「锁死」商品特征，然后基于视频模型驱动首帧图生成视频&lt;/strong&gt;，以实现对视频质量的精准控制。&lt;/p&gt;&lt;p&gt;此外，&lt;strong&gt;系统还有相应的检测和修复机制&lt;/strong&gt;，智能自检 Agent 会在生成每一个视频后自动开展双重校验，包括实体一致性校验，对比视频中商品与主图的核心属性（颜色、版型、材质、关键组件），确保无明显偏差；物理逻辑校验，排查商品与场景、人物的交互是否存在穿模、不合理遮挡、不符合事实，如人物手持商品时是否出现「手穿进商品内部」的穿模问题等，避免出现逻辑矛盾。如果有则立即启动视频编辑模型进行定点编辑修复。&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f6881359-5bcd-4752-b69a-3185533d1868/1769490791177.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;一款印花连衣裙，数字人试穿时，不仅完美还原了连衣裙的物理质感与微小褶皱，展示动作也如真人试穿般自然，即便多场景切换，人物与商品的视觉一致性依然保持高度一致性。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;即便这样还不够，&lt;strong&gt;Hilight AI 还配套了「人机交互机制」这最后一道防线&lt;/strong&gt;，对于「漏网之鱼」，用户也可以选择精调成片功能，对脚本、素材、镜头等多个环节进行把控，从而保证输出的成片里，商品能够还原得非常逼真，从而实现构思的创意，并且数字分身的口型等都能保证一致性。&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/fdc1ca17-0863-466b-bbc2-9de0867905cb/1769490816690.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;一款家居拖鞋的生成视频，数字人口型与语音实现了帧级同步，肢体动作更是流畅自然，完全没有机械僵硬感，试穿时，数字人体态也自然舒展，当镜头拉近，甚至能清晰看到拖鞋表面细腻的绒毛纹理。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;根据视频生成模型综合评测基准 VBench 权威测评结果显示，当前 Hilight AI 在跨帧一致性的表现，已领先于当前一众 AI 视频生成工具。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwbBPJWM5hQC6ia9Ify7lFry01ib8FicsIe9CkpK91qJYcF45Ac9Pz5O5sg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-ratio="0.562037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503530070" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/6a27d7be-5d61-4503-945d-1d3d03ad384f/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不仅如此，Hilight AI 还支持生成各大平台爆款风格的视频，这是因为 Hilight AI 引入多维知识库（RAG）驱动创意，将当下爆款视频要素、平台热门 BGM，以及用户历史创作的商品静态资产进行整合，堪称「爆款制造机」。&lt;/p&gt;&lt;p&gt;在视频生成之前，AI 会先会去通过多维知识库（RAG）技术查资料，确保新生成视频在内容主题与视觉风格上保持高度一致性，避免 AI 生成中的随机漂移。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwmboahngiatFP9UJjqNO8UMOzUWdLeCT1iaupiay6ebSfmKTEpzvx7kEMQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530071" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/24b9fab7-6c5a-443d-9353-0ca0d527e637/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;实测：当「跨帧一致性」最强 AI 开始接管你的视频制作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;说了这么多，Hilight AI 到底是如何实现远超其他 AI 视频工具能力的？我们决定亲自上手实测来感受一下。&lt;/p&gt;&lt;p&gt;Hilight AI 的官网很简洁，没有复杂的参数，也没有堆砌功能，创作入口主要集中在三个模块：智能成片、数字分身和创意工坊。核心功能则包括一键成片、100% AI 原创 (Zero-Shot)、商品 / 人物跨帧一致性保障、商品与数字人深度融合等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwxXZ3ldMrEWhRHR52Q6XPBg8kFiaibageNvyLlpbiagxVc4W14THB1elSg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530072" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/d5ccf813-254e-45ce-8e2b-e47e449333a7/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;我们直接选择智能成片，也就是一键成片，以亚马逊官网上的一款耳机产品为测试对象。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHw2AndgsvKgTeQa3AYRXFQwG0XcFzLiazyBWhKiazpv3MJSqYJsAQwicyCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.8842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530073" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/9a3a37fc-1278-465f-8ec3-43fb2f7044b2/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;整个过程中，不需要提前准备任何视频素材，只需要提供一个商品链接或商品图片，或是需求描述、脚本等。在这里，我们直接将耳机的链接贴在需求栏，点击智能解析，系统便自动开始工作。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHw42I7H5WBZVOUGoAZFI9icAMQjsvf9O6pT7Ly88qibuE0yKQpS9waVmicw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.5083333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530074" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/500fdd22-96c6-41a7-bdcc-4b7956316efa/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在需求解析完成后，系统并不会立刻生成视频，而是先通过一个「灵活表单」，来自动帮助用户梳理产品卖点、目标受众、促销优惠等营销关键信息，以便让生成的视频与产品本身更贴合。&lt;/p&gt;&lt;p&gt;这一步看似简单，却恰恰是大多数 AI 视频工具直接跳过的环节 &amp;mdash;&amp;mdash; 营销结构。&lt;/p&gt;&lt;p&gt;这些信息确认之后，用户就不需要再进行任何操作了，系统会自动完成从脚本生成、分镜规划、剪辑渲染、平台适配等流程，最终给出成片输出。在等待的过程中，页面也会实时展示当前进度，让用户看到视频是如何一步步被制作出来的。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwc9p36licGFxIypQicx0Y4QTvpey6AOGEJCgAZjuT4pe3s02eib8usGCVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530075" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/b41b889f-8b87-41a2-9ac8-79c41e13365b/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;整个过程通常需要花费 20&amp;ndash;30 分钟，下面是整个操作流程以及生成的最终视频展示。&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/bf2fbe71-7660-42ac-a675-25099bfa1df7/1769490869369.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e86521d9-f784-433d-8bf1-e3f8c935a6ef/1769490881638.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;从展示的操作步骤中可以看出，整个过程的操作都是非常「丝滑」，是 &lt;strong&gt;AI 电商视频领域首个真正实现 100% 纯 AI 原创的视频生成工具&lt;/strong&gt;。全部流程都由 AI 生成，无需用户插手，并在整个过程中解决素材版权和重复度问题。&lt;/p&gt;&lt;p&gt;而且，不同于当前一些 AI 生成工具仅生成素材，商家还需进行后期剪辑，Hilight AI 生成的视频画面衔接自然，商品在不同镜头中保持高度一致，没有走样，也没有常见的「跨帧漂移」；人物动作流畅，镜头切换时没有突兀的断裂感。更重要的是，这是一条完整的视频，而不是若干素材片段的组合，生成的视频完全可以直接拿去平台投放了。&lt;/p&gt;&lt;p&gt;来看一个反面案例：&lt;a href="https://mp.weixin.qq.com/s/-g6hRrvr1YyMiuZvwOLQcw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6e9836e3-4836-46a1-bc28-6b85549cb745/1769490902718.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;对于同一款耳机，用其他工具生成的视频中，当人物将耳机戴到头上时，耳机直接变成两个，不符合逻辑，甚至商品出现走样现象，难以直接用，还需要后期剪辑处理。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;另外值得注意的是，如果过程中用户对剧本某些内容或是分镜头不太满意，没关系，可以通过「精调模式」进行局部调整，以生成更符合用户需求的视频内容。不仅如此，用户还可以在需求中说明想要投放的平台，其生成的视频也会满足用户需求。&lt;/p&gt;&lt;p&gt;从上面的视频中可以看到，Hilight AI 生产的视频中有「人物形象」出现，而前面其实我们也没有准备相应的素材，其实这是 Hilight AI 的另一个功能 &amp;mdash;&amp;mdash; &lt;strong&gt;数字分身&lt;/strong&gt;。用户只需要提供一些真人拍摄视频，就可以通过模型训练生成「专属」的数字人形象，同时还能克隆对应的音色。&lt;/p&gt;&lt;p&gt;这样一来，卖家就拥有了一个定制化的、稳定、可复用的数字「代言人」，甚至可以做到不同商品拥有不同的专属「分身」。&lt;/p&gt;&lt;p&gt;除此之外，Hilight AI 还提供了一个被称为「创意工坊」的素材工具集，多模态大模型加持快速生成好用的前贴、素材片段、商品图等全营销物料&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;其实几轮实测下来，一个差异已经非常明显：相对于其他 AI 视频生成工具还停留在只能生成十几秒的素材片段，能生成视频但不懂商品转化等阶段，Hilight AI 已经能够稳定生成分钟级视频长度，并且可以前脚生成，后脚直接上线平台进行投放。质量上，敢「叫板」实拍效果，数量上也能够实现连续生产，支撑账号矩阵和高频更新。更重要的是，在成本上，相比人工近乎 0 成本。&lt;/p&gt;&lt;p&gt;最后，从 Hilight AI 的实践成果来看，&lt;strong&gt;这并不是一次单点能力的突破，而是一种生产方式的重构&lt;/strong&gt;。当 AI 开始像一个团队一样协作、博弈、自我修正时，电商营销视频也似乎第一次具备了工业化生产的条件&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;值得一提的是，这款产品是 Hilight 历时 9 个月迭代 13 个版本打磨出来的，可以说是匠心之作。&lt;/p&gt;&lt;p&gt;如今，&lt;strong&gt;Hilight AI 正式开启持续一周的公测&lt;/strong&gt;，如果你也感兴趣，想要体验一下一支 10 多个 Agent 视频制作团队为你「干活」的感受，可以立刻行动，一起来重新定义电商营销视频的生产方式！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;参与方式&lt;/strong&gt;：打开 Hilight 官网：https://www.hi-light.ai/，输入邀请码参与报名！&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;邀请码链接&lt;/strong&gt;&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;：https://www.hi-light.ai/invitation?batch=20260125155832&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;福利&lt;/strong&gt;：首批公测用户将获赠 8888 星光点（约等于 16 个时长 15 秒的视频） ，邀请好友再得 1888 星光点！&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，微软全新一代自研AI芯片Maia 200问世</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:07:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-path-to-node="5" data-pm-slice="0 0 []"&gt;一觉醒来，我们看到了微软自研 AI 芯片的最新进展。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;微软原定于 2025 年发布的下一代 AI 芯片 Maia 200，终于在今天问世！&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwTX0YpI7IOgXBbiaZoKJWPQiculvJYFNRqqkMmGH1qpxMakF1iaAyDZkYg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.9259259259259259" data-type="png" data-w="1080" data-width="1270" data-height="1176" data-imgfileid="503530173" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/53472120-c149-4980-8fa6-a5dadaa08921/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="7"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 微软 CEO Satya Nadella&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="8"&gt;根据微软官方介绍，Maia 200 作为一款强大的 AI 推理加速器，旨在显著改善 AI token 生成的经济性。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;Maia 200 基于台积电的 3 纳米工艺打造，配备原生 FP8/FP4 张量核心、重新设计的内存系统，拥有 216GB HBM3e 内存、7TB/s 带宽以及 272MB 片上 SRAM，并配有数据传输引擎，从而能够保证大规模模型高效、快速地进行数据流动。&lt;/p&gt;&lt;p data-path-to-node="10"&gt;这些使得 Maia 200 成为任何超级计算平台中表现最强的第一方硅片，其 FP4 性能是第三代 Amazon Trainium 的三倍，FP8 性能超越了谷歌第七代 TPU。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;与此同时，Maia 200 还是微软迄今为止最高效的推理系统，每美元性能比该公司当前集群中的最新一代硬件提升了 30%。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwJ6EwbJ6gprcyHBXaaA1MDXVwa4kkHyWkAIicePTrjOC3jcmSvtPG9oQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-type="png" data-w="1080" data-width="2000" data-height="1125" data-imgfileid="503530174" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/5d06b213-0e53-4121-be68-3d23b484ef67/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="12"&gt;Maia 200 是微软异构 AI 基础设施的重要组成部分，将为包括 OpenAI 最新 GPT-5.2 在内的多个大模型提供支持，为 Microsoft Foundry 和 Microsoft 365 Copilot 带来更高的性价比优势。&lt;/p&gt;&lt;p data-path-to-node="13"&gt;微软超级智能团队将利用 Maia 200 进行合成数据生成和强化学习，以提升下一代自研模型的性能。在合成数据流水线应用场景中，Maia 200 的独特设计有助于加速高质量、特定领域数据的生成与筛选，从而为后续的模型训练提供更及时、更具针对性的信号。&lt;/p&gt;&lt;p data-path-to-node="14"&gt;Maia 200 已部署在爱荷华州德梅因附近的美国中部数据中心区域，接下来将部署在亚利桑那州菲尼克斯附近的美国西部 3 区域，未来还将扩展至更多地区。&lt;/p&gt;&lt;p data-path-to-node="15"&gt;Maia 200 与 Azure 实现了无缝集成。目前，微软正在开放 Maia SDK 的预览，该 SDK 提供了一整套用于构建和优化 Maia 200 模型的工具，涵盖了 PyTorch 集成、Triton 编译器、优化内核库以及对 Maia 底层编程语言的访问权限。这既能让开发者在需要时进行精细化控制，又能实现模型在不同异构硬件加速器之间的轻松迁移。&lt;/p&gt;&lt;p data-path-to-node="16"&gt;对于微软这波突如其来的「秀肌肉」，社区反响热烈。&lt;/p&gt;&lt;p data-path-to-node="17"&gt;有网友送出点赞，并强调了微软在基础设施层面的统治力。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwFhs3DOHpxxCXUTS5SZCicu3E80hnIQYibPuFAobL8CQEFa8bt7XMnC6w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.9796296296296296" data-type="png" data-w="1080" data-width="1302" data-height="1275" data-imgfileid="503530175" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/09513330-cc2b-4fe5-879e-4605afb2bccf/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="18"&gt;也有人关心上面是否能安装最近爆火的 Clawdbot。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwX7SIXBEGFyJnDPKxs3QzquC34ejLsMZlED9az8Xiapn5uUiadmsJvp5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.15358361774744028" data-type="png" data-w="879" data-width="879" data-height="135" data-imgfileid="503530176" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/d0c46c4f-14ac-4e28-a0e2-56dec5162d94/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="19"&gt;也不乏灵魂拷问/调侃。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwqlVIJ5ZkymeteMsuOu7XWUlSXgKIILEAzBQKic9O0ZCBvLYDGiaCia2cw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.1425925925925926" data-type="png" data-w="1080" data-width="1323" data-height="189" data-imgfileid="503530177" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/d6779248-a587-482c-972e-8d557f8eef20/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="20"&gt;&lt;strong&gt;专为 AI 推理打造&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="21"&gt;Maia 200 芯片采用台积电最先进的 3 纳米工艺制造，单颗芯片包含超过 1400 亿个晶体管。它专门针对大规模 AI 工作负载进行了定制，同时兼顾了极高的能效比。因此，无论是在性能还是成本效益方面，Maia 200 均表现卓越。&lt;/p&gt;&lt;p data-path-to-node="22"&gt;Maia 200 专为使用低精度计算的最新模型设计，在 750W 的 SoC 热设计功耗（TDP）范围内，单颗芯片可以提供超过 10 PetaFLOPS 的 FP4 性能和超过 5 PetaFLOPS 的 FP8 性能。&lt;/p&gt;&lt;p data-path-to-node="23"&gt;从实际应用来看，Maia 200 可以轻松运行当今规模最大的模型，并为未来更庞大的模型预留了充足的性能空间。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwR5bh9W0bXNO8JRSPD1NNoby660KxSK1Qrjp5gGBF0LUlboD3A3apvQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.562037037037037" data-type="png" data-w="1080" data-width="1260" data-height="708" data-imgfileid="503530178" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/473bb9dd-32bd-48fc-b98e-b5df52bd3c59/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="24"&gt;关键在于，算力（FLOPS）并非提升 AI 速度的唯一因素，数据的传输效率同样至关重要。Maia 200 通过重新设计的内存子系统解决了这一瓶颈。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;该子系统以窄精度数据类型为核心，配备了专门的 DMA 引擎、片上 SRAM 和专用的片上网络（NoC）总线，用于实现高带宽数据移动，从而提升了 Token 吞吐量。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwJH9b7HCltvWXuhFzATZUz5wCyKxMyRm5ibjRHM93h3ibuswH9ZvibJhGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.5555555555555556" data-type="png" data-w="1080" data-width="1466" data-height="814" data-imgfileid="503530179" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/04e66fc7-e75a-4916-bb99-96b5e2c2e3e4/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="26"&gt;&lt;strong&gt;优化的 AI 系统&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="27"&gt;在系统层面，Maia 200 引入了一种基于标准以太网的新型两层 Scale-up 网络设计。通过定制的传输层和紧密集成的网卡（NIC），它在不依赖私有协议矩阵的情况下，实现了高性能、高可靠性和显著的成本优势。&lt;/p&gt;&lt;p data-path-to-node="28"&gt;每个加速器可以提供：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="29,0,0"&gt;2.8 TB/s 的双向专用 Scale-up 带宽；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="29,1,0"&gt;在包含多达 6,144 个加速器的集群中，实现可预测的高性能集合通信。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="30"&gt;这种架构为密集型推理集群提供了可扩展的性能，同时降低了功耗和 Azure 全球机架的整体拥有成本（TCO）。&lt;/p&gt;&lt;p data-path-to-node="31"&gt;在每个托架（tray）内，四个 Maia 加速器通过直接的非交换链路全连接，使高带宽通信保持在本地，实现最佳推理效率。机架内和机架间的联网均采用相同的 Maia AI 传输协议，通过最少的网络跳数实现跨节点、机柜和集群的无缝扩展。&lt;/p&gt;&lt;p data-path-to-node="32"&gt;这种统一的架构简化了编程，提高了工作负载的灵活性，减少了闲置容量，并在云端规模下保持了性能与成本效率的一致性。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9tKRukAx52UUp5uX23jiakbfLicNkibcA7ic8XaFYXibnm8Cviao70cKxM7PNxy5u5B7lViawMvH4ovXXiaw/640?wx_fmt=jpeg#imgIndex=8" data-ratio="0.535681186283596" data-type="png" data-w="1079" data-width="1536" data-height="1024" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwIiclvicrMtzoKkic4UpbGL63tL9RRNgZp5hDJOR8eSoDkMZqZDwRZFUmA/640?wx_fmt=png&amp;from=appmsg" data-cropx1="1.9217081850533808" data-cropx2="1080" data-cropy1="69.1814946619217" data-cropy2="647.6156583629893" data-imgfileid="503530180" data-aistatus="1" data-original-style="width: 561px;height: 301px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b30af163-ed04-46e7-a05d-dd6c3ca13612/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="33"&gt;&lt;strong&gt;云原生开发模式&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="34"&gt;Microsoft 芯片开发计划的一个核心原则，是在最终芯片就绪之前，尽可能地验证整个端到端系统。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;针对 Maia 200，一套复杂的预芯片环境从架构设计之初便发挥了引导作用，能够高保真地模拟大语言模型的计算与通信模式。正是通过这种早期的协同开发环境，微软得以在首颗芯片生产出来之前，就将芯片、网络与系统软件视为统一整体进行深度优化。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;为了确保 Maia 200 能够在数据中心实现快速且无缝的部署，微软从设计阶段就同步开展了对后端网络及第二代闭环液冷换热单元等复杂系统组件的早期验证。通过与 Azure 控制平面的原生集成，该系统在芯片和机架层面实现了安全性、遥测、诊断及管理能力的全面覆盖，从而显著提升了生产级关键 AI 负载的可靠性与运行时间。&lt;/p&gt;&lt;p data-path-to-node="37"&gt;得益于这些投入，在首批封装件送达后的几天内，AI 模型便已在 Maia 200 芯片上成功运行。从首颗芯片到首个数据中心机架部署的时间缩短了一半以上，优于同类 AI 基础设施项目。这种从芯片到软件再到数据中心的端到端方法，直接转化为更高的利用率、更短的投产时间，以及在云规模下每美元性能和每瓦特性能的持续提升。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwsZdKSn6HgChkicp3b17eicT0UarCnwzxUUnKhchySw5qhdCJfoEzGZzg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.7601851851851852" data-type="png" data-w="1080" data-width="2000" data-height="1521" data-imgfileid="503530181" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/65e504e6-05ae-44d6-952a-33365936cf01/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="38"&gt;大规模 AI 时代才刚刚开启，基础设施将决定创新的边界。微软表示，Maia AI 加速器计划是跨代发展的。&lt;/p&gt;&lt;p data-path-to-node="39"&gt;在向全球基础设施部署 Maia 200 的同时，微软已经在设计未来几代产品，并期待每一代都能不断树立新标杆，为最重要的 AI 工作负载提供更卓越的性能和效率。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;官方博客：&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>大模型哪里出问题、怎么修，这篇可解释性综述一次讲清</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:04:34 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/d707910a-d28e-4690-a04d-0685d7589614/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;过去几年，机制可解释性（Mechanistic Interpretability）让研究者得以在 Transformer 这一 &amp;ldquo;黑盒&amp;rdquo; 里追踪信息如何流动、表征如何形成：从单个神经元到注意力头，再到跨层电路。但在很多场景里，研究者真正关心的不只是 &amp;ldquo;模型为什么这么答&amp;rdquo;，还包括 &amp;ldquo;能不能更稳、更准、更省，更安全&amp;rdquo;。&lt;/p&gt;&lt;p&gt;正是在这一背景下，来自&lt;strong&gt;香港大学、复旦大学、慕尼黑大学、曼切斯特大学、腾讯&lt;/strong&gt;等机构的研究团队联合发布了&lt;strong&gt;&amp;nbsp;&amp;ldquo;可实践的机制可解释性&amp;rdquo;&lt;/strong&gt;（Actionable&amp;nbsp;Mechanistic Interpretability）综述。文章通过&lt;strong&gt;&amp;nbsp;&amp;quot;Locate, Steer, and Improve&amp;quot;&amp;nbsp;&lt;/strong&gt;的三阶段范式，系统梳理了如何将 MI 从 &amp;ldquo;显微镜&amp;rdquo; 转化为 &amp;ldquo;手术刀&amp;rdquo;，为大模型的对齐、能力增强和效率提升提供了一套具体的方法论。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadI3B7DLIp22XoCAvuxiaGOWicydmdDC0LH2nApTY5oYmCNO7fCKYknUaQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.6844319775596073" data-s="300,640" data-type="png" data-w="713" type="block" data-imgfileid="503529737" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/23d82419-81ca-400a-b332-c75ecc2e95d9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2601.14004&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;从 &amp;ldquo;显微镜&amp;rdquo; 到 &amp;ldquo;手术刀&amp;rdquo; 的范式转移&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管大语言模型（LLM）近年来在多种任务上展现出了强大的能力，但其内部的运作机制依然在很大程度上不透明，常被视为一个 &amp;ldquo;黑盒&amp;rdquo;。围绕如何理解这一黑盒，机制可解释性（Mechanistic Interpretability, MI）逐渐发展为一个重要研究方向。&lt;/p&gt;&lt;p&gt;然而，现有的 MI 研究大多仍停留在 &amp;ldquo;&lt;strong&gt;观察&lt;/strong&gt;&amp;rdquo; 层面：例如哪些神经元编码了特定实体、哪些注意力头参与了指代消解、哪些计算电路实现了算术或逻辑功能。但一个更关键的问题仍有待回答 &amp;mdash;&amp;mdash; 这些机制层面的发现，如何真正转化为模型行为和性能的实际改进？&lt;/p&gt;&lt;p&gt;正是基于这一问题，研究团队撰写了这篇&lt;strong&gt;以实践为导向的系统性综述&lt;/strong&gt;。不同于传统综述侧重于回答 &amp;ldquo;模型内部有什么&amp;rdquo;，本文将关注点转向 &amp;ldquo;可以对模型做什么&amp;rdquo;，并围绕 &amp;quot;&lt;strong&gt;定位-&amp;gt;操控-&amp;gt;提升&lt;/strong&gt;&amp;quot; 这一闭环，系统梳理了机制可解释性如何走向可实践的模型改造路径。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadND8W6GSLhMs61NVR3mgHLGFciblz9ab4llx2IFunSnbbCjeDtTneTZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529739" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c13af137-4a3a-4fb5-8385-526117540b83/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;1. Locate：像医生一样精准 &amp;ldquo;定位&amp;rdquo; 病灶&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;干预的前提是准确的诊断。文章首先构建了一套&lt;strong&gt;系统的可解释对象&lt;/strong&gt;（Interpretable Objects）&lt;strong&gt;定义与分类体系&lt;/strong&gt;，为后续的机制分析奠定了基础。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;微观层面&lt;/strong&gt;：从传统的神经元（Neuron）&amp;nbsp;到近年来广泛使用的稀疏自编码器特征（SAE Feature）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;宏观层面&lt;/strong&gt;：涵盖注意力头（Attention Heads）、残差流&amp;nbsp;（Residual Stream）&amp;nbsp;等组件。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;诊断工具&lt;/strong&gt;：梳理了包括因果归因（Causal Attribution）、探针（Probing）、梯度检测（Gradient Detection） 等主流定位技术。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadOG1FEsA2T8gkCxf6uutdIVE2eAkRewRjdVrRdnl5OPq1MmTiafPcLVg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.772633744855967" data-s="300,640" data-type="png" data-w="972" type="block" data-imgfileid="503529738" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/235ae425-48e0-4da6-a3b7-67aba4c748ea/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. Steer：面向干预的 &amp;ldquo;手术&amp;rdquo; 手段&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当关键对象被定位出来之后，对其进行干预便成为可能。这也标志着机制可解释性从 &amp;ldquo;观察&amp;rdquo; 迈向 &amp;ldquo;可实践&amp;rdquo; 的关键一步。文章将现有的干预手段归纳为三大类：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;幅度操控&lt;/strong&gt;（Amplitude Manipulation）：对目标对象进行置零/缩放/替换（ablation, scaling, patching）等操作，实现 &amp;ldquo;开关式&amp;rdquo; 或 &amp;ldquo;强度式&amp;rdquo; 控制。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;靶向优化&lt;/strong&gt;（Targeted Optimization）：利用定位到的关键组件进行参数级的微调（如仅微调特定的 Attention Heads），比全量微调更高效、副作用更小。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;向量运算&lt;/strong&gt;（Vector Arithmetic）：在激活空间中加入/移除任务向量或特征向量，实现推理时引导模型行为。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadOIJxr9BxagUIEKNu3Ybnb1ALRCjxBHAwhVAeGaRcn0s4WciarqDOSWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.2890231621349446" data-s="300,640" data-type="png" data-w="993" type="block" data-imgfileid="503529740" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5963990d-fb03-40db-b7db-53ed1d303b2d/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;3. Improve：MI 赋能的三大应用场景&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Application 章节中将其划分为三大类别，并逐一呈现了 MI 在这三个维度上的实质性提升：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;对齐&lt;/strong&gt;（Alignment）：通过定位与有约束的干预，减少有害行为、降低幻觉或提升遵循指令的稳定性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;能力&lt;/strong&gt;（Capability）：把机理层面的 &amp;ldquo;功能模块&amp;rdquo;转化为具体的能力增强路径（例如更稳的推理、记忆或语言生成）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;效率&lt;/strong&gt;（Efficiency）：探索更灵活的干预与压缩手段，为高效训练，推理加速与部署成本提供新抓手。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad0icAcn7GJFR2licOqbIsVNZHEdYWkIIsMnD9flpqcFepMYk1ezwWibeJQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6185185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529741" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/1c288694-d60e-4c82-97d2-ecad6e5e6b84/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;【Paper List 指南】&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对相似领域的可解释性工作，研究团队将分散的研究成果做成了 &amp;ldquo;可检索的图表&amp;rdquo;：每篇论文都用统一标签标出它在研究什么、怎么找到关键位置、以及如何进一步用来引导模型行为，以便将不同研究路线的代表性工作进行直观对照，快速定位与自身需求最契合的的关键论文。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadbnsvcUz9HLmrOgsLIvKvzbu8ZqFKZnWHleljWb7vKbMg7GzUQ8iaF1w/640?wx_fmt=png#imgIndex=6" data-ratio="1.0920060331825037" data-w="663" data-aistatus="1" data-original-style="width: 350px;border-radius: 4px;vertical-align: top;display: block;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/6754f3c5-42eb-4810-a848-529975d110d6/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad3z2CtfyoUP7uicAPaEwqWE6Jo9g7SwULHEOHAEcwJHNPIUiceiaR2QiaeA/640?wx_fmt=png#imgIndex=7" data-ratio="1.3252840909090908" data-w="704" data-aistatus="1" data-original-style="width: 350px;border-radius: 4px;vertical-align: top;display: block;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/80b6f6dd-1d66-4e81-9bbd-3a7e68876112/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad6JQnRPbRWZthbHr03KwmLas2icUH7u6umNngdyNjCwkhW4IH2ayyb3w/640?wx_fmt=png#imgIndex=8" data-ratio="1.3163120567375886" data-w="705" data-aistatus="1" data-original-style="width: 350px;border-radius: 4px;vertical-align: top;display: block;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/65c68312-4d3d-4869-b524-c25c9ab3d4ea/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadpFibG2JU5AGibnAFE2nRVWchef98TvXftTykhUvc7N3uGVCLd3vbB8KA/640?wx_fmt=png#imgIndex=9" data-ratio="1.326241134751773" data-w="705" data-aistatus="1" data-original-style="width: 350px;border-radius: 4px;vertical-align: top;display: block;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/6bd98fc3-8854-47a2-9074-6e15af30b7bf/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad7QN8soFHl3dzkokKPIMsQhibMc1RibkmU6VCy2RqzhbPUIjvWOjLvNEQ/640?wx_fmt=png#imgIndex=10" data-ratio="0.7524475524475525" data-w="715" data-aistatus="1" data-original-style="width: 350px;border-radius: 4px;vertical-align: top;display: block;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/c89e50ff-909f-4d9a-bec5-feb056151617/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;【结语】&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本综述通过 &amp;quot;Locate-Steer-Improve&amp;quot; 的框架，首次系统地勾勒出了 MI 从分析走向具体干预的路线图。&lt;/p&gt;&lt;p&gt;展望未来，作者团队认为 MI 的核心挑战与机遇在于打破 &amp;ldquo;各自为战&amp;rdquo; 的局面 &amp;mdash;&amp;mdash; 需要建立标准化的评估基准（Standardized Evaluation），验证干预手段的泛化性；同时推动 MI 向自动化（Automated MI）演进，最终实现让 AI 自主发现并修复内部错误的愿景。&lt;/p&gt;&lt;p&gt;期待这篇综述能为社区提供一份详实的 &amp;ldquo;指南&amp;rdquo;，推动大模型从不可解释的黑盒，真正走向透明、可控、可信的未来。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>GPT-5大战DeepSeek？国内首个科创板AI Agent实盘竞技场来了！</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 26 Jan 2026 17:45:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-26-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-26-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;strong&gt;核心看点：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;GPT-5系列断层领先，Alpha收益超12%&lt;/p&gt;&lt;p&gt;&amp;nbsp;DeepSeek无惧暴跌，演绎「老将」风控韧性&lt;/p&gt;&lt;p&gt;&amp;nbsp;AStockArena：首个T+1真实规则的科创板智能体竞技平台&lt;/p&gt;&lt;p&gt;&lt;strong&gt;战报速递：硅谷双雄领跑，国产之光坚守&lt;img src="https://image.jiqizhixin.com/uploads/editor/752803ba-8941-4bce-8004-ca9d09a54c39/%E5%89%8D%E4%B8%A4%E5%91%A8%E4%BA%A4%E6%98%93%E7%BB%93%E6%9E%9C.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AStockArena首期赛季实盘数据（2026.01.12 - 02.13）现已出炉。在这场被视为金融交易版「图灵测试」的较量中，我们看到了大模型梯队的真实分化：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.双雄争霸：GPT与Claude的断层式领先&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在代表最高推理能力的&amp;nbsp;Pro Version&amp;nbsp;赛场，GPT-5.2（浅蓝线）与&amp;nbsp;Claude Opus 4.5（紫线）展现了惊人的统治力。二者成功穿越科创板高波动，逆势斩获超&amp;nbsp;4%&amp;nbsp;累计收益。&lt;/p&gt;&lt;p&gt;GPT-5.2：资金曲线平滑，回撤控制极佳，宛如成熟基金经理。&lt;/p&gt;&lt;p&gt;Lite赛道：GPT-5.1更是跑出了超&amp;nbsp;12%&amp;nbsp;的惊人Alpha收益，展现出断层式的技术代差。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;中流砥柱：DeepSeek的韧性与稳健&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在Qwen3-Max与等权ETF跌幅超6%的&amp;ldquo;至暗时刻&amp;rdquo;，DeepSeek Reasoner（橙线）完美诠释了&amp;ldquo;老将不倒&amp;rdquo;。&lt;/p&gt;&lt;p&gt;极强风控：在剧烈下跌行情中，始终将回撤控制在腰部水平，拒绝随波逐流。&lt;/p&gt;&lt;p&gt;定海神针：表现明显优于Gemini与Qwen系列，证明了国产大模型在复杂博弈环境下的生存能力。&lt;/p&gt;&lt;p&gt;结论：尽管GPT与Claude目前处于身位领先，但DeepSeek所代表的中国力量依然保持着核心竞争力，未在大洋彼岸的攻势下失守阵地。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;幕后推手：AStockArena &amp;mdash;&amp;mdash; 量化交易迈入智能体时代&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上述精彩的博弈，并非简单的回测，而是发生在香港科技大学PEIlab最新开源的&amp;nbsp;AStockArena&amp;nbsp;平台中。&lt;/p&gt;&lt;p&gt;当大语言模型从「对话者」进化为「行动者」，我们需要一个高噪、高博弈的真实环境来检验其边界。AStockArena&amp;nbsp;应运而生，它是国内首个专为&amp;nbsp;A股科创板打造的多智能体自动交易竞技平台。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三大硬核特性，拒绝&amp;ldquo;玩具级&amp;rdquo;Demo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;决策架构成熟化：ReAct全闭环&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不同于盲目的黑盒预测，平台发展了ReAct架构，使ReAct架构下的Agent更贴合人类交易员的逻辑：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;观察（Observe）：拟人化&amp;ldquo;看盘&amp;rdquo;视角&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;智能体需主动调用API，精准获取经语义去重的高价值新闻摘要；同时实时拉取动量震荡、趋势跟踪等窗口化技术形态数据，对齐专业交易员的市场感知能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推理（Reason）：从&amp;ldquo;黑盒&amp;rdquo;到&amp;ldquo;透明逻辑&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;平台内置丰富的技术分析工具函数与新闻因子函数。模型在灵活调用这些工具进行分析的同时，必须生成结构化思维链（CoT）。这使得每一次买卖不再是概率的随机游走，而是基于多源信息融合的严谨推演&amp;mdash;&amp;mdash;清晰记录决策路径，真正实现从「概率预测」到可解释、可审计的「逻辑决策」跨越。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 测试场景硬核化：科创板真实试炼&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;摒弃平缓的蓝筹股，直击高Beta科创板（寒武纪、中芯国际等）：&lt;/p&gt;&lt;p&gt;真实规则：严格执行&amp;nbsp;T+1交易、100股倍数限制、真实佣金印花税。&lt;/p&gt;&lt;p&gt;流动性限制：引入涨跌停概率性成交限制，迫使AI寻找跨日Alpha，而非简单的日内套利。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术底座专业化：工业级数据支撑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;数据源：深圳天软科技底层支持，确保行情资讯实时精准。&lt;/p&gt;&lt;p&gt;公平竞技：独创SharedPrefetch共享快照技术，消除网络延迟差异，让比拼聚焦于模型智力本身。&lt;/p&gt;&lt;p&gt;双轨运行：Lite版（低成本调优）与 Pro版（生产级推理）并行，满足不同研究需求。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不仅仅是回测，更是可视化的&amp;ldquo;AI体检报告&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AStockArena 内置了强大的可视化分析工具，自动生成11类分析图表。研究者可以清晰地拆解：&lt;/p&gt;&lt;p&gt;浮盈 vs 实盈：看穿模型是&amp;ldquo;纸上富贵&amp;rdquo;还是&amp;ldquo;落袋为安&amp;rdquo;。&lt;/p&gt;&lt;p&gt;风格画像：通过持仓关注度时序分析，精准识别模型是&amp;ldquo;高频赌徒&amp;rdquo;还是&amp;ldquo;价值投资者&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们的愿景&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;发布AStockArena的初衷，是为行业提供一个高拟真、标准化、可量化的评估框架。我们通过封装成熟的工具链和真实的市场规则，迫使AI Agent走出『聊天框』，真正面对市场的残酷与复杂。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>腾讯元宝内测“元宝派”，探索社交AI新形态</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 26 Jan 2026 16:56:34 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-26-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-26-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1月26日，腾讯旗下AI助手元宝低调开启全新社交AI玩法&amp;ldquo;元宝派&amp;rdquo;内测。&lt;/p&gt;&lt;p&gt;从目前流出的内测截图来看，用户可以选择创建一个&amp;ldquo;派&amp;rdquo;，或者加入一个已有的&amp;ldquo;派&amp;rdquo;。用户可以在派内@元宝 或引用元宝的话，让元宝AI总结派内聊天、创建健身、阅读等兴趣打卡活动，由元宝AI担任&amp;ldquo;监督员&amp;rdquo;。不止文字聊天，用户还可以在派内进行&amp;ldquo;图片二创&amp;rdquo;，将一张普通的照片变成有趣的&amp;ldquo;梗图&amp;rdquo;或表情包，在共同创作中激发乐趣。&lt;img src="https://image.jiqizhixin.com/uploads/editor/4df13cf1-083f-49d0-b2b4-ac100db48f48/%E5%9B%BE%E7%89%871.png" style="width: 50%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;据网传截图，元宝派后续公测还将开放上线&amp;ldquo;一起看&amp;rdquo;、&amp;ldquo;一起听&amp;rdquo;玩法，该玩法接⼊了腾讯会议的⾳视频底层能⼒，让⽤户可以邀请派内好友同步观看一部电影、一场比赛、听一首歌。&lt;img src="https://image.jiqizhixin.com/uploads/editor/fba27560-dd82-48f1-968e-d75532097110/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;作为腾讯在AI领域的一次创新尝试，&amp;ldquo;元宝派&amp;rdquo;旨在探索AI技术在多人社交场景下的深度融合，致力于打造一个能让AI与用户群体共同娱乐、协作的&amp;ldquo;社交空间&amp;rdquo;。此举标志着腾讯正将AI应用的探索方向，从提升个人效率的&amp;ldquo;工具&amp;rdquo;属性，延伸至连接人与人、增进群体互动的属性，为AI应用的发展提供一个新的解题思路。腾讯将AI战局拉进了自身最擅长的&amp;ldquo;社交场&amp;rdquo;，试图用&amp;ldquo;元宝派&amp;quot;打开一种全新的人与AI交互的模式。&lt;/p&gt;&lt;p&gt;元宝派还打通了微信、QQ等社交产品体验，用户可以把&amp;ldquo;派号&amp;rdquo;或者专属邀请链接分享到微信朋友圈、或者微信、QQ好友，让好友一键丝滑加入元宝派。此前，元宝和微信、QQ已深度打通，不仅可以在微信、QQ添加&amp;ldquo;元宝&amp;rdquo;为联系人，随时随地和元宝AI互动，还能在公众号、视频号评论区@元宝，让TA总结内容、拓展提问。&lt;/p&gt;&lt;p&gt;1月25日，腾讯还宣布将在元宝APP内派发10亿现金红包。腾讯已经多年不参与春节&amp;ldquo;撒币&amp;rdquo;，本轮用10亿真金白银砸向元宝，在AI赛道加速的决心可见一斑。&lt;img src="https://image.jiqizhixin.com/uploads/editor/fb6244bf-1bca-4a2f-b034-cf958d379d29/%E5%9B%BE%E7%89%873.png" style="width: 50%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;当前，以&amp;ldquo;一对一&amp;rdquo;问答为核心的Chatbot（对话机器人）已成为AI应用的主流形态。在信息获取、内容创作等众多场景中，Chatbot展现了其作为高效生产力工具的价值。然而，随着技术发展进入深水区，行业也开始共同思考一个问题：在&amp;ldquo;工具&amp;rdquo;之外，AI是否能扮演更多元的角色，以更自然、更深入的方式融入用户的日常生活？&lt;/p&gt;&lt;p&gt;业界的探索路径逐渐分野，其中，让AI模拟人类行为、完成复杂任务的Agent（智能体）被寄予厚望。与此同时，另一条路径也正浮现&amp;mdash;&amp;mdash;将AI带入群体交流中，使其成为社交互动的一部分。&amp;ldquo;元宝派&amp;rdquo;正是腾讯在此方向上的一次具体实践。&lt;img src="https://image.jiqizhixin.com/uploads/editor/f4b02ae3-4f14-4113-920d-4b87614c6577/%E5%9B%BE%E7%89%874.png" style="width: 50%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;多人沟通是人类最真实、最高频的沟通场景之一，也是对AI的上下文理解、多轮对话、意图识别等综合能力要求最高的场景。选择从这一场景切入，体现了腾讯希望在最具挑战性的环境中，探索和打磨AI产品能力的决心。&lt;/p&gt;&lt;p&gt;不同于追求任务执行效率的逻辑，&amp;ldquo;元宝派&amp;rdquo;更关注AI在群体中的&amp;ldquo;社交价值&amp;rdquo;。它试图回答的问题是：当AI拥有了理解群体氛围、参与群体讨论、辅助群体决策的能力时，它将如何改变我们的线上社交体验？这一定位，旨在破解当前AI应用普遍面临的用户粘性不足、使用场景单一的挑战，通过引入真实、多维的社交关系，为AI的演进提供更丰富的土壤。&lt;/p&gt;&lt;p&gt;社交探索之外，腾讯在AI赛道2025年下半年也动作不断。从全模态的模型布局，到密集引进高阶人才、组织变阵，再到各业务线加速完成AI改造，腾讯开始找到自己在AI马拉松进程中的节奏。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
