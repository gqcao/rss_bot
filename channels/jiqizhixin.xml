<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>突发！Meta官宣收购智能体初创公司Manus</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 10:13:53 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30-2</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/322054e3-8207-4129-b818-c1c88c35bc62/1767060780408.png" style="width: 700%;" class="fr-fic fr-dib"&gt;就在刚刚，Meta 完成了一项大收购，将智能体初创公司 Manus 收入麾下。&lt;/section&gt;&lt;p&gt;目前，双方交易的具体细节（包括具体收购金额等）尚未公布。&lt;/p&gt;&lt;p&gt;自今年 3 月推出全球首款通用 Agent 以来，Manus 迅速走红，成为人工智能领域的一大焦点。据公开资料显示，今年 4 月份，Manus 母公司宣布完成 7500 万美元融资，估值接近 5 亿美元，投资方包括知名风投机构基准资本在内的多家投资主体。&lt;/p&gt;&lt;p&gt;此后，Manus 总部及核心研发团队搬到了新加坡。&lt;/p&gt;&lt;p&gt;如今，「靴子终于落地」，Manus 被 Meta 收购，迎来了新的发展机遇。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrLMCbGjAvKJCsNYOnoNKvrV39v8UOdprK70A5Ig0SOGBqMSyDtmNzZg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.9055555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526030" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/880ad7f6-d781-4dda-b498-12f3109d8fa4/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;Meta 首席 AI 官 Alexandr Wang 表示，「很高兴 Manus 加入 Meta，帮助我们打造令人惊叹的 AI 产品！Manus 团队在探索当前模型的能力潜力方面处于全球领先地位，致力于构建强大的智能体。」&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503526031" data-ratio="0.48055555555555557" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrxqRH6v5yu8XfZA6nhkp2f58dEkvbNMNjibmxEpTaG3oZMHgrxagOxRA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/bbb1bbf2-3a3b-490b-aa22-03aac1bbb4cf/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Manus 创始人兼 CEO 肖弘发文称，「今天是一个我将终生难忘的时刻。当我们创办 Manus 时，很少有人相信通用 AI 智能体能够成功。我们被告知时机太早，目标太宏大，挑战太艰难，但我们依然坚持建设。在怀疑、挫折和无数个夜晚的徘徊中，我们曾质疑自己是否在追逐不可能的梦想。但是，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: justify;margin-bottom: 0px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;我们没有错。&lt;/span&gt;」&lt;/p&gt;&lt;p&gt;他接着说到，「这不仅仅是一次收购。这是对我们一直在构建的未来的验证，证明它是真实的，而且它的到来比任何人预期的都要快。但是，这不是终点。AI 时代的真正开始，不仅仅是会「说话」，更能「行动」、「创造」并「交付」，才刚刚开始。现在，我们将以自己从未想象过的规模来构建它。感谢那些在我们还没有显现出成果时就相信我们的人。最好的还在后头。」&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503526032" data-ratio="0.6814814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrTaxY30ask5iaHjfYdoNLwAqln8domG6MECmY0N1o8FmtyAwUiaicZDVgw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a2d2fc6c-b06e-4aa5-bf9b-a6c2b16d2887/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;评论区的网友们也纷纷送来了祝贺。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr4MhVCRcJ7Fy4hSyeBTLvmPl5weTJDELfLSWHiaaWfh8r1tkaUuMEcTQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.687962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526033" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/ca0a3cd3-02bf-4542-8104-6fbd47aeb050/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;以下为 &lt;strong&gt;Meta 公告内容&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;我们很高兴地宣布，Manus 将加入 Meta，携手将领先的智能体带给全球数十亿人，并为使用我们产品中的企业解锁更多机会。&lt;/p&gt;&lt;p&gt;Manus 已经打造了一个领先的自主通用智能体，可以独立执行复杂任务，如市场调研、编码和数据分析。我们将继续运营和销售 Manus 的服务，并将其整合到我们的产品中。&lt;/p&gt;&lt;p&gt;Manus 已经满足了全球数百万用户和企业的日常需求。今年早些时候，Manus 推出了首个通用 AI 智能体，至今已处理超过 147 万亿个令牌，并创建了超过 8000 万个虚拟计算机。我们计划将这项服务扩展到更多的企业。&lt;/p&gt;&lt;p&gt;Manus 的卓越人才将加入 Meta 团队，为我们的消费品和企业产品提供通用智能体，包括在 Meta AI 中的应用。&lt;/p&gt;&lt;p&gt;我们非常高兴欢迎 Manus 团队的加入，并通过他们的技术帮助改善数十亿人和数百万企业的生活。&lt;/p&gt;&lt;p&gt;以下为&lt;strong&gt; Manus 公告内容&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;我们想要分享一个消息给大家：Manus 即将加入 Meta。&lt;/p&gt;&lt;p&gt;对我们而言，这不只是一条新闻，更是对 Manus 在通用 AI Agent 领域里工作的认可。&lt;/p&gt;&lt;p&gt;自发布以来，Manus 专注于构建通用型 AI Agent，帮助用户高效完成研究、自动化和复杂任务。面对全球越来越多用户的使用需求，团队持续迭代产品，努力使 Manus 在实际使用中更实用、更可靠。根据 12 月初统计的数据，上线至今，Manus 已处理超过 147 万亿个 token，并创建了超过 8000 万台虚拟计算机。&lt;/p&gt;&lt;p&gt;我们深信 AI Agent 的发展前景。此次与 Meta 的携手，将进一步巩固 Manus 在 AI 应用层的战略位置 &amp;mdash;&amp;mdash; 将先进的人工智能能力转化为可规模化、可靠的系统，在实际使用场景中端到端出色地执行用户交给的任务。&lt;/p&gt;&lt;p&gt;当然，确保这一变化不会影响用户的正常使用，是我们最重视的事情 &amp;mdash;&amp;mdash; Manus 将继续通过 app 和网站为用户提供产品和订阅服务，同时公司将继续在新加坡运营。&lt;/p&gt;&lt;p&gt;到目前为止，Manus 已经为全球数百万用户提供服务、创造价值。在接下来的时间里，我们希望将 Manus 的服务带给 Meta 平台上的数百万企业和数十亿用户。&lt;/p&gt;&lt;p&gt;肖弘表示：「携手 Meta 使我们能够在不改变 Manus 运作方式和决策机制的前提下，在更强大、更可持续的基础上发展。我们对 Meta 与 Manus 合作的前景充满期待。我们将继续迭代产品，为用户提供超预期的服务 &amp;mdash;&amp;mdash; 这是 Manus 从上线至今得以存在和发展的根本原因。」&lt;/p&gt;&lt;p&gt;&lt;sup&gt;公告 1 链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.facebook.com/business/news/manus-joins-meta-accelerating-ai-innovation-for-businesses&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;公告 2 原链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://manus.im/zh-cn/blog/manus-joins-meta-for-next-era-of-innovation&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>港大联合字节跳动提出JoVA: 一种基于联合自注意力的视频-音频联合生成模型</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 10:11:02 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/9499f88a-6e10-4733-a4da-ed71cbbbb04e/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;作者介绍：本文第一作者黄小虎同学，目前是香港大学的三年级在读博士生，导师是韩锴教授。黄小虎的研究方向是以视频为中心的领域，包括音视频生成、视频理解以及视频识别。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;视频 - 音频联合生成的研究近期在开源与闭源社区都备受关注，其中，如何生成音视频对齐的内容是研究的重点。&lt;/p&gt;&lt;p&gt;近日，来自香港大学和字节跳动的研究团队提出了一种简单有效的框架 &amp;mdash;&amp;mdash;JoVA，它支持视频和音频的 Token 在一个 Transformer 的注意力模块中直接进行跨模态交互。为了解决人物说话时的 &amp;ldquo;口型 - 语音同步&amp;rdquo; 问题，JoVA 引入了一个基于面部关键点检测的嘴部区域特定损失 (Mouth-area specific loss)。&lt;/p&gt;&lt;p&gt;实验表明，JoVA 只采用了约 190 万条训练数据，便在口型同步准确率、语音质量和整体生成保真度上，达到了先进水平。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrNQaakk7OY3Y6ibHEyIbXdpWBIQoWeVE1HQC7G72jaYIwj74ZmbG7e8A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.40925925925925927" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525803" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/32411780-81df-477c-abd6-04d93560d167/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目主页： https://visual-ai.github.io/jova/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2512.13677&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/ZmID9Gmno3F1N2pypq9mlQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7c1db443-9754-4db9-b48e-4e31bcc3d5eb/1767060547021.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;strong&gt;一、研究背景与动机&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;目前的开源解决方案通常分为两大类别：一类是 &amp;ldquo;级联式&amp;rdquo;，即先生成视频再配音，或者先生成语音再驱动视频生成，这种方式在一定程度上会导致音频和画面的割裂；另一类是 &amp;ldquo;端到端的联合生成&amp;rdquo;，试图同时输出视频和音频。&lt;/p&gt;&lt;p&gt;如下图 a, 现有的端到端方法（如 OVi 和 Universe 等），为了实现双模态对齐，需要在自注意力层 (self-attention) 之外，额外设计融合模块或跨注意力层 (Cross-attention)。这不仅破坏了 Transformer 架构的简洁性，还可能阻碍进一步的数据和模态扩展。&lt;/p&gt;&lt;p&gt;相比之下，JoVA 采用了更加简洁的设计（如图 b），直接使用联合自注意力层 (joint self-attention) 进行两种模态特征的融合与对齐。它同时承担了单模态内的建模以及跨模态的融合任务，无需引入任何新的模块。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525804" data-ratio="0.6527777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrnBHReZeAibbKypzFrDDHrmzCK1DicVDIY5dvQVdUE1YEQP1ObnG7h32Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b7296e55-c19f-4a08-9fdf-6612d26f320f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;二、方法设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 架构描述&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;JoVA 采用 Waver 作为基础模型。为了实现音频生成，JoVA 首先通过复制预训练视频主干网络 (Backbone) 的参数来初始化音频扩散模型。在特征提取方面，采用了 MMAudio VAE 将原始音频转换为声谱图潜在表示 (Latent Representation)。&lt;/p&gt;&lt;p&gt;音频分支的训练沿用了与视频分支相同的流匹配 (Flow Matching) 目标函数。在预训练阶段，视频和音频模态是独立训练的；而在后续阶段，两者被统一整合进同一个架构中进行并行处理。此外，对于视频生成，模型支持参考图像 (Reference Image) 作为条件输入。该图像经由视频 VAE 编码后，在通道维度上与噪声视频潜特征进行拼接。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525805" data-ratio="0.7314814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrG3qbpfdeuqbMGOuic5dg8sj4qy54VWUM3ls2tTj6zTSuXZ778NrS4SA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/259a20bc-f2c9-4d6c-ac5f-96ef1eeb2697/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 音频 - 视频 - 文本联合自注意力层&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了实现模态间的融合，JoVA 在 Transformer 块内部采用联合自注意力机制（Joint Self-Attention）。具体而言，视频 Token、音频 Token 以及对应的文本 Token 被拼接在一起，输入到共享的自注意力层中进行处理。这种设计允许不同模态的 Token 在每一层都进行直接的信息交换，既保留了各自的预训练知识，又实现了特征融合。为了确保视频与音频在时间维度上的精确同步，模型采用了源自 MMAudio 的时间对齐旋转位置编码（Temporal-aligned RoPE），在时间维度上同步了两种模态的位置编码。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 潜空间嘴部区域感知监督（Mouth-Aware Supervision）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了解决人像生成中的唇形同步问题，JoVA 引入了一种针对嘴部区域的增强监督策略。该过程包含三个步骤：&lt;/p&gt;&lt;p&gt;1. 区域定位：首先在原始视频帧上进行面部关键点检测，计算出覆盖嘴部区域的像素级边界框。&lt;/p&gt;&lt;p&gt;2. 潜空间映射：将像素空间的边界框映射到 VAE 的潜空间。这包括空间上的缩放（除以空间下采样因子 s）和时间上的滑动窗口聚合（根据时间下采样因子 t 合并窗口内的边界框），以精确定位潜特征中的嘴部区域。&lt;/p&gt;&lt;p&gt;3. 加权损失：在训练目标函数中引入了专门的嘴部损失项。该损失仅对视频潜特征中的嘴部掩码区域计算流匹配损失，并通过权重系数进行调节。最终的总损失函数由视频损失、音频损失和嘴部区域损失共同构成，从而在不增加推理阶段架构复杂度的前提下，强制模型学习细粒度的唇形 - 语音对齐。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrSl0bibBicLfKiaUzpfibIicTndoQ4ID24oibUhWywiaQEg0W8cwzZGusHyvXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.09537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525794" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5d3f83b7-ec01-4662-9286-3e9bc4e7cb35/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;如下图，我们可以发现，这种映射方式可以很好地在潜空间定位到嘴部区域：&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525795" data-ratio="0.7472222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrdwEJ03tYo9uddaqWu0dCBNI6IibOnKkbUdJ0a5SLEKpITcEj30cE4Hw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/db3d332a-a395-4e38-b3b5-c265f99d079c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;三、训练数据集构建&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作者构建了包含三个部分的训练数据集：Text2Audio（环境音）、Text2Video-Audio（自然场景视听对）以及 Text2Avatar-Speech（数字人 / 说话人视频），总共约 1.9M 的训练样本。数据标注采用了一套自动化流水线：使用 Tarsier2 生成视频描述，Audio-flamingo3 生成音频描述，并利用 Whisper 进行自动语音识别（ASR）以获取语音文本。&lt;/p&gt;&lt;p&gt;在实施细节上，采用两阶段训练策略：先进行语音单模态独立训练（80K 步），再进行联合视听训练（50K 步），并在推理时使用了分类器无关引导（Classifier-Free Guidance）以提升生成质量。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrSuJW78fs7BsZOFzLyrYZ8fbvk9g6dFicgl87zC7l2OEDzZ84Plp0DSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.29814814814814816" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525806" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/c9c9f040-ab0d-4590-8637-4a03da696757/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;四、实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. SOTA 方法对比&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 UniAvatar-Bench（作者精选的 100 个样本）和 Verse-Bench（600 个多样化样本）两个基准上进行了评估。对比对象包括两类：一是使用真实音频驱动的视频生成模型（如 Wan-S2V, Fantasy-Talking），二是联合视听生成模型（如 Universe-1, OVI）。&lt;/p&gt;&lt;p&gt;UniAvatar-Bench 表现：JoVA 在整体性能上表现最佳。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;唇形同步（LSE-C）：得分为 6.64，不仅优于联合生成模型 OVI (6.41) 和 Universe-1 (1.62)，甚至超过了使用真实音频驱动的 Wan-S2V (6.43)，证明了嘴部监督策略的有效性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;语音与音频质量：在文本转语音准确性上，JoVA 取得了最低的词错误率（WER 0.18）；在音频生成指标（FD, KL, CE, CU, PQ）上均取得最佳分数。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;视频质量：在动态程度（MS 0.98）和美学评分（AS 0.47）上均领先。虽然身份一致性（ID 0.78）低于音频驱动模型，但在联合生成任务中处于合理范围。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525807" data-ratio="0.3416666666666667" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr3ic5I8RVwicqZtKUOZRiadiaotzdXsicmvhknrWlicNxMYzXNPMyNk9DFVJQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/86e5124b-c9f9-4f26-8b0a-5d7ee0eb4e87/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Verse-Bench 表现：JoVA 展现了在多样化场景下的鲁棒性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;语音准确性：WER 低至 0.11，验证了其稳健的语音合成能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;视听对齐：LSE-C 得分为 6.51，略低于 OVI (6.61) 但远高于 Universe (1.62)。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;综合质量：在保持最高视频动态（MS 0.80）和美学质量（AS 0.48）的同时，音频生成的一致性（CS, CE）也达到了最优水平。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525808" data-ratio="0.37777777777777777" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrwuuWaXt3dibeDbZof1H34Key0WeEVaF6w5HkYpiauibg3octWV7u5iapmw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/7e0e883d-37e6-46ff-8913-5b024a16c5f3/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;模型扩展性与效率分析&lt;/p&gt;&lt;p&gt;研究进一步对比了基于 Waver-1.6B（总参数量 3.2B）和 Waver-12B（总参数量 24B）主干网络的 JoVA 模型性能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;小模型的高效性：仅使用 3.2B 参数和 1.9M 训练数据的 JoVA 模型，其 LSE-C 得分达到 6.20，显著优于参数量更大（7.1B）且训练数据更多（6.4M）的 Universe-1 模型（LSE-C 1.62），并与 10.9B 参数的 OVI 模型具备竞争力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;大模型的性能上限：随着参数量增加至 24B，JoVA 在各项指标上均达到最佳水平（LSE-C 提升至 6.64，WER 降至 0.18）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525810" data-ratio="0.18888888888888888" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrrsvQ5VvicJMdrNOTMgAN800k33sribx3JpVmVTVxgbR3X3Ob6HepicllQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/d6dcfeed-ea2b-4477-86f8-9148240bfe32/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 融合实验对比&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了验证各模块的有效性，作者进行了多项消融实验：&lt;/p&gt;&lt;p&gt;嘴部感知损失（Mouth-Aware Loss）的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;当权重为 0.0 时，模型无法学习细粒度的唇形对齐（LSE-C 仅为 1.39）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;增加权重至 5.0 时，LSE-C 显著提升至 6.64，且未损害其他音频或视频质量指标。这表明针对嘴部区域的显式监督对于实现精确同步至关重要。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525813" data-ratio="0.3444976076555024" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr5U7yPWwKWTpr8h5MpTxapMoF7wt4Qe0pdeqV4t5mLRyzb3jysia9EAg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-type="png" data-w="836" type="block" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/c6b9aa36-a250-4d65-af04-689658ae97d8/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;时间对齐 RoPE 的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;采用时间对齐的 RoPE（视频和音频共享时间维度的位置编码）相比未对齐版本，LSE-C 从 6.58 提升至 6.64。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;尽管在音频分布相似度（FD）上存在轻微折损（0.58 vs 0.69），但该设计显著增强了帧级的时间对应关系，更利于人像视频生成。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525814" data-ratio="0.27340823970037453" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrl3oOJPBZfne89uia3Fp9dowNe3ZnjeSoqcs2dxzl88hQricav15cfVGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-type="png" data-w="801" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/e1fac8de-039b-40a4-b10b-0c3464fb776b/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;联合自注意力 vs. 交叉注意力：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对比结果显示，联合自注意力（Joint Self-Attention） 机制在唇形同步（LSE-C 6.64）和语音准确性（WER 0.18）上均优于交叉注意力变体。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;特别是带线性适配层的交叉注意力方案表现最差（LSE-C 1.63）。这证实了在统一的注意力空间内直接处理多模态 Token，比通过独立的交叉注意力模块更能促进特征的有效对齐。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525815" data-ratio="0.1925925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrbTyh8QiaGprYO7zTAcTooWYuXt08Viby9l6USpD95xaeztIIh8GyK98w/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/c533d1b8-016c-4547-94ee-ead2fca7a6e1/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AI引爆内存荒：手机电脑不仅要涨价，还要减配</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 16:55:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-11</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/86a70f67-943a-44d5-a5ee-1cbc486d771a/1766998276892.png" style="width: 700%;" class="fr-fic fr-dib"&gt;「我一直在告诉大家，&lt;strong&gt;如果你想买（电子）设备，现在就买&lt;/strong&gt;。我自己要买的 iPhone 17 就已经下手了。」这是咨询公司 TrendForce 高级研究副总裁 Avril Wu 在最近接受采访时说的一句话。&lt;/p&gt;&lt;p&gt;她之所以给出这个建议，是因为他们有一个核心判断：AI 发展带来的内存短缺问题，已经波及到消费电子领域，导致电子设备价格上涨。而且这一问题短期内难以缓解。&lt;/p&gt;&lt;p&gt;上周，我们报道了一个&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651009149&amp;idx=1&amp;sn=2261b03f3e9ad5831fe0a5922c5fa142&amp;scene=21#wechat_redirect" target="_blank"&gt;消息&lt;/a&gt;，电脑内存（RAM）—— 这个长期以来在配置里不占大头的组件，现在的价格已经涨到了令人乍舌的程度，一根 256GB 的内存条比一块 RTX5090 显卡要贵 1000 多美元。今年 2 月份还可以用 899 元买到的两根 32GB 内存条，现在已经涨到了 3499 元。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrE7ibeI3OtwfMuUtEujmtmhdUIcQyyYRTfZyMDnm6RBjlvvXNa6UjVvA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.7335600907029478" data-s="300,640" data-type="png" data-w="882" type="block" data-imgfileid="503525984" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/be7494b2-3710-40a3-8ce4-bda9b3fb2b3b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr8baasw0EGvKucrJePeHIZUeCeiby0D6hNztWjmR5vOOEA6ib0JicjWjzg/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=2" data-ratio="0.2175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525985" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/83760d34-8551-4454-9ba1-0f69709a1659/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;涨价的根本原因在于：&lt;strong&gt;产能都被 AI 截胡了，目前的内存市场正处于一场由 AI 算力需求引发的「结构性紧缺」中。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;TrendForce 的数据表明，当前，RAM 芯片的需求比供应高出 10%—— 而且增长速度如此之快，以至于制造商每个月购买芯片的成本都大幅增加。&lt;/p&gt;&lt;p&gt;仅本季度，他们为 DRAM（最常见的一种内存）支付的费用就比上一季度高出 50%。而且，如果生产商想更快拿到这些芯片，他们要支付的费用会是原来的两到三倍。&lt;/p&gt;&lt;p&gt;Wu 预计，DRAM 价格将在接下来的季度再上涨 40%，并且她不认为 2026 年价格会下降。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvryBA2rMSXSIlNGaeGSWQIiaZRrYueLWZkg9yibIUvibswOGwDaBXjyqOLA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=3" data-ratio="1.3185185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525986" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/e4ce4705-6d4b-40d3-bb9c-ace05d7b7e2c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 网友在 12 月 17 日购买的内存条，8 天后上涨 34%。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 可能吞噬全球 DRAM 用量近 2 成&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要理解这场涨价潮的深层逻辑，需要先了解 AI 对内存的依赖程度。&lt;/p&gt;&lt;p&gt;科技咨询公司 Greyhound Research 的 CEO Sanchit Vir Gogia 解释道：「AI 工作负载是围绕内存构建的。」训练和推理系统需要大容量、持久性的内存占用，极高的带宽，以及与计算单元的紧密协同。「你没办法在不损害性能的前提下缩减这些配置。」&lt;/p&gt;&lt;p&gt;以「100 万 token」的长上下文情境为例，即便采用 FP8 等较省容量的格式，推理过程仍可能需要约 60GB 高速 DRAM 保存中间状态；若采用 FP16，需求可能翻倍到 100GB 以上。相较目前常见的 8K token 约 1GB，等同出现约 60 倍的跳增，形成 AI 扩张的隐形成本。&lt;/p&gt;&lt;p&gt;与此同时，AI 公司正在全球范围内快速建设数据中心，投入数十亿美元。这就是为什么 Gogia 认为&lt;strong&gt;这不是周期性的市场波动，而是一种结构性的转变&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;有媒体引用行业专家的预测，到 2026 年，仅云端高速内存的消耗量就可能达到 3 艾字节（EB）。这个数字由三部分构成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;核心推理工作负载：Google 的 Gemini、AWS 的 Bedrock、OpenAI 的 ChatGPT 等主要平台，实时内存需求约为 750PB，考虑到实际部署所需的冗余和安全余量，这一数字将翻倍至约 1.5EB。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;私有云基础设施：Meta 和苹果的私有云，加上中国国内市场，贡献约 800PB。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;下一代模型训练：包括检查点存储和参数保存在内的训练需求，再增加 500PB。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但真正令人担忧的不是这 3EB 本身，而是它对整个内存产业的「挤出效应」。&lt;/p&gt;&lt;p&gt;AI 竞赛的焦点正在从单纯的算力比拼，转向内存容量和推理成本的较量。随着推理过程中需要存储的中间状态数据激增，每个用户、每个 AI Agent 所需的内存都在成倍增长，对 HBM（高带宽内存）和 GDDR7 等高速内存的需求随之飙升。&lt;/p&gt;&lt;p&gt;问题在于，高速内存的制造远比普通内存更「吃」产能：生产 1GB 的 HBM 所消耗的晶圆产能，相当于 4GB 的标准 DRAM；GDDR7 则是 1.7 倍。这意味着，AI 对制造产能的实际占用，远远超过其内存出货量所显示的比例。&lt;/p&gt;&lt;p&gt;有报告指出，2026 年全球 DRAM 总产能预计为 40EB，而 AI 的「等效消耗」将占到总产量的近 20%。考虑到 DRAM 年产能增长仅有 10% 至 15%，这种需求激增将不可避免地挤压 PC、智能手机和服务器 DDR5 等标准 DRAM 产品的供应，加剧短缺风险和价格上涨压力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;芯片巨头的「二选一」困境&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对 AI 带来的高利润订单，内存芯片制造商纷纷调整产能分配。总部位于爱达荷州的美光科技（Micron Technology）是全球最大的 RAM 制造商之一，上周公布的季度财报超出预期，正是得益于内存芯片价格的上涨。&lt;/p&gt;&lt;p&gt;美光 CEO Sanjay Mehrotra 在财报电话会议上表示：「我们认为，在可预见的未来，整个行业的供应量将大幅低于需求。」&lt;/p&gt;&lt;p&gt;但问题在于：&lt;strong&gt;当芯片厂商将更多产能倾斜给 AI 领域的高端内存时，留给个人电脑、智能手机、游戏设备和电视等消费电子产品的芯片就相应减少了&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;戴尔科技首席运营官 Jeff Clarke 在 11 月 25 日的财报电话会议上直言不讳地谈到了成本压力：「我看不出这些成本如何不转嫁到消费者身上。」&lt;/p&gt;&lt;p&gt;分析人士认为，这一困局短期内没有解决方案。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;内存的短缺，还很有可能连带着让 GPU 的供应也变得紧张。&lt;/strong&gt;根据目前的行业动态和供应链报告，英伟达在 2026 年缩减消费级 GPU（GeForce RTX 系列）产量的消息确实在业界流传，且具有较高的可信度。&lt;/p&gt;&lt;p&gt;根据供应链媒体 Benchlife 和 Board Channels 的爆料，英伟达计划在 2026 年上半年对其最新的 RTX 50 系列（Blackwell 架构） 进行大规模产量缩减。预计相比 2025 年同期，供应量将会减少 30-40%，波及的机型当然首先是显存比较大的高端款，特别是 RTX 5070 Ti 和 RTX 5060 Ti 16GB。&lt;/p&gt;&lt;p&gt;在 2026 年，英伟达预计不会发布全新的 GeForce 架构 GPU（如 RTX 60 系列），而是以 RTX 50 系列的生命周期维护为主，市场的重心将转移到 AI 芯片和「AI 工厂」上。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不光涨价，可能还要减配&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wu 指出，内存芯片行业正面临严重的产能瓶颈。到 2026 年底，芯片制造商将在现有工厂设施中达到产能扩张的极限。&lt;/p&gt;&lt;p&gt;而下一座预计投产的新工厂 —— 美光正在爱达荷州建设的项目 —— 要到 2027 年才能正式运营。&lt;/p&gt;&lt;p&gt;这意味着，在未来至少一年半到两年的时间里，消费者将不得不面对持续上涨的电子设备价格。正如 Wu 所言：「预计供应商在可预见的未来将继续提价。」&lt;/p&gt;&lt;p&gt;对于普通消费者而言，如果近期有购买电脑、手机或其他电子设备的计划，现在或许真的是出手的时机 —— 因为越等，可能越贵。&lt;/p&gt;&lt;p&gt;而且，「更贵」还不一定更好，因为品牌方正在被迫「缩水」产品配置。&lt;/p&gt;&lt;p&gt;根据 TrendForce 最新发布的报告，2026 年第一季度内存价格的涨幅已超出此前预期，终端产品的物料成本（BOM）压力正逼近临界点。面对这种局面，电子设备制造商不得不采取一系列应对措施，包括将原本计划的降价促销叫停、缩减产品规格（比如原本标配 16GB 内存的手机，可能改为 12GB 甚至 8GB），DRAM 规格被迫向下分级等。&lt;/p&gt;&lt;p&gt;报告还指出，不同价位的产品将面临不同的命运：高端产品会通过调整定价和促销策略来转嫁成本；而中低端产品的处境更为艰难 —— 要么被迫涨价，要么直接加速停产退市（EOL）。&lt;/p&gt;&lt;p&gt;你近期有购买电子设备的打算吗？打算何时出手？&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.trendforce.com/research/download/RP251208NA&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.trendforce.com/news/2025/12/26/news-ai-reportedly-to-consume-20-of-global-dram-wafer-capacity-in-2026-hbm-gddr7-lead-demand/&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>全景视觉的Depth Anything来了！Insta360推出DAP，200万数据打造全场景360°空间智能新高度</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 16:49:04 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-10</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/00bbb55f-0d2e-4de9-b0cf-e45b5c261705/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在空间智能（Spatial Intelligence）飞速发展的今天，全景视角因其 360&amp;deg; 的环绕覆盖能力，成为了机器人导航、自动驾驶及虚拟现实的核心基石。然而，全景深度估计长期面临 &amp;ldquo;数据荒&amp;rdquo; 与 &amp;ldquo;模型泛化差&amp;rdquo; 的瓶颈。&lt;/p&gt;&lt;p&gt;近日，&lt;strong&gt;来自 Insta360 研究团队、加州大学圣地亚哥分校 (UCSD)、武汉大学以及加州大学默塞德分校的研究者&lt;/strong&gt;共同推出了 &lt;strong&gt;Depth Any Panoramas (DAP)&lt;/strong&gt;。这是首个在大规模多样化数据集上训练的全景度量深度（Metric Depth）基础模型，不仅统一了室内外场景，更通过 200 万量级的数据引擎与创新的几何一致性设计，刷新了多项 benchmark 纪录，在多种 open-world 场景下保持优异的效果。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525534" data-ratio="0.22685185185185186" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6Isv79ZnDDC7dsJCnfRAxMjUoLRed9QHFXickr8ib0uq0icicMQ2xXRelZQdkA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f178dbcf-5d2f-4bd9-b6e5-57be800ca2d8/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="0 0 []"&gt;论文标题：Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://insta360-research-team.github.io/DAP_website/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://insta360-research-team.github.io/DAP_website/assets/paper.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Demo：https://huggingface.co/spaces/Insta360-Research/DAP&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;模型对由 Gemini 或 DiT-360 等合成的全景图同样展现出了极佳的预测效果，生成的深度图边缘锐利、逻辑自洽，是空间 AIGC 链路中理想的几何基石。 除了静态图像，DAP 在处理全景视频流时同样展现出了极佳的预测效果，具备优秀的帧间一致性与稳定性 。&lt;a href="https://mp.weixin.qq.com/s/1JYzIvqOi-YzhBCahI8MMQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d0f51f3d-c54a-412e-b078-88ccf4dcb1d8/1766997843047.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvvCKUqXmzdut8Ah63vD9fMWekXuKvkSQqtCbc3YulgB9Um5VJ7e4qUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7592592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525525" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/0240ae7a-97c7-4867-8147-a68ab888bd34/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;破局：从「贫矿」到 200 万量级的「数据海洋」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在深度学习时代，数据的规模决定了模型的上限。然而，获取带高精度深度标注的全景数据成本极高，导致学术界长期依赖于几万张规模的小型数据集，如 Stanford2D3D 或 Matterport3D。&lt;/p&gt;&lt;p&gt;为了打破这一僵局，DAP 团队构建了一个规模空前的全景数据引擎，将数据量直接推向了&lt;strong&gt; 200 万&lt;/strong&gt;（2M）级别，除了现有的 Structured3D：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;1.7M 互联网真实全景图&lt;/strong&gt;：从海量网页中收集并精细过滤，覆盖了极为丰富的真实世界场景。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;UE5 模拟器精准补全&lt;/strong&gt;：利用基于虚幻引擎 5 的 AirSim360 模拟器，生成了 90K 张高质量、带像素级深度标签的室外航拍数据，解决了户外训练数据稀缺的痛点。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;AIGC 技术协同&lt;/strong&gt;：引入 DiT360 模型生成了 200K 张室内全景图，进一步增强了模型对多样化室内环境的理解力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvmIGwVibuemico9ibNoiaicaPLdDykQ9XYeiaeLw58niaz31Hftdia6qf2IZSwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.34296724470134876" data-type="png" data-w="1038" data-width="1038" data-height="356" data-imgfileid="503525527" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a6cc431d-7d42-4968-be4f-056ded24353d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;三阶段伪标签管线：让「无监督」变「强监督」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对 1.9M 没有任何标签的原始全景图，如何挖掘它们的价值？&lt;/p&gt;&lt;p&gt;DAP 巧妙地设计了一个&lt;strong&gt;三阶段伪标签精炼管线&lt;/strong&gt;，像漏斗一样层层筛选，最终淬炼出高质量的监督信号：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Stage 1：场景不变标注器&lt;/strong&gt;。先用小规模但精准的合成数据（Structured3D + DAP-2M-Labeled）练出一个基本功扎实的标注器，确立物理意义上的深度基准。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Stage 2：写实性不变标注器&lt;/strong&gt;。引入专门的深度质量判别器（Discriminator），从 1.9M 预测结果中筛选出最靠谱的 600K 样本（300K 室内 + 300K 户外），再次训练标注器，消除合成数据与真实场景之间的纹理鸿沟。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. Stage 3：全量 DAP 训练&lt;/strong&gt;。在汇集了精炼伪标签和原始强监督标签的 2M 数据集上，正式炼成 DAP 基础模型。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525528" data-ratio="0.29259259259259257" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvxnQgxkmk3LB96VXqV4yztAVkFtGf4H3xOWGR6G0N3ibbWBSTGRtAS0w/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/76ee2fce-fa38-46a9-a027-bdb65795b740/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;模型架构细节：DINOv3 骨干+动态距离掩码&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了海量数据，DAP 在模型架构上也进行了设计：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;强大的 &amp;ldquo;大脑&amp;rdquo;&lt;/strong&gt;：采用最新的&amp;nbsp;&lt;strong&gt;DINOv3-Large&lt;/strong&gt;&amp;nbsp;作为特征提取骨干，赋予了模型极强的视觉先验和零样本泛化能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;距离自适应（Range Mask Head）&lt;/strong&gt;：模型内置了即插即用的距离阈值分支，允许用户根据应用场景（如室内扫地机器人 vs 户外无人机）切换深度感知范围，有效解决了全景图中远景区域深度分布不均、预测不稳的问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多维几何优化&lt;/strong&gt;：引入了包括&lt;strong&gt; SILog 损失、锋利度损失（LDF/Lgrad）、表面法线损失&lt;/strong&gt;以及&lt;strong&gt;点云一致性损失&lt;/strong&gt;在内的联合优化。这些损失函数专门针对全景图的等距柱状投影（ERP）进行了畸变补偿，确保预测出的深度图不仅数值准，而且边缘锐利、几何结构不崩塌。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525529" data-ratio="0.26296296296296295" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsveoI18ic7iaOj8ZWogDMYgicFZXto6oLNnqO3th5kt31HD3icSeqQeicCeYw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3b35fef5-74d6-4b6b-95a5-322347506cf0/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;效果：三大主流榜单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在多项严苛的零样本（Zero-shot）测试中，DAP 展现了优异的效果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;室内场景（Stanford2D3D / Matterport3D）&lt;/strong&gt;：DAP 的绝对相对误差（AbsRel）大幅下降，在没有针对目标数据集进行任何微调的情况下，依然保持了极高的预测一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;户外场景（Deep360 / DAP-Test）&lt;/strong&gt;：在极具挑战性的户外测试集中，DAP 显著超越了此前的 DAC 和 Unik3D。它预测出的建筑物边缘清晰，天空区域深度稳定，不再出现传统模型的 &amp;ldquo;深度空洞&amp;rdquo; 或 &amp;ldquo;结构扭曲&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvfjCVx8HE9wa4HTpC2sRojvvJA4U5FdPhysJyliattnIZTvUJ4niaJHKQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.34814814814814815" data-type="png" data-w="1080" data-width="2254" data-height="784" data-imgfileid="503525530" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9513cd28-88d6-440d-8011-3eae39095929/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-height="554" data-imgfileid="503525531" data-ratio="0.5055555555555555" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6Isv2UlxwpDoLqKkyLSriaFQqaBCWlq2Zicic5CaFVyrW3yjqfaicicLwcsRmeA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="1080" data-width="1096" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/abdfbdd1-ba6a-4aab-b4a5-597cc3e0ab14/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;图示对比&lt;/strong&gt;：图中的实测对比中可以看到，对比 baseline 出现的远景模糊和天空深度误判，DAP 无论是复杂的家具纹理还是远处的山脉轮廓，都清晰可见。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525532" data-ratio="0.7314814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvhsBEwY5xK7Wy8k0Q16Et0P4B6yzvDNpnLTJEUMZSZPBIiahS6cjP5vA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/098c241e-084a-496a-be96-df823f25e578/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;全空间智能的新里程碑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DAP 的出现，标志着全景深度估计正式进入了 open-world 时代。&lt;/p&gt;&lt;p&gt;它不仅能为自动驾驶、机器人避障提供更广阔的 &amp;ldquo;全知视角&amp;rdquo;，也为 3D 场景重建、VR/AR 内容创作提供了极低成本的深度获取手段。正如论文总结所言，DAP 通过大规模数据扩展和统一的三阶段管线，成功构建了一个能跨越室内外、统一米制深度的全景视觉基座。&lt;/p&gt;&lt;p&gt;目前，DAP 的项目页面已经正式上线，相关的代码与模型也已开源。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;数据是在全景领域实现 AGI 感知的关键。&lt;/strong&gt;&amp;rdquo;&amp;nbsp;DAP 不仅为机器人全向避障提供了更精准的 &amp;ldquo;眼睛&amp;rdquo;，也为 VR/AR 场景的大规模 3D 重建和场景生成奠定了坚实的技术底座。如果你对全景视觉、空间计算或深度估计感兴趣，DAP 绝对是不容错过的年度之作！&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-height="828" data-imgfileid="503525533" data-ratio="0.5583333333333333" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvXxMOiaC8OBWqFX1iaYzwbEB2UbjBoqgem2icZ0sXudBwkSGgYbrIToEOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" data-width="1482" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/02dfef6e-ad1d-483a-9679-7a9022cbd5d0/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>九章云极完成新一轮战略融资，持续巩固全球智算云第一梯队地位</title>
      <description>&lt;![CDATA[工智能基础设施与智算云提供商九章云极宣布完成新一轮融资。]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 16:42:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-9</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;工智能基础设施与智算云提供商九章云极宣布完成新一轮融资。本轮由北京信息产业发展投资基金与北京市人工智能产业投资基金联合领投，老股东启辰星跟投。此次&amp;ldquo;国家队&amp;rdquo;资本的战略入股，体现了国有资本对建设先进人工智能设施建设的高度重视，并为九章云极夯实技术领先优势和拓展普惠算力生态注入强劲动力。&lt;/p&gt;&lt;p&gt;在资金规划与战略布局上，本次融资将重点聚焦两大核心方向：其一，持续加码AI加速计算优化技术研发，进一步巩固并扩大公司在AI训练、智能体开发、强化学习等关键领域的技术壁垒与领先优势；其二，加大普惠智算云平台建设投入，致力于打造国内领先的普惠智算生态体系，推动企业级AI应用的规模化落地与商业化普及。&lt;/p&gt;&lt;p&gt;北京AI产投基金自成立以来，始终围绕北京市在人工智能领域的总体布局开展直接投资，其重点方向涵盖人工智能芯片、大模型算法创新、具身智能等关键领域。此前已投资智谱AI等大模型企业，此次战略入股九章云极，显示出其对AI基础设施这一核心细分领域的重点布局，对九章云极的战略价值显著。&lt;/p&gt;&lt;p&gt;2025年，随着普惠算力需求爆发，九章云极旗下九章智算云（Alaya NeW Cloud）发展迅猛。6月，公司宣布全面升级Alaya NeW Cloud 2.0，并发布全球首个强化学习平台&amp;ldquo;AgentiCTRL&amp;rdquo;及智算生态基金。市场数据显示，九章云极的普惠算力战略成效卓越：截至目前，其智能算力纳管规模已突破万PFLOP，以13.1%的市场份额稳居华南地区第三方普惠智算云市场首位；算力服务版图同步扩张，覆盖国内多数&amp;ldquo;双一流&amp;rdquo;高校AI研究团队及QS前100高校，并以68%的占比成为中小企业首选；在已部署智算云的百人规模企业中，其占比与AWS、Lambda、Azure等国际巨头形成有力竞争。凭借&amp;ldquo;普惠AI&amp;rdquo;战略，九章云极已跻身亚太最具潜力创新企业，领跑全球AI基础设施云赛道。&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着模型训练与推理需求持续攀升，智算云正逐步显现为算力供给的终局形态，其在弹性调度、跨区域部署和算力利用效率上的优势愈发凸显：Forrester数据显示，62%的企业决策者计划2026年布局AI原生智算云以提升训练灵活性，76%的管理者认定AI训练工作负载云端部署&amp;ldquo;至关重要&amp;rdquo;，智算云已成为企业在&amp;nbsp;AI&amp;nbsp;时代构建核心竞争力的关键基础设施。围绕本土模型训练需求，九章云极构建起以&amp;ldquo;普惠训练&amp;rdquo;为核心的智算云生态。九章智算云不仅提供算力资源，更通过训练流程优化、资源调度与实践经验输出，引导用户&amp;ldquo;会训练、训得好&amp;ldquo;，推动更多主体真正参与到模型训练这一创造智能的事业之中。&lt;/p&gt;&lt;p&gt;北京信息产业发展投资基金与北京市人工智能产业投资基金表示：我们看好九章云极在AI基础设施领域的长期价值。其高度前瞻性的发展路径，不仅具备商业潜力，更对构建健康、自主的AI产业生态具有重要战略意义。&lt;/p&gt;&lt;p&gt;据了解，九章云极已锚定未来三年建成10万PFLOP普惠智算储备的目标，将围绕&amp;ldquo;技术创新+普惠落地&amp;rdquo;的发展理念，不断强化其在全球智算云市场的核心竞争力，向引领AI基础设施发展的领军企业持续迈进。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>药物靶标不再停留在器官层级，首次在全身尺度解析药物结合的单细胞图谱</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Mon, 29 Dec 2025 14:33:20 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-8</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfD9ONDOGU5HYjFWictDQqiapWwoeNxSB8gPvKibMyRibsz6ndXVqkMWjrxw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="318" data-imgfileid="100027009" data-original-style="width: 100%;" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/c370dd4c-0891-4522-9b46-10c7c3e50791/640.png" data-sec-load-status="2" data-report-img-idx="0" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;编辑丨&amp;amp;&lt;/p&gt;&lt;p&gt;随着科研人员对生物系统的理解达到单细胞和高空间分辨率，药理学方法也必须跟上，以匹配这种精确度，理解药物作用。&lt;/p&gt;&lt;p&gt;目前，常用的临床检测只能显示药物在某个器官里大致浓度，但无法看清药物真正结合在哪些细胞上。打个比方来说，药物在体内的去向常像被雾霾遮住的景象&amp;mdash;&amp;mdash;一切都处在模糊的感知中，想要精确定位并不简单。&lt;/p&gt;&lt;p&gt;这种处在黑箱之中的现状被终结了。以美国斯克里普斯研究中心（Scripps Research）与霍华德&amp;middot;休斯医学研究所（Howard Hughes Medical Institute）为主的研究团队开发了一种突破性的成像技术，能够照亮整个小鼠体内药物结合的单个细胞。&lt;/p&gt;&lt;p&gt;相关研究内容以「Mapping cellular targets of covalent cancer drugs in the entire mammalian body」为题，于 2025 年 12 月 22 日发布在《Cell》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfKwS8FZ37LYWZC8HLJzDXXBAbO05Zk7zia5bFT4j5RNRb8PpTu5iaXQ1A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.22685185185185186" data-type="png" data-w="1080" data-width="1105" data-height="251" data-imgfileid="100027004" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/b4335471-54e9-43f0-8eaa-2d1b5c0d743a/640.png" alt="图片" data-before-load-time="1766989966264" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：https://www.cell.com/cell/fulltext/S0092-8674(25)01365-0&lt;/p&gt;&lt;p&gt;&lt;strong&gt;vCATCH&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;体积清除辅助组织点击化学（volumetric clearing-assisted tissue click chemistry），后称为 vCATCH，是该团队所开发的能够在体内精确定位药物、细胞结合情况的成像技术。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfoQ1NmYMgo3pElzz31T3swcIRkckT0qwCtBWIPE9XGg6yO8ZSibYgJcA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.1722745625841184" data-type="png" data-w="743" data-width="743" data-height="871" data-backw="546" data-backh="640" data-imgfileid="100027007" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/8c416b1d-86c5-410c-8e43-91f1ba9b79b5/640.png" alt="图片" data-before-load-time="1766989966277" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：建立深层组织点击反应的原理。&lt;/p&gt;&lt;p&gt;vCATCH 是对 2022 年提出的 CATCH 方法的升级方向，主要采用以下几种关键技术的组合：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内部位标记药物分子&lt;/strong&gt;：将包含可与目标蛋白发生共价作用的过量的铜进行适当的化学改造，以深入染色器官。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内给药与固定&lt;/strong&gt;：药物按常规方式给药后，在体内与目标结合。随后对整个动物组织进行取样与预处理。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;strong&gt;Click Chemistry&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;反应标记结合位点&lt;/strong&gt;：使用高选择性的化学反应给结合了药物的细胞分子进行荧光标记。该反应极具专一性并能穿透深层组织。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内组织清晰化 + 三维成像&lt;/strong&gt;：通过组织清晰化方法让整个组织透明，并使用光片荧光显微成像系统获全身三维数据。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;AI 辅助图像分析&lt;/strong&gt;：成像产生的数据通常以&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;TB 级&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;规模呈现，使用先进的 AI 数据管线自动识别并定位每一个结合细胞。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;测试与结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了测试这一新方法，叶的实验室绘制了两种靶向癌症药物的结合：用于治疗血癌的&amp;nbsp;Ibrutinib（Imbruvica）和用于非小细胞肺癌处方的 Afatinib（Gilotrif）。&lt;/p&gt;&lt;p&gt;对于前者，vCATCH 分辨出的结合定位显示&lt;strong&gt;不仅在血液细胞中&lt;/strong&gt;，还在心脏、血管壁甚至肝脏的免疫细胞中出现明确结合信号，这一发现&lt;strong&gt;可能解释其临床上观察到的心血管副作用&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfmHnJ5LUIjfBlRevyoJSDdyI4iatyMUY67XPA9BBlSETqM8WLO2gLiaVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.248995983935743" data-type="png" data-w="747" data-width="747" data-height="933" data-backw="546" data-backh="682" data-imgfileid="100027005" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/70916bf6-d575-4f9c-9f89-2b47d68942be/640.png" alt="图片" data-before-load-time="1766989966358" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：vCATCH 显示了全体 TKI 分布，具有高空间分辨率。&lt;/p&gt;&lt;p&gt;而对于后者，表现出了如预期结果中的一致性。&lt;/p&gt;&lt;p&gt;除此之外，团队还发现：尽管这两种药物都与肾脏结合，但它们在器官内表现出不同的模式。更高分辨率的 vCATCH 显示了它们在器官内分布上的差异：Afatinib 在肝组织中表现出冠状信号，而 Ibrutinib 则与稀疏散布的单个细胞相关，这一点由光片和共焦成像均有体现。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfjLQIhdFGFlGCS8lXV3QPhJuOQ1RfwpOwnphWEqdI1bzXIk0zgBnUBQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.4318529862174578" data-type="png" data-w="653" data-width="653" data-height="935" data-backw="546" data-backh="782" data-imgfileid="100027006" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/60876c13-157f-435d-905b-1fe39f67d70c/640.png" alt="图片" data-before-load-time="1766989966581" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：药物富集组织中 TKI 参与的细胞类型特征。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;药物开发的全新视角&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统药效评估局限于整体组织层级，而 vCATCH 提供了&amp;nbsp;&lt;strong&gt;逐细胞的药物结合情况&lt;/strong&gt;，有助于识别药物靶标之外的非预期结合位点，这正是许多未知副作用的根源。&lt;/p&gt;&lt;p&gt;在药物开发的早期阶段，研发团队可以凭此快速观察到药物与细胞的交互，也可以从 vCATCH 处获得有关副作用的细胞级别解释。结合 AI 工具分析全身结合数据后，甚至可以对不同小鼠个体乃至未来人体样本执行比对分析，在个性化医学中具备潜在应用。&lt;/p&gt;&lt;p&gt;相关报道：https://www.eurekalert.org/news-releases/1110881&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>全球首个具身 VLTA 多模态数据集开源，它石加速具身智能真实世界落地</title>
      <description>&lt;![CDATA[在具身智能领域，数据是智能的最重要燃料，它石智航正成为“为众人抱薪者”]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 14:20:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-12</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;12&amp;nbsp;月&amp;nbsp;26&amp;nbsp;日，它石智航重磅开源了全球首个大规模真实世界具身&amp;nbsp;VLTA（Vision-Language-Tactile-Action）多模态数据集&amp;nbsp;——World In Your Hands（简称&amp;nbsp;“WIYH&amp;nbsp;数据集”）：https://wiyh.tars-ai.com/。该数据集首次亮相于今年&amp;nbsp;10&amp;nbsp;月，并在刚刚结束的它石智航技术首秀发布上，作为「超级算法」中的核心成果之一得到行业广泛认可。首秀现场展示的世界首台可人工刺绣机器人等成果，其丝滑动作背后的核心正是基于&amp;nbsp;WIYH&amp;nbsp;数据集训练的AWE2.0模型。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/074387bd-db38-4aa8-8259-c447a41c9943/1766999899190.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;WIYH&amp;nbsp;数据集通过首创&amp;nbsp;“Human-centric”（以人为中心）的数据采集新范式，破解了遥操作数据采集规模化成本高、仿真数据在&amp;nbsp;sim2real&amp;nbsp;时存在&amp;nbsp;GAP，难以丝滑迁移到现实世界的痛点，填补了具身智能所需的高质量、可泛化、大规模真实世界数据的空白，为具身基座模型实现&amp;nbsp;Scaling Law&amp;nbsp;提供了关键语料。要知道，当前具身智能所需数据量与现有储备之间至少相差两个数量级，WIYH的开源正为填补这一鸿沟提供了核心解法，加速了具身智能落地真实世界的进程。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/74d4fba5-5fa2-48a4-bef5-5dbdc1758952/1766999923177.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;Human-centric&amp;nbsp;采集范式下形成的&amp;nbsp;WIYH&amp;nbsp;数据集，具备了真实可靠、丰富多元、全面多模态、规模化等特征，并拥有海量数据：包含超过&lt;strong&gt;10&lt;/strong&gt;&lt;strong&gt;万&lt;/strong&gt;条以上的真实人类操作视频、&lt;strong&gt;40&lt;/strong&gt;&lt;strong&gt;余种&lt;/strong&gt;任务类型、&lt;strong&gt;100&lt;/strong&gt;多种人类技能，覆盖了含&lt;strong&gt;520&lt;/strong&gt; 余种真实物品，真实还原商超、酒店、餐饮、工业、办公、家居等多行业的&lt;strong&gt;10&lt;/strong&gt;种核心场景全链路任务，数据将分批次释放。WIYH是业内最大的Human-centric数据集，且仍在不断扩展和丰富中。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/44dbdc0e-4e3c-4fee-9a91-5db715df4a2e/1766999942861.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;场景和任务分布&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/be9d8352-f15e-4f71-917b-3607698b2845/1766999963371.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;各场景技能分布&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f9aa131e-7769-4b84-a4ea-c5983d85730b/1766999976292.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;物体和技能词云&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;采集了丰富的数据之后，行业还面临数据迁移这另一核心难题，为此它石构建了 &lt;strong&gt;TARS Datacore&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;具身数据引擎&lt;/strong&gt;。作为云端大模型，它实现了全流程的自动化标注，将原始视频转化为机器人能理解的“教科书”。这套标注体系不仅涵盖了标定、深度、动作、指令、思维链&amp;nbsp;(COT)、掩码（Mask）和触觉（Tactile）等基础信息，更通过以下三类精准标注，形成了从感知到动作的全链路闭环：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;原子任务标注：理解“做什么”。将复杂动作拆解为“抓取、移动、放置”等不可再细分的逻辑单元，并配以自然语言指令，让机器人明白长流程任务的操作顺序与步骤内涵；&lt;/li&gt;&lt;li&gt;图像感知标注：看清“在哪里”。通过云端语义模型给每个物体打上清晰的边界标签（掩码），并利用&amp;nbsp;3D&amp;nbsp;视觉技术计算画面中物与人的深度距离，赋予机器人精准的空间感，使其能理解物体的边界与远近；&lt;/li&gt;&lt;li&gt;视觉语言标注：思考“为什么”。这是最关键的“大脑训练”，它通过空间明确指代操作对象，防止机器人在杂乱环境中错认目标。同时，让模型预测下一步任务，并设置逻辑陷阱来校验机器人的判断力。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;它石首席科学家丁文超博士表示，&lt;strong&gt;“Human-centric&lt;/strong&gt;&lt;strong&gt;数据采集范式配合&lt;/strong&gt;&lt;strong&gt;TARS Datacore&lt;/strong&gt;&lt;strong&gt;数据引擎，可以记录和生产最高质量、最丰富的具身智能数据，真正使得&lt;/strong&gt;&lt;strong&gt;scaling law&lt;/strong&gt;&lt;strong&gt;成为可能。&lt;/strong&gt;&lt;strong&gt;”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;强大的数据采集与迁移能力，共同打磨出&amp;nbsp;WIYH&amp;nbsp;数据集在空间推理、世界模型、跨本体迁移等方面的独特优势。目前，多项基准测试结果已印证了&amp;nbsp;WIYH&amp;nbsp;数据集的核心价值：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在视言大模型（VLM）的空间推理评测中，通过对&amp;nbsp;GPT-4o、Qwen-VL-Plus&amp;nbsp;等主流模型在空间推理（SR）和空间感知（SP）等维度的对比发现，虽然各模型在通用视觉任务上表现出色，但在处理复杂的以人为中心的空间逻辑时仍存在显著差异。这一基准测试直观地揭示了当前大模型在感知操作空间时的局限，也凸显了&amp;nbsp;WIYH&amp;nbsp;数据集在训练更高阶空间感知能力方面的独特价值。&lt;/li&gt;&lt;li&gt;在世界模型（World Model）的物理一致性验证上，WIYH&amp;nbsp;数据集展现了强大的“物理引擎”属性。评测结果显示，在加入&amp;nbsp;WIYH&amp;nbsp;数据后，COGVIDEO&amp;nbsp;和&amp;nbsp;DYNAMICRAFTER&amp;nbsp;等视频生成模型在一致性、流畅度、动态性和质量等四大关键指标上均实现了全面跨越，其中动态性（Dynamic）指标得分提升了&amp;nbsp;15.6&amp;nbsp;分。配合&amp;nbsp;4D&amp;nbsp;重建技术，通过对“倒酒”、“叠衣服”等任务进行精确的几何重建，为模型理解真实世界的物理动态提供了高真值的监督信号，确保生成的动作既流畅又符合物理常识。&lt;/li&gt;&lt;li&gt;在机器人跨本体迁移实验（Cross-embodiment Experiments）层面，WIYH&amp;nbsp;数据集真正实现了“从人到机器”的能力迁移。通过将人类演示视频与机器人操作数据进行协同训练（Co-training），机器人在复杂场景下的泛化能力得到了质的提升。实验数据表明，在极其杂乱的场景中，仅机器人操作数据只能达到&amp;nbsp;8%&amp;nbsp;成功率的任务，在引入&amp;nbsp;WIYH&amp;nbsp;人类视频辅助后，成功率暴涨至&amp;nbsp;60%。这一显著的性能增益证明了&amp;nbsp;WIYH&amp;nbsp;数据集不仅是视觉语料，更是提升机器人实战能力、解决真实世界非结构化环境操作难题的核心“养料”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ee67720b-5efa-4dfb-97d5-52e786e301a0/1766999994578.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;它石创始人兼 CEO 陈亦伦博士认为，&lt;strong&gt;“&lt;/strong&gt;&lt;strong&gt;在&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;领域，最极致的开放不是开源模型，而是开源数据集，因为所有的&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;模型本质上都可以通过数据和合适的训练方法&lt;/strong&gt;&lt;strong&gt;‘&lt;/strong&gt;&lt;strong&gt;生长&lt;/strong&gt;&lt;strong&gt;’&lt;/strong&gt;&lt;strong&gt;出来。&lt;/strong&gt;&lt;strong&gt;”&amp;nbsp;&lt;/strong&gt;它石WIYH的此番开源，正是以开放的态度，为行业通用具身基座模型训练提供关键语料和基础设施，助力具身智能迈向通用智能的新高峰。&lt;br&gt;&amp;nbsp;它石WIYH数据集于今日起（12.26）可正式访问。该网站构建了从认知到实践的全方位支撑：用户既能通过结构化的数据集全景展示、基准测试结果与典型用例，快速完成对数据集性能与场景的初步了解，又能配合快速入门指南与自动化 SDK，直接进入深度的落地实践与开发部署。网站公布了标准化的 off-the-shelf (OTS) 开源可复现方案，也放出了 TARS商用级Human-centric数据解决方案TARS SenseHub的相关信息。 TARS SenseHub 是由它石自研的超轻量数据采集套件，包含 “眼睛” TARS-Vision 与 “双手” TARS-Glove 两大关键组件。其核心理念是让人类和机器共享同一套感知体系，即机器人通过数据“能看人之所看，感人之所感”。&lt;/p&gt;&lt;p&gt;未来，它石智航将继续坚持以人为中心的具身数据与模型新范式，持续挖掘真实世界的数据价值，让面向万物、赋能万业的具身智能真正从实验室走进千家万户。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>《连线》杂志：2026年将是阿里千问之年</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 14:12:27 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-7</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;12月29日，美国著名科技媒体《连线》（WIRED）发表头条文章《再见，GPT-5。你好，千问》，文章指出，GPT-5未能激起市场热情，而开源开放的阿里千问，性能优异，适于灵活部署应用，2026年将属于千问。&amp;ldquo;衡量任何AI模型价值的关键标准，不应仅限于其&amp;lsquo;聪明&amp;rsquo;程度，更应看它被用于构建其他应用的广度。若以此为尺度，千问等中国开源大模型无疑正处于上升势头。&amp;rdquo;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a88f2b1c-0615-4e76-8b38-957d8c956ddb/%E5%9B%BE%E7%89%8700.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;《连线》（WIRED）是全球顶级科技媒体之一，《失控》作者凯文&amp;middot;凯利是该杂志创始主编，《连线》对科技及行业趋势的精准洞察，在硅谷和华尔街享有&amp;ldquo;数字时代圣经&amp;rdquo;的美誉。每年年末，《连线》会刊出对全球科技预测的前瞻文章，《再见，GPT-5。你好，千问》即是年终重磅文章系列之一。&lt;/p&gt;&lt;p&gt;《连线》杂志观察指出，尽管美国OpenAI的GPT-5、谷歌的Gemini 3以及Anthropic的Claude通常得分更高，但阿里千问、DeepSeek等中国模型性能也稳居第一梯队，并且变得越来越受欢迎，&amp;ldquo;原因在于它们既性能优异，又易于开发者灵活调整和使用&amp;rdquo;。&lt;/p&gt;&lt;p&gt;根据全球最大的AI开源社区HuggingFace数据，中国开源模型的下载量在2025年7月已经超过美国，其中阿里千问位居第一；海外模型API调用第三方平台OpenRouter数据也显示，千问在今年异军突起，调用量一度冲至所有开闭源模型的全球第四。&lt;img src="https://image.jiqizhixin.com/uploads/editor/d4b3235f-d733-49e7-91c9-eb3d42eba2be/%E5%9B%BE%E7%89%87000.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;《连线》杂志采访了数家企业和研究机构，发现千问能够胜任大多数先进AI模型所能完成的任务，已经成为企业落地AI大模型的首选：比如中国&amp;ldquo;电动汽车之王&amp;rdquo;比亚迪，将千问集成到其新款车载仪表盘助手中；智能眼镜Rokid，基于千问微调出适配的大模型，实现多端部署多个千问模型；硅谷公司爱彼迎（Airbnb）、Perplexity和英伟达均已将千问纳入技术栈；甚至曾是开源模型先驱的Meta，如今也被传正在借助千问协助开发新一代模型。&lt;/p&gt;&lt;p&gt;《连线》观察到，相较于不常开源的OpenAI，阿里在模型构建与持续更新方面投入更多精力，且千问技术细节通常公开透明，这让千问成为AI创新的第一策源地。在今年顶级人工智能会议NeurIPS上，阿里千问夺得最佳论文，会上收录的数百篇学术论文均采用了千问Qwen。倡导美国开源模型发展的非营利组织Laude Institute联合创始人安迪&amp;middot;康温斯基（Andy Konwinski）表示：&amp;ldquo;许多科研人员都在使用Qwen，因为它是目前最好的开源大模型。&amp;rdquo;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>上线不到一年，收徒百万，首个真人级AI导师技术底牌首次曝光</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 13:56:37 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-6</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e30ea275-9195-415d-ba58-eefc8813433f/1766987374648.png" style="width: 700%;" class="fr-fic fr-dib"&gt;第一次见到「爱学」前，王佳佳（化名）害怕和老师互动。&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;这个来自安徽阜阳的初三女生，性格内向，在课堂上几乎从不举手。题不会，不敢问，宁愿空着；一被老师点名，就紧张到大脑一片空白。久而久之，数学和英语成了她最不愿面对的两门课。&lt;/section&gt;&lt;p&gt;直到有一天，她开始反复和一个「不会不耐烦」的对象对话。&lt;span data-pm-slice="0 0 []"&gt;一句没听懂，就一直追问，直到彻底弄清楚&lt;/span&gt;。对方有表情，会根据她的反应实时调整讲解节奏，也会在她犹豫、走神时主动追问，把她拉回来。&lt;/p&gt;&lt;p&gt;慢慢地，王佳佳敢开口了，学习也变得主动。最近一次数学随堂考试，她考了 103 分，比上一次整整提高了 40 分。&lt;/p&gt;&lt;p data-pm-slice="2 3 []"&gt;「爱学」所承载的并不是一位真人老师，而是一个真人级 AI 导师。&lt;/p&gt;&lt;p data-pm-slice="3 3 []"&gt;2025 年初，成立不到两年的首批 AI 原生应用企业与爱为舞率先落地了&lt;strong&gt;国内首个真人级 AI 一对一导师&lt;/strong&gt;产品「爱学」。App 上线不到一年，已经被超过百万名学员真实使用。&lt;/p&gt;&lt;p&gt;单次课可能持续 1&amp;mdash;2 小时，没有任何真人介入，完课率却高达 92.4%。单个学员的最长学习时长已达到 9000 分钟。&lt;/p&gt;&lt;p&gt;在&amp;nbsp;AI&amp;nbsp;课堂中，单次课的答题正确率&lt;span data-pm-slice="0 0 []"&gt;也从&amp;nbsp;&lt;/span&gt;59.1%&amp;nbsp;提升至&amp;nbsp;83.2%。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ecb35147-e303-41b3-bbde-c03d52f65d0c/1766987407036.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 学员小苹果学英语，进步明显。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;导师，拒走&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;答题」老路&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;放眼当前的 AI 教育市场，大模型似乎成了基础设施，但真正意义上的突破，却迟迟没有出现。大量产品仍停留在「舒适区」&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;把 LLM 「套进」学习机或 App ，当作一个更聪明的聊天机器人来用。它擅长预测下一个 token，却没能走进教学本身。&lt;/p&gt;&lt;p&gt;最常见的使用方式是被大量用于搜题、批改。用户卡题了，点一下 AI，答案瞬间生成。所谓「讲解」，只是堆叠冗长而晦涩的推理链，对低龄用户来说如同天书。好一点的情况，会调用提前录制好的人类视频，但交互为零。&lt;/p&gt;&lt;p&gt;&lt;em&gt;例如，一个常见的语法题：My sister plans _____ （meet）her teacher after school tomorrow. 这里的 meet 应该填什么形式？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;很多 AI 的回答只是就着答案做解释（尤其像 AI 拍搜）：&lt;/em&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;1.答案：to meet&lt;/p&gt;&lt;p&gt;a.核心原因：固定搭配+非谓语动词用法&lt;/p&gt;&lt;p&gt;b.动词plan的固定搭配规则：plan + to do sth.，即计划做某事&lt;/p&gt;&lt;p&gt;2.语法逻辑解析：&lt;/p&gt;&lt;p&gt;a.plan作为谓语动词，后面需要接&amp;ldquo;宾语&amp;rdquo;来表示&amp;ldquo;计划的内容&amp;rdquo;&lt;/p&gt;&lt;p&gt;b.英语中，动词原形（meet）不能直接做宾语，需要通过不定式（to+动词原形）构成不定式短语（to meet her teacher），才能充当plan的宾语，表示&amp;ldquo;计划的具体动作&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;更常见的是打着「AI 课程」旗号的产品，本质仍是录播，AI 交互浅到几乎可以忽略。这些产品解决了「有教」的一部分，却仍难以触及「会教」，更别提「因材施教」。&lt;/p&gt;&lt;p&gt;要将 AI 真正引入教育，必须在「教学」这件事上下功夫。与爱为舞认为，新一代 AI 教育范式应该&lt;strong&gt;因材施教、授人以渔。&lt;/strong&gt;这套判断，来自一支典型的「互联网大厂 &amp;times; 头部教育」跨界团队。公司创始人兼 CEO 张怀亭，曾担任百度商业化系统「凤巢」核心负责人、高途联合创始人。创始人兼 COO 刘威，曾任高途集团副总裁、高途课堂总经理。&lt;/p&gt;&lt;p&gt;就拿前面那道英语题来说，为什么是 &lt;strong&gt;&lt;em&gt;to meet&lt;/em&gt;&lt;/strong&gt;，而不是其他形式？&lt;/p&gt;&lt;p&gt;「授人以渔」的 AI ，不会一上来就告诉你答案是 &lt;em&gt;to meet&lt;/em&gt;。它会先判断学员卡在哪一层，是不是没搞懂非谓语动词？&lt;/p&gt;&lt;p&gt;随后，通过追问与对比引导学员自己发现：&lt;em&gt;plan&amp;nbsp;&lt;/em&gt;表达的是尚未发生的计划动作，英语里通常用 &lt;em&gt;&lt;strong&gt;to do&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;来承载这种未来意图。&lt;/p&gt;&lt;p&gt;再通过举一反三让学员理解共性规律。当遇到 &lt;em&gt;plan / decide / hope / want&amp;nbsp;&lt;/em&gt;这类动词时，不必死背搭配，也能判断该如何表达。&lt;/p&gt;&lt;p&gt;近两年，AI Agent 概念大热，市场上也出现了一对一 AI 导师的雏形，但仍局限在英语、数学等单一学科 。相比之下，「爱学」已实现&lt;strong&gt;全年龄段&lt;/strong&gt;覆盖、&lt;strong&gt;「多学科 + 长课时」&lt;/strong&gt;深度陪伴。&lt;/p&gt;&lt;p&gt;而支撑这一跨代产品的，是自研的&lt;strong&gt;国内首个 AI 原生教育框架&lt;/strong&gt;&amp;mdash;&amp;mdash;一个打通了&lt;strong&gt;「数字人 + 语音 + 大模型 + 工程」的全栈技术体系&lt;/strong&gt; 。与爱为舞也因此成为国内首个在教育场景下打通全栈技术能力的公司。&lt;/p&gt;&lt;p&gt;那么，一个好的 AI 导师是如何炼出来的？&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.8537037037037036" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="562" data-backh="1042" data-imgfileid="503525698" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGB5swibwqIVI8Ug8VeuibnXag8lDXoiapQuelicbXvpKpyzR7MydoVbI3cQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-original-style="width: 100%;height: auto !important;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6267e2f1-7c85-4106-8706-6373f4a3f3c3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;「爱学」背后的全栈技术能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「筑魂」：&lt;/strong&gt;&lt;strong&gt;从 ChatBot 到 MDP 决策&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了打造一个真正「会教」的 AI 导师，与爱为舞自研了&lt;strong&gt;三大基础模型体系&lt;/strong&gt;&amp;mdash;&amp;mdash;从感知的「皮囊」，到负责决策的「灵魂」&amp;mdash;&amp;mdash;完整覆盖 AI 教学的核心能力闭环。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="325" data-imgfileid="503525699" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGW4L6jI1RSPdyeuNeUwnSMaREvLkiaa5rpLouiaLJGJ8XHyMOz38MKxFA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-original-style="width: 100%;height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4f92bc5f-ef0f-4a16-a7ef-81e56bf27a26/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 与爱为舞认为，一个优秀的AI导师需要具备上面四个方面的核心能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;其中，&lt;strong&gt;爱学教育大模型，第一次让 AI 拥有了真正的「教学之魂」&lt;/strong&gt;，更是与竞对拉开身位的关键赛点。&lt;/p&gt;&lt;p&gt;与市面上仍停留在预测下一个 token 的对话系统不同，「爱学」从一开始就被设计成一套实时教学决策系统&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;「一对一教学」不再只是问答交互，而被抽象为一&lt;strong&gt;个持续演化的&lt;/strong&gt;&lt;strong&gt;马尔科夫决策过程（MDP）&lt;/strong&gt;，一次教学就是一场目标明确的博弈。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;学员的状态&amp;mdash;&amp;mdash;包括理解程度、情绪变化、长时间犹豫等&amp;mdash;&amp;mdash;被视作环境；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;每一次提问、追问、提示、鼓励与纠偏，都是 AI 导师可选择的教学策略；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;系统的奖励，不再是「题对了+1 分」，而是学员是否真的学会了、学得更快、学得更开心。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，AI&amp;nbsp;导师每一次讲解、每一次追问、每一次纠错，都不是「接一句话」，而是在&lt;strong&gt;当前学员状态下做出的最优教学决策&lt;/strong&gt;。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2eae5893-2ab2-40bf-b01d-52ec570f42db/1766987535186.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 数学课上，学员和AI导师一问一答，互动学习。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了让模型真正具备「师魂」，与爱为舞设计了两个进化阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SFT（启发式注入）：不只是喂数据，而是通过思维链（CoT）将大量名师的隐性经验系统化。它不只学「说什么」，更在学「为什么要这么教」。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RL（强化学习进化）：引入定制化 GRPO 算法。围绕教学规划的质量与灵活性构建 Reward 函数，让 AI 在数亿次的模拟试错中，打磨出类似真人的「教学直觉」，做到因材施教。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-ratio="0.4425925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="256" data-imgfileid="503525677" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGwObTwfvkICr1k2Kjre83tDnia2qWyty4ib6f8BrfLRoQmUfFk90GMdPQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-original-style="width: 100%;height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/e2a9f614-5c00-4275-b69c-e7858c0e8b5f/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;AI 导师是如何炼成的？这是在真实课堂 + 仿真课堂中持续进化的完整数据闭环与训练体系。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;b align="" alt="" border="" data-aiimageid="" data-aiimagesource="" data-asynid="" data-backh="" data-backw="" data-before-oversubscription-url="" data-cacheurl="" data-cardimg="" data-copyright="" data-croporisrc="" data-cropselx1="" data-cropselx2="" data-cropsely1="" data-cropsely2="" data-cropx1="" data-cropx2="" data-cropy1="" data-cropy2="" data-fileid="" data-fromlib="" data-galleryid="" data-gallerysupplier="" data-height="" data-imgfileid="503525676" data-imgid="" data-imgqrcoded="" data-oversubscription-url="" data-positionback="" data-ratio="" data-remoteid="" data-retry="" data-s="300,640" data-src="" data-type="png" data-upload="1" data-w="" data-width="" height="" ismap="" sizes="" src="" title="" type="block" usemap="" width=""&gt;如果说模型架构决定了「导师素质」的上限，那么数据工程则构成了能力的下限。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与爱为舞没有简单堆砌题库或对话语料，而是先搭建了一套&lt;strong&gt;可运行的&amp;nbsp;AI&amp;nbsp;教学环境&lt;/strong&gt;，让数据在真实教学逻辑中自然生长。数据被系统性地拆解为三层核心要素。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;学科本体。通过构建覆盖各学科的核心知识图谱、关键考点与解题方法，将教材与考纲转化为&amp;nbsp;AI&amp;nbsp;可理解、可调用的教学结构；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;教学方法。通过名师参与课程设计，沉淀「为什么这么教、先讲什么、后练什么」的课程逻辑；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;真实课堂中的学员交互数据。这也是最关键、最稀缺的一层&lt;/strong&gt;。学员的回答方式、犹豫与卡顿、追问与反馈，都被完整记录下来，形成高价值的实时互动样本。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;三者共同构成一套「可运行的数字课堂」，也为后续模型训练提供了区别于传统题库与对话数据的核心养料：不仅知道教什么、怎么教，更知道该如何根据学员状态去教。&lt;/p&gt;&lt;p&gt;为补齐真实数据的稀缺与长尾问题，研发团队进一步引入了类似 AlphaGo 的自博弈机制：让「学员模拟器」与「AI 导师」在虚拟课堂中反复对弈，自生成千万级训练样本。&lt;/p&gt;&lt;p&gt;真实课堂每周数万小时的数据持续回流，驱动 SFT 与 RL 的高速迭代，形成一套稳定运转的数据飞轮。&lt;/p&gt;&lt;p&gt;最终，具备教学判断力的&amp;nbsp;AI&amp;nbsp;导师，得以在&lt;strong&gt;教学目标、教学路径、课堂交互与作业巩固&lt;/strong&gt;上，实现真正意义上的个性化学习。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.44074074074074077" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="578" data-backh="255" data-imgfileid="503525701" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGw3Jmjz2FDhqageZ3ZXEHibW4ia0J6Xynlqyszvp1x56MBhawaLX7xJpQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-original-style="width: 100%;height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/859ffbdf-1af6-43fa-aef5-e6152228d5a4/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; AI 导师从四个方面实现「个性化」施教：教学目标、路径、交互与作业巩固。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;与爱为舞依靠真实的教学互动数据，通过数据飞轮，推动算法以周为单位迭代。目前已更新 20 多个版本，建立了业界首个在线一对一 AI 教学 Agent，全方位提升了教学效果。&lt;/p&gt;&lt;p&gt;1、持续优化互动频次，每节课 AI 导师都能与学员进行几十次的一对一互动，牢牢抓住学员的注意力。&lt;/p&gt;&lt;p&gt;2、持续优化互动质量，学员在一对一互动中的有效回答率提升到 95% 以上，说明学员的注意力得到显著提升。&lt;/p&gt;&lt;p&gt;3、持续优化个性化教学质量，通过个性化教学目标和个性化教学路径，将学员做题的准确率从不足 60% 提升到 83% ，部分课程正确率超过 95%，说明学员在集中精力学习之后，确实掌握了相关知识点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重做「听说」：&lt;/strong&gt;&lt;strong&gt;上下文 ASR +流式 TTS + 全双工语音交互&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在打磨「师魂」的同时，与爱为舞也在&lt;strong&gt;感知层&lt;/strong&gt;完成了一次彻底重构。&lt;/p&gt;&lt;p&gt;原因并不复杂：教学本质上是一种高频互动行为，语音是所有交互的第一道门槛。老师听得准、反应快，学员才愿意继续说下去。&lt;/p&gt;&lt;p&gt;传统 ASR 只会「听写」，至于这句话是在教几何还是教英语，一概不知。于是，因环境嘈杂或学员口音，「four」被听成「for」，「D答案」被听成「第一答案」，「有理数」被听成「有礼数」也就不足为奇。&lt;/p&gt;&lt;p&gt;与爱为舞不再把 ASR 当作「听写」工具，而是把它升级为「课堂参与者」。&lt;strong&gt;自研多模态语音理解模型，在解码最底层引入教学语境约束，&lt;/strong&gt;让「听」从一开始就带着教学目的。约束来自三方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;教学任务（Task）：如当前正在攻克哪一个知识点？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;教学进度（Step）：目前处于引入、练习还是总结阶段？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;学员画像（Persona）：学员此前的错误分布和表达习惯。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于上下文信息直接参与解码路径计算，模型在第一时间就能排除那些在纯语音层面「听起来合理」，但在教学逻辑上完全荒谬的候选结果，&lt;strong&gt;ASR 准确率从行业最好开放能力约&lt;/strong&gt; &lt;strong&gt;80%&amp;nbsp;左右提升至&amp;nbsp;95%+&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;除了「听懂」，AI 导师说话得有人味儿。因此，TTS 也被重做了一番。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;自研流式 TTS 大模型&lt;/strong&gt;将首字延迟压到 &lt;strong&gt;300ms&amp;nbsp;以内&lt;/strong&gt;，通过语义/声学双 Speech Token + 强化学习联合优化，让语音能随语境动态调整节奏、重音与情绪&amp;mdash;&amp;mdash;讲诗词会留白，讲推导会干净利落，甚至能用少样本快速对齐「名师腔」。&lt;/p&gt;&lt;p&gt;让我们一起听听下面这段音频，猜猜是真人还是 AI。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/aa98279a-ed23-41c2-9ce0-c5dc77dd446d/1766987649696.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;听出来了吗？下面揭晓答案：短短 18 秒的音频，混合了 3 段真人音频和 3 段 AI ，是不是完全听不出来？&lt;/p&gt;&lt;p&gt;更关键的是，AI 导师还能被随时打断。通过全双工语音交互，结合流式语义 VAD 与打断拒识模型，实现真正意义上的边说边听，说话过程中即可识别学员插话，&lt;strong&gt;打断识别准确率&lt;/strong&gt; &lt;strong&gt;90%+&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.8969210174029452" data-s="300,640" data-type="png" data-w="747" type="block" data-backw="578" data-backh="518" data-imgfileid="503525693" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGFI8J0TibgdKlEiahxOz6MVkzM6nsoqkNBz0co3G0jDY8kShGfCNpLCPw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-original-style="width: 100%;height: auto !important;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f1bc6eda-13d6-4d4b-9236-cde3a03a116e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;「肉身」进化：&lt;/strong&gt;&lt;strong&gt;数字人百 FPS 实时，不出戏&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在课堂里，声音与「人」必须严格同步。多数数字人 Demo，十几秒足够惊艳，但一旦拉长到 40 多分钟课堂，就会迅速滑向「恐怖片」：&lt;/p&gt;&lt;p&gt;穿模、口型错位、动作僵硬、抖动、表情漂移、声音输出和其唇部动作之间存在明显延迟&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;能稳稳&amp;nbsp;hold&amp;nbsp;住一整堂课的&amp;nbsp;AI&amp;nbsp;导师，对数字人技术提出了近乎苛刻的要求&amp;mdash;&amp;mdash;&lt;strong&gt;极致的实时互动能力，以及长期一致性&lt;/strong&gt;。围绕这些目标，与爱为舞的数字人系统开启了一次从 1.0 到 6.0 的疯狂进化。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/51f6f98c-fa42-46cf-adaf-95fb6a1e4546/1766987674879.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;一旦 AI 导师出现卡顿，学习心流就会瞬间崩溃。为了把实时性做到极致，他们先在架构上做了彻底解耦&amp;mdash;&amp;mdash;引入 NeRF 与 3D Gaussian Splatting 建模，构建实时驱动框架，将口型、表情、身体动作分离建模；音频不再只是驱动嘴巴，而是在毫秒级联动微表情与姿态变化，让反应真正贴合课堂节奏。&lt;/p&gt;&lt;p&gt;再把性能推到百 FPS 级：系统不再「生成完再播放」，而是云端实时「存活」并持续输出；并实现无需训练的秒级生成，仅凭音频输入即可秒级生成高质量视频，内容生产从「提前制作」走向「实时发生」。&lt;/p&gt;&lt;p&gt;在「一致性」上，核心策略只有一个：锁定人格稳定性。&lt;/p&gt;&lt;p&gt;通过构建跨 ID 动作驱动体系，名师动作可稳定迁移；高精度骨骼提取保证复杂姿态下也不穿模、不崩坏，内容生产效率提升 5 倍。&lt;/p&gt;&lt;section data-pm-slice="1 3 []"&gt;&lt;img data-ratio="0.25" data-s="300,640" data-type="gif" data-w="800" type="block" data-backw="562" data-backh="140" data-imgfileid="503525671" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGwQH62NOLXTjIteUn0mcjjFq0T1Nnd1FLCcZLh4dqbJiaWtZ6OtRxr4w/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-original-style="width: 100%;height: auto !important;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/8f8774e8-371b-4102-9105-9891d7f56f5c/640.gif" data-order="0" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;基于真人动作捕捉与骨架识别，将手势与姿态精准映射到虚拟导师，实现自然同步的数字人教学演示。&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;最新 6.0 架构进一步把语音、文本、动作、情绪与人物 ID 融合进统一多模态模型调度，即便 45 分钟情绪持续变化，外观一致性与动作分布依然自然如初，音素级口型同步也终于告别「永远对不上的嘴型」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;钢铁骨架：&lt;/strong&gt;&lt;strong&gt;万人并发，1&amp;ndash;1.6s 即时响应&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说，「师魂」决定教学逻辑，「皮囊」负责感知，那么，能规模化交付的底线只有两个字：不崩。这意味着&lt;strong&gt;高并发、低延迟&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;现实很残酷。传统方案一旦冲到万人并发，端到端延迟往往直接飙到 3 秒以上，交互体验断崖式下滑。而与爱为舞从零搭建的这套「 AI 课堂操作系统」，硬是把 ASR、教学决策、内容生成、TTS、数字人驱动到音视频推流的整条长链路，压缩到了 &lt;strong&gt;1.0&amp;ndash;1.6&amp;nbsp;秒&lt;/strong&gt;&amp;mdash;&amp;mdash;&lt;strong&gt;万人同时在线，依然做到即问即答&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.7472222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="432" data-imgfileid="503525738" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9XdQ7EuVQP1r9OHHvXQCLuATVfjnhtLmeUSQaLg4RRj2Yt2Vj0ujKBRMRo2tNt0GDdpnicnSV0iaicQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-original-style="width: 100%;height: auto !important;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d9712cf4-bb29-4a1c-9400-54f3a45a2951/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;「爱学」如何真正跑起来？背后一整套工程系统，万人并发下还能实时上课的端到端 AI 教学架构图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;为了把时间一分一秒抠出来，他们做了几项工程改造。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第一刀，砍在 ASR上&lt;/strong&gt;。通过ASR 预判 + 并行执行，语音识别链路延迟被压到 100ms 级。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第二刀，打断意图秒识&lt;/strong&gt;。基于历史先验的意图识别，判断学员是不是要插话、追问、纠错。整条打断链路，1.6 秒内闭环，不会让 AI 导师「慢半拍」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第三刀，落在缓存体系&lt;/strong&gt;，把「每问必算」改成「能不算就不算」。用 Prefill Cache 消灭重复计算，用语义 Cache 复用专家答案，真正把响应时间压到人感知不到的区间。&lt;/p&gt;&lt;p&gt;如果说延迟是体验底线，那成本就是商业底线。&lt;/p&gt;&lt;p&gt;数字人渲染，是典型的 GPU 吞噬型任务。如果一张 GPU 只能服务一两名学员，万人并发意味着服务器成本直接失控。&lt;/p&gt;&lt;p&gt;为此，他们一方面通过GPU 显存全共享，榨干单张 GPU 承载极限。另一方面，通过统一调度「大脑」而非模型堆叠，在万人并发下，对不同形象、不同语音素材进行毫秒级自动分配。&lt;/p&gt;&lt;p&gt;真正难的，其实在运营阶段。当系统跑起来，一万个学员就有一万个进度。学习路径高度碎片化，请求分布不可预测&amp;mdash;&amp;mdash;这是任何 AI 教育系统的「噩梦」。&lt;/p&gt;&lt;p&gt;与爱为舞决定拆解「复杂性」&amp;mdash;&amp;mdash;把教学拆成乐高积木一样的零件，原本「随心所欲」的交互，被重构为「按剧本执行」的自动化指令，复杂调度变得可预测、可收敛。&lt;/p&gt;&lt;p&gt;为对抗「意外中断」，系统还引入可重入（Re-entrant）机制，保证 AI 导师不会「断片儿」，随时恢复状态，陪你学。&lt;/p&gt;&lt;p&gt;在过去不到一年的时间里，像王佳佳这样的变化，并不只发生在安徽阜阳的一间书房。它出现在佳木斯的清晨，也出现在三沙的落日里。有人甚至死磕同一节课12 次、交互 585 次，从「不知道」走到「全部知道」。真人级 AI 导师正在为天南地北的孩子，持续规划各自独一无二的学习路径。&lt;/p&gt;&lt;p&gt;这不只是一次产品能力的展示，更是一种新范式的显形：AI 以导师的身份，进入真实、复杂、对结果高度敏感的学习现场，并稳定发挥作用。它也为中国AI Agent 的规模化落地，定义了一套清晰的范式，甚至走在了世界前列。&lt;/p&gt;&lt;p&gt;当知识的获取不再是刷题、排名与淘汰的赛跑，而是一段被理解、被引导、被尊重的旅程&amp;mdash;&amp;mdash;学习这件事，永远值得投入，也永远值得期待。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>深度拆解沐曦MXMACA软件栈功能，算力自主+生态兼容，破解国产GPU落地难题</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 13:46:47 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-5</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/19a21b2f-2fbe-482f-b785-14f06878d87b/1766987049848.png" style="width: 700%;" class="fr-fic fr-dib"&gt;近日，刚刚 IPO 的国产 GPU 公司沐曦股份，完成了自上市后的首个重大技术发布。&lt;/p&gt;&lt;p&gt;该公司旗下的 MXMACA 软件栈（MACA）正式发布了全新版本 3.3.0.X，沐曦发布了一份 23 页的技术报告，机器之心围绕该报告对 MACA 进行解读。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.7425925925925926" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525603" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGwSd177vSxUZia6ml0xjXITMv8Q7uUPStDPVexx8buBLK0RSbQEDJHkg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-original-style="height: auto !important;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/2b4728a3-c0b2-4129-99aa-21148fea0bb6/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在全自主硬件体系的支撑下，沐曦已经构建起「全栈软件」体系，其对于提升计算引擎的效率起到了关键作用。同时，新一代 MACA 宣告了沐曦软件生态的一次重要跨越，它的核心理念，是如何让国产 GPU 真正「用起来」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;GPU 生态适配的「万能接口」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MACA（MetaX Accelerated Computing Architecture）被定义为「异构计算软件栈核心计算平台、引擎、运维工具和规范化操作范本」，内置了全套自研工具链，涵盖编译器、性能分析工具、格式转换组件等，可实现多语言支持、算子自动优化与跨框架平滑适配。&lt;/p&gt;&lt;p&gt;它面向沐曦的曦云 C 系列、曦思 N 系列 GPU 研发，其定位是连接沐曦自研 GPU 硬件与上层应用生态的关键纽带。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.4398148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525831" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvraqsfVsxEtuukeSd0XxoR3eOnsicTqQoAT4A2JhwQe8asWesBq5sqYmw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-original-style="height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/59336eba-0d65-4179-afaf-011de73c0212/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;MACA 承担着连接硬件算力单元与上层应用生态的纽带作用。据介绍，它覆盖了 AI 芯片工作流程的底层驱动、用户态接口、编译器、算子适配、训练框架、推理框架、行业场景优化等全链路能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在芯片行业，硬件决定算力基础，而软件栈则决定了算力能否被有效释放。长期以来，国产 GPU 落地面临的最大挑战并非纯粹的性能，而是生态兼容问题 &amp;mdash;&amp;mdash;AI 开发者早已习惯在英伟达的 CUDA 生态中开发新技术、构建应用，迁移到新的硬件上意味着高昂的迁移成本。&lt;/p&gt;&lt;p&gt;MACA 3.3.0.X 直击这一痛点，它是一套「生态强化版」软件栈，聚焦场景的深度适配，涵盖底层基础能力的迭代与主流 AI 框架、大模型训练推理、搜索、广告、推荐、科学计算等多维度生态适配，&lt;strong&gt;其核心逻辑是构建一个「万能接口」，让现有生态能够近乎无缝地迁移到沐曦平台上。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具体有多万能？技术报告显示，沐曦团队对 GitHub 上大量 CUDA 项目进行了适配测试。他们筛选了 4490 个「含 CUDA 关键字」的活跃代码仓库进行验证，按应用领域包括 AI 模型 / 应用、高性能并行计算、气象模拟、计算化学等场景。&lt;/p&gt;&lt;p&gt;测试结果显示，4173 个项目可以直接适配运行，成功率高达 92.94%。仅有 260 个项目需要微小调整，占比不足 6%，且修改主要涉及编译配置优化，而非核心业务逻辑。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.4351851851851852" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525605" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGD4NXPe9sSJ98oIRLCrPlEoia2yKXV6QtPQoJpUWyLAWicptaz89wl5Lg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" data-original-style="height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/91b846e2-8d41-4e00-8c02-8dd8b6a59f52/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这意味着，&lt;strong&gt;几乎任何现有的 CUDA 项目都可以近乎「开箱即用」地迁移到沐曦平台上，目前在市面上，还没有第二家能够做到&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在 MACA 的这一通适配之后，GitHub 上海量的 AI、数据处理、科学计算应用工具，可以快速适配在国产异构计算平台上。对于开发者而言，这就意味着面对国产 AI 硬件体系时，学习成本和迁移工作量可以大幅降低。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;框架兼容 &amp;nbsp;拥抱主流 AI 开发生态&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了能够无缝迁移已有的项目工作，新版本 MACA 也强调了对于 AI 框架兼容的特性，它能够帮助开发者构建和探索新技术。&lt;/p&gt;&lt;p&gt;在 AI 开发领域，框架兼容能力决定了平台的可用性。MACA 3.3.0.X 版本完成了对 PyTorch 2.8 的深度适配，覆盖了全部 2650 个核心算子（其中 GPU 算子 2410 个）。涵盖从基本算术运算、线性代数操作、卷积 / 池化类算子、规约操作、随机采样、索引与切片快速傅里叶变换（FFT）、Attention 等所有关键算子类别。它支持多种数据形态，保障了算子能力的完整性与场景适配性。&lt;/p&gt;&lt;p&gt;除了 PyTorch，MACA 还兼容 TensorFlow、PaddlePaddle、JAX 等主流开源框架，以及 Megatron-LM、DeepSpeed 等大模型训练框架，在推理端支持 vLLM、SGLang、Transformers、KTransformer 等推理框架。&lt;/p&gt;&lt;p&gt;在操作系统方面，MACA 兼容了 Ubuntu、CentOS、RHEL、openEuler、Anolis OS 、银河麒麟等主流 Linux 发行版。它同时完整支持混合精度训练、分布式训练、torch.compile 编译优化与图模式任务下发的深度集成等关键特性。&lt;/p&gt;&lt;p&gt;简单来说，这一兼容性列表几乎涵盖了当前 AI 开发的所有主流工具链。技术报告中还特别强调，这种适配是「无需调整工程构建逻辑，即可实现现有模型的无缝使用」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;搭配性能分析与优化工具链，MACA 配合沐曦 GPU 在核心场景上的性能可以对标主流 GPU 水平。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MACA 不仅仅是一个兼容层，而是一个完整的软件栈。它包含了开发效率引擎层和垂直场景赋能层两大核心部分。&lt;/p&gt;&lt;p&gt;在开发效率引擎层，MACA 提供了一系列高性能算子库，如针对矩阵计算的 mcBLAS、针对深度神经网络的 mcDNN、针对注意力机制的 mcFlashAttention 等。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.44907407407407407" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525606" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGDJjXk495pGSaFvx3weibFoHjaibjEy56NQLxX08hogXjc6icvyoibDzRXg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-original-style="height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/66e0461a-65ee-448e-97be-8182fd3b3bbd/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; MACA 套件中开发效率引擎，其旨在降低异构开发门槛。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这些工具针对沐曦 GPU 的多卡拓扑进行了专门优化，编译器工具支持 MACA C/C++、Fortran 等语言，能将高级语言转化为高效的可执行程序。&lt;/p&gt;&lt;p&gt;在垂直场景赋能层，MACA 针对 AI 与科学计算两大方向，通过针对性的优化策略与框架适配解决需求。&lt;/p&gt;&lt;p&gt;其中在 AI 领域，MACA 的训练优化兼容 PyTorch、BMTrain 等框架，通过硬件流水线并行实现通信与计算重叠，优化分布式并行策略。推理优化则适配 ONNX Runtime、vLLM、SGLang 等框架，采用 INT8 量化、KVCache 跨卡管理提升长序列处理效率。&lt;/p&gt;&lt;p&gt;在科学计算领域，MACA 通过重构 MPI、BLAS 库提升内存带宽，定向移植 OpenFOAM、GROMACS 等科学计算框架，结合容器化部署方案，能够确保算力能高效支撑流体仿真、分子动力学等垂直场景。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.3888888888888889" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525607" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGiaYQBjEMSd0hMwccXNPKk1GluFmy6PiaibOgUQcypDGYpArAgg3vE5Z8g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-original-style="height: auto !important;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/e2745ced-8a6d-447e-b149-04dd5f0bc29f/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，MACA 的性能分析工具提供了系统级追踪和核函数指标采集功能，能够帮助开发者定位计算瓶颈。全栈工具链的完整性，使得开发者能够在沐曦平台上完成从开发到部署的全流程工作。&lt;/p&gt;&lt;p&gt;此种能力的背后，是沐曦构建的大模型训推一体化能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;算力到生产力的转化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MACA-3.3.0.X 版本为开发者们构建起了一套全流程的一体化算力支撑底座，通过软硬件协同、核心算子优化以及分布式架构的升级，旨在实现训推效能的跨越式突破。&lt;/p&gt;&lt;p&gt;这一底座的基础是沐曦自研的 GPGPU，其高算力密度与高内存带宽确保了单卡能够高效处理千亿参数模型。通过自研的 MetaXLink 高速互连技术，沐曦在硬件层面构建了低时延、高带宽的分布式通信网络，使得算力供给扩展至万卡级集群，为 AI 大模型的超大规模分布式训练与推理奠定了基础。&lt;/p&gt;&lt;p&gt;在软件层面，MACA 构建起端到端的协同体系。其首要特点是极致的生态兼容性，除此之外，MACA 通过拓扑感知的 MCCL 高性能通信库和自研的编译器优化模块，能够智能地优化多机多卡的数据通信策略，实现算子自动融合、循环展开等编译级优化，深度挖掘出硬件底层潜力。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.4962962962962963" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525608" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGcSLibe8BsoReGibiaQqzu74TxLJTo2ak1ggPiaePkZiaD6KbMf86Xl52ZQQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=6" data-original-style="height: auto !important;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/58adf856-71b0-4156-8bfd-0522573b46ff/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;MACA 套件大模型推理优化技术。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;一体化设计的重要优势，在于打破训练与推理之间的场景壁垒。MACA 支持模型训练后的轻量化转换与直接部署，无需二次适配。&lt;strong&gt;通过统一的模型格式与接口规范，它实现了「训练 - 微调 - 推理 - 部署」全流程链路贯通，大幅缩短了大模型从技术研发到业务落地的周期与成本。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在技术层面上，MACA-3.3.0.X 版本针对几个关键瓶颈进行了深度调优。&lt;/p&gt;&lt;p&gt;在关键算子上，MACA 针对 FlashAttention 优化，大幅减少了向 HBM 显存的数据搬运开销；通过对于分布式集合通信库的优化，MACA 将千卡集群的训练、推理线性度稳定在 95% 以上，专家并行效率提升了 15%；通过异步通信机制，还有通信 - 计算重叠优化，MACA 将数据传输任务与 GPU 计算任务解耦并行，缩短了端到端延迟，提升 GPU 利用率 15%-30%，解决了因等待数据通信而导致的芯片闲置问题。&lt;/p&gt;&lt;p&gt;在软件栈上层，沐曦进一步做了面向易用性和部署的优化：其深度支持 PyTorch 2.0 的 torch.compile 动态图编译，以最大化硬件利用率；针对推理场景打造轻量化引擎，优化批处理策略以同时降低延迟、提升吞吐；全面兼容容器化与云原生架构，支持企业级的大规模弹性部署与便捷运维。&lt;/p&gt;&lt;p&gt;MACA 全面兼容当前主流的大模型生态体系，无需代码修改即可开展训练、推理；针对大规模大模型训练场景，其工具链可以缩短训练周期，在分布式训练中展现出优异线性度，可以长周期无故障稳定运行；在推理时，MACA 针对主流大模型的深度优化降低了延迟，提升了吞吐量；与此同时，MACA 还具备从小规模调试到大规模训推的全场景平滑扩展能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实测数据表明，沐曦通过 MACA-3.3.0.X 构建的一体化算力底座在曦云 C 系列 GPU 上的训推效能已经展现了与国际旗舰 GPU 产品 A 正面竞争的实力。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.4324074074074074" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525609" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGZb7HjpUzZd06k2l2icSeHjiabGnC7EtHwU2herp0EvUxPEcXYyAP6jaQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-original-style="height: auto !important;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/7f7ffbc7-1f43-4453-ba47-0bfe5b4f29e5/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;DeepSeek、GLM、InternLM、Llama、Qwen 等多系列大模型，在不同参数规模（如 7B、13B）及任务类型（SFT、Pretrain）下的训练 TGS 数据，包含&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;旗舰 A TGS&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;（黄色柱）、&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;C550 TGS&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;（紫色柱）及两者效率比值（绿色折线）。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;MACA 不仅是一个技术平台，更是沐曦「1+6+X」战略的重要组成部分。在这一战略中，「1」代表数字算力底座，「6」代表对于六大核心行业的赋能，包括金融、医疗健康、能源、教科研、交通和大文娱等行业的 AI 场景应用及开源生态建设，「X」代表具身智能、低空经济等新兴行业。&lt;/p&gt;&lt;p&gt;技术报告详细介绍了 MACA 对于多个垂直场景的优化：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在搜广推场景，MACA 针对 TensorFlow/JAX 与 XLA 技术栈进行了深度协同适配。在部分模型中，沐曦平台的性能已达到甚至超过国际旗舰产品。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在传统小模型支持方面，MACA 提供了多模型格式兼容和底层计算优化，覆盖计算机视觉、自然语言处理及传统机器学习等核心场景。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在 AI for Science 领域，MACA 适配了 PaddleScience、WRF 数值模式等科学计算工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;除此以外在材料、技术科学、天气模拟、药物研发等领域，MACA 对领域主流 AI 框架都进行了适配。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种场景化优化能力，使得沐曦 GPU 不再仅仅是提供原始的算力，而是能够针对特定行业需求提供优化方案，实现从算力到生产力的高效转化。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;构建生态的长远布局&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为衔接自主 GPGPU 硬件与全栈软件体系的核心载体，MACA 3.3.0.X 的推出不仅是产品版本的常规迭代，更是国产芯片厂商在经历硬件破冰后，试图通过软件定义算力、通过标准重塑生态的长远布局。&lt;/p&gt;&lt;p&gt;值得肯定的是，&lt;strong&gt;沐曦提供的从 AI 芯片到软件核心平台的能力，是全栈自研的 &lt;/strong&gt;&amp;mdash;&amp;mdash; 与部分厂商选择兼容 CUDA 或基于现有开源 ISA 进行微调的方式不同，沐曦选择了最具挑战但也保证了长期安全性的路线：自主指令集。MACA 软件栈具有自己的编程模型和使用范式，但也深度兼容 CUDA 生态，无需大幅修改即可适配海量 CUDA 项目。另外，沐曦的 GPU 基于全自研 GPGPU 核心 IP 及架构，原生支持全精度计算、MetaXLink 高速互连等特性。&lt;/p&gt;&lt;p&gt;凭借自研的体系，沐曦保证了算力体系的安全合规、性能针对性以及演进自主权。与此同时，MACA 并没有将全自研等同于「生态完全推倒重来」，而是通过 MACA 软件栈构建了高度兼容的体系。&lt;/p&gt;&lt;p&gt;这种策略，保证了「算力自主」的战略目标。通过一并兼容已有生态海量的算法模型、软件资产与开发者技能，让更多开发者们无需重复造轮子，就可以在自主算力的底座上跑通业务。这种「高门槛自研、低成本迁移」的模式，最大化地保证了用户的商业效率与效益。&lt;/p&gt;&lt;p&gt;随着技术的不断进步，沐曦正在以最低的迁移成本，将 AI 开发者引入自己的生态轨道。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>QwenLong-L1.5发布：一套配方，三大法宝，让30B MoE模型长文本推理能力媲美GPT-5</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 13:42:22 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-4</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/f43e578e-8cb7-47dd-bfab-84e82a444d0d/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;作为大模型从业者或研究员的你，是否也曾为一个模型的 &amp;ldquo;长文本能力&amp;rdquo; 而兴奋，却在实际应用中发现它并没有想象中那么智能？&lt;/p&gt;&lt;p&gt;你大概率也遇到过以下困境之一：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;虚假的繁荣&lt;/strong&gt;： 模型在 &amp;ldquo;大海捞针&amp;rdquo; (Needle-in-a-Haystack) 测试中轻松取得高分，营造了一种长文本能力已经解决的 &amp;ldquo;虚假繁荣&amp;rdquo;。但一旦任务从简单的信息定位，升级为需要串联分散证据、整合全局信息的多跳推理 (multi-hop reasoning) 时，模型的表现便会急转直下，难以构建起完整的逻辑链条，暴露出其在深度理解上的真实短板。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练的噩梦&lt;/strong&gt;： 长文本、多任务的训练数据就像一个成分复杂的 &amp;ldquo;大杂烩&amp;rdquo;，其多源、多域的特性，让标准的 RL 算法严重 &amp;ldquo;水土不服&amp;rdquo;。你精心设计的奖励函数（Reward Function）很可能因为数据分布的剧烈变化而产生偏差，导致模型性能不升反降。最终，监控图上那剧烈震荡的奖励和熵（Entropy）曲线，无情地宣告着训练过程的 &amp;ldquo;翻车&amp;rdquo; 与崩溃。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;窗口的天花板&lt;/strong&gt;： 即使上下文窗口被扩展到 256K，1M 甚至更长，它也终究是一个有限的 &amp;ldquo;物理内存&amp;rdquo;。然而，现实世界的知识流 &amp;mdash;&amp;mdash; 分析整个代码仓库、研读一份完整的年度财报、或是精读一部专业巨著 &amp;mdash;&amp;mdash; 其信息量轻易就能突破这个上限。这使得模型在处理这些 &amp;ldquo;超框&amp;rdquo;（Out-of-Window）任务时，不得不依赖分块处理等妥协方案，最终导致关键全局信息的丢失和端到端推理能力的降级。&lt;/p&gt;&lt;p&gt;如果这些场景让你倍感熟悉，那么问题很可能不在于你不够努力，而在于业界缺少一套完整、端到端的长文本推理后训练 &amp;ldquo;配方&amp;rdquo;（Post-training Recipe）。&lt;/p&gt;&lt;p&gt;针对这一系列挑战，&lt;strong&gt;通义文档智能团队正式推出 QwenLong-L1.5&lt;/strong&gt;&amp;mdash;&amp;mdash; 一个基于 Qwen3-30B-A3B 打造的长文本推理专家。我们的核心贡献，正是提供了这套缺失的 &amp;ldquo;配方&amp;rdquo;，它系统性地统一了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;可扩展的高质量数据合成管线&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;为长文本定制的强化学习方法&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;突破物理窗口的智能体架构&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这套组合拳，旨在一次性解决从 &amp;ldquo;学不好&amp;rdquo; 到 &amp;ldquo;用不了&amp;rdquo; 的全链路难题。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvcJAuicNUfkHNMb16rtyv3HOE1emYme6RGNgs3nuYZjAibeq2icb13qV3Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3148148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525549" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1d64cf1b-4d2e-402a-abd4-4d08251a6f9f/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;技术报告： https://huggingface.co/papers/2512.12967&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 仓库： https://github.com/Tongyi-Zhiwen/Qwen-Doc&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;深入拆解：我们的三大「法宝」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要让模型真正掌握长文本推理，零敲碎打的优化是远远不够的。我们提出了一套系统性的 &amp;ldquo;组合拳&amp;rdquo;，包含三大核心法宝，从根本上重塑模型的学习与思考方式。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;法宝一：高质量 &amp;ldquo;精神食粮&amp;rdquo; &amp;mdash;&amp;mdash; 多跳推理数据合成流水线&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;模型的 &amp;ldquo;食粮&amp;rdquo; 决定了它的 &amp;ldquo;智商&amp;rdquo;。如果只给模型投喂简单的 &amp;ldquo;大海捞针&amp;rdquo; 式任务，就如同只让学生做单选题，却期望他能写出长篇论述文。&lt;/p&gt;&lt;p&gt;为了教会模型真正的 &amp;ldquo;思考&amp;rdquo;，我们打造了一条新颖的数据合成流水线。其核心思想是 &amp;ldquo;先拆解，后组合&amp;rdquo;，专造需要 &amp;ldquo;多跳溯源 (multi-hop grounding) 和全局推理&amp;rdquo; 的难题。这就像用乐高积木拼城堡：我们先把一本巨著拆解成一个个知识 &amp;ldquo;积木&amp;rdquo;（原子事实），再根据复杂的 &amp;ldquo;图纸&amp;rdquo;（如知识图谱、多文档表格），把这些分布在不同章节的积木拼成一个宏伟的 &amp;ldquo;城堡&amp;rdquo;（复杂问题）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvqkiaibYPYPbvpqI6MiauRTY1mtibhZ9nXLYdlTL0KSwPPj9vrKKys681xw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.4648148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525542" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/27746823-7068-4ff6-aaeb-280c15f2db74/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这条流水线由&lt;strong&gt;三大 &amp;ldquo;出题引擎&amp;rdquo; 驱动&lt;/strong&gt;，能程序化地生成无穷无尽的高质量挑战：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;知识图谱引导 (KG-Guided)&lt;/strong&gt;： 自动挖掘文档间的深层逻辑链，生成环环相扣的多跳推理题，强制模型进行跨段落、跨文档的关联思考。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;跨文档表格引擎 (Cross-document Table Engine)&lt;/strong&gt;： 从多个非结构化文档中自动抽取出数据，整合成统一的结构化表格，据此生成需要聚合、统计与复杂计算的数值推理题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多智能体自我进化 (MASE)&lt;/strong&gt;： 设计一个由 &amp;ldquo;出题者&amp;rdquo;、&amp;ldquo;解题者&amp;rdquo;、&amp;ldquo;检验者&amp;rdquo; 组成的多智能体框架，基于无标签文档自动合成通用长文本任务，通过 &amp;ldquo;出题 - 解题 - 检验&amp;rdquo; 的循环，结合历史合成任务提升任务难度和广度。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;法宝二：稳定高效的 RL 优化策略&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;强化学习（RL）是提升模型推理能力的关键，但在长文本、多任务场景下，标准的 RL 方法会面临两大严峻挑战，极易导致训练崩溃。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第一个挑战源于数据分布的异构性&lt;/strong&gt;。我们的长文本训练数据来自代码、学术文献、财报等多个领域，任务类型也涵盖了问答、计算、分析等。这种复杂性导致在训练的每个批次（mini-batch）内，数据分布都会发生剧烈偏移（distributional drift）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvFNPaTK15mDQy9QZQLVSvowjfb1bZWZaMmgLmObouCjUoQX0Z7oz3lQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.69593147751606" data-s="300,640" data-type="png" data-w="934" type="block" data-imgfileid="503525544" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/365a85c5-077c-4bef-89ad-c0dc648030be/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这种偏移会严重干扰奖励信号（reward）的稳定性，并对优势函数（advantage function）的估计引入巨大噪声，使得梯度更新方向变得极不可靠。为解决此问题，我们采取了双重策略：&lt;/p&gt;&lt;p&gt;任务均衡采样（Task-balanced Sampling）： 在构建每个训练批次时，强制从不同的任务类型（如多跳推理、数值计算、对话记忆等）中均匀抽取样本，从源头上保证了批次内数据分布的相对均衡。&lt;/p&gt;&lt;p&gt;任务专属优势估计（Task-specific Advantage Estimation）： 在计算优势函数时，我们不再对整个批次的奖励进行标准化，而是在每个任务类型内部独立进行。这能有效隔离不同任务间迥异的奖励分布（如 0/1 的稀疏奖励与 0-1 的密集奖励），从而为每个任务提供更准确、更稳定的优势信号。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第二个挑战是长文本推理中的信用分配难题（Credit Assignment Problem）&lt;/strong&gt;。在生成式任务中，一个最终错误的答案（negative response）往往包含了大量完全正确的中间推理步骤。传统的 RL 算法通过一个单一的负向奖励来惩罚整个序列，这种 &amp;ldquo;一刀切&amp;rdquo; 的做法会错误地惩罚那些正确的、具有探索价值的步骤，不仅压制了模型的探索能力，甚至可能导致 &amp;ldquo;熵坍塌&amp;rdquo;（entropy collapse）和训练早停。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6Isv5Fuprtc9ibvaGib7HJD9iamoN602yribzACK3gN3lRXyCuvicVFbGRBInIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7156398104265402" data-s="300,640" data-type="png" data-w="844" type="block" data-imgfileid="503525545" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/aeb990ce-f4be-4bd0-8a34-709ee9a04949/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;为此，我们提出了&lt;strong&gt;自适应熵控制策略优化（Adaptive Entropy-Controlled Policy Optimization, AEPO）算法&lt;/strong&gt;。AEPO 的核心是一种基于模型自身不确定性（以策略熵衡量）的动态梯度屏蔽机制：&lt;/p&gt;&lt;p&gt;当模型在高不确定性（高熵）状态下生成了错误答案时，AEPO 会主动屏蔽（mask）其负向梯度。这保护了模型的探索性行为，避免因惩罚不成熟的尝试而丧失学习潜力。&lt;/p&gt;&lt;p&gt;反之，当模型在高置信度（低熵）状态下依然犯错时，负向梯度会被正常施加，以坚决纠正这些高置信度的错误。&lt;/p&gt;&lt;p&gt;通过这种动态的、智能的梯度控制，AEPO 将模型策略的熵稳定在一个健康的区间，完美平衡了探索与利用，从根本上解决了长文本 RL 中的不稳定性问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;法宝三：突破极限的 &amp;ldquo;外置大脑&amp;rdquo;&amp;mdash;&amp;mdash; 记忆管理框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;256K 的上下文窗口，本质上是一种有限的 &amp;ldquo;短期记忆&amp;rdquo;。当面对浩如烟海的真实世界知识流时，我们需要的不是一个更大的窗口，而是一个全新的工作模式。&lt;/p&gt;&lt;p&gt;为此，我们为模型设计了一套&lt;strong&gt;记忆管理框架 (Memory Management Framework)&lt;/strong&gt;，这相当于给了它一个可无限扩展的 &amp;ldquo;智能笔记本&amp;rdquo;。在阅读超长文档时，模型不再试图将所有内容硬塞进 &amp;ldquo;短期记忆&amp;rdquo;，而是学会了边读边记要点（迭代式记忆更新），形成结构化的记忆，并在需要时高效检索和利用这些 &amp;ldquo;笔记&amp;rdquo;。&lt;/p&gt;&lt;p&gt;但这并非一个孤立的工具。通过巧妙的多阶段融合 RL 训练 (multi-stage fusion RL training)，我们将这种 &amp;ldquo;笔记能力&amp;rdquo; 与模型与生俱来的 &amp;ldquo;过目不忘&amp;rdquo;（窗口内推理）能力无缝地融合在了一起。最终得到的，是一个统一的模型 &amp;mdash;&amp;mdash; 一个既能 &amp;ldquo;深思&amp;rdquo; 又能 &amp;ldquo;博览&amp;rdquo; 的全能选手，真正突破了物理窗口的束缚。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;效果展示&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;性能全面飞跃，30B moe 模型实现媲美顶级旗舰的效果！&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvxRcwqyaLBM5HYbxNfWmqZHmBFaSFNDp6hQgqYDreVqtPyOd1iajn6sw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.537962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525546" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/789d0568-fedd-4774-a577-b9225004e9b3/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;QwenLong-L1.5 在多个权威长文本推理基准上取得了令人瞩目的成绩，其表现可以总结为：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;整体性能飞跃&lt;/strong&gt;： 相比基线模型 Qwen3-30B-A3B-Thinking，QwenLong-L1.5 的平均分&lt;strong&gt;暴涨 9.9 分&lt;/strong&gt;！这证明了我们全套后训练 &amp;ldquo;配方&amp;rdquo; 的巨大成功。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;比肩顶级旗舰&lt;/strong&gt;： 在多个权威长文本榜单上，我们的 30B-A3B 模型取得了与 GPT-5、Gemini-2.5-Pro 等业界顶级闭源模型相媲美的性能，展现了极强的竞争力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;精准的能力跃升&lt;/strong&gt;： 更值得注意的是，我们的性能提升精准地体现在了最能考验深度推理能力的复杂任务上。在需要多跳推理和全局信息整合的 MRCR、CorpusQA 和 LongBench-V2 等基准上，我们分别取得了 &lt;strong&gt;+31.72、+9.69 和 +6.16 的性能增长&lt;/strong&gt;！&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这并非巧合，而是精准地验证了我们 &amp;ldquo;高质量精神食粮&amp;rdquo;（可编程数据合成）的有效性 &amp;mdash;&amp;mdash; 我们专门为模型打造了什么样的难题，它就在解决这些难题上获得了最强的能力！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;意外之喜：通用能力不降反升！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;训练 &amp;ldquo;专才&amp;rdquo; 是否会牺牲 &amp;ldquo;通才&amp;rdquo; 能力？这是大模型微调中常见的 &amp;ldquo;跷跷板&amp;rdquo; 难题。&lt;/p&gt;&lt;p&gt;我们的答案是：不仅不会，反而会相互促进！&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvcFX5nL3LtjTic8B1NAjZRwQgiaKoK86QxdLILFp1EqYstB3LtnCDI2wg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.9168399168399168" data-s="300,640" data-type="png" data-w="962" type="block" data-imgfileid="503525547" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/71a3cbe5-8b47-4e97-a222-6bd25bd70f70/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;实验结果显示，经过长文本强化训练后，QwenLong-L1.5 不仅没有出现 &amp;ldquo;偏科&amp;rdquo; 或 &amp;ldquo;遗忘&amp;rdquo;，反而在一系列通用能力上也获得了显著提升：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在数学推理 (AIME25) 任务上表现更优；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在智能体记忆 (BFCL) 任务中展现出更强的状态追踪能力；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在长对话 (LongMemEval) 场景下，记忆和理解能力大幅增强。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这有力地证明了，提升长程信息整合能力，是一种基础性的 &amp;ldquo;认知升级&amp;rdquo;，其收益会辐射到模型的各项核心能力之中。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战极限：征服 1M~4M Token 超长文本！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当任务长度远超物理上下文窗口时，模型真正的扩展能力才得以体现。&lt;/p&gt;&lt;p&gt;借助我们的 &amp;ldquo;外置大脑&amp;rdquo;（记忆管理框架），QwenLong-L1.5 在处理百万、甚至四百万级别的超长任务时，展现出了卓越的性能。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvDdYTg8NMJKHCCWwPAIrOnJ9Ts04d3dfWecVqRJ50BJKREOzqe3GYgw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.4546296296296296" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525548" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/65363329-84d5-4f4a-8f89-50e742179811/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;结果显示，QwenLong-L1.5 在这些极限挑战中，性能远超同类智能体方法，充分验证了我们框架强大的可扩展性。这表明，我们不仅提升了模型在窗口内的能力，更赋予了它突破物理窗口限制、处理无限信息流的巨大潜力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;：我们提出的 QwenLong-L1.5 及其背后的 &amp;ldquo;数据合成 + RL 优化 + 记忆管理&amp;rdquo; 三位一体的后训练框架，为解决大模型长文本推理难题提供了一条经过验证的、可复现的路径。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;开源呼吁&lt;/strong&gt;：我们相信开放与共享的力量。相关技术细节已在论文中公布，代码也在 https://github.com/Tongyi-Zhiwen/Qwen-Doc 开源。欢迎大家下载使用、交流探讨，共同推动长文本技术的发展！&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AAAI 2026 Oral｜LENS：基于统一强化推理的分割大模型</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 13:37:24 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-3</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg&amp;wxfrom=5&amp;wx_lazy=1&amp;tp=wxpic#imgIndex=0" alt="图片" data-ratio="0.5703703703703704" data-w="1080" data-original-style="height: auto !important;visibility: visible !important;width: 661px !important;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/10a42059-55bf-4f1c-b9b4-f8eef90fc226/640.png" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;文本提示图像分割（Text-prompted image segmentation）是实现精细化视觉理解的关键技术，在人机交互、具身智能及机器人等前沿领域具有重大的战略意义。这项技术使机器能够根据自然语言指令，在复杂的视觉场景中定位并分割出任意目标。&lt;/p&gt;&lt;p&gt;然而，当前主流的技术路径，如基于监督式微调（Supervised Fine-Tuning, SFT）的方法，正面临着根本性的瓶颈。这些方法本质上是静态的模式匹配，虽然在特定数据集上表现优异，但其泛化能力往往受限，形成了一个难以逾越的 &amp;ldquo;能力天花板&amp;rdquo;。尤其是在处理需要多步、复杂推理的未知指令时，性能会显著下降，其根源在于 SFT 方法在训练中忽略了动态的、显式的推理过程。&lt;/p&gt;&lt;p&gt;为了 shatter 这一能力天花板，我们引入了 LENS（Learning to Segment Anything with Unified Reinforced Reasoning）框架。LENS 摒弃了静态的 SFT，转而采用端到端的强化学习（Reinforcement Learning, RL）机制，将高层次的 &amp;ldquo;思考&amp;rdquo; 过程（即思维链推理）与像素级的 &amp;ldquo;执行&amp;rdquo; 过程（即图像分割）进行动态的联合优化。通过这种设计，LENS 旨在赋予分割模型真正的、上下文感知的推理能力，从而在根本上提升其在复杂任务中的鲁棒性和泛化性。&lt;/p&gt;&lt;p&gt;本文将深入介绍一下我们 AAAI 荣获 Oral 的工作，&amp;ldquo;会思考的分割大模型 LENS&amp;rdquo;。有幸在这次 AAAI 2026 得到了审稿人们一致正面的评价，并被 AC 和 PC 一致同意推荐为 Oral 论文。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZTOULeibJSu4mkiaFwAXGe2otZvv2sYRLHpO39xfACRicPA8zOW4h3Vl0w/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.18796296296296297" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525284" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/72f06963-ed99-450b-98c8-30073eb4d428/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：LENS: Learning to Segment Anything with Unified Reinforced Reasoning &amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2508.14153 &amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：https://github.com/hustvl/LENS&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;LENS 框架概览：推理与分割的协同进化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这个工作中，我们研究了分割大模型领域的一大一小两个关键问题，大问题就是老生常谈的 &amp;ldquo;泛化能力&amp;rdquo;，传统分割大模型对未见过的提示和领域的泛化能力往往有限；小问题则是隐藏的 &amp;ldquo;信息瓶颈&amp;rdquo;，此前的分割大模型从 &amp;ldquo;大脑思考&amp;rdquo;（MLLM）到 &amp;ldquo;分割解码&amp;rdquo;（SAM）之间往往只通过单一的分割 Token 传递信息，存在隐形的 &amp;ldquo;信息输送瓶颈&amp;rdquo;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZ0OBwEic5ey0xJcPKd1PxH0mia3njjUrzCSIuD8IGKPopGnsx4vic5F6QA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="0.8740740740740741" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525285" data-original-style="width: 447px;height: 391px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/2257cfe5-53e5-4a4d-b356-4477ee543d7e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;LENS 框架的核心设计在于通过端到端的联合优化，彻底打破传统模型中 &amp;ldquo;思考&amp;rdquo;（推理）与 &amp;ldquo;执行&amp;rdquo;（分割）之间的信息壁垒。&lt;/p&gt;&lt;p&gt;以往的方法，例如同期的优秀工作 Seg-Zero，采用的是非端到端的设计，即先由推理模型生成边界框和点提示，再交由现成的（off-the-shelf）SAM 进行分割。这种分离式流程的主要缺陷在于误差的单向传播。这意味着像 Seg-Zero 这样的非端到端模型是根本上脆弱的；它们的性能上限被其初始猜测的准确性所锁定。一旦推理阶段的定位出现偏差，下游的分割模型将无法纠正，最终必然导致分割失败。相比之下，LENS 通过其端到端的反馈闭环，具备了即便从不完美的初步定位中也能自我纠正的能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZT2ialTkwibUWzhbfFRNsWC2iaBTomLEkpzr65G3kq7x9QenLfcEn2LRSQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.7046296296296296" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525286" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f2651078-54a5-4fa2-9deb-85c4339b8d94/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;LENS 的整体架构由三大核心组件构成，它们协同工作，实现了从高级语义理解到精确像素输出的无缝衔接：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多模态大语言模型 (MLLM) - 推理核心&lt;/strong&gt;：作为系统的 &amp;ldquo;大脑&amp;rdquo;，LENS 采用先进的 MLLM（如 Qwen2.5-VL-3B-Instruct）来处理输入的图像和文本指令。它负责生成详尽的思维链（Chain-of-Thought, CoT）推理过程，并给出一个初步的目标边界框。这一过程不仅是定位，更是对指令的深度理解。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们将系统提示 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZ53Y3r7adoEIYVAKj6FXIh3lgibwOicuNpYGDSiaNEmmBfc96DWe41Xhdw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-ratio="0.55" data-s="300,640" data-type="jpeg" data-w="60" type="block" data-imgfileid="503525287" data-original-style="width: 46px;height: 25px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/1e54901f-c9d3-4daf-b470-5a10ac3d32b7/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 4.96%;"&gt;、文本指令 T 以及图像 I 输入到 MLLM 推理模型 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZB5g2S1SKanesdcqcGlDsrHCeDVBBIibEgVYf1ibtn82OH6TTWJG6DhEA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.40476190476190477" data-s="300,640" data-type="jpeg" data-w="84" type="block" data-imgfileid="503525288" data-original-style="width: 70px;height: 28px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/db99510f-51e3-4c09-b369-144cf4faa72d/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 8.43%;"&gt; 中，得到思维链（COT） &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZRicOCBmaKyFVZ0XQD01XYTBRzOic4TTow3Xwo7sF4aDt7SnYEevDf3EQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=6" data-ratio="0.5753424657534246" data-s="300,640" data-type="jpeg" data-w="73" type="block" data-imgfileid="503525290" data-original-style="width: 49px;height: 28px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/213ce913-870e-48ad-a348-75caa508dab6/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 5.9%;"&gt; 和边界框预测 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZoic9kFfj5SIrDkJoVagjQAor9558iaJVUSC6xKEsRaicxjeGWY3Pic0b1A/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-ratio="0.5909090909090909" data-s="300,640" data-type="jpeg" data-w="66" type="block" data-imgfileid="503525292" data-original-style="width: 40px;height: 24px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/6cdb25bc-f453-44ad-8d81-1f5ec93a31ef/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dii" style="width: 5.17%;"&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZwNFYxfCs18Xagse22R4UfDErN3QUf4JaaJMPZQH8tBV1loXtuibgXicg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=8" data-ratio="0.0920353982300885" data-s="300,640" data-type="jpeg" data-w="565" type="block" data-imgfileid="503525304" data-original-style="width: 368px;height: 34px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/1156aa31-51eb-4ea7-90bb-5275073f7e01/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;上下文模块 (Context Module) - 信息桥梁&lt;/strong&gt;：这是 LENS 的关键创新，它充当了 MLLM 和分割模型之间的信息高速公路。该模块由一组可学习的上下文查询（Context Queries）和一个连接器（Connector）组成，其任务是将 MLLM 生成的丰富推理轨迹和定位信息，转化为分割模型能够高效利用的、信息密集的分割提示。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们随机初始化上下文查询 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZicSRibOyoetlYq0HHicephO0zlh5w4Z3UYibRbM7ic7XPjWriciaynib2rlsEQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=9" data-ratio="0.2792207792207792" data-s="300,640" data-type="jpeg" data-w="154" type="block" data-imgfileid="503525307" data-original-style="width: 106px;height: 30px;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e3b7e6b8-0488-454b-974f-cd86038a9d61/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dii" style="width: 11.18%;"&gt;，其中 M 表示上下文查询的数量，C 表示多模态大语言模型（MLLM）的隐藏维度。 需要注意的是，上下文查询被追加到输入序列和生成序列的末尾，并通过一次前向传播来汇聚信息。随后，我们将 MLLM 输出的上下文查询嵌入 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZ3HGJjrYAEnTLWxQCqTJM0G4y2tnCb9ib8PzOrQBibvJQnacQiaxPywXXA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=10" data-ratio="0.24848484848484848" data-s="300,640" data-type="jpeg" data-w="165" type="block" data-imgfileid="503525309" data-original-style="width: 118px;height: 29px;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/46c3f2b9-10f3-4a2c-88c3-d56058c05445/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dii" style="width: 12.34%;"&gt; 输入到连接器（connector）中。该连接器是一个浅层 Transformer，用于将上下文查询嵌入投影到 SAM 的提示空间中，即 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZwd1HNm3nbFAoH2VfOweoxvNLGt9SLU0g6bqj2UIU02VudFEpAmkZyA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=11" data-ratio="0.6190476190476191" data-s="300,640" data-type="jpeg" data-w="63" type="block" data-imgfileid="503525312" data-original-style="width: 45px;height: 28px;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/b134d38b-62a4-4f43-895e-23c9c89978cf/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dii" style="width: 4.74%;"&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZpgN191w1Su3OLTIYRlIHkoibxUqnJDyPSiaD0xz6KicQyNlByDsnXswRA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=12" data-ratio="0.1312" data-s="300,640" data-type="jpeg" data-w="625" type="block" data-imgfileid="503525316" data-original-style="width: 420px;height: 55px;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/3ce203cb-9196-4f25-adda-5263a1554c22/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;分割模型 (SAM-2) - 像素解码器：作为系统的 &amp;ldquo;双手&amp;rdquo;，LENS 采用高性能的分割模型（SAM2-Large）。它接收来自上下文模块的复杂指令，并结合原始图像信息，执行精准的像素级掩码生成任务，将推理结果精确地体现在图像上。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZdznl9k7fqOl9aBz82suI72yH447hSuP7uOl5dKzPgqpibP9m6frqALQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=13" data-ratio="0.17132867132867133" data-s="300,640" data-type="jpeg" data-w="286" type="block" data-imgfileid="503525325" data-original-style="width: 206px;height: 35px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/d0e7dde5-1734-49bc-b435-a265f4dfba82/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 40%;"&gt;&lt;/section&gt;&lt;p&gt;通过这种 &amp;ldquo;推理 - 桥接 - 分割&amp;rdquo; 三位一体的紧密耦合架构，LENS 实现了推理质量和分割精度的同步提升。这种设计使得最终的分割性能可以直接反作用于推理过程的优化，形成一个完整的闭环，为实现更高水平的通用分割能力奠定了基础。&lt;/p&gt;&lt;p&gt;LENS 框架同时在 &amp;ldquo;思考推理&amp;rdquo; 端也做出了改进，我们基于 Group Relative Policy Optimization（GRPO）方法构建了统一强化学习奖励机制（Unified Rewards Scheme）。该奖励机制是多维度的，同时监督以下三个层级的线索：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;格式奖励（Format Reward）&lt;/strong&gt;：确保 MLLM 的输出（包括推理过程和定位结果）遵循预期的结构和格式一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;边界框 IoU 奖励（Box IoU Reward）&lt;/strong&gt;：衡量预测边界框与真实边界框之间的定位准确性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;分割掩码 IoU 奖励（Segment IoU Reward）&lt;/strong&gt;：评估像素级分割掩码的质量。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZuiaMP9b7XG9d30PibzZBCIcettLkdnW9V8uHSs1c94Ee3tDsUglSJtPA/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.3861111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525327" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/fe95fdef-a71c-4b53-9cf2-d6d5fcbb5445/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;通过我们提出的联合优化（将统一的 GRPO 目标与监督分割损失相结合），LENS 能够从奖励驱动的推理改进和直接的分割监督中同时受益。值得一提的是，LENS 的端到端特性解决了定位错误（Grounding Error）向下游传播的问题，如上图右一右二所示，哪怕有些情况定位框是错的，强大的上下文查询（Context Query）也能带领分割模型走向正确。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;性能评估与分析&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZGZNjGJCA1RyczgPjjuDNGvn4ZnxqXNRcvft1nPibkLRqaM8FytVELIA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5101851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525328" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/493d5b6d-9219-4a8b-87a0-892dfd770d60/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;核心结果方面，LENS 取得了文本提示分割任务的最先进性能（SoTA）：LENS 在 RefCOCO 系列的基准测试中取得了 81.2% 的平均 cIoU，达到了世界最高水平。在 GroundingSuite-Eval 这类更具挑战性的零样本基准测试中，LENS 展现出卓越的域外泛化能力，cIoU 达到 78.3%，超越第二优方法接近 10%。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW98iacuWAiaBTm2Iq243SoVCZfRqIdibkvMj0oZVbFLNbFhiasLrUSwFrukporiaBBEqsRL2mpqcYCuObg/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.3037037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525331" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/8c757852-24ff-4308-96d1-886f98d972b6/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这些成果表明，LENS 这一类基于统一强化学习奖励驱动的 CoT 推理方法，能够显著提升文本提示下的分割能力。我们相信，LENS 为强化学习与视觉分割的无缝集成提供了新的思路，并有望推动更通用、更稳健的视觉 - 语言系统的研究。代码和预训练权重已开源（https://github.com/hustvl/LENS），感兴趣的朋友们欢迎研究和使用。我们也期待在 AAAI 2026 与学术界同行进行深入交流。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
