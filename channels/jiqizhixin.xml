<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>估值35亿美元，LeCun创业公司官宣核心方向，掀起对Next-token范式的「叛变」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:42:56 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;自从图灵奖得主 Yann LeCun 离开 Meta 创立 AMI Labs（Advanced Machine Intelligence） 以来，这家新公司便引发了业界的高度关注。本周，他们终于确认了核心方向：&lt;strong&gt;开发所谓的「世界模型（world models）」，以此构建能够理解现实世界的智能系统。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5gwEYbRp4CHArBHMOYTmZEhdNAziav39lmsHb9cKic6ia7wLn6H83VEibBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.38425925925925924" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529990" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/126849b0-0f47-4681-8a04-2a8f752e494f/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 官网地址：https://amilabs.xyz/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;一直以来，LeCun 都对现有大语言模型的发展持怀疑态度，认为仅靠预测下一个 token 的生成式模型无法真正做到理解现实世界。他提出了&lt;strong&gt;世界模型这一不同路径&lt;/strong&gt;，一种能够准确反映现实动态的新型人工智能架构。这类全新的智能系统，应同时具备四项关键能力：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;理解真实世界；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;拥有持久记忆；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;能够进行推理与规划；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;可控且安全。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一愿景背后，直指当前大模型路线的一个核心局限。&lt;/p&gt;&lt;p&gt;现实世界的数据主要来自摄像头与各类传感器，其特征是连续、高维且充满噪声。过去几年中，基于自监督学习、以预测未来为目标的生成式模型，在语言领域取得巨大成功。然而，当这一路径被直接套用到真实世界的感知数据上时，却会遭遇根本性挑战，因为现实世界中，大量细节本身就是不可预测的。&lt;/p&gt;&lt;p&gt;AMI Labs 给出的答案是：&lt;strong&gt;不再执着于逐像素生成现实，而是构建世界模型&lt;/strong&gt;，让模型学会对真实世界传感器数据进行抽象建模，过滤掉不可预测的噪声信息，并在更高层次的表征空间中进行预测与推理。&lt;/p&gt;&lt;p&gt;在此基础上，AMI Labs 进一步提出带动作条件的世界模型（action-conditioned world models）。这种模型能够预估智能体采取某个行动后可能带来的结果，并在安全约束之内规划行动序列，从而完成具体任务。&lt;/p&gt;&lt;p&gt;这也意味着，其目标不只是理解世界，而是让 AI 能够在真实世界中可靠地行动。&lt;/p&gt;&lt;p&gt;因此，AMI Labs 的应用方向也高度聚焦在对可靠性、可控性和安全性要求极高的领域，包括工业流程控制、自动化系统、可穿戴设备、机器人与医疗健康等场景。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn522LKFzEVwJ4c3v5U7e9PszYVxwEnZHG4cN53ibYAjjkibHYTNaz1Kc5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5722222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529991" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e727b0dc-2e44-485c-9453-a50ed9c7eb4c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;值得一提的是，在业界另一条技术路线中，LeCun 也开始发挥更广泛的影响力。近日，硅谷初创公司 Logical Intelligence 任命 Yann LeCun 为其技术研究委员会创始主席。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5glYicB6HiaV9EpnMVjq6icQbrgicsAicrOicb0wfibKf3a6rMQIJwoAWB4PjQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.2712962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529992" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/775e29c1-6ebc-4b51-a95b-6825ea15ee71/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;近日，该公司推出了一款名为 Kona 的能量 - 推理模型，并宣称其性能比 OpenAI 的 GPT-5 和谷歌的 Gemini 等大语言模型更准确，功耗也更低。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Kona 走的技术路线也是不同于 next token 预测范式，而是一种基于能量的推理模型（EBRM）&lt;/strong&gt;，它通过根据约束条件进行评分来验证和优化解决方案，从而找到能量最低（最一致）的结果。&lt;/p&gt;&lt;p&gt;有意思的是，这一路线与 Yann LeCun 长期以来倡导的思路高度一致。LeCun 多次批评大语言模型依赖 next-token 预测的方式本质上是在猜答案，而真正的智能应建立在目标驱动与能量最小化的机制之上，让模型在约束与物理一致性的框架中寻找最优解。&lt;/p&gt;&lt;p&gt;无论是 Logical Intelligence 的能量推理模型，还是 AMI Labs 正在推进的世界模型，本质上都指向同一个方向：跳出语言生成范式，转向能够理解、预测并作用于真实世界的智能系统。&lt;/p&gt;&lt;p&gt;而这背后，或许正是 LeCun 选择离开 Meta 的重要原因之一。&lt;/p&gt;&lt;p&gt;在他看来，如今整个 AI 行业几乎被大语言模型所占据。在硅谷，所有公司都在做同一件事，挖同一条战壕，彼此争夺工程师，却很少有人敢真正走一条不同的路，因为一旦偏离主流方向，就可能在竞争中落后。&lt;/p&gt;&lt;p&gt;Meta 同样选择了全面押注大语言模型。这或许是一个合理的商业决策，但并不是 LeCun 所感兴趣的研究方向。&lt;/p&gt;&lt;p&gt;在他看来，如果一个系统无法提前预测自身行为可能带来的后果，就无法构建真正的智能体系统（agentic systems）。&lt;/p&gt;&lt;p&gt;正如人类在现实世界中行动，是因为能够预见行为的结果，并据此进行规划与决策，这种能力，才是智能的基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;顶尖科学家入局，「世界模型」赛道吸引 VC 争抢&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;构建连接 AI 与现实世界的基础模型，已经成为当下 AI 领域最令人兴奋的探索方向之一。无论是否已经拥有成熟产品，这一赛道都正在吸引顶尖科学家和资金雄厚的投资者持续加码。&lt;/p&gt;&lt;p&gt;比如由 AI 先驱李飞飞创立的 World Labs，在结束隐形融资阶段后不久便跻身「独角兽」行列，而在其推出首款产品 Marble&amp;mdash;&amp;mdash; 能够生成符合物理规律的 3D 世界后，更是估值飙升，成为资本新宠。&lt;strong&gt;据报道，目前该公司正在洽谈新一轮融资，估值达 50 亿美元。 &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;毫无疑问，风投机构（VC）也同样渴望投资 LeCun，这也进一步佐证了前段时间传出的，关于 &lt;strong&gt;AMI Labs 可能正以 35 亿美元估值进行融资&lt;/strong&gt;传言的可信度。&lt;/p&gt;&lt;p&gt;据报道，正在与这家初创公司洽谈的风投机构包括 Cathay Innovation、Greycroft，以及 LeCun 担任顾问的 Hiro Capital。此外，其他潜在投资者还包括 20VC、Bpifrance、Daphni 和 HV Capital。&lt;/p&gt;&lt;p&gt;其实无论谁出资，投资者或许需要注意一个重要细节：正如 LeCun 所明确指出的，他是 AMI 的执行董事长，而非首席执行官（CEO）。实际上，这一职位是由 Alex LeBrun 担任，他此前是 Nabla 的联合创始人兼首席执行官，而 Nabla 是一家在巴黎和纽约均设有办事处的医疗 AI 初创公司。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5mPnw7CGRVRLo1jnAO8T0fvticcSafFneK4MPtFpKWoe7yNw2HnyPvSQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9916666666666667" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529995" data-aistatus="1" data-original-style="width:437px;height:433px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/1e366986-b93d-48a9-8183-07b133f54dbf/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;Alex LeBrun 之所以从 Nabla 转任 AMI，源于 Nabla 去年 12 月宣布的一项合作项目。据悉，Nabla 致力于开发用于临床护理的 AI 助手，而 LeCun 一直是其顾问。因此，作为交换，Nabla 获得了对 AMI 世界模型的「优先访问权」，其董事会也支持 Alex LeBrun 从 CEO 转任首席 AI 科学家兼董事长，从而为他在 AMI 担任新职务铺平了道路。&lt;/p&gt;&lt;p&gt;作为 AMI Labs 的 CEO，LeBrun 身边有着许多熟悉面孔。此前，他创办的上一家公司 Wit.ai 被 Meta 收购后，他便在 LeCun 的领导下在 Meta 的 AI 研究实验室 FAIR 工作。此外，据传去年 12 月卸任 Meta 欧洲区副总裁一职的 Laurent Solly 也将加入 AMI Labs。&lt;/p&gt;&lt;p&gt;其实，AMI Labs 与 Meta 之间的人才重叠可能远不止于此。&lt;strong&gt;据 LeCun 透露，前雇主 Meta 很可能成为 AMI 的首个客户&lt;/strong&gt;。不过，他也曾公开批评过 Meta 在 Mark Zuckerberg 领导下所做出的部分战略选择。&lt;/p&gt;&lt;p&gt;甚至，从更广泛的角度来讲，在外界看来，&lt;strong&gt;AMI Labs 是对大语言模型 (LLM) 的一种逆向投资&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在 LeCun 看来，LLM 的局限性包括幻觉等问题，而这在医疗等领域是一个严重隐忧，LeBrun 对此也深有体会，他曾在接受媒体采访时提到，他接受这份工作的一个重要原因是希望将世界模型应用于医疗健康领域。&lt;/p&gt;&lt;p&gt;「医疗健康是我的心血之作，我们也清楚目前哪些问题我们无法解决，我们希望 AI 的这一新兴分支能够帮助我们在医疗健康领域超越目前的局限。」&lt;/p&gt;&lt;p&gt;不过，这家初创公司也将目光投向了其他高风险的应用领域。&lt;/p&gt;&lt;p&gt;正如官网所宣称的那样：「AMI Labs 将致力于推进 AI 研究，并开发那些对可靠性、可控性和安全性要求极高的应用，特别是在工业过程控制、自动化、可穿戴设备、机器人技术、医疗健康以及更多领域。」&lt;/p&gt;&lt;p&gt;该初创公司计划将其技术授权给行业合作伙伴以用于实际应用，但同时也表示，计划「通过公开出版物和开源项目，与全球学术研究界」共同构建 AI 的未来。&lt;/p&gt;&lt;p&gt;LeCun 表示，他计划保留在纽约大学（NYU）的教授职位，他目前在那里每年教授一门课程，并指导博士生和博士后研究员。&lt;/p&gt;&lt;p&gt;这意味着这位出生于法国的研究人员将继续常驻纽约，但他也透露，AMI Labs「将成为一家总部位于巴黎的全球性公司」。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://techcrunch.com/2026/01/23/whos-behind-ami-labs-yann-lecuns-world-model-startup/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/rohanpaul_ai/status/2014731323638997011&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>挑战Claude Code？OpenAI Codex发布月将至，今先揭秘智能体循环</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:38:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Panda&lt;/section&gt;&lt;p&gt;刚刚，OpenAI CEO 山姆・奥特曼发了一条推文：「从下周开始的接下来一个月，我们将会发布很多与 Codex 相关的激动人心的东西。」他尤其强调了网络安全这个主题。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529974" data-ratio="1.6698872785829308" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5wChp1spqkWWBibGbV14uUvnLwvVhnYksCbAicaT0MkKFp0WY3CiapqgrA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="621" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/cd501859-163b-4ecb-bc82-bd35103a9ec9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;当然，和奥特曼的很多推文一样，这条推文也收获了网友的各式各样的评论：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5krPSZxjGL9XYBaukh1tVcyGbUqoOsfTsWX3MOux8C1TriaYznz0ycmQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.2866779089376054" data-s="300,640" data-type="png" data-w="593" type="block" data-imgfileid="503529973" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/ff550d55-54cf-4989-b168-c4db8d88896f/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5fvXibjZYtUwtGxFndqJdjkTADYh73NoPg7MVTqu6JSKick6ThmG3gdtw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.2505592841163311" data-s="300,640" data-type="png" data-w="447" type="block" data-imgfileid="503529972" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/0a2ce45a-7bcc-4392-843c-cc0ac8103881/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5jv66O86b3AkbQZ6sdgzXgYYDIPy6wOjR48AWwSUb7t6Dq3VzkMYvpQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.21442495126705652" data-s="300,640" data-type="png" data-w="513" type="block" data-imgfileid="503529975" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/2e9de98a-7a46-4aa1-b236-05a60c9d7453/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;似乎是响应奥特曼的 Codex 发布预告，OpenAI 官方也发布了一篇技术博客，以「&lt;strong&gt;揭秘 Codex 智能体循环&lt;/strong&gt;」为题，深入揭秘了 Codex CLI 的核心架构 &amp;mdash;&amp;mdash; 智能体循环（Agent Loop）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5atF8ZEa9IepHyqe7OwScZkxfwhmdPVQOCibozsN8cXuUI44PKSprtHQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.2238648363252376" data-s="300,640" data-type="png" data-w="947" type="block" data-imgfileid="503529976" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/b95eb7ce-c7f5-4202-a382-cacf88ec2f97/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;博客地址：https://openai.com/index/unrolling-the-codex-agent-loop/&lt;/p&gt;&lt;p&gt;具体来说，其中详细介绍了它如何通过 Responses API 协调用户指令、模型推理与本地工具执行（如 Shell 命令），并重点阐述了通过保持「提示词前缀一致」来触发缓存优化性能，以及利用自动压缩技术管理上下文窗口，从而在保证数据隐私（ZDR）的前提下，实现安全、高效的自动化软件开发。&lt;/p&gt;&lt;p&gt;下面我们就来详细看看这篇博客的内容。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;揭秘 Codex 智能体循环&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Codex CLI 是 OpenAI 的跨平台本地软件智能体，可以生成相当高质量的软件变更。&lt;/p&gt;&lt;p&gt;OpenAI 表示：「自今年 4 月首次发布 CLI 以来，我们在构建世界级软件智能体方面积累了大量经验。」&lt;/p&gt;&lt;p&gt;为了分享这些见解，OpenAI 推出了这个系列博客，本文即是第一篇。&lt;/p&gt;&lt;p&gt;在这个系列中，OpenAI 将探讨 Codex 的工作原理以及那些来之不易的教训。（如果您想更深入地了解 Codex CLI 的构建细节，请查看 OpenAI 的开源仓库：。OpenAI 的许多设计决策细节都记录在 GitHub 的 Issue 和 Pull Request 中。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5Dbm37j5X2icuwXib98l2JtaZ1sGLXeYAvMWN5xibDocdDDgmjpoB4hVCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5083333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529977" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/68388fdd-9a02-4f67-9fed-b870abc22811/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;仓库地址：https://github.com/openai/codex&lt;/p&gt;&lt;p&gt;第一篇，OpenAI 将聚焦于&lt;strong&gt;智能体循环（Agent Loop）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这是 Codex CLI 的核心逻辑，负责协调用户、模型以及模型为执行软件任务而调用的工具之间的交互。&lt;/p&gt;&lt;p&gt;OpenAI 表示：「我们希望这篇文章能让您清晰地看到 OpenAI 的智能体（harness）在利用 LLM 时所扮演的角色。&lt;/p&gt;&lt;p&gt;在开始之前，先简要说明一下术语：在 OpenAI，「Codex」涵盖了一系列软件智能体产品，包括 Codex CLI、Codex Cloud 和 Codex VS Code 扩展。本文重点讨论 Codex Harness，它提供了支持所有 Codex 体验的核心智能体循环和执行逻辑，并通过 Codex CLI 呈现。为了方便起见，下文中将交替使用「Codex」和「Codex CLI」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智能体循环&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个 AI 智能体的核心都是所谓的「智能体循环」。智能体循环的简化图示如下：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5dwe7xwg5V3bnKGT00t7NXmbOGicGVibOmWtW9yBb5PwgibVL5LHB7fm4Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.46140939597315433" data-s="300,640" data-type="png" data-w="596" type="block" data-imgfileid="503529978" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/958368e5-c2df-44ec-8945-e08ffcfb5ae9/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;：智能体获取用户的输入，并将其整合到为模型准备的一组文本指令中，这被称为提示词。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推理（Inference）&lt;/strong&gt;：下一步是查询模型。OpenAI 将指令发送给模型并请求其生成回复。在推理过程中，文本提示词首先被转化为一系列输入 Token（映射到模型词汇表的整数）。这些 Token 被用来对模型进行采样，生成新的输出 Token 序列。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;解码&lt;/strong&gt;：输出 Token 被转换回文本，成为模型的回复。由于 Token 是增量生成的，这种转换可以随着模型的运行同步进行，这就是为什么许多 LLM 应用会显示流式输出。在实践中，推理通常被封装在处理文本的 API 之后，隐藏了 Token 化的细节。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;决策&lt;/strong&gt;：作为推理步骤的结果，模型要么 (1) 针对用户的原始输入生成最终回复，要么 (2) 请求 工具调用（Tool Call）（例如，「运行 ls 并报告输出」）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;执行与重试&lt;/strong&gt;：在情况 (2) 下，智能体执行工具调用并将输出附加到原始提示词中。该输出用于生成新的输入以重新查询模型；智能体随后可以考虑这些新信息并再次尝试。&lt;/p&gt;&lt;p&gt;这个过程会一直重复，直到模型停止发出工具调用，而是为用户生成一条消息（在 OpenAI 模型中称为助手消息 / Assistant Message）。在许多情况下，这条消息会直接回答用户的原始请求，但也可能是对用户的一个后续提问。&lt;/p&gt;&lt;p&gt;由于智能体可以执行修改本地环境的工具调用，其「输出」并不局限于助手消息。在很多情况下，软件智能体的主要输出是它在您机器上编写或编辑的代码。尽管如此，每个轮次（Turn）总是以助手消息结束（例如「我已经添加了你要求的 architecture.md」），这标志着智能体循环的终止状态。从智能体的角度来看，它的工作已经完成，控制权交还给用户。&lt;/p&gt;&lt;p&gt;图中所示的从「用户输入」到「智能体回复」的过程被称为对话的一个轮次（Turn）（在 Codex 中称为 Thread）。一个对话轮次可以包含模型推理和工具调用之间的多次迭代。每当您向现有对话发送新消息时，对话历史记录都会作为新轮次提示词的一部分，其中包括之前轮次的消息和工具调用。&lt;/p&gt;&lt;p&gt;这意味着随着对话的进行，用于模型采样的提示词长度也会增加。&lt;/p&gt;&lt;p&gt;长度非常重要，因为每个模型都有上下文窗口，即单次推理调用中可以使用的最大 Token 数（包括输入和输出）。你可以想象，智能体在一个轮次中可能会决定进行数百次工具调用，从而耗尽上下文窗口。&lt;/p&gt;&lt;p&gt;因此，&lt;strong&gt;上下文窗口管理&lt;/strong&gt;是智能体的众多职责之一。&lt;/p&gt;&lt;p&gt;现在，让我们深入了解 Codex 是如何运行智能体循环的。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;模型推理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Codex CLI 通过向 Responses API 发送 HTTP 请求来运行模型推理。OpenAI 将检查信息如何流经 Codex，并利用 Responses API 驱动智能体循环。&lt;/p&gt;&lt;p&gt;Codex CLI 使用的 Responses API 端点是可配置的，因此它可以与任何实现了 Responses API 的端点配合使用：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;使用 ChatGPT 登录 Codex CLI 时，端点为&amp;nbsp;https://chatgpt.com/backend-api/codex/responses。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;使用 OpenAI 托管模型的 API 密钥认证时，端点为&amp;nbsp;https://api.openai.com/v1/responses。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行 codex --oss 以配合 ollama 或 LM Studio 使用 gpt-oss 时，默认指向本地运行的&amp;nbsp;http://localhost:11434/v1/responses。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;也可以配合云服务商（如 Azure）托管的 Responses API 使用。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来，让我们探索 Codex 如何为对话中的第一次推理调用创建提示词。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;构建初始提示词&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作为终端用户，你在查询 Responses API 时并不会逐字指定用于采样的提示词。相反，你会在查询中指定各种输入类型，而 Responses API 服务器决定如何将这些信息结构化为模型设计的提示词。你可以将提示词看作一个「项目列表」。&lt;/p&gt;&lt;p&gt;在初始提示词中，列表中的每个项目都关联一个角色（Role）。角色指示了相关内容的权重，取值如下（优先级从高到低）：system（系统）、developer（开发者）、user（用户）、assistant（助手）。&lt;/p&gt;&lt;p&gt;Responses API 接收包含多个参数的 JSON 负载。OpenAI 重点关注这三个：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;instructions&lt;/strong&gt;：插入模型上下文的系统（或开发者）消息。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;tools&lt;/strong&gt;：模型在生成回复时可能调用的工具列表。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;input&lt;/strong&gt;：输入给模型的文本、图像或文件列表。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Codex 中，instructions 字段读取自～/.codex/config.toml；否则使用模型自带的 base_instructions（例如 gpt-5.2-codex_prompt.md）。&lt;/p&gt;&lt;p&gt;tools 字段是符合 Responses API 架构的工具定义列表。对于 Codex，这包括 CLI 提供的工具、API 提供的工具以及用户通过 MCP（模型上下文协议） 服务器提供的工具。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5wr9Q3iabeodfnelAhuwyzLFSGI25q7sXMyZoNEia6shwJDib1oz03JZ3Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.4655172413793103" data-s="300,640" data-type="png" data-w="928" type="block" data-imgfileid="503529979" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/5a2d4e6f-da6e-4d0d-9083-45690ab79661/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;最后，JSON 负载的 input 字段是一个项目列表。Codex 在添加用户消息之前，会在 input 中插入以下项目：&lt;/p&gt;&lt;p&gt;1. 一条 role=developer 的消息，描述仅适用于 tools 部分定义的 Codex 提供之 shell 工具的沙箱环境。也就是说，其他工具（如 MCP 服务器提供的工具）不受 Codex 沙箱限制，需自行负责执行安全准则。该消息是根据模板构建的，其中关键内容来自捆绑在 Codex CLI 中的 Markdown 片段，如 workspace_write.md 和 on_request.md：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5gj7gRJDB7vecScO40mCZMNZh9GwibmScXesSyVesSLp0xgeVOOxM6XQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.18549511854951187" data-s="300,640" data-type="png" data-w="717" type="block" data-imgfileid="503529980" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/25b3fabc-5233-41a4-9ce1-80d0fb315496/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;2.（可选）一条 role=developer 的消息，其内容是从用户的 config.toml 文件中读取的 developer_instructions 值。&lt;/p&gt;&lt;p&gt;3.（可选）一条 role=user 的消息，其内容是「用户指令（User Instructions）」，这些指令并非来源于单一文件，而是从多个来源汇总而来的。通常，更具体的指令会出现在后面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;$CODEX_HOME 中 AGENTS.override.md 和 AGENTS.md 的内容。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;受限于一定大小（默认为 32 KiB），在从当前工作目录（CWD）的 Git / 项目根目录到 CWD 自身的每个文件夹中查找：添加任何 AGENTS.override.md、AGENTS.md 或 config.toml 中 project_doc_fallback_filenames 指定的文件内容。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果配置了任何 Skills：关于 Skill 的简短序言、每个 Skill 的元数据、关于如何使用 Skill 的章节&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;4. 一条 role=user 的消息，描述智能体当前运行的本地环境。这指定了当前工作目录和用户的 Shell：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5gRxw59qULb3QsB5LO3HdlBbtNXVBsanDhEuWSMeFEBqkuNqVq7QxRQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.29916897506925205" data-s="300,640" data-type="png" data-w="361" type="block" data-imgfileid="503529981" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/86e2400d-98d1-4042-a7b3-21156cb68114/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;一旦 Codex 完成上述所有初始化输入的计算，它就会附加用户消息以开始对话。&lt;/p&gt;&lt;p&gt;之前的例子集中在每条消息的内容上，但请注意，input 的每个元素都是一个具有 type、role 和 content 的 JSON 对象，如下所示：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5CbMJ5qahlStAd8vUzYypE3amRTibl1sWiaJZy2S5N7Zib8FC9PeoafzoQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.5632798573975044" data-s="300,640" data-type="png" data-w="561" type="block" data-imgfileid="503529982" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/bbb547cd-b4aa-4f4e-a6d4-dd6244bbd714/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;一旦 Codex 构建好发送给 Responses API 的完整 JSON 负载，它就会发出带有 Authorization 标头的 HTTP POST 请求，该标头取决于～/.codex/config.toml 中 Responses API 端点的配置方式（如果指定了额外的 HTTP 标头和查询参数，也会一并添加）。&lt;/p&gt;&lt;p&gt;当 OpenAI Responses API 服务器收到请求时，它会使用 JSON 按如下方式推导出模型的提示词（当然，自定义的 Responses API 实现可能会有不同的选择）：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5Y7mypzATEyddHG5q95jBNlbUu0Gf31iaz5gXjNm1Nk2TBkgNgzwuJ2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.5985037406483791" data-s="300,640" data-type="png" data-w="802" type="block" data-imgfileid="503529984" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/6464e1ff-78ec-4ca8-96f8-d1ce2bdd8745/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;如你所见，提示词中前三项的顺序是由服务器而非客户端决定的。即便如此，在这三项中，只有 system 消息的内容也受服务器控制，因为 tools 和 instructions 是由客户端决定的。紧随其后的是来自 JSON 负载的 input 以完成提示词。&lt;/p&gt;&lt;p&gt;既然有了提示词，OpenAI 就可以开始采样模型了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第一轮对话&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对 Responses API 的这个 HTTP 请求启动了 Codex 对话的第一个「轮次」。服务器以服务器发送事件（SSE）流的形式进行回复。每个事件的数据都是一个 JSON 负载，其 type 以 response 开头，可能类似于这样（事件的完整列表可以在 OpenAI 的 API 文档中找到）：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5tt0o4whYwnEyZemycNR6ySx2EwGNHqhSo2PJqsbzIwiaic97SjMAz74g/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.29237947122861585" data-s="300,640" data-type="png" data-w="643" type="block" data-imgfileid="503529983" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/8ffa4978-26cb-45bb-842a-98c11d8c1c66/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Codex 消费这些事件流，并将它们重新发布为可供客户端使用的内部事件对象。像 response.output_text.delta 这样的事件用于支持 UI 中的流式显示，而像 response.output_item.added 这样的事件则被转换为对象，附加到后续 Responses API 调用的 input 中。&lt;/p&gt;&lt;p&gt;假设对 Responses API 的第一个请求包含了两个 response.output_item.done 事件：一个类型为 reasoning（推理），一个类型为 function_call（函数调用）。当 OpenAI 再次使用工具调用的结果查询模型时，这些事件必须体现在 JSON 的 input 字段中：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5dicoCqr5Vu8FHLVrDyVEhDEI3EdGPjTrNUeEyuf2b5t2LxlzEJpyRrw/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.7301204819277108" data-s="300,640" data-type="png" data-w="830" type="block" data-imgfileid="503529985" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/f765b772-1eb5-48f9-8ada-aa250a8bcb42/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在随后的查询中，用于采样模型的提示词将如下所示：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5xl0PmvGQ0SmvXjT4xkPKPqHOooQ2uIaVn7eyfEQH97VB34p0E18z1Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.3877805486284289" data-s="300,640" data-type="png" data-w="802" type="block" data-imgfileid="503529987" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/9990af80-0df8-4c7f-8906-5eee2a61d095/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;特别要注意的是，旧提示词是新提示词的精确前缀。这是有意为之的，因为这使得后续请求更加高效，因为它使 OpenAI 能够利用&lt;strong&gt;提示词缓存&lt;/strong&gt;（Prompt Caching）（OpenAI 将在下一节关于性能的内容中讨论）。&lt;/p&gt;&lt;p&gt;回顾 OpenAI 的第一张智能体循环图，OpenAI 看到在推理和工具调用之间可能存在多次迭代。提示词可能会持续增长，直到 OpenAI 最终收到一条 Assistant 消息，表明该轮次结束：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5Hfo8XjXPjZliczVTZEpH9zTHlakEZ27ib8P0GeiaukSx6Q3ia7bcUESvNA/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.08066581306017925" data-s="300,640" data-type="png" data-w="781" type="block" data-imgfileid="503529986" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/98624025-7f63-484b-965d-db4bc2725c5c/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在 Codex CLI 中，OpenAI 将 Assistant 消息呈现给用户，并聚焦编辑器以向用户表明轮到他们继续对话了。如果用户回复，则前一轮的 Assistant 消息以及用户的新消息都必须附加到 Responses API 请求的 input 中，以开始新轮次：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5ASHjBuDMr8pucM8McVhibqjv3QD0GVUsvOqFVzD5Gw04s62eIf4G5CA/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.8942857142857142" data-s="300,640" data-type="png" data-w="700" type="block" data-imgfileid="503529988" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/b31484de-93e0-4947-93f6-ce9af67cd86f/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;再一次，由于 OpenAI 正在继续对话，OpenAI 发送给 Responses API 的输入长度会不断增加：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9ZIK7vDox8JI3Lianicq7rn5AK0JmvFU82a7BicUyNDCxzTsGZD5tpzl4tClHd0lut92aRz1Thnwx5A/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.5087281795511222" data-s="300,640" data-type="png" data-w="802" type="block" data-imgfileid="503529989" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/83e6fa94-c308-4203-8001-4f59c57b712a/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;让 OpenAI 来看看这种不断增长的提示词对性能意味着什么。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;性能考虑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;你可能会问自己：「等等，&lt;strong&gt;智能体循环在对话过程中发送给 Responses API 的 JSON 量难道不是呈二次方增长吗？&lt;/strong&gt;」&lt;/p&gt;&lt;p&gt;确实如此，虽然 Responses API 确实支持一个可选的 previous_response_id 参数来缓解这个问题，但 Codex 目前并未使用它，主要是为了保持请求完全无状态，并支持零数据保留（ZDR）配置。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;避免使用 previous_response_id 简化了 Responses API 提供者的工作，因为它确保了每个请求都是无状态的。这也使得支持选择零数据保留（ZDR）的客户变得简单，因为存储支持 previous_response_id 所需的数据会与 ZDR 冲突。请注意，ZDR 客户并不会失去从前几轮的专有推理消息中受益的能力，因为相关的 encrypted_content 可以在服务器上解密。（OpenAI 会保留 ZDR 客户的解密密钥，但不会保留其数据。）有关 Codex 支持 ZDR 的相关更改，请参见 PR #642 和 #1641。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;通常，采样模型的成本远高于网络传输的成本，因此采样是 OpenAI 提高效率的主要目标。这就是为什么提示词缓存如此重要，因为它使 OpenAI 能够重用之前推理调用的计算结果。当 OpenAI 命中缓存时，采样模型的时间复杂度是线性的而非二次方的。&lt;/p&gt;&lt;p&gt;OpenAI 的提示词缓存文档对此进行了更详细的解释：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;缓存命中仅适用于提示词内的精确前缀匹配。为了获得缓存收益，请将静态内容（如指令和示例）放在提示词的开头，并将变量内容（如用户特定信息）放在末尾。这也适用于图像和工具，它们在请求之间必须完全一致。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;考虑到这一点，让 OpenAI 看看哪些类型的操作会导致 Codex 中的「缓存未命中」：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在对话过程中更改模型可用的工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;更改 Responses API 请求的目标模型（实际上，这会改变原始提示词中的第三项，因为它包含模型特定的指令）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;更改沙箱配置、批准模式或当前工作目录。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Codex 团队在 Codex CLI 中引入可能破坏提示词缓存的新功能时必须保持严谨。例如，OpenAI 最初对 MCP 工具的支持引入了一个 Bug，即 OpenAI 未能以一致的顺序排列工具，导致了缓存未命中。&lt;/p&gt;&lt;p&gt;请注意，&lt;strong&gt;MCP 工具可能特别棘手&lt;/strong&gt;，因为 MCP 服务器可以通过 notifications/tools/list_changed 通知随时更改它们提供的工具列表。在长对话中间响应此通知可能会导致昂贵的缓存未命中。&lt;/p&gt;&lt;p&gt;如果可能的话，对于对话中发生的配置更改，OpenAI 通过在 input 中附加一条新消息来反映更改，而不是修改之前的消息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;如果沙箱配置或批准模式发生变化，OpenAI 会插入一条新的 role=developer 消息，格式与原始的 &amp;lt;permissions instructions&amp;gt; 项目相同。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;如果当前工作目录发生变化，OpenAI 会插入一条新的 role=user 消息，格式与原始的 &amp;lt;environment_context&amp;gt; 相同。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了性能，OpenAI 竭尽全力确保缓存命中。此外，OpenAI 还必须管理另一个关键资源：上下文窗口。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;上下文管理与压缩&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;OpenAI 避免耗尽上下文窗口的总策略是：&lt;strong&gt;一旦 Token 数量超过某个阈值，就对对话进行压缩（Compaction）。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具体来说，OpenAI 用一个代表对话的小型新项目列表替换 input，使智能体能够在理解迄今为止发生的事情的情况下继续工作。早期的压缩实现需要用户手动调用 /compact 命令，该命令会使用现有对话加上自定义的总结指令来查询 Responses API。Codex 将生成的包含总结的 Assistant 消息作为后续对话轮次的新 input。&lt;/p&gt;&lt;p&gt;自那以后，Responses API 已演进为支持一个特殊的 /responses/compact 端点，该端点能更高效地执行压缩。它返回一个项目列表，可用于替代之前的输入以继续对话，同时释放上下文窗口。此列表包含一个特殊的 type=compaction 项目，带有一个不透明的 encrypted_content 项，它保留了模型对原始对话的潜在理解（latent understanding）。&lt;/p&gt;&lt;p&gt;现在，当超过 auto_compact_limit 时，Codex 会自动使用此端点来压缩对话。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;下期预告&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本博客介绍了 Codex 智能体循环，并详细讲解了 Codex 在查询模型时如何构建和管理其上下文。在此过程中，OpenAI 强调了适用于任何在 Responses API 之上构建智能体循环的开发者的实际考虑因素和最佳实践。&lt;/p&gt;&lt;p&gt;虽然智能体循环为 Codex 提供了基础，但这仅仅是个开始。&lt;/p&gt;&lt;p&gt;在接下来的文章中，OpenAI 表示将深入探讨 CLI 的架构，探索工具调用的具体实现方式，并详细了解 Codex 的沙箱模型。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>不止于Prompt：揭秘「神经网络可重编程性」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:31:39 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/5a534c14-8160-4c3a-a7f2-dbb18aedf105/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;从模型重编程（Model Reprogramming），到参数高效微调（PEFT），再到当下大模型时代的 Prompt Tuning ，Prompt Instruction 和 In-context Learning，研究者和从业人员不断地探索一个核心问题：&lt;strong&gt;在尽量不改动模型参数的前提下，如何最大化地复用预训练模型的能力？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去几年，这类方法在不同社区中以各自独立的形式快速发展 &amp;mdash;&amp;mdash; 有的来自对抗鲁棒性与迁移学习，有的服务于下游任务适配，有的则成为大模型对齐与应用的基础工具。然而，这些看似分散的技术路线，背后是否存在一个更统一、更本质的理论视角？&lt;/p&gt;&lt;p&gt;近期，来自墨尔本大学可信赖机器学习与推理（TMLR）研究小组和 IBM AI 研究所的研究者系统性地提出了「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;神经网络可重编程性（Neural Network Reprogrammability）&lt;/strong&gt;」这一统一主题，在最近的一篇 survey 中，将模型重编程，Prompt Tuning、Prompt Instruction 和 In-context Learning 纳入同一分析框架，从操纵位置、操纵类型、操纵算子和输出对齐四个维度进行了系统梳理与对比。同时，该团队也在 &lt;strong&gt;AAAI 2026&lt;/strong&gt; 上带来同名 Tutorial，帮助研究者与工程实践者全面理解这一正在重塑模型使用范式的关键能力。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKthhaZfstBYEWww653MgCZW9vHj5tRISAbqJNFiakkxQApOvIRVHmhXg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3314814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529488" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/53b2cd60-6877-4527-b26c-26fd91096cf1/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Tutorial 标题：Neural Network Reprogrammability: A Unified Framework for Parameter-Efficient Foundation Model Adaptation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文标题：Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Arxiv: https://arxiv.org/pdf/2506.04650&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub: https://zyecs.github.io/awesome-reprogrammability/tutorial-AAAI26/&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;1. 模型训练范式的转变&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在本文中，我们认为随着预训练模型（pre-trained model）规模的增长，其适配下游任务（downstream tasks）的范式已经发生了根本性转变：从传统的基于模型参数调整的适配（图 1a）转变为了基于模型可重编程性的适配（图 1b）。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKyEJAkSFh7ibv8HVWLicS5icKzCwqalF8M6cbA8fhKKIz91BibicQz1RY8dQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5055555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529489" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/021808a1-c747-4f2d-965b-9c4dbad23995/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;传统适配技术（parameter-centric adaptation, PCA）通过重新训练预训练模型，&lt;strong&gt;修改模型内部参数&lt;/strong&gt;，使其适用于新的下游任务。例如，将 ImageNet 预训练的图像分类器应用于猫狗分类任务时，需要至少改变分类头，甚至重新训练其他层的参数，即我们通常所说的 fine-tuning，本质上改变了模型学习到的内部表征（representation），并需要为每个下游任务维护一份新的参数拷贝。&lt;/p&gt;&lt;p&gt;新兴适配技术（基于模型可重编程性的适配，reprogrammability-centric adaptation, RCA）则采用了一种不同的理念：保持模型参数冻结，转而&lt;strong&gt;策略性地修改任务呈现的方式&lt;/strong&gt;，通过精心设计下游任务的输入变换（包括模型输入（input）、提示（prompt）或上下文信息（context）），以及模型输出对齐方式（output）来使其兼容下游任务，使用极少量可训练参数（甚至完全不引入新参数），在不触及模型权重的情况下「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;重编程&lt;/span&gt;」预训练模型的行为。&lt;/p&gt;&lt;p&gt;核心转变体现在&lt;strong&gt;理念上的转换&lt;/strong&gt;：从「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;修改模型以适应任务&lt;/strong&gt;」转向「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;修改任务以适应模型&lt;/strong&gt;」，从而使我们能以最小的计算开销在不同任务中重复使用预训练模型，同时保持其原有能力。同一个冻结的模型仅通过改变与其「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;对话」的方式，就能处理多种不同的任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 可重编程性范式的效率优势&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具体实验数据表明（图 2），相较 PCA，RCA 在参数效率上有明显优势。将 ImageNet 预训练的视觉 Transformer（ViT-B/32）适配到遥感图像分类任务（EuroSAT）。柱状图显示不同 fine-tune 策略的参数需求：从左到右分别对应 fully fine-tune 到逐步减少可训练层数的各种配置，训练参数量随之下降。但即便是最轻量的 PCA 方案仍需要大量参数。&lt;/p&gt;&lt;p&gt;形成对比的是，&lt;strong&gt;红色虚线&lt;/strong&gt;显示 RCA 需要的训练参数始终比任何 PCA 配置少 2-3 个数量级。这些参数用于输入变换和输出对齐，而不是修改预训练模型的内部权重。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKpDVb56yxjO5qicxEoyqbEzO8LDXSJBdbphS2c8pkpUIj8yXzA2adx5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.7064814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529490" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/63a68e04-7a94-40dc-9a0e-220a98936494/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;这表明，在可以实现 comparable performance 前提下，RCA 的参数效率更高，使得在资源受限环境中适配大模型成为可能，并支持同时适配多个任务而不会出现灾难性遗忘。在预训练模型规模与能力不断提升、获取方式日趋不透明（如商业模型仅提供 API 接口）的背景下，RCA 的优势愈发突出。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 可重编程性范式的「&lt;/strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;多种称谓」&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;然而，我们发现相似甚至相同的模型适配方法在不同研究社区却有着截然不同的命名：NLP 社区常称之为「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;prompt tuning」，而 ML 文献中研究者更倾向于使用 「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;model reprogramming」指代这类方法。经验上，这种术语混乱也经常引发 「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;哪种方法更优」、&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;为何不比较其他方法」&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;等争论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;核心问题在于：prompt tuning，model reprogramming，甚至 in-context learning 真的代表不同的模型适配方法吗？答案是否定的。尽管表现形式各异，这些方法实质上都利用了神经网络的同一固有属性 -- neural network reprogrammability （神经网络&lt;strong&gt;可重编程性&lt;/strong&gt;，图 3）。基于这一认识，我们提出统一框架来连接三个独立发展的研究领域，并系统性地描述和归类这些适配方法。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKAwib5hdtiacl6aByiaUq1fupXxLAyIKHBawB6O8y1qJZ7yiaekufssrJDg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5101851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529492" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/3264161f-ca68-43a3-864b-a1fdfc57db0d/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键点 1. 可重编程性的普适性。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;它具备架构无关性和模态无关性，跨越三个核心维度：适配方法，预训练模型架构（单模态类型、多模态模型、专门架构），以及数据类型（图像、文本、音频、图结构等） -- 无论具体实现细节如何，&lt;strong&gt;围绕模型接口的信息操作&lt;/strong&gt;（information manipulation at model&amp;rsquo;s interfaces）这一共同的底层原理，我们都能将任意预训练模型适配到任意下游任务。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4. 可重编程性范式的首次提出（ICLR 2019）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;那么什么是 reprogrammability 呢？下面这张图片展示了从神经网络对于对抗样本的脆弱性（sensitivity to adversarial examples）向可重编程性（reprogrammability）的演进。图片来自文章《Adversarial reprogramming of neural networks》由 G. F. Elsayed, I. Goodfellow, and J. Sohl-Dickstein. 发表于 ICLR 2019.&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKxLrVLcyd1m11oiayic31960YHicicYwoBxibIo4LwpYc6lpNkMCXLWpHEIw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.20185185185185187" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529494" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/d6201345-6572-4d13-b21a-185e9cc021c1/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;左侧（传统对抗样本 adversarial example）：展示了经典对抗攻击，在熊猫图像上添加不可察觉的噪声，就能使 ImageNet 分类器将其错分为长臂猿，置信度高达 99.3%，尽管图像在人眼看来没有变化。&lt;/p&gt;&lt;p&gt;右侧（对抗重编程 adversarial reprogramming）：展示了如何将这种脆弱性转化为建设性用途。我们不仅欺骗模型，同时将其「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;重编程」以执行完全不同的任务：&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（a）&lt;/span&gt;展示了一个黑白格图像的计数任务，我们可以人为将不同的动物类别映射到方块数量类别（1-10 个方块）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（b）&lt;/span&gt;展示了「对抗程序」（adversarial program） -- 精心设计的噪声，充当指导模型行为的指令（可以理解为 prompt）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（c）&lt;/span&gt;将（a）和（b）结合后，仅在 object recognition 任务上预训练的 ImageNet 分类器被「重编程」以执行方格计数任务，可以输出「4 个方格」的预测结果（从源域的「虎鲨」类映射得到）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;关键点 2. 巧妙利用神经网络的敏感性。&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由对抗样本发现的神经网络敏感性（理论背景包括决策边界的不连续性等），正是可重编程性的基础。我们不再将这种敏感性仅视为安全缺陷，而是建设性地利用它，在不重新训练的情况下将预训练模型重定向到新的任务。精心设计的 program/prompt 可以将神经网络感知的弱点转化为高效的适配机制。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKSKe3tNiawheo7zPhNb7laV4uMkLbicHZ7JHh1jY72jSSjBCR7g77jKUw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.425" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529495" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/d69d9eba-e8c8-4f0e-8e4c-4c4d2a171fbf/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;5. 可重编程性范式的数学表达&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如上，我们给出 neural network reprogrammability 统一框架的定义，涵盖了文章中讨论的各类模型适配方法。定义如下：&lt;/p&gt;&lt;p&gt;给定源域（source domain）上预训练的模型&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKNwbMzxBMRcgbYa1snbNhXcdpCatOTAH8IcpUuUxtyzd8SrRshyXLgA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.4350282485875706" data-s="300,640" data-type="png" data-w="354" type="block" data-imgfileid="503529497" data-aistatus="1" data-original-style="width:52px;height:23px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/5e5383d8-160b-4007-ad35-af336b783700/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dii" style="width: 7.06%;"&gt;，该模型从源域输入空间&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKo3M6HibHmCznRXFFia4UUs0S1zxP41GMyPUiabKHS9lT5xo7pjrZMMjWA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.85" data-s="300,640" data-type="png" data-w="200" type="block" data-imgfileid="503529498" data-aistatus="1" data-original-style="width:29px;height:25px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/2513e3eb-0521-4d27-9c35-b8e0dc19a01d/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 3.76%;"&gt;映射到源域输出空间&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKrABG7Gg4ibs7tZ3zEp20qjIpSJ2t6icoANk2AKVLtZ6qTA0H6ZI709QA/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.794392523364486" data-s="300,640" data-type="png" data-w="214" type="block" data-imgfileid="503529499" data-aistatus="1" data-original-style="width:31px;height:25px;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/92d96cde-ef65-4350-bee5-aa2fcf982e37/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dii" style="width: 4.86%;"&gt;。神经网络可重编程性使这个固定模型（参数不再改变）能够通过两个&lt;strong&gt;可配置&lt;/strong&gt;的变换在&lt;strong&gt;完全不同&lt;/strong&gt;的目标域（target domain）实现由该域输入 / 输出空间&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKziaAsjbKibUDffZlSBW2c8zhLFz1wG9GCkhBGWibyNIcpWqysialG5Y8rA/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.40594059405940597" data-s="300,640" data-type="png" data-w="404" type="block" data-imgfileid="503529500" data-aistatus="1" data-original-style="width:56px;height:23px;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/b64ddcbe-2a6e-40e3-aa74-2ffa98aab49d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dii" style="width: 7.15%;"&gt;定义的目标任务：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;输入操作（input manipulation）&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMK9Rv3rXR1ma7Ih2xssdSxqjWibRGRiaqA7MxWMW4P1cpYPDYuCcYZaZIA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.3053435114503817" data-s="300,640" data-type="png" data-w="524" type="block" data-imgfileid="503529501" data-aistatus="1" data-original-style="width:65px;height:20px;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/576fbdd7-dda3-4c61-a9dd-5668eb6e8d1f/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dii" style="width: 9.99%;"&gt;该变换将目标任务的输入转换为预训练模型可处理的格式，这可能是通过添加可学习的 prompt、拼接 demonstration examples 或应用 adversarial perturbation 到目标样本上。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;输出对齐（output alignment）&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKMV5fkKldO91iauqOwaXegBgJbicicKxyEduB0vwP6iatXpYM6AK2h9lX6g/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.3383458646616541" data-s="300,640" data-type="png" data-w="532" type="block" data-imgfileid="503529502" data-aistatus="1" data-original-style="width:70px;height:24px;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/3d0c8566-a446-4c9d-b2fc-6a06090718d2/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dii" style="width: 10.74%;"&gt;该变换将预训练模型的源域预测映射到目标任务的输出格式。这可能涉及到 label mapping, structured parsing 或 linear projection 等。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;将这两个变换与预训练模型结合，我们得到重编程后的预训练模型&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKWmmiaAwXbEE9qCK6drVLSSJ1872EqoPR957tH4Y0luyK2GibUmIiafo5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.25076452599388377" data-s="300,640" data-type="png" data-w="654" type="block" data-imgfileid="503529503" data-aistatus="1" data-original-style="width:110px;height:28px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/8792d7ad-1d63-4ff7-b906-f48896cc2c2d/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dii" style="width: 15.49%;"&gt;。这个看似简单的复合函数可以描述上述模型适配技术的本质，这些看似不同的方法实际上只是同一基本原理的不同实例。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6. 可重编程性范式的具体案例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;以视觉 - 语言模型（Vision-Language Model）为例，说明三种可重编程方法在实现上的差异（如图 4 所示）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;（4a） model reprogramming （MR）&lt;/strong&gt;：主要在模型&lt;strong&gt;原始输入层&lt;/strong&gt;操作。可学习的扰动直接添加到输入图像上。模型通过图像和文本编码器处理这些修改后的输入，需要&lt;strong&gt;输出对齐&lt;/strong&gt;将模型的原始预测映射到新的目标任务。这种方法适用于可访问模型的输入和输出，但对内部模型组件控制有限的情况。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;（4b） prompt tuning （PT）&lt;/strong&gt;：主要在&lt;strong&gt;中间表示层&lt;/strong&gt;操作。可学习的 tokens 或嵌入（embedding）被插入到模型的内部层（包括图像编码器和文本编码器）。这些「软提示」可以在嵌入层（embedding layer）或隐藏层（hidden layers）进行前置或插值，在保持核心参数冻结的同时允许对模型内部处理进行更直接的控制。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;（4c） prompt instruction （PI）&lt;/strong&gt;：通过&lt;strong&gt;上下文演示（contextual demonstration）&lt;/strong&gt;操作。该方法不使用可学习参数，而是提供多个示例图像和明确的文本指令来引导模型行为。模型从提供的演示中「上下文」学习任务，无需任何参数更新。该方法的有效性主要在 LLMs 和 large vision-language model/multi-modal LLMs 上可观察到。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;操作位置&lt;/strong&gt;：输入空间 （MR） &amp;rarr; 嵌入 / 隐藏空间 （PT） &amp;rarr; 输入空间 （PI）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;参数需求&lt;/strong&gt;：可学习扰动 （MR） &amp;rarr; 可学习 tokens（PT） &amp;rarr; 无新参数 （PI）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;访问要求&lt;/strong&gt;：输入访问 （MR） &amp;rarr; 白盒访问 （PT） &amp;rarr; API 级访问 （PI）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本质上，三种方法都实现了相同目标 -- 将冻结模型重新用于新任务 -- 通过计算图中的不同路径实现。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKEoUltQibKhmfORt2TpvbyEslh3KxiajaibkeqglwhuRSeNqQEIicibRKxaQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.4398148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529504" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/caf6085b-cfd1-45f9-b609-5ebb47ff12d7/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;Neural network reprogrammability 如何在不同模态和任务中具体实现呢？&lt;/p&gt;&lt;p&gt;（a） model reprogramming for 图像分类任务（图 5a）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;输入操纵：目标图像经过调整大小并与可学习扰动模式 &amp;lambda; 结合。这将目标任务输入转换为预训练分类器可处理的格式。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;预训练模型：冻结的图像分类器 （如 ResNet, ViT） 处理操纵后的输入。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;输出对齐：将分类器的原始类别预测转换到目标任务的标签空间（不同类别，可能不同数量的类别）。即实现了 Label Mapping 步骤，不需要额外的训练参数。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;训练：仅通过反向传播优化扰动参数 &amp;lambda;，模型权重保持冻结。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;（b） prompt tuning for 文本生成任务（图 5b）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;输入操纵：可学习的 prompt tokens &amp;lambda; 通过拼接操作前置到目标文本输入。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;预训练模型：冻结的 language generator（如 GPT）处理提示增强的输入。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;输出对齐：因为模型已经在目标文本空间输出，无需额外转换。&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;训练：仅优化提示参数 &amp;lambda;，保持生成器完全冻结。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKVTjf28AIQhR2L3sPDiaicwKPXEGT47noEHhbHib5AQdw70uxMsqWzMRnA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529505" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/a2cdd8eb-28a4-44a6-9279-da09ee29ff23/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;关键点 3. 数学框架下的一致性。 &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管操纵不同模态（视觉 vs 语言）、任务类型（分类 vs 生成）并使用不同的输入操纵策略（加性扰动 vs 连接提示），两种方法都遵循完全相同的数学框架。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7. 基于可重编程性范式，归纳现有方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于这个特性，我们进一步提出了一个分类法（taxonomy），将过往的研究工作组织为跨四个维度的连贯结构，并展示了 neural network reprogrammability 这一框架的泛用性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;操纵位置：定义输入操纵发生在预训练模型的哪个接口，包括原始输入空间（input space），嵌入空间（embedding space），以及隐藏空间（hidden space）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;操纵类型：定义输入操纵的类型，分为可优化（learnable）和固定（fixed）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;操纵算子：定义输入操纵如何被应用到目标数据（target input）上，包括加性（additive）、拼接（concatenative）、参数化（parametric）算子&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;输出对齐：定义是否模型输出需要进行额外操作以对齐目标任务（target output），包括恒等映射 （identity mapping）、结构变换（structural alignment）、统计变换（statistical alignment）、线性变换（linear alignment）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对应地，MR，PT 和 PI 对应的研究方法可以被系统归类，如表格 2 所示。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKo3p4ibP5s269JSLskDBfQ9tqHcdDBiceojztb852a2ribqckTN5twP8JQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.42314814814814816" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529506" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/d6539237-411e-45fa-b195-e6bbc7a19e7d/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;8. 如何用可重编程性范式来理解 In-context learning 和 Chain-of-Thought Reasoning&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKtzavdST55JRHNZokVraSQIOCDIKRMx4zqho374CgutdiaEHnQ25JV5A/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.387037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529510" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/be8059d0-1600-41b7-97cd-447896ec488d/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;特别地，LLM 的上下文学习 in-context learning （ICL） 在该框架下可以描述为&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;固定输入操纵：无训练参数，依赖人为设计的 demonstration examples&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;原始输入空间操纵：demonstration example 直接与模型的 text query 拼接&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;拼接操纵算子：demonstration example 通过拼接操作&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;隐式输出对齐：无需额外显式映射，预训练模型直接生成目标输出或依靠模型自身能力对输出进行基于规则的格式、结构调整（见下图示例，ChatGPT 可以直接对模型输出的 natural language 进行格式限制，e.g., bullet list, LaTeX）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKibHIrfaLl7jhm6ibVQguIfZUSsD6OgJ5KjQib1g0Zxvq3eD4M2mEhaU5g/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.5092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529507" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/dea20abf-9534-49a9-89aa-94fa888903f1/640.png" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;因此，模型通过这些示例在「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;上下文」中学习目标任务的模式，且无需任何参数更新。Demonstration examples 本质上是一种输入操纵，通过策略性构造输入，从而重编程模型行为。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKMcUmr6beXockcfTyyxWMgvOaEVO5A989cI610KuITl5zvH3ZtahseQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-ratio="0.4148148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529508" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/f0036c53-8ed6-4985-83df-5904ede6e4ae/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;对应地，思维链推理（Chain-of-Thought Reasoning）可被认为是一种通过融入结构化、与输入样本特定相关的（sample-specific）「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;推理形式」的输入操纵。&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;输入操纵：具备增强的上下文信息，不仅包含输入 - 输出对，还包含&lt;strong&gt;明确的中间推理步骤&lt;/strong&gt;。例如，解决数学问题时，CoT 会包含「问题 -&amp;gt; 第一步计算 -&amp;gt; 第二步计算 -&amp;gt;&amp;hellip;-&amp;gt; 最终步骤」的完整推理过程。另外，每个目标输入 query 都会触发模型生成与&lt;strong&gt;该具体 query 匹配&lt;/strong&gt;的推理链。比如解决「23&amp;times;47=?」时，模型会生成针对这两个具体数字的逐步计算过程，而不仅是通用的乘法公式。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;输出对齐：由于模型输出完整的推理序列（「首先计算 23&amp;times;40=920，然后计算 23&amp;times;7=161，最后 920+161=1081」），因此需要结构化、基于规则的解析机制（structural alignment）从这个推理文本中提取最终数值答案。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;9. 资源分享：Awesome Neural Network Reprogrammability 资源库&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了方便社区追踪这一飞速发展的领域的最新进展，我们维护了一个 Awesome 风格的资源库，收录并持续更新 Neural Network Reprogrammability 领域的最新论文和代码实现。希望这个资源库能让你少走弯路！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GitHub: https://zyecs.github.io/awesome-reprogrammability/&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你正在做相关方向，欢迎在 GitHub 上 star 支持，或者来仓库一起补全与更新！&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>1月28日，直播预约！来聊聊具身评测中的科学与乱象</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:23:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD91ejQabpM8dh70KoznYia0L1xXmDrmXlX1RJrD0IhDFvy26mdTGGYjQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD91ejQabpM8dh70KoznYia0L1xXmDrmXlX1RJrD0IhDFvy26mdTGGYjQ/0?wx_fmt=png&amp;from=appmsg" data-cropselx2="549" data-cropsely2="309" data-imgfileid="503529964" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/868ccbfe-4ebd-4379-8aca-3ec2cd126bd9/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;过去一年，我们几乎每周都能看到各种惊艳的机器人 Demo：机器人会叠衣服了、会做咖啡了、会跳各种舞了。但在繁荣的背后，有一个问题越来越频繁地被提起，那就是：&lt;strong&gt;我们到底怎么判断一个具身模型是真的进步了&lt;/strong&gt;&lt;strong&gt;？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身评测是具身智能产业发展的&amp;ldquo;度量衡&amp;rdquo;，是技术从实验室走向产业化的必经之路。&lt;/p&gt;&lt;p&gt;但一走出实验室，面对真实世界的复杂、多变和不确定性时，那些号称接近完美的成功率往往会瞬间&amp;ldquo;缩水&amp;rdquo;。&amp;ldquo;刷榜容易，落地难&amp;rdquo;，成为了悬在具身智能商业化路上的达摩克利斯之剑。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;1月28日（下周三）晚19:00&lt;/strong&gt;，直播即将开启。&lt;/p&gt;&lt;p&gt;本次圆桌对话由&lt;strong&gt;机器之心创始人兼CEO 赵云峰&lt;/strong&gt;主持，特邀四位产业与学术专家：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;范浩强，Dexmal 原力灵机Co-Founder&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;李永露，上海交通大学副教授、上海创智学院全时导师&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;沈宇军，蚂蚁灵波科技首席科学家&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;赵行，星海图联合创始人、清华大学助理教授&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;共同深入探讨具身智能评测的真实现状与核心挑战。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDY22wadPMwbFWPs8x5xdzlLCCCSvPCwhyXkxibpcSUca6aiayoJvvkqPQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.7777777777777777" data-s="300,640" data-type="png" data-w="1080" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDY22wadPMwbFWPs8x5xdzlLCCCSvPCwhyXkxibpcSUca6aiayoJvvkqPQ/0?wx_fmt=png&amp;from=appmsg" data-cropselx2="555" data-cropsely2="986" data-imgfileid="503529963" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/56c4528a-6b71-4cf5-9894-f68e45c26085/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;圆桌嘉宾&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDAlZYrZagBXKM7FiaGulia4j8CHw9dpPAkqoraK15Ib6AHE7yvkhgVNBA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="1" data-s="300,640" data-type="png" data-w="785" data-imgfileid="503529949" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b16caec3-21d2-47e6-b69f-d759d45fec7f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;范浩强 | Dexmal 原力灵机Co-Founder&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;范浩强毕业于清华姚班，高二时即作为第6号员工加入旷视团队，曾任旷视研究院助理院长，主导多项CV（计算机视觉）技术从实验室到千万级产品的转化，擅长软硬结合交叉领域技术。其核心学术成果在CVPR、ICCV、AAAI等国际顶级学术会议上多次发表，研究方向覆盖人脸识别、活体检测、3D 重建、计算摄影等多个领域。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDbYKYYXGWib05Kg15U0j3DRiceXRIAtBibsTdgxmY85khpLZeK60hplCnQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503529951" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b9f96794-b49c-4629-9b98-463f11cb1a6e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;李永露 |&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;上海交通大学副教授、上海创智学院全时导师&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;李永露博士，上海交大副教授、上海创智学院全时导师，博导，研究具身智能、物理推理、行为理解。在TPAMI、CVPR、NeurIPS、CoRL等发表成果50余篇，引用100+论文8篇，获ICRA 2025 Best Paper Award（HRI，独立通讯），开源项目获Github star 1.3万+；代表工作HAKE（引用1.48k+，Github Star 2.18k+，官网全球访问16.8万+次）、AlphaPose（引用780+，Github Star 8.3k+）。任NeurIPS、ICLR Area Chair，上海交大ACM班《计算机视觉》课程教师， VALSE EACC，中国人工智能学会-具身智能专委会副秘书长。主持、参与多项国家级项目，如青基、科技部重点研发计划等。获上海市海外高层次人才、中国人工智能学会吴文俊人工智能科学技术奖-优秀博士学位论文、蚂蚁Intech奖、WAIC云帆奖-璀璨明星、明日之星、AI100青年先锋、世界互联网大会领先科技奖、NeurIPS&amp;rsquo;20/21杰出审稿人、百度奖学金、华人AI新星百人等。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD7SEaoRbmUqh4J6ucoWjpXXVqKlUkuZVRZwNPhxRQSoLREdW6eG0kFw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1" data-s="300,640" data-type="png" data-w="480" data-imgfileid="503529948" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/afaf25b6-d459-423b-b3ec-06dc07a6e658/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;沈宇军 | 蚂蚁灵波科技首席科学家&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;沈宇军，博士毕业于香港中文大学，研究方向是计算机视觉和生成模型，现任蚂蚁灵波科技首席科学家，努力探索计算机视觉在机器人行业的落地之路。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDWGx61BwxGIAvuqA751L0v5Z2M15YbpI7e6NvXtMIdNLRRCHTwKu4Hg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1" data-s="300,640" data-type="png" data-w="851" data-imgfileid="503529954" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/dde92642-8f47-4e00-99bc-7eae48d3b0b1/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;赵行 | 星海图联合创始人、清华大学助理教授&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;赵行博士毕业于美国麻省理工学院MIT，后于谷歌无人车项目Waymo担任研究科学家。赵行长期致力于机器人学习和自动驾驶的研究。曾获机器人学习顶会CoRL 2023最佳系统论文奖提名，ICCP最佳论文奖，麻省理工科技评论&amp;rdquo;35岁以下创新35人&amp;rdquo;，世界人工智能大会最高奖项&amp;ldquo;SAIL之星&amp;rdquo; 奖等荣誉。&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;&amp;nbsp;直播预约&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;直播主题：&lt;/strong&gt;&lt;strong&gt;「聊聊具身评测：科学与乱象」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;直播时间：&lt;/strong&gt;&lt;strong&gt;1月28日 19:00 - 20:00&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;直播预约：&lt;/strong&gt;&lt;a href="https://mp.weixin.qq.com/s/ZofR4pwm6zUEPsmwvO_EgA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/db11a792-9f20-4abc-904b-a4e7c2196bf3/1769257343860.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;p data-pm-slice='3 3 ["para",{"tagName":"section","attributes":{"style":"font-size: 14px; color: rgb(67, 65, 65); letter-spacing: 1px; line-height: 2; padding: 0px; box-sizing: border-box; font-style: normal; font-weight: 400; text-align: justify;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"style":"color: rgb(0, 0, 0); text-align: left; padding: 0px 10px; box-sizing: border-box;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;我们不见不散！&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>LeCun、谢赛宁团队重磅论文：RAE能大规模文生图了，且比VAE更好</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:20:28 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Panda&lt;/section&gt;&lt;p&gt;在文生图模型的技术版图中，VAE 几乎已经成为共识。从 Stable Diffusion 到 FLUX，再到一系列扩散 Transformer，主流路线高度一致：先用 VAE 压缩视觉信息，再在潜空间中完成生成。这条路径被反复验证、规模化扩展，也几乎没有再被认真挑战过。&lt;/p&gt;&lt;p&gt;但挑战者其实早已到来，它就是谢赛宁团队提出的&lt;strong&gt;表征自编码器（RAE）&lt;/strong&gt;，详见我们去年十月的报道《&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650995317&amp;idx=1&amp;sn=893a75b049fe049a6e9b9ea46e3049cb&amp;scene=21#wechat_redirect" target="_blank"&gt;VAE 时代终结？谢赛宁团队「RAE」登场，表征自编码器或成 DiT 训练新基石&lt;/a&gt;》。&lt;/p&gt;&lt;p&gt;现在，RAE 方向又诞生了一项新的重磅成果。并且是来自 Rob Fergus、Yann LeCun 以及谢赛宁三位业内知名学者领导的一个联合团队。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDytx4rsv4EbvYWYkcaX1RNR9m2iatCITWUHXvGRLfR0CtprUFYdalCQw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.2037037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529931" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/c2dae336-1760-4ff8-b868-a6570dfc379b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.16208v1&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码地址：https://github.com/ZitengWangNYU/Scale-RAE&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目页面：https://rae-dit.github.io/scale-rae/&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;他们解答了一个更加基础的问题：&lt;strong&gt;我们真的需要 VAE 才能做好大规模文生图吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这篇工作给出的答案颇为激进。该团队系统性地扩展了「表征自编码器」这一思路，在冻结的语义表征编码器之上构建扩散模型，从 ImageNet 一路扩展到大规模自由文本生成场景。&lt;/p&gt;&lt;p&gt;结果显示，在从 5 亿到近百亿参数的多个尺度上，&lt;strong&gt;RAE 不仅在预训练阶段全面优于当前最强的 VAE 方案，还在高质量数据微调时展现出惊人的稳定性&lt;/strong&gt;，而 VAE 模型却在短短 64 个 epoch 后出现灾难性过拟合。&lt;/p&gt;&lt;p&gt;可以说，这篇论文释放出了一个相当具有颠覆性的信号：当理解与生成共享同一套语义表征空间时，扩散模型的复杂工程设计反而可以被大幅削减。更进一步，这个思路或许有望打开&lt;strong&gt;多模态统一模型&lt;/strong&gt;的想象空间。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;架构设计：以表征自编码器重塑潜空间&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在传统的潜向扩散模型（LDM）中，VAE 的作用是将图像压缩进一个极低维度的空间。然而，RAE 采用了截然不同的逻辑：它直接耦合一个预训练且冻结的视觉表征编码器（如 SigLIP-2），并仅训练一个轻量化的 ViT 结构解码器用于像素重建。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDQuDGRdkdCibBcKyTCln4niagzib6oN9qibW8r7W0Nvj8p2X162ibQoWZDLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3675925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529932" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b4f06b2f-6ddc-40c7-bde3-a05f03c455bf/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;以研究中采用的 SigLIP-2 So400M 为例，它会将一幅图像转化为 16&amp;times;16 个 token，每个 token 的维度高达 1152。这一维度远超主流 VAE 方案（通道数通常小于 64），为生成过程提供了极高保真度的语义起点。为了将这一思路从 ImageNet 推广至复杂的文本生成场景，研究团队进行了三项深度的架构探索。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;超越规模的数据组成策略&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究发现，单纯增加数据量并不能让 RAE 完美处理文生图任务。团队构建了一个包含约 7300 万条数据的大规模数据集，涵盖了 Web 图像、由 FLUX.1-schnell 生成的高美感合成图像以及专门的 RenderedText 文本渲染数据。&lt;/p&gt;&lt;p&gt;实验数据揭示了一个关键的技术细节：虽然在 Web 规模数据上训练能提升模型对自然图像的泛化能力，但对于「文本渲染」这一特定领域，数据的组成比例至关重要。&lt;/p&gt;&lt;p&gt;如表 1 所示，若缺乏针对性的文本渲染数据，解码器即使在数千万张 Web 图片上训练，也无法还原出清晰的字形细节。只有引入了文本专项数据后，其在 Text 域的 rFID 分数才出现了质的飞跃。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDYGS27TtuBdhOgLCJe3QjceC7VzP1NuJLKiaJ2VCP2OXFUyTIGFIHibGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5287221570926143" data-s="300,640" data-type="png" data-w="853" type="block" data-imgfileid="503529934" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9b98b8d7-c08e-4215-9222-e76765707fe1/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;除了数据组成，研究团队还对比了不同视觉编码器作为 RAE 后端的重建质量。如表 2 所示，在 ImageNet、YFCC 以及文本（Text）这三个维度上，RAE 方案展现出了极具竞争力的保真度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDcUHGLicoacXSqzpnMBXQaYjz0krGnv1Yg2jaKE7tzKpsx5b1IjeOP1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.523943661971831" data-s="300,640" data-type="png" data-w="1065" type="block" data-imgfileid="503529935" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/f46b0507-8168-4eec-8bcb-b70c40e927c9/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDjnibricdgIKpnqUDl7OwPiaBqd9ctX8JPDkpjiahEas36GYNUKBzhqx8vA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6555555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529937" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/425bccc9-c3a3-4af7-a637-87594955f7db/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;虽然 RAE 在绝对重建指标上目前还稍逊于顶尖的 FLUX VAE，但它已经全面超越了此前文生图领域的标杆 SDXL VAE。实验进一步发现，基于自监督学习（SSL）训练的 WebSSL ViT-L 编码器在图像重建任务中比 SigLIP-2 表现更优。这证明了 RAE 框架具备极佳的通用性，能够适配不同预训练目标的视觉编码器。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;潜空间维度相关的噪声调度&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;由于 RAE 操作的是极高维度的语义表征，传统的扩散模型噪声调度方案会因为维度灾难而失效。为了解决这一数学难题，研究团队引入了维度敏感的&lt;strong&gt;噪声调度平移（Noise Schedule Shift）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其核心逻辑是根据有效数据维度 m（即 token 数量 N 与通道维度 s 的乘积）对基础调度 t_n 进行重缩放。其计算公式如下：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD9oTf42GucibZv0NYKzibSibZTeKDic6Wg80MUfcOkXnwDVmuSzln4KG68g/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.17958412098298676" data-s="300,640" data-type="png" data-w="529" type="block" data-imgfileid="503529936" data-aistatus="1" data-original-style="width: 302px;height: 54px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/d65185ed-5319-4d0e-be8c-b14a760d114e/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;其中 &amp;alpha; 是比例因子，n 为参考基准维度。实验证明，应用这一平移变换对模型收敛至关重要，不带平移的模型在 GenEval 上的表现甚至不及带平移模型的一半。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;大模型时代的结构化减法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 RAE 最初针对 ImageNet 的设计中，为了增强模型能力，曾引入过复杂的「宽扩散头（DiT^DH）」以及「噪声增强解码（Noise-augmented decoding）」。然而，这篇论文通过严谨的消融实验发现，当扩散 Transformer（DiT）的规模扩展至十亿参数以上时，这些复杂设计反而成了冗余。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDZA41DX4O5sh1jMUBSUWUdmht7xoeGViaoL20Kmia1ogcKne2O9a5Rg1Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="1.171951219512195" data-s="300,640" data-type="png" data-w="820" type="block" data-imgfileid="503529938" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/58f1e904-c8e0-41a1-8013-ec23f2f84e2b/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;架构冗余&lt;/strong&gt;：DiT^DH 这种窄骨干配合宽头的设计在 0.5B 规模下能带来 11.2 的 GenEval 提升，但当 DiT 扩展至 2.4B 以上时，其增益会迅速消失。这是因为大模型本身的隐藏维度（d&amp;ge;2048）已经天然覆盖了 RAE 的潜空间需求。&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练简化&lt;/strong&gt;：原本用于缩小训练与推理分布差距的噪声增强解码，在训练后期（约 120k 步后）提供的增益也趋于零。这表明在大规模预训练下，模型能够自行学习到足够健壮的潜流形，从而摒弃繁琐的正则化手段。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;实验表现：从极速收敛到无惧过拟合&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队在从 0.5B 到 9.8B 参数的多个 DiT 尺度上，将 RAE 与目前最先进的 FLUX VAE 进行了系统性对比。&lt;/p&gt;&lt;p&gt;在相同的算力与数据条件下，RAE 展现出了显著的收敛速度优势。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDHRiby7gmw5P0Lbqb3zT8Wk5a24sNpjgtJdeia1ej989GAvTeUB5wrYZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.682741116751269" data-s="300,640" data-type="png" data-w="788" type="block" data-imgfileid="503529939" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/c471d511-69db-4a83-a43e-a5004700c76a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;在 1.5B LLM 与 2.4B DiT 的基准测试中，RAE 达到同等生成质量所需的时间仅为 VAE 的四分之一左右。在 GenEval 评测中实现了 4.0 倍加速，在 DPG-Bench 上更是达到了 4.6 倍加速。&lt;/p&gt;&lt;p&gt;这种由 RAE 带来的效率提升与性能增益，在模型规模扩展过程中表现出了极强的鲁棒性。研究团队通过图 5 系统性地评估了 DiT 规模以及 LLM 骨干规模对最终生成效果的影响。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDn3KAQP5cP9MekoEzS09JQUpaNLeAj9OeBBkdRdMibIc4246FYX8lvvw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="1.1502890173410405" data-s="300,640" data-type="png" data-w="865" type="block" data-imgfileid="503529941" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/2bc77f0a-4b97-4dbe-b947-dccf54f1a242/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;在 0.5B 到 9.8B 参数的所有 DiT 尺度下，RAE 均能稳定且大幅度地优于 VAE 方案。即便是在 DiT 隐藏维度仅略大于 RAE 潜空间维度的 0.5B 小模型上，这种优势依然清晰可见。此外，当 LLM 骨干从 1.5B 升级至 7B 时，RAE 模型能够更好地利用更丰富的文本表征，从而获得进一步的性能跨越。&lt;/p&gt;&lt;p&gt;这一发现极具启发意义。以往研究往往认为 LLM 规模的增加对文生图任务的增益有限，但本论文通过微调 LLM 骨干，证明了当生成与理解在同一个语义潜空间中对齐时，更大的语言模型确实能释放出更强的生成潜力。&lt;/p&gt;&lt;p&gt;而在针对高质量数据集（如 BLIP30-60k）进行的精细化微调中，RAE 与 VAE 方案的表现分化更是令人震惊。传统的 VAE 模型在训练至 64 个 epoch 左右后，会发生灾难性的过拟合，性能指标呈断崖式下跌。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDe8b2Go73N3LcbS25zTIJ7iclEticnMPlAjjiaLfI8oYyB28ibJLFV5ibugg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.6821808510638298" data-s="300,640" data-type="png" data-w="752" type="block" data-imgfileid="503529940" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/c172421a-fe15-482b-b1b1-e3af839436c3/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;损耗曲线显示 VAE 的 Loss 会迅速跌至近乎为零，这意味着模型正在机械地死记硬背训练样本。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDtyBskek0Ym1ytpLKNKWeKHKNibiaOonDRx69c0wEZJoicd8CibQpEVXz3A/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.5990740740740741" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529942" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/10273935-fe63-404a-a347-8398d339f2db/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;相比之下，RAE 表现出了极强的鲁棒性。即使持续微调至 256 个甚至 512 个 epoch，RAE 依然能保持稳定的生成质量。这种「天然」的防过拟合特性，或许得益于高维语义空间提供的隐式正则化作用。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;迈向多模态统一的新可能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;RAE 的意义不仅在于生成，它还让&lt;strong&gt;理解与生成&lt;/strong&gt;在同一套语义特征空间中运行。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD3cLJibI2icibO3hAFzY7GbxROOObB5GLbKPj5qQxO6CjwfIc0byllianHA/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.6916666666666667" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529943" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/c7e140fe-6077-4b18-b399-4183bfc5460e/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;理解能力保全&lt;/strong&gt;：实验结果显示，在加入生成训练后，模型在 MME、MMMU 等视觉理解榜单上的性能保持完好，甚至略有提升。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;潜空间测试时缩放（TTS）&lt;/strong&gt;：得益于共享表征，LLM 无需将图像解码为像素，即可直接对扩散模型生成的潜变量进行「理解」和「打分」。通过这种 Best-of-N 策略，模型能显著提升生成图像与提示词的匹配度。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;文生图技术栈的下一站&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这篇论文为大规模文生图提供了一个全新的基础范式。&lt;/p&gt;&lt;p&gt;通过将 RAE 扩展至百亿参数规模，该团队证明了：我们不仅不需要 VAE 来实现高质量生成，甚至可以利用 RAE 获得更快的收敛速度、更高的训练稳定性和更好的多模态统一潜力。&lt;/p&gt;&lt;p&gt;当理解与生成不再需要依靠两个互不相通的潜空间（如 CLIP 与 VAE）来回切换时，扩散模型真正开始学会以「视觉语义」的角度去构建世界。&lt;/p&gt;&lt;p&gt;RAE 的成功，标志着潜向扩散模型正在从繁复的结构堆砌回归到更简洁、更本质的语义建模。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>音频-视觉全模态的未来预测，FutureOmni给出了首份答卷</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 24 Jan 2026 20:14:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-24</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-24</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/6fb8cfde-373b-4232-98fe-3142cb2b2b06/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;p&gt;复旦大学、上海创智学院与新加坡国立大学联合推出首个全模态未来预测评测基准 FutureOmni，要求模型从音频 - 视觉线索中预测未来事件，实现跨模态因果和时间推理。包含 &lt;strong&gt;919&lt;/strong&gt; 个视频和 &lt;strong&gt;1,034 &lt;/strong&gt;个多选题问答对，在 &lt;strong&gt;13 个全模态模型&lt;/strong&gt;和&lt;strong&gt; 7 个纯视频模型&lt;/strong&gt;上的评估显示，当前系统在预测未来事件方面存在显著困难，最佳准确率仅为 64.8%。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在日常生活中，人类不仅能理解「发生了什么」，更重要的是能够预测「将会发生什么」。看到乌云密布、听到雷声渐近，我们会主动关窗收衣；看到老师眉头紧皱，反复强调某个知识点（听），我们知道接下来可能会有提问；看到球员起跳的动作和听到观众的惊呼，我们能够预判这是一个精彩的扣篮。&lt;/p&gt;&lt;p&gt;然而，现有的多模态大语言模型（MLLMs）虽然在全方位感知方面展现出强大的能力，但它们从音频 - 视觉线索中预测未来事件的能力仍然很大程度上未被探索。现有的音视频模态基准主要关注回顾性理解 ⸺ 「视频中发生了什么」，而非前瞻性预测 ⸺ 「接下来会发生什么」。&lt;/p&gt;&lt;p&gt;现在，这一空白终于被填补了！复旦大学、上海创智学院与新加坡国立大学联合发布 &lt;strong&gt;FutureOmni&lt;/strong&gt;，不仅重新定义了多模态模型的「未来预测」评测范式，更通过精心设计的全模态因果推理任务，首次系统评估模型通过「融合视觉观察与听觉线索」来「预测未来」的能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKUwkmRJAYBoLlVsl8qDVIuvZnBjNwkmwqB0HcUQkghXjOgbknXNLOOA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.36666666666666664" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529446" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/05deefa2-70fd-4df1-96cc-ac71ecd196c4/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址： https://arxiv.org/pdf/2601.13836&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码地址： https://github.com/OpenMOSS/FutureOmni&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;数据集地址： https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页： https://openmoss.github.io/FutureOmni&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;评测范式革命：从回顾理解到未来预测&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKx1ZC8dibbARwK8HEr4ECVxqtoosV6iaAabskuJBHujZ8HPyytAxEkD1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3638888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529448" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4ca7c7fe-484a-4710-ab42-82132c4a35d7/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 1：FutureOmni 数据示例。模型需要基于给定的前提事件（premise event），从多个选项中选择最可能的未来事件（future event）。 &lt;/sup&gt;&lt;/p&gt;&lt;p&gt;当前主流的 MLLMs 评测基准存在两大局限：（1）现有基准大多关注「发生了什么」，要求模型描述、理解或分析已经发生的事件，无法评估模型预测未来事件的能力。（2）现有方法严重依赖于视觉信息，即便使用音频，也往往作为辅助信息，未能充分挖掘音频 - 视觉之间的因果关系对预测未来事件的关键作用。&lt;/p&gt;&lt;p&gt;这意味着，过去的多模态模型是一个擅长「事后分析」的观察者，而非一个能&lt;strong&gt;未卜先知&lt;/strong&gt;的智能伙伴。&lt;/p&gt;&lt;p&gt;FutureOmni 提出的&lt;strong&gt;全模态未来预测&lt;/strong&gt;（omni-modal future forecasting）新范式，旨在彻底改变这一现状。它要求模型能像人类一样，主动融合音频对话、环境声音和视觉观察，从多模态上下文中推断出未来最可能发生的事件。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从回顾到预测&lt;/strong&gt;： 不再是回答「视频中发生了什么」，而是预测「接下来最可能发生什么」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从单模态到全模态&lt;/strong&gt;： 同时理解音频中的语义信息（如语音内容、说话人身份、情感倾向）、环境声音（如门铃、警报、音乐）以及视觉观察（画面中的物体状态和人物关系）的因果关系。&lt;/p&gt;&lt;p&gt;这不再是简单的视频理解，而是让模型具备了真正的未来预测能力。它就像一个贴心的智能助手，能够从一段对话、一个动作和周围的环境中读懂「潜台词」，预测未来最可能的发展。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;FutureOmni 数据集：为「未来预测认知」量身打造的大规模评测基准&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKk5Syt04o1L0Hacb5Z2JSpvMh9jTmZJTs3EFaaq3Y8wj0KQAJzicUaAQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.24259259259259258" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529449" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/617663ae-189c-4d66-babe-9efe8e48b2b0/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 2：FutureOmni 评测结果。评估了 13 个全模态模型和 7 个视频模型。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;研究团队构建了 &lt;strong&gt;FutureOmni&lt;/strong&gt; ⸺ 首个大规模全模态未来预测评测基准，包含基于音频 - 视觉因果关系、日常序列、主题蒙太奇的未来事件预测任务。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;海量规模与丰富多样性&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKUdZV9KvpuqDyjmuUDI423XpzNWOM4UcH31mTejhicicgjXjC5xDg6cCw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-ratio="0.5333333333333333" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503529450" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/502d7724-69d4-4ac1-a78d-c06768777a90/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 3：FutureOmni 数据统计分布。&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;919 &lt;/strong&gt;个视频，&lt;strong&gt;1,034 &lt;/strong&gt;个多选题问答对&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;8 &lt;/strong&gt;个主要领域：教育、紧急情况、监控、日常生活、纪录片、电影、游戏、卡通&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;100% 原创视频率&lt;/strong&gt;，确保零污染，所有视频均为首次收集&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;3 种音频类型&lt;/strong&gt;：语音（Speech）、声音（Sound）、音乐（Music）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;八大视频领域：精心设计的「预测考题」&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKvON9yiaBfjQOZl6R9oILFJCLF3ASKdHib7XGyiaIQgP1IwjMaI1W12vqQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3611111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529453" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/4a97604e-c9dc-4186-81a1-b08a4ba336de/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 4：FutureOmni 数据构建流程。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为确保数据的真实性与高质量，研究团队采用三阶段流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;阶段一：视频收集与筛选。&lt;/strong&gt;从多个来源收集原始视频，确保 100% 原创，避免数据污染。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;阶段二：因果对构建。&lt;/strong&gt;使用 LLM 辅助识别具有明确因果关系的视频片段，生成高质量的前提 - 结论对。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;阶段三：问题生成与审核。&lt;/strong&gt;人工和大模型审核质量，确保每个问题都测试模型的未来预测能力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;实验结果：当前模型在「预测未来」上仍面临巨大挑战&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队在 &lt;strong&gt;13 个全模态模型&lt;/strong&gt;和&lt;strong&gt; 7 个视频模型&lt;/strong&gt;上进行了广泛评估，揭示了当前系统在未来预测任务上的显著不足。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;整体性能：SOTA 模型依然不合格&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKGqVKlcvMEqTjXq1RtlUdEfER8ucmmh7U77uY9yYnvyM5wNl8XNxVzQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5731481481481482" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529455" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/780f2ffb-6170-45e1-82dc-2022035cdf06/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 5：FutureOmni 评测结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结论：即便是最强的 Gemini 3 Flash，准确率也仅为 &lt;strong&gt;64.8%&lt;/strong&gt;。开源最强模型 Qwen3-Omni 表现不及格，仅为 &lt;strong&gt;53.05%&lt;/strong&gt;。视觉大模型 GPT-4o 也只达到 &lt;strong&gt;49.70%&lt;/strong&gt;。这表明，现有的多模态大模型在面对复杂的全模态未来预测任务时，距离人类水平仍有不小差距。&lt;/p&gt;&lt;p&gt;细粒度分析：语音场景最具挑战性&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMK8BHBVPTEgzFjdglibXNiaGnGbzXaQpOEE4tbPQqAiaMo3ibcZX3ib4OkyGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.42592592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529461" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/47a37a4f-59fd-4948-a623-10c090637c90/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 6：不同音频类型（语音、声音、音乐）对模型性能的影响。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果显示： - &lt;strong&gt;语音场景&lt;/strong&gt;最具挑战性，模型表现普遍较低（最佳模型 Gemini 3 Flash 仅 60.52%） - &lt;strong&gt;音乐场景&lt;/strong&gt;相对容易，模型表现较好（Gemini 3 Flash 达到 68.31%） - &lt;strong&gt;声音场景&lt;/strong&gt;处于中等难度（Gemini 3 Flash 达到 67.13%）&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKwgG9bqkBiaKJpztfAIefyOic8vnWnibeZPRaYfhXEpMicpKAjVVl9ia2ZAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.425" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529464" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/364df7f5-449b-4770-88e2-db3bbc78f686/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 7：不同视频时长对模型性能的影响。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;模态消融研究：音频信息至关重要&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMK3KbuoYlV7JwicHGEqUhrRAM5U43rfVJt6hemzuTibpsTvMdCuk8VykDw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.22962962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529466" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/fffbe231-0b2d-47b4-9ffe-9a8f9b462d31/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 8：模态消融实验结果。评估不同模态组合对性能的影响。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键发现：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;音频 + 视频&lt;/strong&gt;的组合显著优于单独使用视频。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;音频信息对于未来预测至关重要，缺失音频会导致性能大幅下降。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;跨模态融合能力是成功预测未来的关键。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这证明了 FutureOmni 设计的合理性：未来预测需要同时理解音频和视觉信息之间的因果关系。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;OFF 训练策略：让模型真正「学会预测未来」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了缓解当前模型的局限性，研究团队提出了&lt;strong&gt;全模态未来预测（OFF）&lt;/strong&gt;策略，并精心策划了一个 &lt;strong&gt;7K 样本的指令微调数据集&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;OFF 策略的核心在于：通过专门的未来预测训练，让模型不仅提升未来预测能力，还增强通用感知能力。这与传统的视频理解训练不同，它要求模型学习音频 - 视觉之间的因果关系，并利用这些关系预测未来事件。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练效果：显著提升未来预测和通用能力&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKpkrzesGo4ByeGAwy2zcy53Z2mug2BBI8sJ1MWPU7y5HS8AcLNicqLXg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.40370370370370373" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529467" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ed930d75-4d8d-405e-9dff-52cb4a4e5bd5/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 图 9：使用 OFF 策略训练后，模型在不同音频类型上的性能提升。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKflpwXXicW8qPPodqocsdG316xG5PyBtUk8EsQFssVzdx1w3ygBu6lCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.25833333333333336" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529468" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/2d4f4fe5-7649-48e3-98ef-ad70b4f7163f/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 10：使用 OFF 策略训练后，模型在不同视频类别上的性能提升。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKAF5G8ib1EIM1c7CszFBP8PHUNmcL1KGicWWLztQmRtDsDvib1cB9zSe7w/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.28888888888888886" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529469" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/87dfa28d-9f8a-411c-a76d-8b3dc4455e7c/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 11： OFF 策略在通用能力基准上的泛化效果。证明未来预测训练不仅提升了预测能力，还增强了模型的通用感知能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键发现&lt;/strong&gt;：在 &lt;strong&gt;FutureOmni &lt;/strong&gt;和流行的音频 - 视觉（如 WorldSense、DailyOmni）以及纯视频（如 Video-MME）基准上的评估表明，&lt;strong&gt;OFF&lt;/strong&gt; 策略显著提升了未来预测和通用感知能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键帧差异分析&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW86SLPI8ke4rQDiaibQ4jyDMKZaewX4QvdEfwMsbfjcUEHHnIAnKmeuxIzTaXDAqicy6f98DtwVIQyuw/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.4861111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529471" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/8ed7a8ae-051d-4bcf-82a8-5501117592c0/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 12： 关键帧选择对未来预测的影响分析。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;研究团队利用注意力可视化技术进一步分析 &lt;strong&gt;OFF &lt;/strong&gt;泛化的原因，发现该策略显著增强了模型在深层网络中对关键关键帧的聚焦能力。 如图所示，与基线相比，OFF 模型（蓝线）在网络的深层表现出大幅提升的注意力分数差值。这意味着模型学会锁定包含未来事件线索的关键时刻，即使在最终输出层之前仍能保持对关键信息的高度关注。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;未来展望：让 AI 真正「未卜先知」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;FutureOmni 为多模态大语言模型的未来预测能力提供了首个系统性评估基准。我们期待：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更多模型参与&lt;/strong&gt;，希望更多研究团队在 FutureOmni 上评估他们的模型，共同推动多模态场景下未来预测能力的发展。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;方法改进&lt;/strong&gt;，基于研究团队的发现，开发更强大的未来预测方法，特别是针对语音场景和跨模态因果推理的改进。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;应用拓展&lt;/strong&gt;，将未来预测能力应用到实际场景中，如智能助手、自动驾驶、机器人等，让 AI 真正具备「未卜先知」的能力。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>OpenAI：以后大家用AI赚的钱，我可能要抽成</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 23 Jan 2026 16:46:07 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜杜伟、泽南&lt;/section&gt;&lt;p&gt;今天一早，OpenAI CEO 奥特曼就发推晒收入，「仅我们的 API 业务而言，上个月就增加了超过 10 亿美元的 ARR（年度经常性收入）。」&lt;/p&gt;&lt;p&gt;他继续说到，大多数人只看到了 ChatGPT 的成绩，但 API 团队的工作表现同样令人惊叹。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDGyEELjkG3kLh4qGI1qHqExUgeKy0vFfUlVbGS5bn6JQZmG1UvEeWPA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.4824074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529918" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6a55da28-2746-4e8d-ab95-c512e7ccb2fb/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;奥特曼此举或许是为了提振投资者的信心。这几天，OpenAI 被曝正计划寻求融资 500 亿美元，新的估值预计在 7500 亿美元到 8300 亿美元之间。&lt;/p&gt;&lt;p&gt;与此同时，在外媒 The Information CEO Jessica Lessin 主持的一场达沃斯论坛上，OpenAI CFO Sarah Friar 讨论了另一种商业机会 &amp;mdash;&amp;mdash;「价值共享」（value sharing）。&lt;/p&gt;&lt;p&gt;她提出，在药物研发领域，其他公司可以使用 OpenAI 的技术来发现药物。但一旦新药研发成功，OpenAI 将从自己的 AI 技术为客户创造的收益中分取一部分利润。&lt;/p&gt;&lt;p&gt;这意味着，OpenAI 可能正在考虑从「卖工具」转向「分利润」的商业模式。OpenAI 似乎不满足于只收「软件使用费」，而是想在客户发财时「抽成」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDbCa69SrqrmaUG5ASPDQ0RYyEOtEhuILMMCm0d5oqDEazriajLuDrfiaA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529920" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/6de88620-ccb4-4546-8a51-2a4cbb20ae3a/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; OpenAI CFO Sarah Friar 在达沃斯论坛上。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;最近大家都在讨论 OpenAI 的收入压力，预测他们今年会改变策略想办法盈利。没想到改变商业模式，是这么个改变法？&lt;/p&gt;&lt;p&gt;此番报道一出，真可谓一石激起千层浪，OpenAI 又被推上了风口浪尖。&lt;/p&gt;&lt;p&gt;有人认为，这可能是对 AI 工具化认知的一次巨大颠覆。想象一下，如果你在使用 Photoshop，而 Adobe 却要求对你创作的每一件设计作品抽成。&lt;/p&gt;&lt;p&gt;如果这种做法成为行业标准，对于那些基于 AI API 构建业务的初创公司来说，整个商业模式的成本计算逻辑都将被彻底改变。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDXnL0H7rPEpDM90yOGBcg6PIZp5pcDIibwWWE2dW08G67TPDoiaTUes2w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.40925925925925927" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529921" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4cdb1852-6be0-4200-a133-7e8fbe8556da/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;也有人分析道，「这听起来可能像是异想天开，但科学家们确实已经对大语言模型作为「想法合成器」和「研究助手」的潜力感到痴迷，并且 OpenAI 也确实在积极寻求获取生物、制药等领域的私有数据授权，用于模型训练。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDb0I6Na5FQ6N0kXTPr3F72goSsKWIUp4XuexaFQ83slebnI4jq1ybCw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.26944444444444443" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529922" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a0d96427-dac5-4005-bc38-44a7107f800b/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;不过，更有业内人士感叹道，「一家以非营利性质起家的公司竟然走到了这一步，真是令人尴尬，简直是一种耻辱。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDXGduIuCicym6p4Sbia7DPG2pyT5oObmmicLp0ulq66s97mOmiaXqUS90kA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0453703703703703" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529923" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/990e46f8-9850-4529-88f2-6178e4fe34dc/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;「所以，OpenAI 打算从用户使用其软件开发的知识产权（IP）中抽取分成，而他们自己的软件，却又是通过侵犯他人的知识产权构建出来的。」&lt;/p&gt;&lt;p&gt;一直以来，OpenAI 面临着 AI 训练数据的来源争议，其模型使用的训练数据中包括受版权保护的文章、书籍、代码和艺术作品。此前，《纽约时报》以及包括多位作家在内的个人就曾对 OpenAI 未经授权使用其数据来训练模型提起过诉讼。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDqjACB7iavOe4ac7nXNyRgibN11jF1VZicPwyXCaTerEuzhUPOBFPA1nnA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.25092592592592594" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529924" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e026a181-51f5-4d8d-b7ee-7205412f7ca5/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;对于企业来说，OpenAI 的这种模式是可接受的吗？答案或许是肯定的。并且，这可能会令 OpenAI 损失更多商业客户。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD6Xl5dzySmuK3bNAd1ibDKNX5IO4mVicOgaNR3y0uZ0vxzWxfP3W5Ojqw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.21481481481481482" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529925" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/8cceab91-bf49-49c4-89db-4191dd8edecf/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDMdec3u0iarAXqkv2lDUoVs0cMWbQYJ02jP6PnZyBII5RTTYvd8BFv7Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.21203703703703702" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529926" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/d446b6e3-6efb-4803-ae8a-5f656bd12aa8/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;OpenAI 转变思路的背后，AI 正在加速药物研发&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;OpenAI 抛出收入分成论的原因，一部分看到是如今 AI 作为科研工具已经已经开始起效了。&lt;/p&gt;&lt;p&gt;最近，制药和生物技术公司纷纷开始使用各种形式的 AI 进行药物研发。已有多家大型医药公司宣布了与 OpenAI 的深度合作，尝试使用 OpenAI 的模型来分析大量数据并提出假设或测试方法。去年 10 月，为制药公司提供服务和设备的企业 Thermo Fisher Scientific 表示，它将使用 OpenAI 模型来加速药物开发，并识别哪些疗法不太可能成功。&lt;/p&gt;&lt;p&gt;OpenAI 似乎也在开发越来越复杂、专门用于生物学和药物方向的 AI 模型，以推动 AI 在药物发现过程中更直接地协助制药公司。&lt;/p&gt;&lt;p&gt;例如，OpenAI 最近与生命科学诊断供应商 Revvity、Xero 以及其他生物技术公司进行了洽谈，想要获得授权以使用它们更专业的数据来训练自家 AI 模型。&lt;/p&gt;&lt;p&gt;在 AI + 医药研发方向，OpenAI 并不是唯一的一家，Anthropic 以及谷歌 DeepMind 等也与一些早期生物技术初创公司就数据许可或合作事宜展开了讨论。&lt;/p&gt;&lt;p&gt;Sarah Friar 无疑熟悉像 Recursion 这样较早期的 AI 药物研发公司，这些公司曾与制药企业达成交易，若其技术成功识别出药物，将获得巨额奖金。不过，目前这样的成功案例即使有，也寥寥无几。&lt;/p&gt;&lt;p&gt;竞争虽然才起步，但已非常激烈：OpenAI 的对手 Anthropic、Google DeepMind 以及 Alphabet 旗下专注于利用 AI 进行药物研发的子公司 Isomorphic Labs，也都已与早期生物技术初创公司就数据许可或合作关系进行了讨论。&lt;/p&gt;&lt;p&gt;上周末，Sarah Friar 在一篇博客文章中做了某些暗示，OpenAI 也可以在能源和金融领域达成这种价值共享类型的安排。「基于知识产权（IP）的许可协议和基于结果的定价，将分享所创造的价值，」Sarah Friar 这样写道。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDRggS2jBriaAasibCo0peJWAluZ0vUjUtlHGyNzt1WfCB0DiboZ3SBAibTw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5435185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529928" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/d4432112-f81a-4e34-8de4-ad7a318b9096/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 博客地址：https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;大语言模型已经很擅长发现人类可能错过的架构和形态。OpenAI 的模型有时可以将不同领域的概念联系起来，提出新型实验建议，涉及从核聚变到病原体检测的各个方面。尽管这些模型存在许多局限和错误，但科学家们似乎仍对它们充满热情。&lt;/p&gt;&lt;p&gt;虽然这听起来可能有些天马行空，想象 OpenAI 通过 IP 许可或版税获得的收入比广告还多，但 Sarah Friar 的言论极其清晰地释放了他们想要做什么的信号。&lt;/p&gt;&lt;p&gt;问题在于，在 OpenAI 完成目前正向投资者寻求的数百亿美元融资之后，她是否还会继续谈论这一点。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.theinformation.com/newsletters/applied-ai/openai-plans-take-cut-customers-ai-aided-discoveries?rc=jn0pp4&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.tipranks.com/news/the-fly/openai-spoke-to-revvity-about-licensing-data-the-information-reports-thefly&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>陈天奇、贾扬清点赞：Vibe Coding版PyTorch，连论文都是AI写的</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 23 Jan 2026 16:42:35 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Panda、泽南&lt;/section&gt;&lt;p&gt;前两天，Node.js 之父 Ryan Dahl 在 X 上断言：「&lt;strong&gt;人类编写代码的时代已经结束了&lt;/strong&gt;。」该帖引发广泛讨论，浏览量更是已经超过了 700 万。而现在，我们迎来了一个对这一判断的有力证明。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529847" data-ratio="0.5400254129606099" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDlXAsWZTBI8zJJfHILDY7ia26riaiciaoYQwLvicSXKH9UwqHDB5EpnJiaWMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="787" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/9b890084-f026-4076-b1ba-eabc571b9f08/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;刚刚，英伟达杰出工程师许冰（Bing Xu）在 GitHub 上开源了一个新项目 &lt;strong&gt;VibeTensor&lt;/strong&gt;，让我们看到了 AI 在编程方面的强大实力。&lt;/p&gt;&lt;p&gt;从名字也能看出来，这是 Vibe Coding 的成果。事实也确实如此，这位谷歌学术引用量超 20 万的工程师在 X 上表示：「&lt;strong&gt;这是第一个完全由 AI 智能体生成的深度学习系统，没有一行人类编写的代码。&lt;/strong&gt;」&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529848" data-ratio="0.6615776081424937" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDQHenXRVBYTSyc96xBiaeiczVqJM5kJVxvEYJwqWzKNyaOGia75l0oKibIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="786" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/5ee5114b-9eeb-45e7-8161-418c7bed53be/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;更具体来说，VibeTensor 是一个可运行的深度学习系统，配备了 RCU 风格的调度器、缓存分配器和反向模式自动微分器。该智能体还发明了一种&amp;nbsp;&lt;strong&gt;Fabric 张量系统&amp;nbsp;&lt;/strong&gt;&amp;mdash;&amp;mdash; 这是目前任何框架中都不存在的新东西。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529857" data-ratio="0.5342592592592592" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDRibMfOBHBgcm2V2edLU1lmicXFP6bugkHoWjTM8RTMPJ6lgGOx34nrfQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/32951d9c-aed9-4d82-98ad-482cb81871ab/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 很明显，许冰分享的这张项目架构图也是 AI 生成的&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;其 Vibe Kernel 包含 13 种不同类型、总计约 4.7 万行代码的自动生成内核，这些内核使用 Triton 和 CuteDSL 编写，并且具有很强的性能表现。&lt;/p&gt;&lt;p&gt;许冰表示，VibeTensor 由&lt;strong&gt;英伟达的第四代智能体&lt;/strong&gt;生成。但它也呈现出了一种「&lt;strong&gt;弗兰肯斯坦效应（Frankenstein Effect）&lt;/strong&gt;」：系统本身是正确的，但某些关键路径的设计效率低下。因此，其性能无法与 PyTorch 相媲美。&lt;/p&gt;&lt;p&gt;更重要的是，许冰强调：「自 2025 年夏天以来，我一行代码都没写过。」他说这项工作是他看过 Andrej Kaparthy 的播客之后开始的。「我当时并不认同他的观点，所以我和 Terry Chen（英伟达首席工程师）开始用它来测试我们的智能体的能力。弗兰肯斯坦效应最终暴露了我们智能体的一些局限性 &amp;mdash;&amp;mdash; 但方向很明确。」&lt;/p&gt;&lt;p&gt;该项目在 X 上引起了不少关注，许冰的几位著名英伟达同事（也被列为参与者）也有分享点评。&lt;/p&gt;&lt;p&gt;比如陈天奇表示：VibeTensor 很有意思，它表明 AI 智能体能够构建深度学习框架这样复杂的东西。「生成的代码还有一些需要改进的地方，但它能够做到这一点本身就非常有趣。」&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529850" data-ratio="0.42857142857142855" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDCm86TUxoMTTOJfMvDQ2XzCfF30zqKBOGibUGXF04ibaYACiaBwqzMUfCg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="784" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/cbdc2ac9-07b3-464a-8f2a-31a911296563/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;贾扬清的评价则更高，他表示该项目的出现罕见地验证了一个根本性问题：AI 能否编写复杂的系统代码？而该项目给出的答案是「能，但是&amp;hellip;&amp;hellip;（仍有问题）」。他说 AI 正以惊人的速度前进，「如果我们能掌握更多正确的原则，AI 终将完全超越人类程序员。这就像 2015 年 1 月的 AlphaGo。」&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529851" data-ratio="0.45616264294790343" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD0tic1X17bGibywfTYwRVHYVETws69QgvVA44XLdc2Hib2mQWvgBCQ78gw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="787" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f58825f7-8071-49cb-a890-d59a6a89ae8a/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;目前，许冰已经在 GitHub 上 NVlabs 帐号下发布了 VibeTensor 的相关内容，其中也包含一篇论文。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529853" data-ratio="0.375" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDT6e4kVu6Zm18kd0qSx1icAF9b7vmKHdTBR4uuq6B5hgkVTC8snSbiazA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/01e0ce16-3512-490d-996f-b7721fa7cc7b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：VibeTensor: System Software for Deep Learning, Fully Generated by AI Agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://github.com/NVlabs/vibetensor/blob/main/docs/vibetensor-paper.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目链接：https://github.com/NVlabs/vibetensor&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有意思的是，当我们初看这篇论文时，我们发现论文中有一些 AI 生成的内容。于是我们询问了许冰本人，而他给出的答案让我们非常震惊：&lt;strong&gt;这篇论文竟也是 100% 由 AI 撰写的！&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529852" data-ratio="0.5421940928270043" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD6Ig1zmToY5kuMapM8gNfZcDQCiasXZWt94cBic5J9qcbiaBmu2icC4kUOQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="474" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/bdfae72e-a737-4190-a4c3-5760743c3741/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 许冰的回复&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;下面我们就来详细看看这个 AI 编写的项目究竟是什么。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;VibeTensor：全球首个完全由 AI 智能体生成的全栈系统&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;VibeTensor 可不仅仅是又一个深度学习库。它是全球首个完全由 AI 智能体生成的全栈系统。从 Python/Node.js 的上层绑定，到 C++ 核心调度器，再到最底层的 CUDA 内存管理，每一行代码的增删改查、每一次 Bug 的修复、每一轮构建验证，全部由英伟达第四代智能体（Agent）独立完成。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529855" data-ratio="0.6657407407407407" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDxlLjJapWbvJV5jpIzN8yjcibherFa4raO4WPmibffut79wJOawatCo3w/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/721be502-d9ec-4f4d-91b9-1754bd6e1345/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;而人类的作用是提供了高层级的需求指导，然后像监工一样看着 AI 智能体在两个月内疯狂输出。下面就来拆解一下这个氛围编程版的 PyTorch：VibeTensor。&lt;/p&gt;&lt;p&gt;首先，性能上虽然 VibeTensor 目前还无法与 PyTorch 这种经过多年磨砺的框架抗衡（根据论文测试，部分场景慢了约 1.7 到 6.2 倍），但作为一个功能完整的技术原型，其设计的完整度令人吃惊。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529854" data-ratio="0.6194444444444445" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDBUibket7lkeiaUh6WJY0QEDnXc9IJzJ8liaMB0KF3Swrwm5oMmmOoticBg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/a21361c3-9555-4331-9a43-f01c72d29434/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529856" data-ratio="0.3888888888888889" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDYn5NAxWPSeiaPlc6RiaHTnecdzGVUXXZib6gQcrzEjaFDDru3oG5k78Kg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/53f2b295-c5db-4ea6-9fa1-87b6fa0693b7/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;根据论文描述，VibeTensor 并不是一个简单的包装库，它拥有极其硬核的底层架构。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心运行时的「暴力美学」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;VibeTensor 的 C++20 核心并非简单的库调用。它实现了一个完整的 TensorImpl 架构，作为参考计数的 Storage 之上的视图。令人惊讶的是，AI 赋予了它支持非连续视图（Non-contiguous views）和 as_strided 语义的能力，并引入了原子版本计数器来确保原地（In-place）操作的安全性。&lt;/p&gt;&lt;p&gt;在算子调度层面，AI 构建了一个 schema-lite 调度器，能够将 vt::add 这样的操作名精准映射到 CPU 或 CUDA 的内核实现上。这种设计支持锁定（Boxed）和非锁定（Unboxed）调用路径，并通过不可变的快照状态（Snapshot states）实现了稳态下的无锁调用，极大地压低了调度开销。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;独创的 Fabric 张量系统：不属于任何现有框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 VibeTensor 的所有组件中，最令人振奋的莫过于名为 Fabric 的实验性子系统。这是目前市面上任何主流深度学习框架（如 PyTorch 或 TensorFlow）中都不曾以这种形式存在的概念。&lt;/p&gt;&lt;p&gt;Fabric 本质上是一个显式的多设备抽象层。它的核心使命是打破单卡运行时的限制，直接接管硬件拓扑的自动发现过程。根据论文描述，Fabric 能够主动识别 CUDA P2P（点对点）和 UVA（统一虚拟地址）支持情况。&lt;/p&gt;&lt;p&gt;不同于传统框架将多卡通信隐藏在复杂的分布式 API 后，Fabric 提供了一套透明的可观测原语，允许研究者直接控制内存的放置与同步策略。&lt;/p&gt;&lt;p&gt;在 VibeTensor 的 Blackwell 评估中，AI 甚至基于 Fabric 构建了一个可选的环形全归约（Ring-allreduce）插件。这种插件直接绑定了 CUTLASS 的实验性内核，完全绕过了 NCCL。这意味着 AI 已经开始尝试从底层通信协议层面，去重构大规模分布式训练的逻辑。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529861" data-ratio="0.649074074074074" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDZCNa38fcNSE3ReKofxUwaemBXuiaIGVJXZxPsoFktlBnhMM0n0TErsQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/52236822-b80d-4daa-9c61-0b500832105a/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;异步优先的「Node.js + Python」双前端&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在用户界面上，AI 并没有止步于复刻一个 PyTorch。它不仅利用 nanobind 打造了一个高度兼容的 Python 覆盖层（vibetensor.torch），还开创性地引入了一个基于 Node-API 的 Node.js 插件。&lt;/p&gt;&lt;p&gt;这个 JavaScript/TypeScript 界面采用了纯粹的「异步优先」设计。所有的重负载任务都被调度至 napi_async_work 以避免阻塞 Node 事件循环，并通过一个全局在途任务上限（VBT_NODE_MAX_INFLIGHT_OPS）来精细控制排队压力。这种横跨数据科学（Python）与后端工程（Node.js）的选型，体现了 AI 智能体在处理异构开发环境时的灵活性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 内核套件：从算子到显存的全自动进化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在最底层的算子实现上，VibeTensor 附带了一个由 AI 生成的庞大内核套件。这里包含了 200 多个源文件，涵盖了从基础的 LayerNorm 到复杂的 Fused Attention 等各类算子。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529858" data-ratio="0.32407407407407407" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD62IBsoMhR0GQRrgnhCzic5YqHMf67jPficCme5FnaHskpLUp7QOYUrXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/9c9d81a6-4aa1-44b6-badb-23341dc445b5/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这些内核利用了 Triton 和英伟达自家的 CuTeDSL 编写。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529859" data-ratio="0.6268518518518519" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDK8GfgPXbEJcLKDTFnSVz6xbEJjmMIVwgxMkvuqA5Y9TXEmjc8lhezQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/761e26ad-62e0-4b46-bef8-9684a8d54fa6/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;值得注意的是，AI 生成的内核并非只是「能用」，在 H100 的实测中，其生成的 Fused Attention 内核在特定形状下，前向计算比 PyTorch 的原生 FlashAttention 快了 1.54 倍，后向计算快了 1.26 倍。尽管这只是孤立算子的表现，但它证明了 AI 在掌握硬件特性（如 Hopper 架构的 TMA 或 Tensor Cores）方面的巨大潜力。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529860" data-ratio="0.3638888888888889" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDRiazbbbAchmg6mawTT0LEZ7iaYChcecU6gG34CicpH4vmicxjY1ibic5ltLA/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/e0b428cf-feb5-4541-a071-9e04cba18f85/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;弗兰肯斯坦效应：AI 编程的隐形墙&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管 VibeTensor 能够跑通复杂的神经网络模型，但许冰和团队在论文中诚实地提出了一个引人深思的概念：「&lt;strong&gt;弗兰肯斯坦效应（Frankenstein Effect）&lt;/strong&gt;」。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529862" data-ratio="0.6268518518518519" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDf31SRicMvoPv0qrSuHBM2qe036VQzOgHzMh1mq3wWXKlLRZfibbjCAuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/09bf0e52-c6a8-4683-90ba-20f38c49ff53/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这是 AI 智能体在构建复杂系统时暴露出的核心局限性。简单来说，AI 能够确保每一个局部子系统（如调度器、分配器、算子）在逻辑上是正确的，且能通过单元测试。但当这些局部组件拼凑成一个庞大的全局系统时，它们之间会产生意想不到的「摩擦」，形成性能瓶颈。&lt;/p&gt;&lt;p&gt;例如，AI 为了确保多线程环境下的安全性，在 Autograd 引擎中设计了一个非重入的全局互斥锁。这个设计从局部看非常稳健、安全，但在全局运行时却成了「扼杀」并行性能的元凶，导致原本高效的显卡内核因数据等待而频繁空转。这种「正确但低效」的代码，正是目前智能体在系统级架构设计上的天花板。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 辅助的开发方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;VibeTensor 的诞生并非源于一次简单的提示词工程，而是一场长达两个月的、由高层级人类指令驱动的 Agent 自主演化过程。许冰也让 AI 在论文中用一个章节专门总结了「AI 辅助的开发方法」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 彻底的「黑盒」工作流&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这场实验中，人类的角色从「程序员」彻底转变为「监工」与「策略制定者」。许冰及其团队并没有进行任何代码层面的 Diff Review（差异审查），也没有手动运行过任何验证命令。&lt;/p&gt;&lt;p&gt;相反，开发流程被简化为一个持续循环的闭环：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;目标设定&lt;/strong&gt;： 人类指定一个作用域明确的目标和必须遵守的约束条件。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;代码生成&lt;/strong&gt;： AI 智能体自主提议代码更改，并以 Diff 的形式应用到仓库中。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具校验&lt;/strong&gt;： Agent 会自动调用编译器、测试框架和差异检查工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多智能体评审&lt;/strong&gt;： 为了弥补单体 AI 可能存在的盲点，团队引入了多 Agent 协作评审机制，用于捕捉缺失的边界情况、冗余的抽象或是潜在的安全隐患。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. 测试驱动的「硬核」规范&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 Agent 驱动的开发中，&lt;strong&gt;测试不再是锦上添花，而是唯一的「真理来源」&lt;/strong&gt;。VibeTensor 的每一行代码都必须经过 C++（CTest）和 Python（pytest）双重测试套件的洗礼。&lt;/p&gt;&lt;p&gt;更具创新性的是，AI 智能体还利用 PyTorch 作为一个「参考原件」，建立了一套自动化的 API 对齐检查器。当 AI 编写的算子出现数值偏差或内存泄漏时，Agent 会自主分析报错日志，添加一个最小化的回归测试用例，并重新进入修复循环。这种「测试即规格说明」的模式，确保了即使在缺乏人工干预的情况下，生成的 16 万行代码依然保持了极高的逻辑一致性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 跨层级调试的挑战&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;论文揭示了一个有趣的现象：&lt;strong&gt;AI 在处理「单次正确」的任务时表现卓越，但在处理系统的「组合稳定性」时却面临巨大挑战&lt;/strong&gt;。例如，在 Fused Attention 算子的移植过程中，Agent 经历了多次挫败：从最初的参数超限、显存对齐错误，到运行数千次后才暴露出的缓冲区初始化隐患。&lt;/p&gt;&lt;p&gt;这种跨越 C++ 运行时、CUDA 驱动程序和 Python 封装层的多级调试能力，正是此次英伟达第四代智能体展示出的最核心竞争力。它证明了 Agent 已经能够理解复杂的内存语义和硬件约束，而不仅仅是模仿代码片段。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 工程师的「AlphaGo 时刻」？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;VibeTensor 的出现并非为了取代 PyTorch，而是一场关于「生成式软件工程」的宏大实验。&lt;/p&gt;&lt;p&gt;正如前文所述，许冰提到这项工作的灵感源于 Andrej Karpathy 的播客。当时他并不完全认同 Karpathy 关于「AI 编程」的某些激进观点，于是决定和首席工程师 Terry Chen 一起，用最硬核的系统开发来测试智能体的极限。&lt;/p&gt;&lt;p&gt;现在，方向已经明确。虽然「弗兰肯斯坦效应」依然存在，但 VibeTensor 的诞生标志着一个新时代的开启：未来的系统软件可能不再是工程师逐行敲出来的，而是由人类定义需求、由 AI 在「氛围」中生成出来的。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/bingxu_/status/2014354974986408138&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/tqchenml/status/2014360719534227561&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/jiayq/status/2014373196934590593&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/rough__sea/status/2013280952370573666&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>思维链太长拖慢推理？把它「画」进隐空间！新框架RoT探索大模型隐空间推理新范式</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 23 Jan 2026 16:37:03 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/a039cb52-11c0-414a-a597-8978d41c0203/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="2" data-pm-slice="0 0 []"&gt;在 LLM 时代，思维链（ CoT）已成为解锁模型复杂推理能力的关键钥匙。然而，CoT 的冗长问题一直困扰着研究者&amp;mdash;&amp;mdash;中间推理步骤和解码操作带来了巨大的计算开销和显存占用，严重制约了模型的推理效率。&lt;/p&gt;&lt;p data-path-to-node="3"&gt;为了解决这个问题，研究界近期尝试了「隐式 CoT」（Implicit CoT），即让模型在内部隐状态中完成推理，而不输出具体的文本。这种方法虽然快，但却是个「黑盒」：我们无法知道模型到底想了什么，也难以进行监督。&lt;/p&gt;&lt;p data-path-to-node="4"&gt;&lt;strong&gt;有什么方案既保证推理速度快，又使得过程可分析，还无需昂贵的预训练？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="5"&gt;针对这一挑战，腾讯内容服务部 BAC 联合清华大学与北京大学，提出了一种名为 &lt;b data-index-in-node="39" data-path-to-node="5"&gt;Render-of-Thought (RoT)&lt;/b&gt; 的新框架。RoT 的核心思想非常巧妙：&lt;strong&gt;利用多模态模型（VLM）已有的视觉编码器作为「语义锚点」，将文本推理步骤「渲染」为图像的视觉嵌入（Visual Embeddings）。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="6"&gt;这种方法不仅将推理过程压缩到了致密的视觉潜空间中，还通过视觉渲染让隐式推理过程变得可分析且可追踪。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDu68qibiaSoWEIOLO1oIjB0prvjZ6AdUibZDyr6BmdWgZWNLoKc2k8kZZg/640?wx_fmt=jpeg#imgIndex=1" data-ratio="0.5083333333333333" data-type="png" data-w="1080" data-width="1706" data-height="812" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDkypVhEUPwLvO0TQn2rv2Adc0icF8AQysF6yUDm65ic3MpXKO7a5wUWhg/0?wx_fmt=png&amp;from=appmsg" data-cropx1="91.49826989619378" data-cropx2="1599.7439446366782" data-cropy2="767.4048442906574" data-imgfileid="503529781" data-aistatus="1" data-original-style="width: 511px;height: 260px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/dcbb546b-c11d-4a10-b296-3e7ddc8b3be3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.14750&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Github 地址：https://github.com/TencentBAC/RoT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Huggingface地址：https://huggingface.co/collections/TencentBAC/rot&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="7,1,0"&gt;&lt;strong&gt;显式太慢，隐式太黑盒？RoT 走出第三条路&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="10,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="10,0,0"&gt;显式 CoT (Explicit CoT)：&lt;/b&gt; 让模型把每一步推理都写出来，就像学生做数学题写步骤一样。生成几百个 Token 的中间步骤不仅费时，还极其消耗显存。&lt;/p&gt;&lt;p data-path-to-node="10,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="10,1,0"&gt;隐式 CoT (Implicit CoT)：&lt;/b&gt; 模型直接在内部隐状态中进行推理，不输出具体文本。这种方式就像把思考过程扔进了一个「黑箱」，缺乏中间过程的监督。&lt;/p&gt;&lt;p data-path-to-node="10,2,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="10,2,0"&gt;Render-of-Thought (RoT)：&lt;/b&gt; 另辟蹊径，&lt;strong&gt;把「思考」变成了「作画」&lt;/strong&gt;。利用视觉信息的高密度特性，将冗长的文本压缩成紧凑的视觉向量。这不仅有迹可循，还大幅提升了推理速度。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadYGQvTdaHZMdJriamtG2rMLpMRwOGNWtk69uz9OMYw7bb6jSEq14dP9w/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.785929648241206" data-type="png" data-w="995" data-width="995" data-height="782" data-imgfileid="503529627" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e1a0b74e-6bb2-4b2d-9fa9-fbd0986bde60/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="11"&gt;&lt;strong&gt;拒绝「黑盒」：让隐式推理「看得见、摸得着」&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="12"&gt;RoT 是一种将文本思维链通过光学渲染（Optical Rendering）和视觉知识蒸馏转化为紧凑视觉表征的新范式。&lt;/p&gt;&lt;p data-path-to-node="13"&gt;与以往需要从头学习「推理 Token」的隐式方法不同，RoT 直接利用了现有 VLM（如 Qwen-VL, LLaVA）中冻结的视觉编码器。通过将 LLM 的隐状态与渲染文本的视觉嵌入对齐，RoT 实现了&lt;strong&gt;即插即用（Plug-and-Play）&lt;/strong&gt;，无需额外的预训练开销。渲染方案将文本推理步骤转化为单行图像，隐空间推理方法通过投影头将 LLM 生成的隐状态与视觉特征对齐。&lt;/p&gt;&lt;p data-path-to-node="14"&gt;为了适应自回归思维链的序列化建模，研究团队摒弃了固定尺寸的图像渲染方案，采用了&lt;strong&gt;单行图像&lt;/strong&gt;渲染。该策略可以根据文本长度动态修改所需的图像宽度。此外，单行的渲染方式确保图像的 Patch 严格按照从左到右的方式提取，自然地将视觉序列与文本顺序对齐。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadGiaYE8CKNNC49CxsGg8gIUSzXxFE8pY74kxnYPoa9uricibOn0OcIQsDA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5370370370370371" data-type="png" data-w="1080" data-width="1093" data-height="587" data-imgfileid="503529628" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ab332645-f31d-4464-be5f-afd2f0676707/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="15"&gt;&lt;strong&gt;移花接木的艺术：两步训练实现「降维打击」&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="16"&gt;RoT 的实现过程主要分为两个阶段，旨在逐步将 LLM 的离散推理能力转化为连续的视觉隐空间推理能力。&lt;/p&gt;&lt;p data-path-to-node="17"&gt;&lt;b data-index-in-node="0" data-path-to-node="17"&gt;阶段一：视觉对齐 (Visual Alignment)&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="18"&gt;这一阶段冻结了 LLM 和视觉编码器，仅训练一个轻量级的「视觉投影头」（Visual Projection Head）。目标是将 LLM 的文本隐状态映射到由视觉编码器提取的「渲染 CoT 图像」的特征空间上。&lt;/p&gt;&lt;p data-path-to-node="19"&gt;在推理步骤 &lt;span data-index-in-node="6" data-math="t"&gt;t&lt;/span&gt; 时，生成的 latent embedding 可以记为&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadyhJ6qNsmGWdhjichy46z1GCml4N7vBIv2EeQJv6JPRylZIOv4FmsVug/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.09814814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529633" data-aistatus="1" data-original-style="width: 175px;height: 20px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/84d7489e-3fec-4b38-baec-f9c91cc7085e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 36.02%;"&gt;，target vision embedding 记为 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadXPXco9ZicicKeiaicZhNh8oTjdph5qHEQveYw76dmRAZSrGtVicfEKFrZUw/640?wx_fmt=jpeg#imgIndex=5" data-ratio="0.8571428571428571" data-s="300,640" data-type="png" data-w="105" type="block" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadgibg7iaP7yZfM8lQW54W2K2VoMUL90D8NSsPrvktXXrG0A5EicNlOibRTg/0?wx_fmt=png&amp;from=appmsg" data-cropx2="105" data-cropy2="89.55882352941175" data-imgfileid="503529634" data-aistatus="1" data-original-style="width: 21px;height: 20px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/e1265094-e96a-4d0e-9800-db24469042cb/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dii" style="width: 3.21%;"&gt;。此时 vision embedding 的对齐损失可以记为：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadf9AwazQkfyLCzIQzqseNXSqfYibAJNvb5SdFqSKA72IqklJDFHZloNQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.3114942528735632" data-s="300,640" data-type="png" data-w="870" type="block" data-imgfileid="503529636" data-aistatus="1" data-original-style="width: 207px;height: 64px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/549be7e6-a41c-4791-b805-237f794a64b9/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 30%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="21"&gt;此外，在第一阶段中，为了使模型与所提出的推理模式保持一致，同时对 &lt;code data-index-in-node="33" data-path-to-node="21"&gt;&amp;lt;|img_end|&amp;gt;&lt;/code&gt; 这一 special token 和答案的交叉熵损失进行了建模：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadBarAZlo3no8O8UL8BhLqIUTpXt2EdbmzrSX2NRj33jOTtYUaTI6m9w/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.11574074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529637" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/2c555f39-41f5-4191-8465-2b52638e2431/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="23"&gt;其中&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadqoD8FuWNAGgHlTgL9CufmomvDAq4KnXCjOadd5pickrwqibvtHnbDVKQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.0305343511450382" data-s="300,640" data-type="png" data-w="131" type="block" data-imgfileid="503529638" data-aistatus="1" data-original-style="width: 20px;height: 21px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/bb0ef910-e116-428b-b39b-960677afc775/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.57%;"&gt;是生成的 latent visual tokens，&lt;span data-index-in-node="46" data-math="y"&gt;y&lt;/span&gt; 为问题 &lt;span data-index-in-node="52" data-math="x"&gt;x&lt;/span&gt; 的 ground truth 答案。阶段一的整体损失函数为上述两者加权：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadL98zUED0Qbn7qLqBz74GmucDqDOoooTsqqJE0gbsJjq7goRTDGaoDg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.20588235294117646" data-s="300,640" data-type="png" data-w="850" type="block" data-imgfileid="503529639" data-aistatus="1" data-original-style="width: 192px;height: 40px;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e4260102-339b-43a9-939e-14de13d44f9b/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 30%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="25"&gt;&lt;b data-index-in-node="0" data-path-to-node="25"&gt;阶段二：潜在监督微调 (Latent Supervised Fine-Tuning)&lt;/b&gt;&lt;/p&gt;&lt;p data-path-to-node="26"&gt;在对齐之后，第二阶段通过 LoRA 微调 LLM，并且冻结已经训练对齐的投影头。此时，模型不再生成文本 Token，而是自回归地生成一串连续的「潜在视觉 Token」（Latent Visual Tokens）。这些 Token 在隐空间中模拟了视觉编码器的输出，最终引导模型解码出正确的文本答案。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadg2WZOSgzBhphMMh7cL9vxlGAhJuREiaVyia5b4CZfiaOWcBJ9drOp9CYA/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.16203703703703703" data-type="png" data-w="1080" data-width="1094" data-height="177" data-imgfileid="503529641" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/5b161342-5664-464c-a99e-3e0776f41c9f/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="27"&gt;&lt;strong&gt;推理与解码策略&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="28"&gt;推理过程要求模型自主地从连续的潜在推理空间导航到离散的文本解空间。研究团队探索了两种方案：&lt;strong&gt;基于 Special Token 的动态终止策略以及固定 Token 预算的静态终止策略。&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="29"&gt;基于 Special Token 的动态终止策略&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="30"&gt;推理阶段在第一个时间步长 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadUmh0N9fka7qlFdMztTjBwDznkqiby6UViay5JnAvgxRRiab89J5NnbQLw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.6043478260869565" data-s="300,640" data-type="png" data-w="230" type="block" data-imgfileid="503529644" data-aistatus="1" data-original-style="width: 40px;height: 24px;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/f5138232-4cb8-43e7-adc1-c9bf587bdc45/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dii" style="width: 4.31%;"&gt;&amp;nbsp;结束，此时终止标记的概率达到最大值：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad8tjseC36YIAI9tvNY28x2YnxDrHoFiaBCq7JBn5FAmSjib9p8cZpibkXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.10277777777777777" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529645" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/fba7ae8d-55f6-410d-8c31-68c26f6c7a83/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="32"&gt;其中&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadfuql4lPbKzxu3wia2B7gZpq4cRCfAv0EqtjL8fJoA0pJIiceaWR4MKNw/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="1.0810810810810811" data-s="300,640" data-type="png" data-w="111" type="block" data-imgfileid="503529646" data-aistatus="1" data-original-style="width: 20px;height: 22px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/d57c2e09-d8fb-4ec1-bff3-c0d210971133/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dii" style="width: 2.57%;"&gt; 表示 Token 集，&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadNhCnmcpt0qfKmQ36nPibSj34fHrbSkHSOfSSl0Svr5lM7gZrMt3Vgmw/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="1.238938053097345" data-s="300,640" data-type="png" data-w="113" type="block" data-imgfileid="503529647" data-aistatus="1" data-original-style="width: 20px;height: 25px;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/c17fb0ab-c7b7-41b6-b61e-1493dfc80599/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dii" style="width: 2.75%;"&gt; 表示在时间步长 t 时的隐藏状态。模型从后续状态&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadOHqGuJZWJ7yNmReWGyrNPibUAQRyzWwJIwXOHs89qGSv3sjFDsa6q8Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5015197568389058" data-s="300,640" data-type="png" data-w="329" type="block" data-imgfileid="503529648" data-aistatus="1" data-original-style="width: 55px;height: 28px;" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/3349a054-f3d2-4944-8b89-380caa4dbefa/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dii" style="width: 5.96%;"&gt;开始对文本答案进行解码。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="33"&gt;固定 Token 预算的静态终止策略&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="34"&gt;该策略将潜在思维链的长度限制为一个固定的超参数。达到这个阈值时，会手动添加 &lt;code data-index-in-node="38" data-path-to-node="34"&gt;&amp;lt;|img_end|&amp;gt;&lt;/code&gt; 这一 special token，以触发从潜在推理到文本生成的转换。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;研究团队在实验中发现，动态终止策略的性能明显低于固定 Token 预算策略。这种性能差距可能源于连续潜空间中自我调节停止机制的&lt;strong&gt;内在不稳定性&lt;/strong&gt;。在生成潜空间推理嵌入时，隐藏状态可能无法始终如一地为终止标记生成高置信度的预测，从而导致过早或延迟的转换，破坏推理流程。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;此外，采用固定 Token 预算策略时，每个数据集的最优 Token 预算各不相同。在 GSM8k-Aug 数据集上，32 个 Token 能实现最佳性能，而 MATH 数据集则需要 64 个 Token 才能达到峰值准确率。研究者推测这种差异的出现是因为 MATH 数据集更具挑战性，需要更长的推理链。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadCC0XZfdcXJQqZZxNzVwkx66xBFicvBFLiahfJIs04bdMBSMYbmW8Eh2w/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.6639784946236559" data-type="png" data-w="372" data-width="372" data-height="247" data-imgfileid="503529649" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/12fa8b7a-90a8-49eb-93d1-4247980375c6/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="37"&gt;&lt;strong&gt;实测数据说话：推理速度「狂飙」&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="38"&gt;研究团队在 GSM8k、MATH、SVAMP 等多个数学和逻辑推理基准上对 RoT 进行了广泛测试。实验基于 Qwen3-VL 和 LLaVA-V1.6 等主流架构。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="39,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,0,0"&gt;显著的压缩与加速：&lt;/b&gt; 相比于显式 CoT，&lt;strong&gt;RoT 实现了 3-4 倍的 Token 压缩率&lt;/strong&gt;。在推理速度上，RoT 展现出了巨大的优势。例如在 Qwen3-VL-4B 模型上，Pass@1/&lt;a data-topic="1" href="javascript%3A;"&gt;#L&lt;/a&gt;（准确率与长度比）指标显著优于基线。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="39,0,0"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiad0fDWHp3P8KmZlOFpHJjXicibTHMgWzRVVjN5r1tkbcvQYjkkqlDHOlrA/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.35833333333333334" data-type="png" data-w="1080" data-width="1094" data-height="392" data-imgfileid="503529651" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/d77688dc-3c72-4d3b-a7cc-d1b2b58eaa7e/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="39,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,1,0"&gt;优于现有的隐式推理方法：&lt;/b&gt; 与 Coconut、CoLaR 等最新的隐式推理方法相比，RoT 在准确率上表现出色。特别是在 MultiArith 数据集上，RoT (Qwen3-VL-4B) 达到了 97.2% 的准确率，显著优于同等规模下其他隐空间推理方案。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="39,1,0"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadTPBREseibFicPia8JK372yaYK294wRk4nEdqcRVQ7cgpch8mAxxiaJWQWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.3490740740740741" data-type="png" data-w="1080" data-width="1094" data-height="382" data-imgfileid="503529650" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/37113efa-9a50-4ee0-8dd7-eabcc4e10c3c/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadpqAPrud2evia53J4l5c9EqxDicr4UQ2lArIhVxjYuf1YdMrH0ZKIWfVw/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-ratio="0.398876404494382" data-type="png" data-w="1068" data-width="1068" data-height="426" data-imgfileid="503529652" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/ae1c1458-3c7b-4e02-ac8e-e8d27e006d5c/640.png" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="39,2,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,2,0"&gt;隐空间推理的可分析性：&lt;/b&gt; RoT 的一大亮点在于其&lt;strong&gt;可分析性&lt;/strong&gt;。由于隐状态被对齐到了视觉空间，可以通过热力图（Heatmap）等来观察模型的「思考过程」。研究团队展示了 MATH 数据集的一个案例。可以看到，生成的潜在 Token 呈现出明显的结构化模式，Token 相似度矩阵显示了推理的阶段性。这证明模型并非在随机生成向量，而是在进行有逻辑的隐式推理。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadNx89qV8oKvyaBNJ1sg6RyVDEKWDH3m7swCnSpfiaovAxlv1NufgIEVA/640?wx_fmt=png&amp;from=appmsg#imgIndex=20" data-ratio="0.5351851851851852" data-type="png" data-w="1080" data-width="1791" data-height="958" data-imgfileid="503529653" data-aistatus="1" data-original-style="null" data-index="22" src="https://image.jiqizhixin.com/uploads/editor/fc8440e1-874a-4db8-a0ce-d2e9c35bcfeb/640.png" alt="图片" data-report-img-idx="20" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="40"&gt;&lt;strong&gt;单行渲染 vs. 多行渲染&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="41"&gt;在 RoT 中，传统的固定尺寸的多行渲染会导致文本在图像中频繁换行。对于模型来说，这种换行在视觉空间中引入了不必要的「空间跳跃」，打断了语义的连续性。&lt;/p&gt;&lt;p data-path-to-node="42"&gt;为了验证这一点，研究团队对比了「固定尺寸的多行渲染图像」与 RoT 文中使用的「单行动态宽度图像」。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadRnRZNGFW9p8Gicz4TJUCgMRd3raCnPacwsL3fGspdDofpvMo44fJlzw/640?wx_fmt=png&amp;from=appmsg#imgIndex=21" data-ratio="0.5859106529209622" data-type="png" data-w="582" data-width="582" data-height="341" data-imgfileid="503529654" data-aistatus="1" data-original-style="null" data-index="23" src="https://image.jiqizhixin.com/uploads/editor/98bb8e5d-1297-4f56-9235-94e59665b9b4/640.png" alt="图片" data-report-img-idx="21" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="42"&gt;如上图所示，单行渲染相比多行渲染收敛更快，同时能够更好地契合语言模型从左到右的序列生成特性。&lt;/p&gt;&lt;p data-path-to-node="43"&gt;&lt;strong&gt;两阶段训练缺一不可&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="44"&gt;为了评估渐进式训练策略的效果，研究团队分别对每个阶段进行独立消融实验。&lt;/p&gt;&lt;p data-path-to-node="45"&gt;去除第一阶段会导致 MATH 的准确率从 33.2% 降至 22.2%，表明视觉对齐对于构建潜在空间结构以及在复杂任务中防止表示坍缩至关重要。同样，排除第二阶段也会导致性能显著下降，这会导致模型难以从连续的潜在空间中推导出最终答案。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8uYMdNaRAc4S0K5BxQVoiadK1ibDPev5jJibQNt99ZPqlmLGggm5ibIqvspmiafFT3AzUWLX48vD1boibw/640?wx_fmt=png&amp;from=appmsg#imgIndex=22" data-ratio="0.2594142259414226" data-type="png" data-w="717" data-width="717" data-height="186" data-imgfileid="503529655" data-aistatus="1" data-original-style="null" data-index="24" src="https://image.jiqizhixin.com/uploads/editor/4077d247-7701-4402-b9e7-0fcc2c7115b7/640.png" alt="图片" data-report-img-idx="22" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="46"&gt;&lt;strong&gt;展望&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="47"&gt;Render-of-Thought 提出了一种极具前景的「视觉化思维」范式。它打破了文本模态的限制，利用视觉信息的高密度特性来压缩推理过程。&lt;/p&gt;&lt;p data-path-to-node="48"&gt;这项工作不仅大幅提升了推理效率，更重要的是，它通过「&lt;strong&gt;将思维渲染为图像&lt;/strong&gt;」这一直观的想法，为理解大模型神秘的内部隐空间提供了一扇新的窗口。对于未来在端侧设备等资源受限场景下部署强推理模型，RoT 提供了一条切实可行的技术路径。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>启动经费550万起！全球顶级AI人才看过来</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 23 Jan 2026 15:05:14 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img data-aistatus="1" data-backh="211" data-backw="578" data-imgfileid="100005278" data-ratio="0.3648148148148148" data-src="https://mmbiz.qpic.cn/mmbiz_gif/Oax1aO3yZ9swXKItI7vROicnE9r2aDCTs0dNgXBVGmBMIcW1T2rAiczkviaLlyMWlu44hwl5hOm9bGPTRvBu56z5w/640?wx_fmt=gif&amp;from=appmsg#imgIndex=0" data-type="gif" data-w="1080" type="block" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ba464ce4-f7f8-441a-b292-747ace86b246/640.gif" data-order="0" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;strong&gt;致全球英才：&lt;/strong&gt;&lt;/p&gt;&lt;section data-role="outer" label="edit by 135editor"&gt;&lt;section data-id="167079" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section data-autoskip="1"&gt;&lt;p&gt;北京中关村学院是全新的高等教育科研机构，&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-tools":"135编辑器","data-id":"167079"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"style":"margin: 10px auto;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"style":"background-color: #f5f5f5;padding: 30px 10px;box-sizing:border-box;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-autoskip":"1","style":"text-align: justify;line-height:1.75em;letter-spacing: 1.5px;font-size:14px;color:#333333;background-color: transparent;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"margin-left: 5px;margin-right: 5px;margin-bottom: 15px;display: block;line-height: 2;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"font-size: 15px;letter-spacing: 0.5px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;与全国31所双一流高校共建，专注于人工智能与交叉学科的人才创新培养。&lt;/span&gt;中关村人工智能研究院是年轻的探索型研发机构，深耕前沿技术研发与产业转化。中关村两院秉持&amp;ldquo;极基础、极应用、极交叉&amp;rdquo;的颠覆式理念，以&amp;ldquo;培养AI领军人才&amp;rdquo;为使命。&lt;/p&gt;&lt;p&gt;我们拥有各层级人才项目自主评审权，将于2月6日面向全球英才召开人才线上交流会暨第四届中关村国际青年论坛宣导会，提供最直接权威的人才政策解读、在线答疑交流，诚邀全球顶尖人才参加！&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;strong data-brushtype="text"&gt;&lt;span data-mpa-action-id="mkq9ir60u71" data-pm-slice="0 0 []"&gt;会议邀请&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-id="166163" data-role="title" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;时间：2026年2月6日&amp;nbsp;&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;p&gt;&lt;strong&gt;形式：线上宣讲&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;报名：扫描下方二维码&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-ratio="0.9959349593495935" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/ZiaENxymVJPibRh2b99mSyibCWVjp305sJwtGtiaCguaJxfW17g1BZYyE55NkevZa6hZVATPLO5JaNFlTVicD3kOqYg/640?from=appmsg#imgIndex=0" data-w="246" data-width="88px" data-original-style="box-shadow: #979899 3.53553px 3.53553px 8px;margin: 0px 8px 8px 0px;border-radius: 0px;width: 88px;vertical-align: baseline;box-sizing:border-box;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/7d6ea6c9-3d77-40d9-9bbf-8d0215789c15/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 30%;"&gt;&lt;/section&gt;&lt;section data-role="outer" label="edit by 135editor"&gt;&lt;section data-pm-slice='6 6 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]' data-role="paragraph"&gt;&lt;p&gt;&lt;strong&gt;截止时间：2026年2月1日&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;咨询邮箱：talent@bjzgca.edu.cn&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;p&gt;*本次宣讲会为邀约制，报名成功后将发送会议链接&lt;/p&gt;&lt;p&gt;期待与您在云端相会，共同探讨您的未来发展之路！&lt;/p&gt;&lt;/section&gt;&lt;section data-id="166163" data-role="title" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong data-brushtype="text"&gt;&lt;span data-mpa-action-id="mkq9j2plrgb" data-pm-slice="0 0 []"&gt;会议议程&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;（&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"line-height: 1.75;margin-bottom: 10px;display: block;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"font-size: 15px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;一&lt;/span&gt;）&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;两院整体情况介绍&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;p&gt;（二）&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;两院科研情况介绍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;（三）&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;海优人才政策介绍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（四）中关村国际青年论坛介绍&lt;/span&gt;&lt;/p&gt;&lt;p&gt;（五）&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;海优获得者经验分享&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"section","attributes":{"data-role":"outer","label":"edit by 135editor"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"section","attributes":{"data-role":"paragraph"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"line-height: 1.75;margin-bottom: 10px;display: block;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"font-size: 15px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;（六）在线答疑&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-id="166163" data-role="title" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong data-brushtype="text"&gt;&lt;span data-mpa-action-id="mkq9j75i1z10" data-pm-slice="0 0 []"&gt;申请条件&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;p&gt;&lt;strong&gt;国家级领军人才：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.&amp;nbsp;一般应当取得博士学位。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2. 在国外著名高校、科研机构等担任相当于&lt;strong&gt;副教授及以上职务、具有较高科研水平和较强科技创新能力&lt;/strong&gt;，或者在国际知名企业担任&lt;strong&gt;高级职务的专业技术人才&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;3. 专业方向：从事&lt;strong&gt;人工智能领域以及部分交叉学科&lt;/strong&gt;（如自然学科、部分人文社会科学领域）的高层次人才（含非华裔外籍人才）。&lt;/p&gt;&lt;p&gt;4. &lt;strong&gt;2025年1月1日后回国（来华）工作，或者尚未全职来华。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5. &lt;strong&gt;任职要求&lt;/strong&gt;：在引进后须辞去海外工作或者在海外无工作，且全职回国（来华）工作不少于3年。&lt;/p&gt;&lt;p&gt;6. &lt;strong&gt;破格条件&lt;/strong&gt;：对业绩特别突出或者从事&amp;ldquo;卡脖子&amp;rdquo;关键技术领域的急需紧缺人才，可放宽学历等申报条件。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;国家级青年人才：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;1. &lt;strong&gt;尚未全职回国（来华）工作，或者2025年1月1日后回国（来华）工作。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2. 在取得博士学位（在国内、海外取得博士均可）后至2026年4月15日前，在海外知名高校、科研机构、企业研发机构等获得&lt;strong&gt;正式教学或者科研职位，且具有连续36个月以上工作经历&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;3. 要求&lt;strong&gt;具有博士学位&lt;/strong&gt;，&lt;strong&gt;取得同行专家认可的科研或技术等成果&lt;/strong&gt;，且具有成为该领域学术带头人或杰出人才的发展潜力。&lt;/p&gt;&lt;p&gt;4. 40周岁以下，在海外取得博士学位且业绩特别突出的，可放宽工作年限要求（不适用于通过中外联合培养方式取得海外博士学位的情况）。&lt;/p&gt;&lt;/section&gt;&lt;section data-id="166163" data-role="title" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;strong data-brushtype="text"&gt;&lt;span data-mpa-action-id="mkq9jgtfw9" data-pm-slice="0 0 []"&gt;支持保障&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;p&gt;&lt;strong&gt;1. 薪酬待遇：&lt;/strong&gt;提供极具竞争力的优厚薪酬待遇。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 获聘岗位：&lt;/strong&gt;入选国家级领军人才项目，获聘教授岗位、博导资格；入选国家级青年人才项目，获聘高级职称、博导资格。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 启动经费：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;国家级领军人才项目：给予每人100万元一次性生活补助+国拨300万元科研经费支持+市级生活补贴配套100万元+区级生活补贴配套50万元+两院配套项目经费。特别优秀者，科研启动经费一事一议。&lt;/p&gt;&lt;p&gt;国家级青年人才项目：给予每人50万元一次性生活补助+国拨经费资助100-300万元+市级配套生活补贴50万元+区级配套生活补贴50万元+两院配套项目经费。特别优秀者，科研启动经费一事一议。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4. 科研支持：&lt;/strong&gt;&lt;/p&gt;&lt;section data-role="list"&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;两院提供充足的算力支撑；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;支持申报国家重大项目、院立项目及自立项目，科研项目优先支持搭建博士生及博士后团队；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;学术生态涵盖31所共建一流高校、国家级科研机构及优质科创企业。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;5. 生活保障：&lt;/strong&gt;&lt;/p&gt;&lt;section data-role="list"&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;引进落户：落户海淀，配偶及子女随迁；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;子女教育：优先保障海淀区中小学优质教育资源；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;安居保障：提供低于市场租赁价格的精装高级人才公寓保障；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;商业医疗：为本人、配偶及子女提供医疗保障。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong data-brushtype="text"&gt;&lt;span data-mpa-action-id="mkq9jlj73x5" data-pm-slice="0 0 []"&gt;关于两院&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-id="166163" data-role="title" data-tools="135编辑器"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;北京中关村学院：全新的高等教育机构&lt;/strong&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role="paragraph"&gt;&lt;section data-role="list"&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;携手顶尖高校，培养人工智能领军人才；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;推动人工智能与交叉学科领域的科研创新。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;中关村人工智能研究院：新型研发机构&lt;/strong&gt;&lt;/p&gt;&lt;section data-role="list"&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;推动创新成果产业化落地，孵化创新企业；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;打造创新生态，赋能千行百业。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="3 3 []"&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2c6adc63-9454-4ff2-a2dd-af3e0d3eb192/1769151823374.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5ace1f8f-208d-4ce1-8009-5a448722649a/1769151839828.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;img data-aistatus="1" data-backh="567" data-backw="578" data-imgfileid="100005116" data-ratio="0.9814814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Oax1aO3yZ9tx9ibdAEBTVq1KoCrHfqqYIRVgqvUuYshjibgjZcxPbs61NKUJ3WYHnRJAUt4KibvsOX4ccJTo9T4HQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a152b75c-7d30-48b0-99a8-e5ca3c73a82e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>三星爆火递归模型TRM唯一作者被迫离职，内部不认可？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 23 Jan 2026 14:58:57 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜冷猫&lt;/section&gt;&lt;p&gt;还记得三个月前，来自三星的一位研究员的独作论文发布即爆火，颠覆了递归推理模型架构，让一个仅包含 700 万个参数的网络，性能比肩甚至超越 o3-mini 和 Gemini 2.5 Pro 等尖端语言模型，震惊了大量业内研究人士。&lt;/p&gt;&lt;p&gt;这篇论文是大名鼎鼎的《Less is More: Recursive Reasoning with Tiny Networks》，带来了影响深远的&lt;strong&gt;微型递归模型 TRM&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;关于这篇论文和模型的相关信息，可以参阅&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650994404&amp;idx=1&amp;sn=145d2c814af6a15e537dbf1028a2a8ff&amp;scene=21#wechat_redirect" target="_blank"&gt;我们之前的报道&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;按理说，发布成果的&lt;strong&gt;唯一作者 Alexia Jolicoeur-Martineau&lt;/strong&gt; ，在三星应当平步青云，带领全新的团队继续后续研究，用 TRM 的后续研究助力三星在人工智能领域的进步。&lt;/p&gt;&lt;p&gt;可惜一切似乎都不尽如人意。突然间，Alexia 就发推说要离职。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDDS2LKxXEC7QTdZYviaEpuBAicE1kC99b0HcubIOKz4KCJ0wzSOmHSSzA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5018518518518519" data-type="png" data-w="1080" data-width="1148" data-height="576" data-imgfileid="503529824" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e991230d-a8a5-4672-a248-93c685b4bb45/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;从推文中明显能看出 Alexia 的怨气。&lt;strong&gt;「在 TRM 取得巨大成功（为公司赚取数十亿美元）后，我在三星的生活变得一团糟。」&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDib7Mn1iczTQY9GtkWSmSeUeepbkL6iaicYcLcBD2o2qRsGXsDSjx4IVWMA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.362962962962963" data-type="png" data-w="1080" data-width="1162" data-height="422" data-imgfileid="503529826" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/63bb6cf6-8573-4512-bcf2-632074fed4a9/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;止不住的怨气来源于， Alexia 的工作，加拿大蒙特利尔三星先进技术研究所人工智能实验室（SAIL Montreal）取得的最大成绩，并未能够被内部认可。&lt;/p&gt;&lt;p&gt;听上去令人匪夷所思。TRM 在学术界的成绩有目共睹，一度被视为三星在 AI 领域崛起的信号。可以确定的是，在 TRM 论文发表后，三星的股价的确产生了相当可观的涨幅。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDib2HPJrg6mLTACLxpRZWygT003jibibibNLnOaia9IjzWyGJFGOGzC9UZ9g/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.8601851851851852" data-type="png" data-w="1080" data-width="1290" data-height="1110" data-imgfileid="503529827" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/00993a0e-281c-4498-b93e-2b5ef1317c59/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;此外，Alexia 与她的论文也获得了 2025 年的 ARC Prize 论文奖的头名，获得了五万美元的奖金。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDB9cic3uF6NSoGnjXw9iasosdibbiciaJah4F55fTCfTx6UCqXGZYECMja4Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.26296296296296295" data-type="png" data-w="1080" data-width="1270" data-height="334" data-imgfileid="503529828" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b3a37162-e1fa-4204-b985-7531d1ec5eee/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在获奖后，ARC Prize 与 Alexia 做过一次专访，当时她已提及，她未能与韩国的高管们联系，而他们是从韩国文章中知道这件事的。或许这已经暗示了一些三星内部对于 TRM 和 SAIL 的态度。感兴趣的读者们可以查看专访视频。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;专访链接：https://www.youtube.com/watch?v=P9zzUM0PrBM&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDBficplibeK2vkbwU2pFPIVr57pZfS9b2nzHpwnhE2yQzA9Sn8xEUicsYg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1" data-type="png" data-w="374" data-width="374" data-height="374" data-imgfileid="503529829" data-aistatus="1" data-original-style="width: 283px;height: 283px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f9437a51-00b1-4e3e-8056-02f6d593b165/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;关于 Alexia Jolicoeur-Martineau 本人，目前在三星从事人工智能研究工作。2019 年冬季，开始在蒙特利尔学习算法研究所（MILA）攻读人工智能博士学位。学术背景涵盖统计学与计算机科学（数学与计算机科学学士、统计学硕士）。&lt;/p&gt;&lt;p&gt;Alexia 此前的研究工作包括：GottaGoFast、对抗式得分匹配（Adversarial Score Matching）、相对式 GAN（Relativistic GANs）、采用交替优化训练的 LEGIT 模型，以及对贝叶斯树先验的研究。此外，还使用多种生成对抗网络（GAN）生成过猫咪图像（Meow Generator）。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDMiadz2mMadHByyGA5PPvZibp4ZUtCYdq9qciblaQuUs6CTkT1KiaKg7iaGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="1" data-type="png" data-w="530" data-width="530" data-height="530" data-imgfileid="503529830" data-aistatus="1" data-original-style="width: 344px;height: 344px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/346e31f9-ed76-4a98-a1cf-ad7175c0710d/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 这下认识了。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;据 Alexia 本人所述，2018 年，她被大学拒绝，所以选择了用一块 GPU 开始自己做 AI 研究。撰写的第二篇论文被 DeepMind 研究员 Ian Goodfellow 选中，于是命运的齿轮开始转动。Ian 帮助 Alexia 进入 AI 领域，随后她便开始和 Ioannis Mitliagkas 攻读博士学位。&lt;/p&gt;&lt;p&gt;在官宣离职的推文下，我们看到了熟悉的身影。Sebastian Raschka 对她的离职表示遗憾：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDqUTfqIdTP8ic2a0Ld7wMtoHovIbr9ibn00oMU5yTxIWGPrpfkV8L83wA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.3731481481481482" data-type="png" data-w="1080" data-width="1174" data-height="438" data-imgfileid="503529832" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/e8200800-b8f2-4694-92ce-eacc58d194c1/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;而 Liquid AI 则直接开始挖人：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="248" data-imgfileid="503529836" data-ratio="0.21296296296296297" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDqubbiaicYpYibyzwhtaKoSic3G6AajiayIFvwKyzLuFoJj1hekkIibricrjtA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" data-width="1166" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/67bda997-2130-4a9b-ace8-82b384d0c788/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;最终 Alexia 会在哪继续她的研究之路，让我们拭目以待。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Nature子刊｜上智院、复旦、无限光年发布MAPLE框架，破解甲基化衰老与疾病风险预测的泛化难题</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Fri, 23 Jan 2026 14:04:23 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-23-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-23-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;作者丨论文团队&lt;/p&gt;&lt;p&gt;编辑丨ScienceAI&lt;/p&gt;&lt;p&gt;衰老是具体而实在的：它既体现在皱纹增多、体力下降这些多数人能感受到的变化上，也发生在身体内部细胞和分子水平的缓慢累积之中。过去十多年里，科学家逐渐认识到，DNA 甲基化作为一种稳定而系统的表观遗传标记，能够记录个体真实的生物学衰老状态，并与多种慢性疾病的发生风险密切相关。因此，表观遗传时钟（Epigenetic Clock）不仅被视为衡量「人老得快还是慢」的工具，也逐渐成为评估衰老干预效果、预测疾病风险、以及开展个体化健康管理的重要量化手段。&lt;/p&gt;&lt;p&gt;然而，一个长期制约该领域发展的核心难题在于泛化能力。不同研究队列、不同测序平台、不同预处理流程乃至不同组织来源之间，都会引入显著的技术差异和系统偏移。许多经典的衰老时钟（Aging Clock）在原始研究数据中表现良好，但一旦应用到新的数据集或真实临床场景，预测精度便明显下降。这使得表观遗传时钟在临床转化、跨队列研究以及长期健康随访中的应用受到限制。&lt;/p&gt;&lt;p&gt;在这一现实背景下，上海科学智能研究院（下称上智院）与复旦大学人类表型组研究院、复旦大学人工智能创新与产业研究院（下称复旦大学 AI&amp;sup3; 院）、无限光年技术有限公司（下称无限光年）等进行联合研究，提出了一个稳健的基于成对学习的甲基化年龄与疾病风险预测框架&amp;nbsp;MAPLE（A Robust Computational Framework for&amp;nbsp;Methylation&amp;nbsp;Age and Disease-risk&amp;nbsp;Prediction&amp;nbsp;Based on&amp;nbsp;Pairwise&amp;nbsp;LEarning），从方法学上引入成对学习思想缓解了高维小样本条件下的过拟合问题，并为跨平台、跨组织的统一建模提供了可行路径。&lt;/p&gt;&lt;p&gt;在全部 31 项测试中，MAPLE 的平均绝对误差为 1.6 年，显著优于多种现有主流方法，并且在疾病识别上曲线下面积均值达 0.97，对疾病前驱状态检测也达到 0.85，显示其精准识别早期风险的能力。MAPLE 不仅在数值精度上取得了突破，更重要的是在方法层面提供了一种可泛化的表观遗传建模范式，为衰老干预评估、慢性病早筛以及长期健康管理奠定了更加可靠的量化基础。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.5435185185185185" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027226" data-aistatus="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmFicnqVlYiaoFLrJia2Siab4tm8zdwXkaaoo11BdLVLWK7LoOKwfiaNicFqrgc3mL8g0No808j7AFlEdYw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-original-style="height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/878ba106-1108-48d0-97ac-49bf201c8e70/640.png" alt="图片" data-before-load-time="1769148150815" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="2 2 []"&gt;论文题目：A robust computational framework for methylation age and disease-risk prediction based on pairwise learning&lt;/p&gt;&lt;p&gt;论文地址：https://www.nature.com/articles/s43588-025-00939-x&lt;/p&gt;&lt;p&gt;代码地址：&lt;/p&gt;&lt;p&gt;https://aistudio.ai4s.com.cn/galaxy-model/partner/galaxy-model-frontend/model/1221437&lt;/p&gt;&lt;p&gt;https://github.com/Drizzle-Zhang/MAPLE&lt;/p&gt;&lt;p&gt;该研究成果已发表于 Nature Computational Science。上智院研究员张雨、无限光年算法科学家姚易辰，为共同第一作者。复旦大学金力院士，上智院首席科学家、复旦大学特聘教授漆远，上智院领域科学家何莹，无限光年联合创始人、复旦大学 AI&amp;sup3; 院研究员徐盈辉，为共同通讯作者。无限光年实习生唐元昊，上智院生命科学方向负责人、复旦大学 AI&amp;sup3; 院研究员程远，为共同作者。&lt;/p&gt;&lt;p&gt;研究项目由星河启智科学智能开放平台（https://aistudio.ai4s.com.cn/）和复旦大学 CFFF 智算平台提供技术和算力支持。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不再直接「算年龄」，而是先理解样本之间的相对衰老关系&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;既往的表观遗传衰老模型大多遵循一个直接的建模范式：从单一样本的甲基化谱出发，预测一个对应的「绝对年龄」或「绝对风险分数」。这种做法在数据条件理想、训练与测试分布高度一致时往往有效，但在真实研究和临床应用中却面临明显挑战。&lt;/p&gt;&lt;p&gt;其根本原因在于，甲基化数据高度敏感于测序平台、预处理流程以及组织来源等非生物因素。在这种情况下，模型往往更容易学习到「样本来自哪个实验体系」，而非真正反映个体衰老或疾病风险状态的生物学信号，导致跨队列、跨组织应用时性能迅速下降。&lt;/p&gt;&lt;p&gt;针对这一问题，研究团队在方法学上采取了不同的建模视角：不再要求模型直接输出绝对数值，而是让模型先学习样本之间的相对关系 &amp;mdash;&amp;mdash; 哪一个样本更老、哪一个样本疾病风险更高。通过在训练阶段构建大量样本对，模型被迫关注那些在不同数据来源中始终保持一致的变化趋势，从而有效弱化技术噪声和系统偏差的影响。&lt;/p&gt;&lt;p&gt;成对学习策略带来了两个直接收益。一方面，它显著降低了平台和预处理差异对模型的干扰，提高了跨数据集的稳定性；另一方面，通过样本成对组合，模型在有限样本规模下获得了更充分的监督信号，有效缓解了高维小样本条件下的过拟合问题。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.1305555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027227" data-aistatus="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmFicnqVlYiaoFLrJia2Siab4tmW3I9ol4VqCB1QHGIoHmP2X2icgR1BTDZq1VGp2y4CdrnZL8HsYrlG7w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-original-style="height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8bb824c3-e107-4c25-9869-20700a28df71/640.png" alt="图片" data-before-load-time="1769148151532" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;衰老不仅能「算得准」，还能「对得上生物学」&lt;/p&gt;&lt;p&gt;在系统评估中，该方法在来自不同研究、不同测序芯片、不同数据标准化流程以及多种组织类型的 31 组独立测试中展现出高度稳定的性能。整体来看，其甲基化年龄预测的中位绝对误差约为 1.6 年，显著优于多种现有主流方法；即使在非血液组织（如脑、肌肉、脂肪和皮肤）中，预测精度依然保持在较高水平，显示出良好的跨组织泛化能力。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.6326530612244898" data-s="300,640" data-type="png" data-w="784" type="block" data-imgfileid="100027228" data-aistatus="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmFicnqVlYiaoFLrJia2Siab4tm7MHiblA9pdYy1atTRDJgptcugFQva7hiaaxtNwxUafg3XaibNfSATEDZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-original-style="height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/19b8254e-e1a7-4090-8ca2-69d5aae1f74c/640.png" alt="图片" data-before-load-time="1769148151650" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;除了数值精度，该框架在生物学解释层面同样表现突出。通过对模型关注的关键甲基化位点进行分析，研究发现这些位点在不同独立研究之间具有高度一致性，其关联基因显著富集于发育调控、组织重塑、免疫调节、神经功能及认知等经典衰老相关生物过程。这表明，模型并非仅依赖统计相关性进行拟合，而是优先捕捉具有明确生物学意义的调控信号。&lt;/p&gt;&lt;p&gt;进一步的人群与疾病分析显示，该方法能够识别一系列细微但具有生物学指向性的衰老特征。例如，在女性人群中，模型捕捉到围绝经期附近出现的显著衰老节律变化；在吸烟、肥胖、唐氏综合征、HIV 感染以及阿尔茨海默病等人群中，模型一致检测到明显的衰老加速信号。值得注意的是，在阿尔茨海默病分析中，该方法在脑组织中识别出的衰老加速特征，在血液样本中并不显著，提示其具备区分组织特异性衰老信号的能力。&lt;/p&gt;&lt;p&gt;这些结果共同表明，该框架不仅在预测层面表现稳定，也能够真实反映衰老相关的生物学过程。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.0398148148148147" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027229" data-aistatus="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmFicnqVlYiaoFLrJia2Siab4tmOML9fHo7zr5teia2A2ia9ptyMI5zaUdkYicGicibqSL89SzLQdiaxmKYiavfA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-original-style="height: auto !important;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/dcda12b2-aae6-4ae1-99c4-ec4b97088cad/640.png" alt="图片" data-before-load-time="1769148151831" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;从衰老测量走向疾病风险预测&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;衰老评估的最终价值，并不止于刻画「生物年龄」，而在于揭示疾病风险的累积与演变。基于同一成对学习框架，研究团队进一步将模型扩展至心血管疾病和 2 型糖尿病等常见慢性疾病的风险评估任务，使表观遗传信号能够直接服务于疾病风险建模。&lt;/p&gt;&lt;p&gt;在多项独立测试中，该方法能够有效区分健康人群、疾病前驱状态以及确诊患者。在心血管疾病任务中，模型在疾病识别和动脉粥样硬化等前疾病状态的识别性能均明显优于传统风险模型；在 2 型糖尿病相关分析中，模型同样能够区分系统性胰岛素抵抗、前驱糖尿病等状态与确诊患者，显示出对疾病连续进展过程的良好刻画能力。&lt;/p&gt;&lt;p&gt;更进一步的分析表明，这种性能优势并非仅来自年龄信息的叠加。即便在控制不同人群年龄分布后，模型的判别能力依然保持稳定，说明其捕捉到的是与疾病发生和进展直接相关的表观遗传变化。模型所强调的关键甲基化位点，其关联基因在血管结构重塑、免疫炎症反应、代谢调控和胰岛素信号通路等疾病相关生物过程中显著富集，提示模型不仅能够区分疾病状态，也在分子层面识别出与病理机制一致的信号。&lt;/p&gt;&lt;p&gt;这一特性使得该框架在慢性病早筛、风险分层以及长期健康管理等场景中具备潜在应用价值，同时也为将表观遗传信息更系统地纳入疾病生物学研究提供了新的计算工具。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.4435185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027230" data-aistatus="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmFicnqVlYiaoFLrJia2Siab4tm9gSzibyNq6Mbgxz6uFiaLr9Dj24iaD2s3Z3Gx3XaNxGF6IQQgNFDQ7eUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-original-style="height: auto !important;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/5a4e0d87-7652-451d-8cbc-fe913e08a0f6/640.png" alt="图片" data-before-load-time="1769148153082" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;总体而言，MAPLE 的意义并不局限于在既有基准上取得更优的预测指标，更在于为表观遗传建模提供了一种可推广的方法论范式。通过成对学习，模型将建模重心从不稳定的「绝对数值预测」转向更具跨数据集一致性的「相对关系学习」，在高维、样本规模受限且来源高度异质的甲基化数据条件下，有效缓解了过拟合与批次效应对模型泛化能力的制约。这一设计使模型能够在不同测序平台、预处理流程和组织来源之间提取稳定的生物学信号，为基于表观遗传信息的衰老时钟和疾病风险预测工具走向真实世界应用奠定了方法学基础。&lt;/p&gt;&lt;p&gt;从更长远的科学智能发展视角来看，MAPLE 也为机制发现与方法融合打开了空间。一方面，模型在不同数据集中稳定聚焦的关键甲基化位点，为解析衰老与疾病相关的调控通路提供了更高信噪比的候选集合；另一方面，该框架具有良好的可扩展性，随着纵向随访队列和多组学数据的不断积累，该框架将被应用在更多的表观遗传数据检测场景，有望成为连接分子层面衰老过程、疾病演进机制与干预评估之间的重要计算桥梁。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
