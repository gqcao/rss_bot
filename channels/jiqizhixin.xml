<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>当运维遇上“春运时刻”，Chaterm破解移动远程运维操作难题</title>
      <description>&lt;![CDATA[“声控”运维、技能复用，合合信息Chaterm实现多场景双端智能运维]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 11:47:12 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;随着AI基础设施布局速度加快，企业运维面临跨终端、全链路管理的新挑战。近日，上海合合信息科技股份有限公司旗下的AI Agent产品Chaterm推出移动端应用，同步在PC端上线&amp;ldquo;Agent Skills&amp;rdquo;功能，帮助云计算行业从业者解决移动场景操作受限、运维知识难以复用等难题。通过打通移动端与PC端的场景协同服务，Chaterm为运维管理向全场景、智能化方向演进提出了新的落地方案。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;解决远程运维难题，Chaterm移动端实现&amp;ldquo;说话即操作&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在算力设施日益复杂的背景下，保障核心业务系统的全时运转已成为企业发展的生命线。然而，面对春节等节假日、外出差旅、日常通勤等非固定办公场景，IT部门往往面临团队分散、网络环境复杂等挑战。传统移动端运维工具受限于物理屏幕尺寸，主要以虚拟键盘为操作方式，难以支撑复杂的代码输入与多键组合操作，导致运维人员操作效率低下，在关键时刻无法进行有效应急响应。&lt;/p&gt;&lt;p&gt;针对这一行业痛点，Chaterm率先在移动终端管理工具中落地语音指令识别功能，让运维指令&amp;ldquo;言出必行&amp;rdquo;。基于&amp;ldquo;ASR与热词增强+LLM纠错&amp;rdquo;双层架构，Chaterm不仅能精准&amp;ldquo;听清&amp;rdquo;运维专业术语，更能深度&amp;ldquo;听懂&amp;rdquo;用户意图，将模糊的口语描述转化为准确、可执行的操作，避免了因术语别名或环境干扰导致的误操作风险。&lt;/p&gt;&lt;p&gt;据Chaterm团队技术人员介绍，目前，Chaterm移动端具备两种模式，在Terminal模式下，用户可以通过语音命令输入和Snippets（快捷命令），快速输入指令；在对话模式下，则可以用自然语言描述运维需求，在高铁、机场等受限环境下，也能快速完成核心业务的故障排查与应急响应。&lt;/p&gt;&lt;p&gt;&lt;img width="174" src="https://image.jiqizhixin.com/uploads/editor/96468c7a-32f6-4f87-9d17-4cb9edc3c444/1770176327436.jpeg" alt="Chaterm" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;图说：Chaterm移动端将用户模糊发音精准转化为标准运维指令&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Agent Skills&lt;/strong&gt;&lt;strong&gt;为运维人员打造&amp;ldquo;技能库&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在提升移动端运维效率的同时，Chaterm同步推进PC端升级，聚焦运维经验在系统内部的标准化复用。在传统运维工作模式中，关键系统的稳定性往往高度依赖资深专家的个人经验，这种隐性知识难以规模化传承，且容易因人员流动或操作失误引发风险。&lt;/p&gt;&lt;p&gt;为应对上述管理难题，Chaterm PC端推出Agent Skills功能，运维工程师可以将运维经验与业务逻辑，例如日常的检查清单、应用/数据库部署流程、故障排查流程、性能优化步骤等，封装为可复用的&amp;ldquo;技能包&amp;rdquo;，当AI面对用户提出的需求时，能像一位经验丰富的专家一样，查阅对应&amp;ldquo;技能包&amp;rdquo;后自主执行任务，提升运维工作效率，助力企业构建更稳健的自动化运维体系。&lt;/p&gt;&lt;p&gt;&lt;img width="415" src="https://image.jiqizhixin.com/uploads/editor/80f922ac-a0c2-4ce5-8ff8-1bb3b89e1b15/1770176327441.png" alt="90f88b67-7a81-41e4-b9c3-01bbeea68a45" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;图说：Chaterm产品主要功能介绍&lt;/p&gt;&lt;p&gt;随着大模型技术不断向垂直业务场景渗透，AI Agent成为提升企业效率的关键。在此趋势下，Chaterm也在积极探索运维智能化落地，相关实践已获行业认可。此前，在全球增长咨询公司沙利文与头豹研究院联合发布的《2025年中国生成式AI行业最佳应用实践》中，Chaterm凭借其在跨平台云资源智能管理方面的创新应用，入选2025年中国生成式AI最佳实践案例。未来，Chaterm将持续拓展AI技术在复杂运维场景中的应用，助力企业构建更高效、稳健的自动化体系。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | 腾讯混元团队联合 KCL 提出 WildToolBench，评估 Wild 场景下 LLM 的 Agentic 能力</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 11:19:21 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/490c3433-8113-4246-af0b-c222447651af/1770174901251.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;作者：Peijie Yu, Wei Liu, Yifan Yang, Jinjian Li, Zelong Zhang, Xiao Feng, Feng Zhang&lt;/p&gt;&lt;p&gt;单位：Tencent HY、King&amp;#39;s College London&lt;/p&gt;&lt;p&gt;链接：&lt;a href="https://openreview.net/forum?id=yz7fL5vfpn"&gt;&lt;u&gt;Benchmarking LLM Tool-Use in the Wild | OpenReview&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Github：&lt;a href="https://github.com/yupeijei1997/WildToolBench"&gt;&lt;u&gt;GitHub - yupeijei1997/WildToolBench: Benchmarking LLM Tool-Use in the Wild&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3d108a18-080b-4a40-82b0-b5b23e387f02/1770174912241.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;通过大型语言模型（LLM）的多轮、多步骤工具调用满足用户需求，往往并非简单直接的过程。真实用户交互本质上具有 &amp;ldquo;野生性&amp;rdquo;，复杂、杂乱且灵活。我们从用户行为中识别出三大核心挑战：一是组合式任务，需高效编排工具调用拓扑结构；二是隐含意图，分散于多轮对话中，需结合上下文推理；三是指令转换，用户会混合任务查询、澄清提问与日常交流，迫使 LLM 实时调整策略。现有基准测试忽视了这些行为，导致 LLM 在工具调用方面的表面进展存在误导性。为此，我们提出 WildToolBench&amp;mdash;&amp;mdash; 一个基于真实用户行为模式的 LLM 工具调用基准测试。对 58个 LLM 的综合评估显示，无任何模型的准确率超过 15%，这表明 LLM 智能体能力的稳健性仍存在巨大差距。受控实验与深度分析进一步表明，LLM 工具调用的真正挑战并非人为设计的复杂任务，而是用户行为的 &amp;ldquo;野生性&amp;rdquo;，这也凸显了重新审视 LLM、用户与工具三者间交互关系的必要性。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e1103d43-1ffa-4096-9d0b-fe83a05c1af7/1770174923131.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;一直以来，现有 LLM 工具使用基准测试（如 BFCL 系列、TauBench等）虽在推动技术发展中发挥了重要作用，但普遍存在场景理想化的问题。它们往往将用户需求简化为明确、独立的任务，忽略了真实对话中用户行为的复杂性 &amp;mdash;&amp;mdash; 用户可能在一次对话中提出包含多个简单需求的复合任务，需要 Agent高效协调多种工具；也可能将真实意图隐藏在多轮对话的上下文之中，要求 Agent主动推断；更会在任务查询、澄清疑问与日常闲聊之间灵活切换，迫使 Agent实时调整应对策略。这些被现有基准忽视的 &amp;ldquo;野生&amp;rdquo; 用户行为，恰恰是 LLMs 在实际应用中面临的核心挑战，也让此前 LLM 工具使用能力的进步显得有些 &amp;ldquo;虚高&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/5dbfb4c1-294a-4f1f-aa2f-dbb9583cc67a/1770174934303.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;WildToolBench 的诞生，正是为了填补这一空白。它以真实用户行为模式为基石，通过精心设计的数据 pipeline，结合人工验证与标注，构建了 256 个场景、1024 项任务的庞大测试集。与其他基准测试不同，WildToolBench 牢牢抓住真实用户交互的三大核心特性：复合任务的工具协同需求、上下文隐含意图的推断要求，以及指令类型灵活切换下的策略适配能力。这一设计理念直指行业痛点 &amp;mdash;&amp;mdash; 真正考验 LLMs 工具使用能力的，并非人为构建的复杂场景，而是看似简单却贴合真实用户习惯的交互模式。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/30783c9d-8c93-4684-89e6-7dbcd1fee909/1770174945105.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;为验证 WildToolBench 的有效性，研究团队对 58款主流 LLMs 展开了全面评估，涵盖闭源通用模型（如 Gemini 系列、Claude系列、GPT 系列）、开源通用模型（如 GLM-4.5、Kimi-K2）以及开源专用工具模型。令人惊讶的是，所有模型的会话准确率均未超过 15% ，即便表现最佳的闭源模型，在任务准确率上也大多低于 60%。这一结果彻底打破了人们对当前 LLM 工具使用能力的乐观认知，揭示出 LLMs 在真实场景下的巨大能力缺口。&lt;/p&gt;&lt;p&gt;值得注意的是，实验还呈现出诸多具有行业启示性的发现。闭源模型整体表现优于开源模型，但头部开源模型（如 GLM-4.5）已能逼近部分顶尖闭源模型的水平，为开源社区的发展注入信心；专用工具模型虽针对工具使用进行优化，却因泛化能力不足，表现反而不及通用模型；具备强化推理能力的模型变体，在工具协同与意图推断任务中优势明显，证明推理能力是提升 LLM 工具使用能力的关键。这些发现不仅为模型开发者指明了优化方向，更让行业意识到，未来 LLM 智能体的发展，不能仅聚焦于工具调用的 &amp;ldquo;执行能力&amp;rdquo;，更需强化对用户意图的 &amp;ldquo;理解能力&amp;rdquo;，而这依赖于模型在指令跟随、长上下文 comprehension 等基础能力上的突破。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/70ef93eb-4244-4846-be35-0181f0188324/1770174986438.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战一：组合式任务的工具调用拓扑编排难题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实场景中，用户的需求往往不是单一指令的简单实现，而是需要多工具、多步骤协同完成的组合式任务。这类任务的核心难点，在于需要LLM具备高效的工具调用拓扑编排能力&amp;mdash;&amp;mdash;也就是说，不仅要明确&amp;ldquo;需要调用哪些工具&amp;rdquo;，更要精准规划&amp;ldquo;工具调用的顺序、时机、优先级&amp;rdquo;，甚至要根据前一步工具的返回结果，动态调整后续的调用逻辑。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e632e30d-2c3a-41d1-bfde-01bd3573992c/1770174992453.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;深入分析实验数据，更能发现 LLMs 在工具使用中的关键短板。在复合任务处理上，无论是顺序调用多工具、并行调用多工具，还是混合调用模式，LLMs 的表现都不尽如人意。以最复杂的混合多工具任务为例，最高准确率仅为 25%，且工具执行的最优路径率不足 43%，说明 LLMs 在工具协同规划与效率优化上仍有极大提升空间。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战二：多轮对话中隐含意图的上下文推理困境&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实用户与LLM的交互，很少会一次性将所有需求细节完整表述，更多是通过多轮对话逐步传递需求，甚至会在对话中隐含核心意图&amp;mdash;&amp;mdash;这就对LLM的上下文推理能力提出了极高要求：LLM需要全程捕捉对话中的关键信息，整合多轮对话的上下文，精准挖掘用户未明确说出的隐含需求，而非仅局限于单轮指令的表面含义。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;挑战三：指令转换下的实时策略调整压力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;真实的用户对话具有极强的灵活性，用户不会始终围绕单一任务指令展开交流，而是会出现频繁的指令转换：可能在提出任务查询后，突然插入澄清提问，或是切换到 casual 交流，随后又回归核心任务。这种指令转换的随机性，迫使LLM必须具备实时调整策略的能力&amp;mdash;&amp;mdash;既要在任务查询时保持工具调用的专业性，又要在澄清、闲聊时灵活回应，同时还要记住对话主线，确保在指令回归后能够快速衔接之前的任务逻辑，不出现思路断裂。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0b87603b-88d0-47b6-bed2-4c29a3676bc3/1770175007335.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在意图推断方面，面对用户通过部分信息省略、指代关联或长距离上下文依赖隐藏的意图，LLMs 尤其是在长距离依赖任务中，准确率普遍低于 50%，暴露出其上下文理解与推理能力的不足。此外，当用户在对话中频繁切换指令类型时，LLMs 的任务准确率最高可下降 30%，其 &amp;ldquo;自我条件反射&amp;rdquo; 式的决策偏差（如之前使用工具后倾向于继续调用工具），严重影响了应对真实用户灵活需求的能力。&lt;/p&gt;&lt;p&gt;WildToolBench 的意义，远不止于提供一个更具挑战性的评估基准。它通过结构化的评估维度与详细的错误分析（如 &amp;ldquo;错误工具选择&amp;rdquo;&amp;ldquo;冗余调用&amp;rdquo; 等高频错误），为模型迭代提供了清晰的改进路径；更以真实用户行为为核心的设计理念，重新定义了 LLM 工具使用评估的标准，推动行业从 &amp;ldquo;理想化测试&amp;rdquo; 迈向 &amp;ldquo;真实场景验证&amp;rdquo;。对于企业而言，WildToolBench 可帮助其更精准地评估 LLM 智能体的实际应用潜力，避免技术选型偏差；对于科研人员，它则为 LLM 工具使用能力的研究提供了贴近实际需求的 &amp;ldquo;试验场&amp;rdquo;。&lt;/p&gt;&lt;p&gt;如今，WildToolBench 的数据集、评估脚本及可控多智能体数据合成框架已全部开源，诚邀全球 AI 研究者与开发者共同探索 LLM&amp;nbsp;Agentic&amp;nbsp;能力的突破之道。&lt;/p&gt;&lt;p&gt;数据集：&lt;a href="https://github.com/yupeijei1997/WildToolBench/blob/main/wild-tool-bench/data/Wild-Tool-Bench.jsonl"&gt;&lt;u&gt;WildToolBench/wild-tool-bench/data/Wild-Tool-Bench.jsonl at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;评估脚本：&lt;a href="https://github.com/yupeijei1997/WildToolBench/tree/main/wild-tool-bench/wtb"&gt;&lt;u&gt;WildToolBench/wild-tool-bench/wtb at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;可控多智能体数据合成框架：&lt;a href="https://github.com/yupeijei1997/WildToolBench/tree/main/multi-agent-framework"&gt;&lt;u&gt;WildToolBench/multi-agent-framework at main &amp;middot; yupeijei1997/WildToolBench &amp;middot; GitHub&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>钉钉北京峰会展示AI落地多行业样本，一批企业集中签约</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 04 Feb 2026 10:58:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2月3日，以&amp;ldquo;AI时代的工作方式&amp;rdquo;为主题的钉峰会在北京隆重举行。峰会由阿里巴巴钉钉和中关村国际共同举办，汇聚了来自制造业、农业、供应链、环保、文媒等多行业的领军企业代表、数字化先锋及专家逾三百人，深入探讨人工智能浪潮下工作方式的重塑与进化。&lt;/p&gt;&lt;p&gt;现场，一批北京区域企业与钉钉集中签约，共同迈入AI时代的工作方式，包括博锐尚格科技股份有限公司、天津市神州商龙科技股份有限公司、北京健坤餐饮集团、北京央视瑞安技术服务有限公司等。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;智能体操作系统：从&lt;/strong&gt;&lt;strong&gt;工具&lt;/strong&gt;&lt;strong&gt;到伙伴的进化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;钉钉华北大区总经理刘浩在致辞中指出，当前正处在AI转型的关键时间点。钉钉以解决真实问题为原点，致力于从传统的应用平台升级为面向未来的&amp;ldquo;智能体操作系统&amp;rdquo;（Agent OS）。他强调，AI时代的工作方式需要根本性的重新思考，即从&amp;ldquo;人操作工具&amp;rdquo;转向&amp;ldquo;人调教AI&amp;rdquo;，让智能体（Agent）成为业务执行与协同的核心单元。&lt;img src="https://image.jiqizhixin.com/uploads/editor/572106a8-a4d3-428c-b800-542ff65b7bf2/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 钉钉华北大区总经理刘浩&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;中关村国际控股有限公司总经理卢江表示表示，中关村国际正通过覆盖全球的创新网络，携手钉钉，表示，以AI为纽带、出海服务为桥梁，共同优化AI时代的服务模式，让创新价值在全球范围内绽放。&lt;/p&gt;&lt;p&gt;钉钉华北大区解决方案总经理刘啸天在主题分享中阐释了&amp;ldquo;智能体协同&amp;rdquo;的核心逻辑。他认为，AI规模化落地的关键瓶颈不再是模型能力，而在于缺乏企业级统一的运行环境。钉钉正以Agent OS为内核，构建下一代操作系统，通过智能体统一调度数字工具与物理设备，并实现智能体间的交互与协同。人在这一系统中的角色，从执行者转变为&amp;ldquo;教练&amp;rdquo;，负责调教与设定目标，而将重复性、流程化的工作交由智能体执行。&lt;/p&gt;&lt;p&gt;全网爆火的钉钉首款AI硬件DingTalk A1录音卡也在本次峰会上亮相，A1产品负责人夏治朋详细介绍了这款&amp;ldquo;网红&amp;rdquo;产品设计的初衷和用户自发探索出的花式用法，现场种草。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;跨界实践：AI赋能千行百业，客户亲述转型心声&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;峰会邀请了多位来自一线企业的代表，分享了其利用钉钉AI能力进行数字化转型的前沿实践，并在现场亲述了转型过程中的真实洞察。&lt;/p&gt;&lt;p&gt;佳沃集团知识管理副总裁李萌带来《一颗蓝莓的AI之旅&amp;mdash;&amp;mdash;佳沃集团AI应用实践》主题分享。&lt;img src="https://image.jiqizhixin.com/uploads/editor/6bc84482-8169-4482-a75c-7232d383d1d6/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 佳沃集团知识管理副总裁李萌&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;佳沃集团有限公司创立于2012年，是联想控股旗下的现代农业和食品产业集团，位列中国农业企业500强第36名，更是中国农业数智化转型的标杆企业。&lt;/p&gt;&lt;p&gt;自2020、年起，佳沃与钉钉共建了以知识管理为内核的数智化生态，通过数据上钉、经验上钉、办公全场景上钉，特别是AI助理&amp;ldquo;小佳&amp;rdquo;的应用，让员工满意度不断提升，新员工培训成本降低50%，一线农人也能随时获取专业建议，经验实时更新，行政事务一站式解决。&lt;/p&gt;&lt;p&gt;2025年钉钉推出AI表格后，佳沃蓝莓也第一时间成为AI表格到深度用户。利用AI表格，佳沃把过去的N张表格变成一张看板，把过去来源于口头、微信、备注等各处无法统计的非结构化信息，变成标签化、可计算的结构化信息。&lt;/p&gt;&lt;p&gt;&amp;ldquo;钉钉AI表格现在可以通过语音录入来输入了，这个功能非常好！&amp;rdquo;李萌说，这意味着田间地头的农人都可以把经验和信息沉淀到AI表格上。&lt;/p&gt;&lt;p&gt;蜀海供应链管理有限责任公司产品总监邢禺分享了蜀海作为餐饮供应链公司的AI实战经验。蜀海曾是海底捞供应链部门，2014年起独立为第三方供应链服务公司。2020年起，蜀海引入钉钉，帮助解决组织的协同问题，2025年起重点推进AI能力，深度赋能多个智能体应用场景，助力AI转型。&lt;/p&gt;&lt;p&gt;邢禺展示了AI在复杂物流配送、智能补货计划及全国仓储会议管理中的深度应用。&amp;ldquo;AI不只是替代人，而是解放人，让从业者脱离重复劳动，聚焦核心的服务。&amp;rdquo; 通过AI算法优化配送路线，将履约率提升至99%；利用AI听记与表格自动分析海量会议内容，挖掘共性问题，使问题闭环率超90%，显著提升了供应链响应速度与管理透明度。&lt;img src="https://image.jiqizhixin.com/uploads/editor/a436d962-34fb-4063-b1d2-7789cc39573c/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 北控水务AI创新部负责人刘连波&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;北控水务是北控集团旗下专注于水资源循环利用和水生态环境保护事业的旗舰企业，在香港主板上市，集产业投资、设计、建设、运营、技术服务与资本运作为一体，水处理规模位居国内行业前列。已入选恒生香港中资企业指数成份股、摩根斯坦利资本国际指数等多只重要国际成分股。&lt;/p&gt;&lt;p&gt;北控水务AI创新部负责人刘连波在钉峰会现场分享了其以&amp;ldquo;补位&amp;rdquo;思路推进AI落地的策略。刘连波表示，&amp;ldquo;近一年来，我们在不断研究从场景探索到能力补强，AI应用从价值验证到构建核心能力，从AI+业务、AI+组织到AI+员工，与钉钉一起继续深挖AI场景和共建AI能力和文化。&amp;rdquo;&lt;/p&gt;&lt;p&gt;他展示了一组数据：基于钉钉知识库，公司迅速孵化AI助手，在很多个超过500人的群里接入了AI助手，目前每月调用3000次以上，回答准确率93%以上。AI工具对近半数员工提高效率20%到50%。&lt;/p&gt;&lt;p&gt;央视瑞安信息化工程师杨过分享了国企信息化落地的经验和方法。央视瑞安作为中央广播电视总台全资子公司，是广播电视领域动力运维与综合技术服务的标杆企业。&amp;ldquo;解决工作中协作问题的关键就是从&amp;lsquo;传纸条&amp;rsquo;变成&amp;lsquo;共看一张图&amp;rsquo;，要善于利用时代最先进的工具为自己所用。&amp;rdquo;杨过说。通过钉钉OA审批和AI表格的联动，央视瑞安搭建了外出保障工作管理平台，实现了流程驱动数据，数据反哺流程的闭环。&lt;/p&gt;&lt;p&gt;&amp;ldquo;AI表格是真的好用，既有科技感，还不用写代码&amp;rdquo;，杨过说，&amp;ldquo;像外出保障工作管理平台，就是我和另一个工程师两个人搭建起来的。&amp;rdquo;&lt;/p&gt;&lt;p&gt;整场峰会气氛热烈，并传递出一个明确信号：AI时代的工作方式变革已进入深化实践阶段，&amp;ldquo;智能体协同&amp;rdquo;不再是一个遥远的概念，而是正在千行百业中落地生根，成为提升组织韧性、激发创新活力、锻造未来竞争力的关键引擎。企业与个人唯有主动拥抱这一变革，方能立于AI浪潮之巅。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，真正好用的Windows版「Cowork」上线了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 10:31:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜杜伟、泽南&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;section&gt;天工 Skywork 桌面版旗帜鲜明地将 Windows 平台作为首发阵地，为全球用户提供开箱即用的「Cowork 平替」。&lt;/section&gt;&lt;/blockquote&gt;&lt;p&gt;终于，Windows 原生「Cowork」问世了！&lt;/p&gt;&lt;p&gt;过去两周，AI 圈被火遍硅谷的 ClawdBot（现已改名为 OpenClaw）持续刷屏。&lt;/p&gt;&lt;p&gt;人们一边震撼于这个智能体助理带来的自动化效率提升，另一边也在吐槽其对 Windows 系统的适配。比如，根据一些用户的反馈，如果严格按照官网提供的命令行在 Windows 上安装 ClawdBot，将导致 Skills 功能彻底失效。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGBIjW0qbxQ4lR0tgCrib0JkUFBTXQZdKR7piciaP9tocIpzkrnO0RE8q1A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.8379629629629629" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531376" data-aistatus="1" data-original-style="height: auto !important;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/43771591-a720-48a7-82f4-06a6af909c83/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;这并不是 ClawdBot 一个智能体助手的选择性倾向，上个月发布的 Claude Cowork 以及 OpenAI 昨天亮相的智能体式 Codex 应用同样优先适配 macOS 系统。这种生态上的失衡在今天迎来了转机。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;国产大模型玩家昆仑天工正式发布了全新的 Agent 产品 &amp;mdash;&amp;mdash; 天工 Skywork 桌面版，旗帜鲜明地将 Windows 平台作为首发阵地&lt;/strong&gt;，为全球用户带来了开箱即用的「Cowork 平替」。&lt;a href="https://mp.weixin.qq.com/s/nKe1iObLWXxawodTrFumnw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1e92a794-82fa-4c5e-8151-211a194e2ff8/1770171920011.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Skywork 原生支持 Windows 系统，无需繁琐的迁移或适配，即可对海量本地历史文件和复杂项目场景展开自动化处理。这种高度的兼容性打通了个人 Agent 进入真实办公场景的「最后一公里」。&lt;/p&gt;&lt;p&gt;在优先适配 Windows 平台之外，Skywork 在&lt;strong&gt;基础模型选择、功能支持、能力扩展&lt;/strong&gt;等其他维度同样有亮眼表现。&lt;/p&gt;&lt;p&gt;首先，与 Claude Cowork 仅支持自家 Claude 模型不同，Skywork 增加了对谷歌 Gemini 的支持，充分发挥该系列模型的原生多模态理解与生成优势。&lt;/p&gt;&lt;p&gt;现在，用户可以自由选择 Gemini 3 Pro 以及 Claude Opus 4.5、Claude Sonnet 4.5 等不同模型。当然也可以启用智能路由「auto」模式，系统自动识别任务类型并匹配最适合的模型，最大化执行效率。&lt;/p&gt;&lt;p&gt;其次，&lt;strong&gt;Skywork 在主打办公场景的同时，也能 hold 住创作场景&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531377" data-ratio="0.6851851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGKgqAyr1w566FIb9FZRkWxr1RqF35vib2kfjdvMIXfvp05JTvQCabhhw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f462d30f-935f-4eac-930f-1e49893b07e5/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; Skywork 界面&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了全方位满足桌面级真实办公需求，Skywork 桌面版做到了对全类型文件的智能管理。不管是图片、视频、表格、Word、Excel、PDF、PPT 还是其他格式，它都能跨文件、跨格式直接读取并理解，遵循用户任务指令对它们进行归类整理或生成新内容。&lt;/p&gt;&lt;p&gt;并且，它能&lt;strong&gt;同时响应并执行多项复杂任务&lt;/strong&gt;，效率拉满。此外也无需上传云端，&lt;strong&gt;在本地环境完成即可&lt;/strong&gt;，消除了用户对数据泄露和文件安全的担忧。&lt;/p&gt;&lt;p&gt;面向图像与视频生成的创作场景，比如制作 PPT、宣传素材，整理可视化报告，创作多媒体内容，Skywork 相较于 Claude Cowork，在语义遵循以及表现力、专业性等多方面均更胜一筹。&lt;/p&gt;&lt;p&gt;最后是能力扩展，&lt;strong&gt;Skywork 内置了 100+ 个经过精选的、真正有用的 Skills 技能包&lt;/strong&gt;，并将控制权交给用户，既可以手动选择也可由系统根据任务类型自动筛选，操作灵活，覆盖了 Office 三件套生成、网页生成以及图像与视频生成。&lt;/p&gt;&lt;p&gt;此外，Skywork &lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;在价格上也极具杀伤力。&lt;strong&gt;用户只需 19.99 美元的 Basic 会员，就能解锁完整的产品体验&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;下载地址：https://skywork.ai/desktop&amp;nbsp;&lt;/p&gt;&lt;p&gt;在全球竞品偏向 macOS 开发的当下，昆仑天工的 Skywork，打响了一场针对「Windows 生产力人群」的抢位战。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;上手实测：这会是办公自动化的「奇点」吗？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;话不多说，我们直接开测。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;告别「碎片化」办公&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上个星期，昆仑天工开源了 SkyReels-V3 视频生成模型，我们对此进行了报道。当时是以文章的形式进行了介绍，用到了大量图片、视频素材。&lt;/p&gt;&lt;p&gt;假如我们现在转换一下身份，&lt;strong&gt;用 PPT 向别人介绍这个模型怎么办？&lt;/strong&gt;这正是 Skywork 的用武之地，模型选用「gemini-3-pro-preview」。&lt;/p&gt;&lt;p&gt;只需要告诉它你要制作这样一个 PPT，文档的 Word 版和视频素材一股脑地放在文件夹里，它就会自行分析需求，把素材找好做成一个可供使用的版本出来。&lt;/p&gt;&lt;p&gt;可以看到，在生成中间，它还会问你要选什么样的风格，这是一个需要互动的过程。&lt;a href="https://mp.weixin.qq.com/s/nKe1iObLWXxawodTrFumnw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/151750a9-6fa8-4a84-a7cf-eff5a99fb2ad/1770172033027.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最后在生成的 PPT 的基础上，你可以进行修改，节省了大量的前期工作。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531411" data-ratio="0.56" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGx3VQzXmicR7QGIeBqOT3IOiaKVV7H0g2a9licmMcOk2y5icKcz8JAXdzgA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-type="gif" data-w="800" type="block" data-original-style="height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/645f8bf6-125d-4343-a6f5-ffbe4f2bb68d/640.gif" data-order="0" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;接下来尝试一个更复杂的案例，&lt;strong&gt;让 Skywork 在不删除任何原始文件的基础上，将文件夹内的所有内容整理成一套清晰直观的目录结构&lt;/strong&gt;，并在任务完成后给出一份简要报告，列出新的文件夹结构和变更日志。&lt;/p&gt;&lt;p&gt;这项任务要求 Agent 具备强大的环境感知能力与跨格式解析能力，这次选用 Claude Opus 4.5。&lt;/p&gt;&lt;p&gt;在实际跑的过程中，Skywork 首先「穿透」不同层级的子目录，精准识别出了 Word、Excel、PDF 等不同格式的文件数据。然后在语义理解的基础上，它自动剔除冗余信息，重组碎片化信息，并生成逻辑严密的结构化报告。&lt;a href="https://mp.weixin.qq.com/s/nKe1iObLWXxawodTrFumnw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/030f5739-7df7-4a7d-9ef5-930e7dc0ebb3/1770172056846.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最终生成的「目录结构整理报告」是这样的：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531420" data-ratio="0.6132723112128147" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG8fE4koCry5TOvHoTqPBhafVZqmJNHR1ySYIAncib7Y0kictS3u62ibxTg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-type="gif" data-w="874" type="block" data-original-style="height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5cafac8d-b789-441d-9556-83d11cbf9d64/640.gif" data-order="1" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;多模态能力融入工作流&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;英伟达最近和 OpenAI 之间的新闻引发了科技圈的关注。目前多路媒体的报道认为，英伟达已经暂停了向 OpenAI 投资高达 1000 亿美元的计划。而 OpenAI 也正在寻求构建 GPU 以外的算力体系。&lt;/p&gt;&lt;p&gt;那么作为一家「以先进 AI 推理芯片为主要产品的公司」，机器之心能否在下一轮 OpenAI 的融资上找到机会呢？我们&lt;strong&gt;让 Skywork 来生成一份意向报告，要求图文并茂。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;怎么看这都是一个大工程。这回它分析需求以后上网进行了搜索，在知乎上搜到了机器之心以前发过的一些文章。&lt;/p&gt;&lt;p&gt;接着它整理了大致的撰写路径之后开始工作，自己给自己列出了要完成的事项，为了写 Word 文档，还自己下载了 docx 的库。&lt;/p&gt;&lt;p&gt;可见最后生成的「下一代 AI 推理算力基础设施提案」有那么一点可行性。&lt;a href="https://mp.weixin.qq.com/s/nKe1iObLWXxawodTrFumnw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8ae96629-20b3-4008-a95f-a16d204cb8ab/1770172089215.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最后尝试一个更有挑战性的场景，看看 Skywork 能不能&lt;strong&gt;根据文档和图片生成一个精美的 SEO 网页&lt;/strong&gt;。这类任务要求 Agent 既能理解内容，还会工程实现。&lt;/p&gt;&lt;p&gt;一要精准提取文档中的核心语义，为撰写高质量网页文案做好准备。二要利用视觉处理能力来裁剪、优化图片并嵌入到合适的布局中。&lt;/p&gt;&lt;p&gt;最后也是最关键的一步，它要能自主编写出符合 SEO 逻辑的代码，将原本孤立的图文素材转化为信息齐全、布局合理、观感友好的网页。&lt;a href="https://mp.weixin.qq.com/s/nKe1iObLWXxawodTrFumnw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e8a15a48-250e-44b8-981c-ab3898e5f259/1770172131643.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;简单的几个案例跑下来，我们发现：Skywork 这样的智能体助手已经不再只是一个等待执行用户指令的对话框，它们在拥有更高的系统操作权限之后，自主性得到史诗级强化，从而在极少的人为参与下高效地完成工作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从「编程智能体」走向「个人助理智能体」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2025 年被普遍认为是 Agent 落地元年，而刚过去一个月的 2026 年，我们正在见证新一轮的爆发。&lt;/p&gt;&lt;p&gt;一开年，Agent 赛道的竞争便趋于白热化，国外 Anthropic 发布 Cowork、OpenClaw 引爆 A I 社区，国内大厂阿里先后上线千问 APP 任务助理、桌面端 QoderWork，其他大模型独角兽也陆续推出桌面智能体应用。&lt;/p&gt;&lt;p&gt;业界玩家们你追我赶的发布节奏，一定程度上可以验证 OpenClaw 创建者 Peter Steinberger 近日接受采访时表达的一种观点。他认为，&lt;strong&gt;2025 年是编程智能体元年，而今年将是个人助理智能体元年&lt;/strong&gt;。这位搅动硅谷的开发者还放出狂言，「我们手机里 80% 的 APP 将被取代。」&lt;/p&gt;&lt;p&gt;Steinberger 的判断只是一家之言，但通过 Agent 打造超级个体的行业趋势是显而易见的。不管是聚焦职场人士还是普通用户，Agent 正在以前所未有的速度与深度重构数字世界的底层逻辑。&lt;/p&gt;&lt;p&gt;在桌面端，Agent 连接本地操作系统，实现跨文件管理、跨应用操作、复杂任务并行执行；在移动端，打破 APP 孤岛，实现主动意图感知、跨软件调度与全链路执行。与人类生活、工作、学习息息相关的双端并进，将加速驱动人类走向以 Agent 为主导的超级个体时代。&lt;/p&gt;&lt;p&gt;可以预见的是，对个人更友好、易用性与专业性更强的 Agent，势必收获更多潜在用户群体的青睐与认可。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;当赢家无法通吃，相对优势或成护城河&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如 ChatGPT 问世之后的数年一样，2026 年 AI 领域仍是「步履不停」。对于所有入局者，尤其是以 AGI 为终极目标的大模型厂商们，谁都不愿在新一轮的狂飙中落后。&lt;/p&gt;&lt;p&gt;尤其是在行业焦虑大模型算力溢出、预训练 Scaling Laws 边际效益增长放缓的当下，大家都在期待下一次 AI 奇点的到来。两周前，DeepMind CEO 哈萨比斯在一次播客节目中表示，「实现 AGI 的路上可能还需要一两个重大创新」。&lt;/p&gt;&lt;p&gt;在那之前，Agent 成为释放与扩展基础大模型潜能的「第二曲线」。通过对个人生活与工作、企业生产范式的变革，Agent 将人类从繁琐、重复性的任务中解脱出来，将更多时间与精力放在能够发挥创造力与想象力、产生高价值的板块。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;随着 Agent 一步步踏入深水区，或许不会出现「赢家通吃」的局面。&lt;/strong&gt;现如今，全球开源社区已经对 Agent 底层模型和执行框架进行了大量解构，该赛道几乎不存在「技术秘密」。主流大模型厂商都有能力自研出 Agent 产品，这样一来竞争的胜负手转向了场景垂直与生态适配。&lt;/p&gt;&lt;p&gt;昆仑万工早在 2025 年 5 月即发布了「AI 版 Office」&amp;mdash;&amp;mdash; 天工超级智能体（&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650970454&amp;idx=1&amp;sn=7b453aa27f07c6e6b897f3a73e815d67&amp;scene=21#wechat_redirect" target="_blank"&gt;Skywork Super Agents&lt;/a&gt;，即 Skywork 网页版），成为国内较早布局 Agent 的厂商，积累了丰富的办公场景效率优化经验，拥有了庞大的 AI 办公用户基础，并让他们对其产品有了很强的认知。&lt;/p&gt;&lt;p&gt;现在，Skywork 桌面版依托 Windows 这个全球最大生产力平台，灵活支持 Gemini 3 Pro 多模态与 Claude 4.5 逻辑推理能力，在继续对办公场景的效率优化以及与其他应用场景的联动中构筑起护城河。&lt;/p&gt;&lt;p&gt;在这场愈演愈烈的 Agent 之战中，谁能更快地建立起无法替代的相对优势，或许就是最后的赢家。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Attention真的可靠吗？上海大学联合南开大学揭示多模态模型中一个被忽视的重要偏置问题</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 04 Feb 2026 10:22:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-04</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-04</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/29adb014-f2c3-404c-a5a8-513f4619da0e/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;近年来，Vision-Language Models（视觉 &amp;mdash; 语言模型）在多模态理解任务中取得了显著进展，并逐渐成为通用人工智能的重要技术路线。然而，这类模型在实际应用中往往面临推理开销大、效率受限的问题，研究者通常依赖 visual token pruning 等策略降低计算成本，其中 attention 机制被广泛视为衡量视觉信息重要性的关键依据。&lt;/p&gt;&lt;p&gt;近日，上海大学曾丹团队联合南开大学研究人员，从 attention 可靠性的角度出发，系统揭示了 Vision-Language Models 中普遍存在的 attention 偏置问题，并提出了一种无需重新训练的 attention 去偏方法，在多个主流模型、剪枝策略及图像与视频基准上验证了其有效性，为多模态模型的高效、可靠部署提供了新的思路。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGwzIrjPM0GgdxqXYqMGlJCXrU5ibEgqDzAicYj0kahDhtLFHjxntqh0Ag/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.17222222222222222" data-type="png" data-w="1080" data-width="1582" data-height="272" data-imgfileid="503531357" data-aistatus="1" data-original-style="background-color: transparent;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1205a911-344c-48e5-b637-35f0d834da7b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml6a4plr35o" data-pm-slice="0 0 []"&gt;论文标题：Attention Debiasing for Token Pruning in Vision Language Models&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://arxiv.org/abs/2508.17807&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml6a1f7d1u8d" data-pm-slice="0 0 []"&gt;代码链接：https://github.com/intcomp/attention-bias&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml6a5idrtp2" data-pm-slice="0 0 []"&gt;&lt;strong&gt;一、研究意义&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;近年来，视觉 &amp;mdash; 语言模型（Vision-Language Models，VLMs）在图像理解、视觉问答、多模态对话等任务中表现突出，并逐渐成为通用人工智能的重要技术基础。然而，这类模型在实际部署时往往面临一个现实挑战：&lt;strong&gt;模型推理成本高，速度慢。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为提升效率，研究者通常会采用 &lt;u&gt;visual token pruning（视觉 token 剪枝） &lt;/u&gt;技术，即在不显著影响性能的前提下，丢弃不重要的视觉信息。其中，attention 机制 被广泛用作判断 &amp;ldquo;哪些视觉 token 更重要&amp;rdquo; 的核心依据。&lt;/p&gt;&lt;p&gt;但上海大学曾丹团队在研究中发现：&lt;strong&gt;attention 并不总是可靠的 &amp;ldquo;重要性指标&amp;rdquo;&lt;/strong&gt;。在多模态模型中，attention 往往受到多种结构性偏置的影响，这些偏置与真实语义无关，却会直接左右剪枝结果，从而影响模型性能。&lt;/p&gt;&lt;p&gt;针对这一问题，该团队系统分析了 VLM 中 attention 的行为特性，提出了一种 &lt;strong&gt;Attention Debiasing（注意力去偏）方法&lt;/strong&gt;，在无需重新训练模型的前提下，有效提升了多种主流剪枝方法的稳定性与可靠性。如下图所示，提出的方法应用于目前基于 attention 的剪枝方法上之后，都有提升。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGvDIibWffz5hHngZgXjq8LYYqVlejXvjjzCcL69evrxe33Dpk3VUgqAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.8462962962962963" data-type="png" data-w="1080" data-width="2783" data-height="2354" data-imgfileid="503531358" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/7ffefe18-a7f1-4fe1-8f63-265ed8e8c2f6/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;二、研究背景&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在直觉上，attention 机制往往被理解为 &amp;ldquo;模型更关注哪里&amp;rdquo;，因此被自然地视为语义重要性的体现。然而，曾丹团队的研究表明，在 Vision-Language Models 中，attention 往往并非只由内容决定，而是隐含着多种系统性偏置。&lt;/p&gt;&lt;p&gt;其中最典型的有两类：&lt;/p&gt;&lt;p&gt;第一类是 &lt;strong&gt;位置偏置（recency bias）&lt;/strong&gt;。研究发现，language-to-vision attention 会随着视觉 token 在序列中的位置不断增大，也就是说，模型更倾向于关注 &amp;ldquo;后面的 token&amp;rdquo;。如图所示，这通常表现为模型对图像下方区域给予更高 attention，即便这些区域并不包含关键信息。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG2whGlT1Gy65hCcWhRqSNAh1aVqUlsNKhqyVhFiag8c1xBY2n4YrDzEg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.4203703703703704" data-type="png" data-w="1080" data-width="1414" data-height="595" data-imgfileid="503531359" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/6b7cd923-ee59-45eb-8a88-129bebf79b17/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;第二类是&lt;strong&gt; padding 引发的 attention sink 现象&lt;/strong&gt;。在实际输入中，为了统一尺寸，图像往往需要 padding，但这些区域在语义上是 &amp;ldquo;空白&amp;rdquo; 的。然而，由于 hidden state 中出现异常激活，padding 对应的 token 反而可能获得较高 attention，从而被错误地保留下来。下图是 pad 区域填充不同的数值时，pad 区域对应的 attention score 数值以及 hidden states 的激活值。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGl5WPocMg79WcDliazzMje8C0G9vrXVDCGvcwCbQVfT7ia5kIqmeQxTaQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.4916666666666667" data-type="png" data-w="1080" data-width="2774" data-height="4137" data-imgfileid="503531360" data-aistatus="1" data-original-style="background-color: transparent;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/42ade913-7119-450b-b8d3-4dbff0a0b2de/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;更值得注意的是，当 attention 被用于剪枝排序时，这些偏置并不会被削弱，反而会被进一步放大，最终导致剪枝结果偏离真实语义需求。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、研究方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;针对上述问题，上海大学曾丹团队并没有提出新的剪枝算法，也没有对模型结构进行修改，而是从一个更基础的角度出发：&lt;strong&gt;既然 attention 本身是有偏的，是否可以先对 attention 进行修正？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该团队观察到，attention 中的偏置并非随机噪声，而是呈现出&lt;strong&gt;稳定的整体趋势&lt;/strong&gt;。因此，他们通过对 attention 随 token 位置变化的趋势进行拟合，构建了一条反映 &amp;ldquo;位置偏置&amp;rdquo; 的曲线，并在此基础上对原始 attention 进行去偏修正，显式削弱与内容无关的位置因素，使 attention 更接近真实的语义重要性。如下图所示。&lt;/p&gt;&lt;p&gt;与此同时，在剪枝阶段显式抑制 padding token 的影响，避免语义为空的区域干扰剪枝排序。整个过程无需重新训练模型，也不依赖特定的剪枝策略，可作为 &lt;strong&gt;plug-and-play 模块&lt;/strong&gt; 直接集成到现有方法中。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGxXJicJyvtNGh2zgSsRd0MgsT5hQZ9e7Et4jFJiaFFASjXR3SQd7riagHw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6111111111111112" data-type="png" data-w="1080" data-width="2800" data-height="1711" data-imgfileid="503531361" data-aistatus="1" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/d5de95f5-3a76-4358-8218-11799230bc9b/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;四、实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在实验验证中，该团队将 Attention Debiasing 方法集成到 FastV、PyramidDrop、SparseVLM、HiMAP、TokenCarve、iLLaVA 等 6 种主流 attention-based 剪枝方法中，在 10 个图像理解基准与 3 个视频理解基准 上进行了系统评估，并覆盖 LLaVA-7B / 13B 等多种主流 Vision-Language Models。&lt;/p&gt;&lt;p&gt;实验结果表明，在几乎所有设置下，经过 attention 去偏修正后，剪枝模型都能获得一致且稳定的性能提升，且在剪枝更激进、token 预算更紧张的情况下效果尤为明显。这说明，对 attention 进行去偏处理，有助于模型在 &amp;ldquo;更少信息&amp;rdquo; 的条件下做出更可靠的判断。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGhfAOdzicic8nYUA8iaOKOwMpuCWYib6WC4wEQXPj14I9bslrKNUfD8tH2w/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.7444444444444445" data-type="png" data-w="1080" data-width="5897" data-height="4389" data-imgfileid="503531362" data-aistatus="1" data-original-style="background-color: transparent;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/859ee5d0-f477-41b2-b4f9-fd4ac60cee03/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG4v0uiaSS5ia63KV2rrEbzdyw92a7x8Ckyy5lrplBISbqlR3iapkXicQewQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.762962962962963" data-type="png" data-w="1080" data-width="2749" data-height="2099" data-imgfileid="503531363" data-aistatus="1" data-original-style="background-color: transparent;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/7c60104d-3a7c-47d3-9814-34447f582d6d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;此外，通过对实验结果的可视化分析，原始 attention-based 剪枝方法往往保留了大量位于图像下方或 padding 区域的视觉 token，而与问题语义密切相关的关键区域却容易被忽略。引入 attention 去偏修正后，模型保留的视觉区域更加集中于目标物体及关键细节位置，有效减少了无关背景的干扰。该结果直观验证了 attention 去偏在提升剪枝合理性和可解释性方面的作用。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGZVsJey8ibvLicV4cficZNVnj75bRlZAdV4ZAico69Y5Wj0zJ1t30p6z5Fg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=8" data-ratio="0.9805555555555555" data-type="jpeg" data-w="1080" data-width="10190" data-height="9988" data-imgfileid="503531364" data-aistatus="1" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/d5aad272-5614-47c9-a79d-36ab7082e71e/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;五、总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该研究表明，attention 并非天然等价于语义重要性，尤其在 Vision-Language Models 中，如果忽视 attention 中潜在的结构性偏置，基于 attention 的剪枝策略可能会被误导。上海大学曾丹团队通过简单而有效的 attention 去偏方法，显著提升了多模态模型在效率与可靠性之间的平衡能力。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>大道至简，何恺明团队新作pMF开启像素级「无潜、单步」生成范式</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 23:29:46 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;何恺明团队新论文，再次「大道至简」。&lt;/p&gt;&lt;p&gt;此次研究直指当前以 DiT 为代表的主流扩散模型与流匹配模型存在的通病，并&lt;strong&gt;提出了一种用于单步、无潜空间（Latent-free）的图像生成新框架&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531379" data-ratio="0.22685185185185186" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGnEx31ugHbKJ2T7VKvjY4fEAEUlKE9SxBDo8pNiatrfK52C5GMBafLrg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/7cdaf6f2-e045-41fa-85b5-0a303c848bd0/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：One-step Latent-free Image Generation with Pixel Mean Flows&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;arXiv 地址：https://arxiv.org/pdf/2601.22158v1&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在生成式 AI 领域，追求更高效、更直接的生成范式一直是学界的核心目标。&lt;/p&gt;&lt;p&gt;当前，以 DiT 为代表的主流扩散模型与流匹配模型主要依赖两大支柱来降低生成难度，一是通过多步采样将复杂的分布转换分解为微小的步进，二是在预训练 VAE（变分自编码器）的潜空间中运行以降低计算维度。&lt;/p&gt;&lt;p&gt;尽管这些设计在图像质量上取得了巨大成功，但从深度学习「端到端」的精神来看，这种对多步迭代和预置编码器的依赖，无疑增加了系统的复杂性和推理开销。&lt;/p&gt;&lt;p&gt;面对这些挑战，&lt;strong&gt;何恺明团队提出了用于单步、无潜空间图像生成的 pixel MeanFlow（pMF）框架&lt;/strong&gt;。该框架继承了改进均值流（improved MeanFlow，MF）的思路，通过在瞬时速度（即 v）空间内定义损失函数，来学习平均速度场（即 u）。&lt;/p&gt;&lt;p&gt;与此同时，受 Just image Transformers（JiT）的启发，pMF 直接对类似于去噪图像的物理量（即 x-prediction 值）进行参数化，并预期该物理量位于低维流形上。&lt;/p&gt;&lt;p&gt;为了兼容这两种设计，团队引入了一种转换机制，将 v、u 和 x 三个场联系起来。实验证明，这种设计更符合流形假设，并且产生了一个更易于学习的目标（见下图 1）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGiaZggwV4geVZVVexd8RDtveqjEeHBVbkDfGTP9KtLJCaorMibVpG9hgQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.42592592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531380" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/eff7bbdd-aee4-4a7f-bc1f-2851979e49f6/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;概括来说，&lt;strong&gt;pMF 训练了一个能将噪声输入直接映射为图像像素的网络&lt;/strong&gt;。它具备「所见即所得」的特性，而这在多步采样或基于潜空间的方法中是不存在的。这一特性使得感知损失能够自然地集成到 pMF 中，从而进一步提升生成质量。&lt;/p&gt;&lt;p&gt;实验结果显示，pMF 在单步、无潜空间生成方面表现强劲，在 ImageNet 数据集上，256x256 分辨率下的 FID 达到 2.22，512x512 分辨率下达到 2.48。团队进一步证明，选择合适的预测目标至关重要：在像素空间直接预测速度场会导致性能崩溃。&lt;/p&gt;&lt;p&gt;本文验证了：&lt;strong&gt;单步、无潜空间生成正变得既可行又具竞争力，这标志着向构建单一、端到端神经网络形式的直接生成建模迈出了坚实的一步。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;框架方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了实现单步、无潜空间的生成，团队引入了 pMF（pixel MeanFlow），它的核心设计在于建立 u、 v 和 x 这三个不同场之间的关联。团队希望网络能像 JiT 那样直接输出 x，而单步建模则像均值流 (MeanFlow) 一样在 u 和 v 空间内进行。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;去噪图像场&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;iMF 和 JiT 都可以被视为在最小化 v-loss，不同之处在于 iMF 执行的是 u-prediction，而 JiT 执行的是 x-prediction。团队在 u 与广义形式的 x 之间引入了一种联系。&lt;/p&gt;&lt;p&gt;原论文等式 (5) 中定义的平均速度场 u 代表了一个潜在的基准真值（ground-truth），它取决于 p_data、p_prior 以及时间调度，但与网络无关（因此不依赖于参数 &amp;theta;）。团队引出了一个定义为 x (z_t, r, t) 的新场：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGsRe2QvicnWiaM7lBXzSgut1uWI5UGPJlg3t0ElCubW2YtsZ7WovItPYQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.17777777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531382" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/c590c57f-1062-44eb-87f8-714e38d65016/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;可泛化的流形假设&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上图 1 通过模拟从预训练流匹配（FM）模型中获得的一条 ODE 轨迹，可视化了 u 场和 x 场。u 包含噪声图像，这是因为作为速度场，u 同时包含了噪声和数据成分。相比之下，x 场具有去噪图像的外观：它们或是近乎清晰的图像，或是因过度去噪而显得模糊的图像。接下来，团队讨论了如何将流形假设泛化到一物理量 x 上。&lt;/p&gt;&lt;p&gt;请注意，MeanFlow 中的时间步 r 满足：&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGmloB9m6Hvuv5UZqld1YWCbBBLtGFKKp7BFvuUlgECcZY4orITfdSCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.2789115646258503" data-s="300,640" data-type="png" data-w="588" type="block" data-imgfileid="503531383" data-aistatus="1" data-original-style="width:60px;height:20px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/2d1b0234-2e39-4e16-b18b-6eb82e41b4a8/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 8.89%;"&gt;。团队首先展示了 r=t 和 r=0 这两种边界情况可以近似满足流形假设；随后讨论了 0＜r＜t 的情况。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;算法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上文公式 (8) 中导出的 x 场为 MeanFlow 网络提供了一种重参数化方法。具体而言，团队让网络 net_&amp;theta; 直接输出 x，并根据公式 (8) 计算出相应的速度场 u：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGUpJiaiaoaeaS4Cq2fwLSovA8JFkoTHiarE0hlZNeyqgwPRIgA4ZuVRNEw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.1259259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531385" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/5cbdd4ec-323d-46cb-bcc6-ff46f78cc4af/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;接着将公式 (11) 中的 u_&amp;theta; 纳入 iMF 表述中，即结合 v-loss 使用原论文公式 (7)。具体的优化目标如下：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGzib2fw20CnBaiaMm87DFC4n1vicgA2HTsgqjDFrx2nFeiaiaf642f4f6wOw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.1814814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531387" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/eb696b0a-fa1d-4193-b494-683826b19080/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;从概念上讲，这是基于 x-prediction 的 v-loss，其中 x 通过 x&amp;rarr;u&amp;rarr;v 的关系转换为 v 空间，从而对 v 进行回归。相应的伪代码见算法 1。遵循 iMF 的思路，该算法可以扩展以支持无分类器引导（CFG）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG3SHphiaLOj7vialBqcRD0yKiaNRUvo5lWibCRiahfia9FOlw6pib4A8jGulJw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="1.0842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531388" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/8371ae76-0235-41c5-8831-4b27954a7c94/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;带有感知损失的像素均值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;网络 x_&amp;theta;(z_t,r,t) 直接将噪声输入 z_t 映射为去噪图像，这使得模型在训练时具备了「所见即所得」的特性。因此团队进一步引入了感知损失，基于潜空间的方法在 tokenizer 重构训练中获益于感知损失，而基于像素的方法此前尚未能轻易利用这一优势。&lt;/p&gt;&lt;p&gt;在形式上，由于 x_&amp;theta; 是像素空间下的去噪图像，团队直接对其应用感知损失（例如 LPIPS ）。整体训练目标为&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG9fHW5ECmDuHXQS8Rno3ltaMf8n22iajGkpRMqZyAfnRrSDZu31vWcQA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.16698292220113853" data-s="300,640" data-type="png" data-w="1054" type="block" data-imgfileid="503531389" data-aistatus="1" data-original-style="width:99px;height:20px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/ec107187-a8f2-4bde-91ab-fb30097ee8b3/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dii" style="width: 16.5%;"&gt;。在实践中，感知损失可以仅在所添加噪声低于特定阈值（即 t&amp;le;t_thr）时应用，从而确保去噪后的图像不会过于模糊。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;玩具（Toy）实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队首先通过一个 2D 玩具实验表明，「当底层数据位于低维流形上时，在 MeanFlow 中使用 x-prediction 更加理想。」&lt;/p&gt;&lt;p&gt;图 2 显示，x-prediction 的表现相当出色，而随着维度 D 的增加，u-prediction 的性能迅速退化。团队观察到，这种性能差距反映在训练损失的差异上：x-prediction 的训练损失低于对应的 u-prediction。这表明，对于容量有限的网络而言，预测 x 更加容易。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGmQWCVvSX0wx0U3vZdSS3qNhTCbSGHXS1PGW7Lypz5PsUGAjIcH8ZCQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.9490740740740741" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531390" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/81d0d120-56db-46ba-ad1f-a94fe9528279/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;团队默认在分辨率为 256x256 的 ImageNet 数据集上进行消融实验。团队采用了 iMF 架构，它是 DiT 设计的一个变体。除非另有说明，团队将 Patch 大小设置为 16&amp;times; 16（表示为 pMF/16）。消融模型从零开始训练了 160 个 Epoch。&lt;/p&gt;&lt;p&gt;关于&lt;strong&gt;网络预测目标&lt;/strong&gt;，团队的方法基于流形假设，即假设 x 处于低维流形中且更易于预测。表 2 验证了这一假设。&lt;/p&gt;&lt;p&gt;首先将 64&amp;times;64 分辨率作为较简单的设置。当 Patch 大小为 4&amp;times;4 时，Patch 维度为 48（即 4&amp;times;4&amp;times;3）。这一维度远低于网络容量（隐藏层维度为 768）。因此，pMF 在 x-prediction 和 u-prediction 下均表现良好。&lt;/p&gt;&lt;p&gt;接下来考虑 256&amp;times;256 分辨率。按照惯例，Patch 大小设为 16&amp;times;16，Patch 维度达到 768（即 16&amp;times;16&amp;times;3）。这导致了更高维的观测空间，增加了神经网络建模的难度。在这种情况下，只有 x-prediction 表现良好，表明 x 位于更低维的流形上，因此更易于学习。&lt;/p&gt;&lt;p&gt;相比之下，u-prediction 性能彻底崩溃：作为一种含噪物理量，u 在高维空间中具有全支撑，建模难度大得多。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGc3M6Ga5IcYwlndpVaUL76THvbn918Zl9o2saAOWfqHlial8lYIZbgTQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.6620370370370371" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531392" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/c5eb6454-e10c-48cc-9da0-0346b9e0c0a0/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;关于&lt;strong&gt;高分辨率生成&lt;/strong&gt;，团队在表 4 中研究了分辨率在 256、512 和 1024 下的 pMF。在保持序列长度不变（16^2）的情况下，不同分辨率下大致维持了相同的计算成本。这样做会导致极其激进的 Patch 大小（例如 64^2）和 Patch 维度（例如 12288）。&lt;/p&gt;&lt;p&gt;结果显示，pMF 可以有效处理这种极具挑战性的情况。尽管观测空间是高维的，但模型始终预测 x，其底层维度并不会成比例增长。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGTW39eRCUkxBBG3v9icaZV8iaibg0WLCgYwB3cBxfI2CSNy5hJEZFNgKHA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.42962962962962964" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531394" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/a8923ea0-9727-40db-996c-8acdb89902a1/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;关于&lt;strong&gt;可扩展性&lt;/strong&gt;，团队在表 5 中报告了增加模型大小和训练 Epoch 的结果。正如预期的那样，pMF 从这两个维度的扩展中均有获益。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531395" data-ratio="0.3287037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGIAIBmkrhaVshqGIoPNa3x4hg2e9LMyVQd4tNus0HcRR4IRJmPpuRicg/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/cb79cae6-c0b7-408c-8363-4d219e2374e3/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;最后，团队在表 6（256&amp;times;256）和表 7（512&amp;times;512）中 ，将 pMF 与之前的模型进行了对比。&lt;/p&gt;&lt;p&gt;其中，在 &lt;strong&gt;256&amp;times;256 分辨率&lt;/strong&gt;下，团队的方法达到了 2.22 FID（在 360 个 Epoch 时），如表 6 所示。据团队的了解，该类别中（单步、无潜空间扩散 / 流模型）唯一的其他方法是最近提出的 EPG，它在自监督预训练下达到了 8.82 FID。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531396" data-ratio="1.4333333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGAzGxib26ChlxwXnehtbUpzbRiaiboRcNoGtUf6FMuLHbich8oceZKVSrQA/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/d0319ca4-822b-47dc-8b69-23729eea6e43/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在&lt;strong&gt; 512&amp;times;512 分辨率&lt;/strong&gt;下，pMF 达到了 2.48 FID，如表 7 所示。这一结果的计算成本（参数量和 Gflops）与 256&amp;times;256 版本相当。事实上，唯一的额外开销仅来自通道数更多的 Patch 嵌入层和预测层，所有的 Transformer 模块都维持了相同的计算成本。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531398" data-ratio="1.0935185185185186" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGQnibzC3u1TFsvAIFHZDOJXZ64kIkicHk0w16o3zVRmyyICI7ZdISf8fg/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/011aa59e-9ccf-45ff-80d8-e59f0bc90c94/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;更多实验细节请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>谷歌给「AI解数学题」神话降温：能摘低垂果实，但过程依然痛苦</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 23:25:17 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜张倩&lt;/section&gt;&lt;p&gt;刚刚，谷歌发布了一项新的研究进展：他们用 Gemini 做了一次系统性的数学攻关实验，把目标对准了著名的 Erdős Problems 数据库里 700 个仍被标注为 open（未解决）的猜想。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531332" data-ratio="0.7869158878504673" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGyhOZlbDQKia7Sj1Jrs8QOJ4LIfNkFOSWkp0UlZaOQVFuCrP3dVzdfkg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1070" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/0db9ffb1-ee35-4fd2-94ab-0e99fb1c9c61/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;结果相当亮眼：Gemini 在这批问题中一共推进了 13 个 &amp;mdash;&amp;mdash; 其中 5 个是模型自主给出的全新解法，另外 8 个则是模型在文献中挖出了早已存在、但此前被遗漏的解答。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGqMlV1oF2KDCkLEpAn6O8OCrLSF3YHCeTY8gPiaG9MV3icBdO49WX8Xyg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.44907407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531333" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c77e7a35-397c-4ac9-aedc-9c65a64449f1/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/pdf/2601.22401&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Erdős Problems 数据库以数学家 Paul Erdős 的名字命名。他是 20 世纪最多产的数学家之一，留下了大量论文和未解决的猜想，涵盖数论、组合数学、图论等。2023 年，数学家 Thomas Bloom 推出了 ErdosProblems.com 网站，这是一个集中式数据库，旨在整理这些猜想并跟踪其研究进展。目前，该数据库共收录 1179 个问题，其中 483 个（41%）被归类为已解决。&lt;/p&gt;&lt;p&gt;然而，该数据库中标注「open」的问题并不一定代表问题真的未被解决，而是意味着至少有一位专业数学家尝试通过网络搜索寻找已发表的解决方案，但以失败告终。&lt;/p&gt;&lt;p&gt;事实证明，很多问题并非「未解决」，而是答案被淹没了。去年 10 月份，OpenAI 宣布 GPT-5 在该网站上发现了 10 个标记「open」的问题，但其实它们的答案已经存在于相关文献，只是之前未被搜到。&lt;/p&gt;&lt;p&gt;这一发现使得 Bloom 的数据库受到了广泛关注，同时促使陶哲轩近期创建了一个社区维基，专门跟踪人工智能辅助解决 Erdős 问题的相关动态。&lt;/p&gt;&lt;p&gt;如今，谷歌的研究把 Erdős 问题的解决又往前推了一步。但他们也坦言，这并不意味着 AI 已经能「自动做数学研究」了，背后的脏活累活远超普通人想象。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;研究方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作者团队在 2025 年 12 月 2&amp;ndash;9 日部署了一个基于 Gemini Deep Think 的定制数学研究智能体 Aletheia，对 Bloom 数据库中当时仍标注为「Open」的约 700 个 Erdős 问题进行半自动探索。Aletheia 内置自然语言验证器（verifier），用于在大规模生成后先做第一轮筛选，将候选问题从 700 个快速收敛到 212 个「看起来可能正确」的回答。&lt;/p&gt;&lt;p&gt;接下来进入人工评估阶段。研究团队先由非该领域专家的数学家进行快速过滤，尽可能在可控时间内剔除明显错误解，从而把候选规模压缩到 27 个，再交由内部领域专家逐一严审；当解法的正确性明确但新颖性存疑时，还会咨询外部专家核对文献。&lt;/p&gt;&lt;p&gt;最终统计显示，在可明确判定的约 200 个候选解中，137 个（68.5%）存在根本性错误；63 个（31.5%）在形式上成立，但&lt;strong&gt;其中只有 13 个（6.5%）真正回答了 Erdős 原本想问的问题&lt;/strong&gt;。其余 50 个虽然「技术上正确」，却因为误读题意而导致数学意义有限，作者计划对这些问题提出更严谨的修订表述；此外还有 12 个回答因问题本身开放或表述不清而被标记为「歧义」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGhMZj0ict93knqiacKdruiaftWxSpW9iaDicibSPruyLz0BibjBGKhe9ePaLQw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.32037037037037036" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531334" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/5ff20f33-cc2d-4cae-9d57-d13adb241ca6/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531335" data-ratio="0.4444444444444444" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGehIzEZ3DEVCSt9wFEtiaSnIAUiboiccszSBhXNvPfXuKzhFic343Lz6lzA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/f01ea04a-fef8-421c-9800-19aef16c351a/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;根据陶哲轩的建议，作者着重列出了上述数据以保证透明度。这也是为了更完整地呈现 AI 辅助数学研究的真实成本：除了少数正例之外，大量时间会消耗在核验、纠错、排查细微错误，以及检索文献以排除「无意重复」上。&lt;/p&gt;&lt;p&gt;这表明，业内广为流传的&lt;strong&gt;「AI 正在加速科学」的论断有一定片面性&lt;/strong&gt;：人们通常只展示少数成功案例，强调 AI 在某个任务上比人类更快，从而声称 AI「加速」了这一结果；但这类叙事很少把负例纳入计算。&lt;/p&gt;&lt;p&gt;更具挑战性的是最后一步 &amp;mdash;&amp;mdash; 确认解答是否已在文献中出现、以及是否真正契合 Erdős 的原始意图。许多问题的困难不在数学推导，而在题面细节的抄录误差、遗漏、以及符号与定义约定的歧义；模型若不了解 Bloom 网站的定义惯例，往往会在多个「各自合理」的解释之间混淆。&lt;/p&gt;&lt;p&gt;作者指出，在深入做文献核查与语义对齐后，「真正有意义的正确解」数量会显著下降，这也提醒未来的 AI 数学发现工作必须对题意一致性与文献溯源保持高度谨慎。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;作者将 13 个有意义的正确结果分为四类：&lt;/p&gt;&lt;p&gt;1、AI 自主解决。对于这些问题，Aletheia 找到了首个正确的解决方案，且解决方案具有实质性的数学意义。其中包括 Erdős-652 和 Erdős-1051，但需要说明的是，Erdős-652 的解决是通过直接引用现有文献中的结果实现的。&lt;/p&gt;&lt;p&gt;2、部分由 AI 解决。对于这些包含多个子问题的复杂问题，Aletheia 找到了其中一个子问题的首个正确解决方案。其中包括 Erdős-654、Erdős-935 和 Erdős-1040。&lt;/p&gt;&lt;p&gt;3、独立重发现。对于这些问题，Aletheia 找到了正确的解决方案，但人类审核者随后发现文献中已存在独立的解决方案。其中包括 Erdős-397、Erdős-659 和 Erdős-1089。这些解决方案似乎是模型独立重发现的：作者仔细检查了 Aletheia 的推理过程日志，确保该解决方案并非直接从文献中提取。当然，&lt;strong&gt;该解决方案也有可能是通过中间来源或预训练过程间接从文献中获取的&lt;/strong&gt;。这凸显了 AI 生成数学内容所伴随的一个新风险：模型可能会再现预训练过程中习得的文献知识，却不注明来源，即存在「潜意识抄袭」的风险。&lt;/p&gt;&lt;p&gt;4、文献识别。对于这些问题，尽管在模型部署时 Bloom 网站将其标记为「open」，但 Aletheia 识别出文献中已明确存在相关解决方案。其中包括 Erdős-333、Erdős-591、Erdős-705、Erdős-992 和 Erdős-1105。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGehIzEZ3DEVCSt9wFEtiaSnIAUiboiccszSBhXNvPfXuKzhFic343Lz6lzA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.4444444444444444" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531336" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/ea6dca1f-ab30-42c8-b5b3-16d0b45b7aab/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;需要明确的是，研究团队并未声称后两类结果具有创新性。上述提到的 5 个自主生成的解决方案分别对应 Erdős-652、Erdős-654、Erdős-935、Erdős-1040 和 Erdős-1051。&lt;strong&gt;根据专家的评估，这 5 个解决方案均未达到学术论文的水平。事实上，其中一些解决方案仅相当于研究生习题的难度&lt;/strong&gt;（基于现有文献）。&lt;/p&gt;&lt;p&gt;他们初步认为，Aletheia 对 Erdős-1051 的解决方案是 AI 系统自主解决具有一定普遍性（温和）数学意义的重要开放 Erdős 问题的早期案例 &amp;mdash;&amp;mdash; 虽然存在关于密切相关问题的过往文献，但这些文献均未完全解决 Erdős-1051。&lt;/p&gt;&lt;p&gt;此外，与许多之前讨论的案例不同，作者认为 Aletheia 的解决方案并非直接受任何先前人类论证的启发，但该方案确实采用了经典思路：转向级数尾部并应用马勒准则（Mahler&amp;rsquo;s criterion）。在 Aletheia 与人类数学家以及 Gemini Deep Think 的协作下，Erdős-1051 的解决方案得到了进一步推广，并形成了研究论文。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;研究意义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究结果表明，&lt;strong&gt;Erdős 问题中存在「低垂的果实」，而 AI 已发展到能够摘取这些果实的水平&lt;/strong&gt;。尽管这为 AI 研究人员提供了一种新的、有趣的数学基准，但作者提醒&lt;strong&gt;人们不应过度夸大其数学意义。本文解决的所有开放问题，任何相关领域的专家都能轻松完成&lt;/strong&gt;。另一方面，人类专家的时间有限。如果能够提高 AI 的可靠性，它已展现出加速数学发现中注意力瓶颈环节的潜力。&lt;/p&gt;&lt;p&gt;在本文的案例研究中，作者遇到了一些最初未预料到的困难。绝大多数技术正确的自主生成解决方案都源于对问题陈述的误解或解读缺陷，而诊断这些问题有时需要花费大量精力。&lt;/p&gt;&lt;p&gt;此外，人类专家面临的最具挑战性的步骤并非验证解决方案的正确性，而是确定这些解决方案是否已存在于文献中。随着人工智能生成数学内容的增多，&lt;strong&gt;学术界必须警惕「潜意识抄袭」&lt;/strong&gt;，即 AI 再现训练过程中习得的文献知识，却未给予适当引用。需要注意的是，形式化验证无法解决这些问题。&lt;/p&gt;&lt;p&gt;尽管 AI 自主解决 Erdős 问题的尝试取得了一定成功，但也引发了误导性的炒作和彻头彻尾的虚假信息，并在社交媒体平台上被放大，这对数学界造成了损害。除了 Erdős 问题，未来可能还会有许多其他数学猜想列表成为（半）自主研究的目标。作者恳请相关研究人员关注本文提出的这些问题。&lt;/p&gt;&lt;p&gt;更多信息请参考原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | Rebuttal 是一场「带着镣铐的舞蹈」？港科 RebuttalAgent 用心智理论「读懂」审稿人</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 23:21:45 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/b2ba1ae2-9303-4b55-9bdb-d424bc9b8aa4/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;面对同行评审，许多作者都有过这样的经历：明明回答了审稿人的每一个问题，态度也足够谦卑，为什么最终还是没能打动对方？&lt;/p&gt;&lt;p&gt;现有的通用大模型在处理这类任务时，往往陷入一种 &amp;ldquo;表面礼貌&amp;rdquo; 的陷阱：它们擅长生成流畅、委婉的 &amp;ldquo;Thank you for your insightful comment&amp;rdquo;，却缺乏对审稿人言外之意的深度洞察，导致回复虽然客气，但缺乏直击痛点的说服力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;究竟什么样的回复策略，才能在有限的篇幅内，有效消除误解、赢得共识？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;针对这一问题，来自&lt;strong&gt;香港科技大学的研究团队&lt;/strong&gt;提出了一种全新的框架 &amp;mdash;&amp;mdash;&lt;strong&gt;RebuttalAgent&lt;/strong&gt;。该研究首次将认知科学中的 &lt;strong&gt;心智理论（Theory of Mind, ToM）&lt;/strong&gt; 引入学术 Rebuttal 任务，让 AI 能够像资深学者一样 &amp;ldquo;读懂&amp;rdquo; 审稿人，从而生成兼具战略性与说服力的回复。&lt;/p&gt;&lt;p&gt;目前，该论文已被&amp;nbsp;&lt;strong&gt;ICLR 2026&lt;/strong&gt;&amp;nbsp;接收。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibedd573Ode2ibgkaeS7ibiazfRuS466iaNibamXOSoic2b6NQs2uUV30twszu9iabxkeEEibGHRoZuQTJ5Eg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3685185185185185" data-type="png" data-w="1080" data-width="1698" data-height="626" data-imgfileid="503530712" data-aistatus="1" data-original-style="background-color: transparent;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6cda6875-c08e-4910-a1f2-e3b171a5efd3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://arxiv.org/pdf/2601.15715&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://github.com/Zhitao-He/RebuttalAgent&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz29kmtk9k" data-pm-slice="0 0 []"&gt;&lt;strong&gt; Rebuttal 需要怎样的博弈智慧？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在学术界的博弈论视角下，Rebuttal 是一个典型的 &lt;strong&gt;&amp;ldquo;不完全信息动态博弈&amp;rdquo;（Dynamic Game of Incomplete Information）&lt;/strong&gt;。作者不仅要面对审稿人显性的质疑，还要应对隐性的信息不对称，你不知道审稿人的知识背景、潜在偏见，也不知道你的解释会引发怎样的连锁反应。&lt;/p&gt;&lt;p&gt;现有的基于监督微调的模型，大多止步于对人类回复的&amp;lsquo;语言学拟态&amp;rsquo;。它们精准复刻了礼貌的&amp;lsquo;外壳&amp;rsquo;，却未能触及审稿人意图的&amp;lsquo;内核&amp;rsquo;，即缺乏对审稿人的深度建模。 针对这一痛点，研究者提出了 RebuttalAgent，其核心洞察：&lt;strong&gt;有效的说服机制，必须建立在对他人的&amp;lsquo;心智理论&amp;rsquo;建模之上。&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibedd573Ode2ibgkaeS7ibiazfrDWfLsPFRZPXWPBqzJiasx1gk46icnsRISWuCf9AQz4UiaI76P9xslGDg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6453703703703704" data-type="png" data-w="1080" data-width="1276" data-height="824" data-imgfileid="503530713" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e03e05c2-7a8c-4638-acdf-6903d25d4aaf/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2bo51m3e" data-pm-slice="0 0 []"&gt;&lt;sup&gt;图一：RebuttalAgent 框架总览图，展示 Data Preparation, TSR Framework 和 Agent Training 三个阶段&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2bwfj10je" data-pm-slice="0 0 []"&gt;&lt;strong&gt;TSR 框架 &amp;mdash;&amp;mdash; 先 &amp;ldquo;读心&amp;rdquo; 再 &amp;ldquo;落笔&amp;rdquo;，&lt;/strong&gt;&lt;/span&gt;&lt;span data-mpa-action-id="mkz2bwfj10je" data-pm-slice="0 0 []"&gt;&lt;strong&gt;重构 AI 的思考链路&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;RebuttalAgent 并没有直接端到端地生成回复，而是模拟了人类专家的思维过程，通过 &lt;strong&gt;ToM-Strategy-Response (TSR) &lt;/strong&gt;框架来拆解这一复杂任务：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. ToM（心智理论建模）&lt;/strong&gt;：不仅仅是读文本 AI 首先充当一名 &amp;ldquo;分析师&amp;rdquo;，对审稿意见进行分层剖析。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;宏观层面（Macro-level）： 判断审稿人的整体立场（接受 / 拒绝）、态度（建设性 / 消极）以及领域专业度。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;微观层面（Micro-level）： 拆解每一条评论背后的核心关切（是方法论缺陷？还是单纯的表达不清？）。 这种建模让 AI 不再盲目回复，而是先构建出审稿人的 &amp;ldquo;心理画像&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. Strategy（谋定而后动）&lt;/strong&gt;：基于上述画像，AI 会生成一个明确的战略计划。例如，面对一个 &amp;ldquo;专业度高但态度怀疑&amp;rdquo; 的审稿人，策略可能是 &amp;ldquo;先承认局限性以建立信任，再用补充实验数据进行强力反击&amp;rdquo;；而面对 &amp;ldquo;误解型&amp;rdquo; 评论，策略则是 &amp;ldquo;澄清概念，重述核心贡献&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. Response（精准打击）&lt;/strong&gt;：最后，AI 结合原始论文片段、战略计划和审稿人画像，生成最终的回复。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibedd573Ode2ibgkaeS7ibiazfbp5Cfvtt2ErRI4op0KHg6HibbUw1LFbyOkl1bDOWE8ZoO1icLPof3lZQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.28055555555555556" data-type="png" data-w="1080" data-width="1460" data-height="410" data-imgfileid="503530714" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/effae19f-65d4-4f4f-8c29-a3c4af688d47/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2cno920nc" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表一：评估的一致性：Rebuttal-RM 在对齐人类偏好上超越 GPT-4.1&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2cytny05" data-pm-slice="0 0 []"&gt;&lt;strong&gt;无需外部导师，&lt;/strong&gt;&lt;/span&gt;&lt;span data-mpa-action-id="mkz2cytny05" data-pm-slice="0 0 []"&gt;&lt;strong&gt;&amp;ldquo;自我博弈&amp;rdquo; 中习得说服的艺术&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;为了训练这样一个能够 &amp;ldquo;运筹帷幄&amp;rdquo; 的 Agent，研究团队面临的最大挑战是数据的稀缺与主观性。为此，他们构建了 &lt;strong&gt;RebuttalBench&lt;/strong&gt;，包含超过 7 万条高质量的 &amp;ldquo;分析 - 策略 - 回复&amp;rdquo; 链条数据。&lt;/p&gt;&lt;p&gt;更进一步，研究者引入了 &lt;strong&gt;Self-Reward 机制&lt;/strong&gt; 的强化学习策略。与传统的依赖外部奖励模型不同，RebuttalAgent 利用自身生成的评价信号进行迭代：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;格式与逻辑奖励&lt;/strong&gt;： 确保 AI 真的在进行思考和布局，而不是形式主义。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多样性奖励&lt;/strong&gt;： 这是一个关键设计。为了防止 AI 偷懒生成 &amp;ldquo;万金油&amp;rdquo; 式的套话（如反复使用 &amp;quot;We thank the reviewer...&amp;quot; 模板），研究者设计了多样性惩罚，迫使模型探索更多样、更像人类专家的表达方式。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2ecl115ix" data-pm-slice="0 0 []"&gt;&lt;strong&gt;从 &amp;ldquo;辞藻堆砌&amp;rdquo; 到 &amp;ldquo;攻心为上&amp;rdquo;：&lt;/strong&gt;&lt;/span&gt;&lt;span data-mpa-action-id="mkz2ecl115ix" data-pm-slice="0 0 []"&gt;&lt;strong&gt;当 AI 学会了换位思考&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;为了量化评估效果，研究团队开发了 &lt;strong&gt;Rebuttal-RM&lt;/strong&gt;，这是一个专门针对学术反驳场景训练的奖励模型。在与人类专家评分的一致性测试中，Rebuttal-RM 的表现超越了 GPT-4.1。&lt;/p&gt;&lt;p&gt;在这一评估体系下，RebuttalAgent 展现出了显著优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在综合得分上，RebuttalAgent 达到了&lt;strong&gt; 9.42&lt;/strong&gt;，显著优于 GPT-4.1 和 O3 。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在 &lt;strong&gt;说服力（Persuasiveness）&lt;/strong&gt; 这一核心指标上，提升尤为明显，表明引入 &amp;ldquo;心智理论&amp;rdquo; 确实增强了模型在观点交锋中的有效性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibedd573Ode2ibgkaeS7ibiazfUGcBUXLrdfjWJKlibVEUehAibCkbTYGuD1nouIabbjV0Skicy5yvYdhuQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.3787037037037037" data-type="png" data-w="1080" data-width="1478" data-height="560" data-imgfileid="503530715" data-aistatus="1" data-original-style="background-color: transparent;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a6a6e8b3-66a1-49ad-9f24-d2dc5241bab3/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2fayv47p" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表二：RebuttalAgent 与其他强基线的性能对比&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz3nfvn19kt" data-pm-slice="0 0 []"&gt;&lt;strong&gt;&amp;ldquo;即插即用&amp;rdquo; 的思维外挂：&lt;/strong&gt;&lt;/span&gt;&lt;span data-mpa-action-id="mkz3nfvn19kt" data-pm-slice="0 0 []"&gt;&lt;strong&gt;让小模型也能像专家一样思考&lt;/strong&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;RebuttalAgent 生成的 &amp;ldquo;分析（Analysis）&amp;rdquo; 和 &amp;ldquo;策略（Strategy）&amp;rdquo; 是否具有普适性？研究者设计了一个巧妙的实验：将 RebuttalAgent 生成的策略作为上下文（Context），喂给参数量较小的基础模型（如 Qwen3-8B 和 Llama-3.1-8B），观察它们的表现变化 (Average Score)。&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验发现，这是一个通用的 &amp;ldquo;思维外挂&amp;rdquo;。仅需引入 RebuttalAgent 的策略指导，&lt;strong&gt;Qwen3-8B 在 &amp;ldquo;表达清晰度&amp;rdquo; 上的得分就暴涨了 21.0% &lt;/strong&gt;，这有力地证明了 TSR 框架的可迁移性。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibedd573Ode2ibgkaeS7ibiazfzFkm7lmweh1icJG9wcIk5P6SJzFgZG9BqwNnaNt9TDJ1I4CQSfVnrSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.45582329317269077" data-type="png" data-w="996" data-width="996" data-height="454" data-imgfileid="503530717" data-aistatus="1" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/bd429796-738f-4710-a73c-d6f522c2e484/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz2hcbxfwq" data-pm-slice="0 0 []"&gt;&lt;strong&gt;做科研路上的 &amp;ldquo;理性副驾驶&amp;rdquo;，&lt;/strong&gt;&lt;/span&gt;&lt;span data-mpa-action-id="mkz2hcbxfwq" data-pm-slice="0 0 []"&gt;&lt;strong&gt;而非 &amp;ldquo;幽灵写手&amp;rdquo;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;RebuttalAgent 的提出，展示了 LLM 在处理高阶认知任务，特别是涉及复杂人际博弈和战略沟通场景的巨大潜力。但 Agent 无法替你完成实验，也不会凭空捏造数据，模型在训练之初就刻意剥离了涉及实验结果生成的指令，杜绝了 &amp;ldquo;幻觉造假&amp;rdquo; 的可能。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;打破 &amp;ldquo;新手墙&amp;rdquo;&lt;/strong&gt;： 对于许多刚踏入学术圈的新手而言，面对犀利甚至尖锐的审稿意见，往往容易陷入恐慌或产生防御性心态。RebuttalAgent 的&lt;strong&gt;价值正是在于提供战略性的建议与实用的技巧，帮助作者克服情绪干扰，理清逻辑脉络，组织得体的语言。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;促进学术交流&lt;/strong&gt;： 论文的核心价值在于 &amp;ldquo;提升学术对话的清晰度与建设性&amp;rdquo;。它致力于消除因表达不当或沟通策略缺失而造成的误解，让审稿人与作者的对话回归真理本身，而非陷入情绪对抗或单纯的语言技巧博弈。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;RebuttalAgent 本质上是对 &lt;strong&gt;大语言模型在严重信息不对称条件下战略说服能力&lt;/strong&gt;的一次探索性研究。最终的科学判断与责任，始终掌握在人类作者手中。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz3v1ny12fv" data-pm-slice="0 0 []"&gt;&lt;strong&gt;作者介绍：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkz3v1ny12fv" data-pm-slice="0 0 []"&gt;何致涛，香港科技大学计算机系博士生，导师 Yi R. (May) Fung。曾在中国科学院自动化研究所、清华大学 AIR、蚂蚁集团从事研究，并在 ACL、NeurIPS、COLM、ICLR 等机器学习与自然语言处理顶级会议上发表多篇论文。&lt;/span&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，腾讯姚顺雨署名首篇论文发布，「下半场」先搞上下文学习</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 19:01:33 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;不久前在 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651011350&amp;idx=1&amp;sn=ce3087a30d2765b6f1b15a48cb85d1d6&amp;scene=21#wechat_redirect" target="_blank"&gt;AGI-Next&amp;nbsp;&lt;/a&gt;前沿峰会上，姚顺雨曾分享过一个核心观点：模型想要迈向高价值应用，核心瓶颈就在于能否「用好上下文（Context）」。&lt;/p&gt;&lt;p&gt;这与最近 OpenAI &amp;nbsp;Jiayi Weng 在访谈中的看法不谋而合。Jiayi Weng 认为，上下文决定了模型和人类认知的边界。只要信息足够对等，普通人大概也能在 OpenAI 胜任工作，人和人的差距往往只是源于信息的不对称。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;而近日，混元团队和复旦联合团队发布了首篇论文《CL-bench》，在「重视上下文」的基础上又往前推了一大步。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;值得一提的是，这也是姚顺雨加入腾讯后首次署名的研究论文。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGpRfm74d9f3aluoliaHicia1YLj4RcBpKj6nvnlTw9wWMSUcFiaxvFKpDgA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531368" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e3dda214-84ad-40fa-a83e-c7d613ebdfcd/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文题目：CL-bench: A Benchmark for Context Learning&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：www.clbench.com&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;论文证实了一个更棘手的问题：即便抹平了上下文带来的信息差，模型也未必能解决问题。这说明模型在上下文利用上，依然存在显著的能力短板。&lt;/p&gt;&lt;p&gt;具体来说，作者认为上下文「给到位了」并不等同于任务就能「做对」。这中间考验的是模型的学习能力：就像两个学习天赋不同的人，读的是同一本武功秘籍，有人能瞬息间领悟招式精髓，有人却始终不得要领。&lt;/p&gt;&lt;p&gt;这种差异的本质在于模型的上下文学习能力不同。 如果模型缺乏从上下文中学习新知识、掌握新技能的能力，哪怕解决任务所需的逻辑和范例都近在咫尺，它也依然无从下手。&lt;/p&gt;&lt;p&gt;本文将结合腾讯混元官网首次发表的技术博客《Learning from context is harder than we thought》的中文版，聊聊在上下文学习这件事上，模型面对的真实困境。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;博客链接：https://hy.tencent.com/research&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;从上下文中学习，远比我们想象的要难&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们需要 AI 成为「上下文学习者」（Context learners）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去几年，大语言模型的进化速度快得令人惊叹。如今的前沿模型，已经是顶级的「做题家」：它们能解开奥数级别的难题，能推演复杂的编程逻辑，甚至能通过那些人类需要苦读数年才能拿下的专业资格考试。&lt;/p&gt;&lt;p&gt;然而，这些耀眼的成绩单可能掩盖了一个真相：&lt;strong&gt;能在考场拿满分的学生，未必能胜任真实世界的工作。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;回看我们人类的日常工作：开发者扫过从未见过的工具文档，就能立刻开始调试代码；玩家拿起新游戏的规则书，在实战中边玩边学；科学家从复杂的实验日志中筛选数据，推导出新的结论和定律。我们发现在这些场景中，人类并不只依赖多年前学到的「死知识」，而是在实时地从眼前的上下文中学习。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGabXDVaw7j9MKgq5ewJdf1Xh9ZNrkqAmw4XJFnuHrdFObEWzYbI0dwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.37962962962962965" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531369" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/36539c54-8eb1-4096-9222-7d5e3d7015ed/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;三个人类日常生活和工作场景的例子。这三个例子分别为：（1）面对 SkyNet 无人机 SDK 文档 (~70K 字)，将自然语言所表达的飞行请求转成安全、合规的 SDK 伪代码； （2）直接上手玩一款游戏：给定一款新游戏的完整规则 (~15K 字)，分析隐藏房间场景并给出可能结果；（3）分析 300 份原始实验日志，验证数据、推导关系式并估计共振常数。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;然而，今天的语言模型并非如此。它们主要依赖「参数化知识」&amp;mdash;&amp;mdash; 即在预训练阶段被压缩进模型权重里的静态记忆。在推理时，模型更多是在调用这些封存的内部知识，而不是主动从当前输入的新信息中汲取营养。&lt;/p&gt;&lt;p&gt;这揭示了当前模型的训练范式和在真实场景中应用之间是不匹配的：我们优化出的模型擅长对自己「已知」的事物进行推理，但用户需要的，却是让模型解决那些依赖于杂乱、动态变化的上下文的任务。&lt;/p&gt;&lt;p&gt;简而言之：&lt;strong&gt;我们造出了依赖「过去」的参数推理者，但世界需要的是能吸收「当下」环境的上下文学习者&lt;/strong&gt;。要弥合这一差距，我们必须从根本上改变模型的优化方向。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyet0lcIkZDzTDD3XzBgm8iaTaJElOZSlFibCBKl2oNgpgl7tGECya1n8Eg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531248" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4a974c7a-abf8-4804-8063-f31a3a27289d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;语言模型的范式转变。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;CL-bench: 衡量模型的上下文学习能力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了衡量现有模型距离真正的「上下文学习者」还有多远，我们构建了 &lt;strong&gt;CL-bench&lt;/strong&gt;。这是一个专门评测语言模型能否&lt;strong&gt;从上下文中学习新知识并正确应用&lt;/strong&gt;的基准。&lt;/p&gt;&lt;p&gt;CL-bench 包含由资深领域专家精心制作的 500 个复杂上下文、1899 个任务和 31607 个验证标准。CL-bench 只包含一个简单但苛刻的要求：&lt;strong&gt;解决每个任务要求模型必须从上下文中学习到模型预训练中不存在的新知识，并正确应用。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;模型需要学习的知识非常广泛。它包括新的领域知识、不熟悉的规则系统、复杂的产品工作流，甚至是必须从实验数据中推导归纳出的定律或结论。&lt;/p&gt;&lt;p&gt;所有这些知识要么是由领域专家完全新构建的，要么是取自那些不太可能出现在当前前沿模型训练数据中的小众、长尾来源。因此，模型无法通过回忆静态的参数化知识来解决任务，都要求模型从提供的上下文进行学习并应用。&lt;/p&gt;&lt;p&gt;具体来说，CL-bench 涵盖了四种广泛的现实世界上下文学习场景：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye1PKLxS0BCTJZgSCvJR4v410muIq8Micl2tRsuvvRwH6YzdIFYZ5ks2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4009259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531257" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/0d77320d-b9a0-4885-8b12-7c34d9d3c23b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; CL-bench 的上下文分类体系。&lt;/sup&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;领域知识推理&lt;/strong&gt;：&amp;nbsp;上下文提供特定的领域知识（例如 虚构的法律体系、创新的金融工具或小众专业知识）。模型需要利用这些知识来推理并解决具体问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;规则系统应用&lt;/strong&gt;：&amp;nbsp;上下文提供新定义的正式系统（例如 新的游戏机制、数学形式体系、编程语法或技术标准）。模型必须理解并应用这些规则来执行任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;程序性任务执行&lt;/strong&gt;：&amp;nbsp;上下文提供复杂的过程系统（例如 工作流、产品手册和操作指南）。模型必须理解并应用这些程序性信息来完成任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;经验发现与模拟&lt;/strong&gt;：&amp;nbsp;上下文提供复杂系统内的实验数据、观测记录或模拟环境。与前几类涉及演绎推理不同，这一类专注于归纳推理，也是最具挑战性的。模型必须从数据中发现潜在的定律或结论，并应用它们来解决任务。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGKPuYjN7w5dUnZZaiaBwyyia1nrapGBqgibKYxicw7UlMx9belAQPljF45g/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.4759259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531308" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c28d4a69-c396-464f-9318-deedcfd9ba79/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;CL-bench 示例。解决这些任务要求语言模型从提供的上下文中学习。图中这四个案例分别是：（1）在一部长达 2.3 万字、刚刚生效的新法律下判一起真实纠纷；（2）基于一门新设计的教育编程语言规范，实现一个带有时间条件终止的周期性程序；（3）在一套从未见过的编程框架中执行代码；（4）在给定技术规格和长期环境政策情景的条件下，模拟关键技术金属的可持续全球供应。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这些类别包含了大部分现实世界工作中常见的演绎推理和归纳推理任务，能充分衡量模型的上下文学习能力。关于 CL-bench 的更多细节，请参阅我们的论文 [1]。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;CL-bench 的设计原则和特性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;CL-bench 围绕一个简单但严格的设计原则构建：&lt;strong&gt;每个任务都必须要求从上下文中学习新知识&lt;/strong&gt;。 CL-bench 中的每个上下文都是&lt;strong&gt;完全自包含（Self-contained）&lt;/strong&gt;的。解决任务所需的所有信息都显式地提供在上下文本身之中：不需要外部检索，也不允许隐藏假设。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531372" data-ratio="0.4101851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGegrsicWiaqYrK5gGZuOXlHia7aETTiciaIzciarPGAfqfpKjCjyrMvJcqw1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/5da9aa61-d63c-4775-bfbc-ca6fe2f848d9/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 解决CL-bench 中的任务需要模型从相应的 context 中学习新知识。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了确保性能真正反映上下文学习，而不是记忆或数据泄露，CL-bench 采用了无污染（Contamination-free）设计：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;虚构创作&lt;/strong&gt;：&amp;nbsp;专家创作完全虚构的内容，例如为虚构国家设计一套完整的法律体系（包括新颖的判例和法律原则），或创建具有独特语法和语义的新编程语言。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;现有内容的修改&lt;/strong&gt;：&amp;nbsp;专家修改现实世界的内容以创建变体，例如更改历史事件、改变科学和数学定义，或修改技术文档和标准。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;整合小众和新兴内容&lt;/strong&gt;： 专家纳入了在预训练数据集中代表性极低的小众或近期新兴内容，如前沿研究发现、新发布的产品手册或技术文档，以及来自专门领域的特定知识。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在不提供任何上下文的情况下，最先进的模型 &lt;strong&gt;GPT-5.1 (High) &lt;/strong&gt;仅能解决&lt;strong&gt;不到 1%&lt;/strong&gt; 的任务。这有力地证明了数据是无污染的，模型若不从上下文中学习，几乎完全无法解决这些任务。&lt;/p&gt;&lt;p&gt;此外，CL-bench 的设计具有高复杂性和序列依赖性。&lt;strong&gt;51.1% 的任务&lt;/strong&gt;需要序列依赖，意味着后续任务的解决方案取决于早期交互的结果。这种多轮次设计显著增加了任务难度。平均而言，领域专家花费约 &lt;strong&gt;20 小时&lt;/strong&gt;标注每个上下文，以确保任务构建的质量和深度。&lt;/p&gt;&lt;p&gt;CL-bench 中的每个任务都是完全可验证的。平均而言，每个上下文关联&lt;strong&gt; 63.2 &lt;/strong&gt;个验证标准，每个任务包含 16.6 个评估标准。每个任务的正确性都从多个角度进行评估，确保了评估的全面性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;部分实验发现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们在 CL-bench 上评估了十个最先进的语言模型。结果揭示了清晰且一致的差距。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyePxuWssOuS3FUCeSZ7AgdMfcicDgtoicqgvO6cUqvPiaJ5CWiaRuhPoXXFw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.387037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531258" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/747f34fa-d1cd-45ce-850a-1990f2e52643/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表：十个前沿模型在 CL-bench 上的任务解决率。所有模型均在推理模式下进行评估，结果报告为三次运行的平均值 &amp;plusmn; 标准差 (%)。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;平均而言，模型仅解决了 &lt;strong&gt;17.2% &lt;/strong&gt;的任务。即便是表现最好的模型 &lt;strong&gt;GPT-5.1 (High)&lt;/strong&gt;，也仅达到了&lt;strong&gt; 23.7%&lt;/strong&gt;。换句话说，尽管上下文中拥有解决每个任务所需的全部信息，模型在绝大多数任务上都失败了。这表明&lt;strong&gt;当前的 SOTA 模型几乎不会从上下文中学习&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;还有几个额外的现象值得注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;忽略或误用上下文是导致失败的主要原因。&lt;/strong&gt; 许多错误并非源于信息缺失，而是源于模型忽视了上下文中的关键细节，或错误地应用了它们。在许多情况下，模型只会利用预训练学习到的静态知识来解决任务，即使上下文明确定义了新的规则、概念或程序，模型也不会学习和利用。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyebKvU6O4gYfibfbQ1WQmUroQr5gvhR6pjAWMJpWrVuhUicZ7icIFCUvicaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.38744075829383884" data-s="300,640" data-type="png" data-w="844" type="block" data-imgfileid="503531252" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b88fc5c2-a006-4101-bf03-499b65e09466/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表：各模型错误类型的分布（因为一个 solutions 可能有多种错误原因，所以每行错误率总和大于 100%）。&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;长上下文推理和指令遵循是必要的，但不是充分条件。&lt;/strong&gt; 案例分析表明，那些难以跨长上下文追踪依赖关系或难以精确遵循约束的模型，往往表现得更差。然而，即使是能够处理长输入并可靠遵循指令的模型，仍然在许多任务上失败。上下文学习需要的能力，远不止长上下文理解和指令遵循能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;从实验数据和环境模拟中进行归纳推理比演绎应用更困难。&lt;/strong&gt; 演绎任务让模型根据 context 中明确给出的规则和流程进行应用，而经验发现和环境模拟类任务则要求 归纳推理 &amp;mdash;&amp;mdash; 从数据中总结规律或在虚拟环境中探索。模型在这类任务上的表现明显较差，任务解决率通常低于 10%，且结果波动大。这表明发现规律远比应用规则更具挑战性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGq2sEiauf3miac6OQVuRsBj6WpeNFWk3yybWkxtsOPnwKX6QiaO953tAuQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.44814814814814813" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531370" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/6ff8d689-beca-49cd-9f6a-41d449dc786a/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; GPT-5.1 在高 / 低推理强度设置下，各子类别表现对比。&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更高的推理强度通常能提升 context 学习效果。 &lt;/strong&gt;对部分模型来说，增加 推理强度 可以改善表现，使模型更深入地理解复杂 context 。例如，GPT-5.1 在管理类和实验数据类任务上的表现提升约 6%。但其他模型提升有限甚至可能下降，说明单靠更多推理并不足够，模型还必须能够正确吸收和组织 context 信息。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGrZib1oiaeW7941ddR1jLfkSTHLsHlDqNSN2f6PxTybGnEib9Qwicu8MHXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5481481481481482" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531371" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/beccea51-148b-4432-a717-365c82f50ce2/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;不同输入长度下模型上下文学习表现的变化趋势。（不同 context 下模型的表现变化呈现相似趋势。）&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Context 学习的难度与 context 长度相关，但短 context 也可能很复杂。&lt;/strong&gt; 较长的 context 通常让所有模型的任务更难，这验证了长 context 处理仍是关键瓶颈。然而，即使是短 context ，如果包含信息密集、规则隐含、依赖复杂或约束严格的内容，也依然很具挑战性，说明 context 学习的难度不仅仅来源于长度，也来自于其复杂度。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;更多发现请参见我们的论文 [1]。综上所述，CL-bench 揭示了一个不能被忽视的现象：&lt;strong&gt;当今的前沿语言模型还仍然不会利用上下文，从上下文中学习。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;CL-bench 充分解释了语言模型在真实场景中为什么经常出错：即使有了上下文工程，给模型准备好了所需的上下文，模型也会失败。如果模型不能真正从中学习，仅仅提供上下文是不够的。上下文学习作为一项模型基础的学习能力，很大程度上被忽视了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;展望未来&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果上下文学习显著提升，人类在 AI 系统中的角色将发生变化：我们不再是主要的数据提供者（training data provider），而变成了 context 提供者（context provider）。竞争的焦点将从 &amp;ldquo;谁能把模型训练得更好&amp;rdquo;，转向 &amp;ldquo;谁能为任务提供最丰富、最相关的 context &amp;rdquo;。&lt;/p&gt;&lt;p&gt;但其实这里还有一个挑战。即便上下文学习足够强大，它目前依然是&lt;strong&gt;临时性的（Ephemeral）&lt;/strong&gt;：模型的上下文窗口一旦清空，学到的知识随之消失。因此，我们还要关注如何让从上下文中习得的知识&lt;strong&gt;持久化&lt;/strong&gt;？这种知识不仅是事实，还包括能帮助模型跨任务迁移的技能、经验和模式等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeaYmkNKNZeffvicB68xX6CMRxb9O4qeGwOPCW1MjicQU1wCqGp0VzJx2g/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.5583333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531253" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/c9b09d0e-7bb9-4e22-865f-1c8b6b31b1ac/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 记忆巩固是语言模型通过上下文学习经验的关键&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;因此，如何记忆很可能成为 2026 年的另一个核心主题。 要充分发挥语言模型的潜力，可能需要新的架构、新的优化方式来决定「该保留什么」。&lt;/p&gt;&lt;p&gt;一旦上下文学习与记忆变得可靠，模型或许就能实现自主学习：它们将自主准备上下文，从中学习并自我巩固。&lt;/p&gt;&lt;p&gt;这听上去多么令人兴奋！但当下我们的目标很明确：&lt;strong&gt;让「上下文学习」真正走向现实！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;写于 2026 年 1 月，正值新年来临之际。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;[1] CL-bench: A Benchmark for Context Learning&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>致敬Kimi K2：基于slime的全流程INT4量化感知RL训练</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 18:54:12 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/d1756c60-302a-42b1-88e5-83ca358ffdb4/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;受 Kimi K2 团队启发，SGLang RL 团队成功落地了 INT4 量化感知训练（QAT） 流程方案。通过 &amp;ldquo;训练端伪量化 + 推理端真实量化（W4A16）&amp;rdquo; 的方案组合，我们实现了媲美 BF16 全精度训练的稳定性与训推一致性，同时 INT4 极致压缩也将 1TB 级超大模型的采样任务容纳于单机 H200 (141G) 显存内，消除了跨机通信瓶颈，显著提高了 Rollout 效率，为社区提供了兼顾高性能与低成本的开源参考。&lt;/p&gt;&lt;p&gt;近期，SGLang RL 团队在强化学习的训练稳定性，训练效率与适用场景方面取得了重要进展，具体包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Unified multi-turn VLM/LLM 多轮采样范式：我们提供了 VLM 多轮采样范式的实现&lt;span data-mpa-action-id="ml4n7xgd20kt" data-pm-slice="0 0 []"&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fzhaochenyang20%2FAwesome-ML-SYS-Tutorial%2Fblob%2Fmain%2Frlhf%2Fslime%2Fvlm-multi-turn%2Freadme.md" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4muh7b-2pezt0" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;blog&lt;/a&gt;&lt;/span&gt;，开发者只需编写一套定制化的 rollout 函数，即可像训练 LLM 一样，轻松开启 VLM 的多轮强化学习。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;稳定性提升：我们实现了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FTHUDM%2Fslime%2Fblob%2Fmain%2Fdocs%2Fzh%2Fadvanced%2Fslime-router.md%2322-rollout-routing-replay-r3-for-moe" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4mx7pv-r18f3o" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Rollout Router Replay&lt;/a&gt; 机制，显著提升了 MoE 模型在 RL 训练过程中的稳定性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;低精度训练：我们在 RL 场景中成功实现了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Flmsys.org%2Fblog%2F2025-11-25-fp8-rl" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4mz7bu-gq35el" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;全流程 FP8 训练与采样&lt;/a&gt; ，进一步释放了硬件性能。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;投机采样&lt;/strong&gt;：我们在 RL 场景中成功实践了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fthudm.github.io%2Fslime%2Fadvanced%2Fspeculative-decoding.html" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n0v6w-u09i45" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;投机采样&lt;/a&gt;，实现了大规模训练的无损加速。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此基础上，我们更进一步，在 slime 框架上成功复现并落地了&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FTHUDM%2Fslime%2Fblob%2F58525eb986c66a271aa31077e17b8afebe704b4f%2Fscripts%2Flow_precision%2Frun-kimi-k2-Thinking-int4.sh" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n1wmw-1zr50z" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;INT4 量化感知训练（QAT）&lt;/a&gt;全流程方案。该方案深受 Kimi 团队 K2-Thinking 技术报告中关于 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F1969558404759544488%2Fanswer%2F1970539327902679960" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n31mf-ib1f6c" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;W4A16 QAT (Quantization-Aware Training)&lt;/a&gt;实践的启发。为了致敬先行者并回馈社区，本文将详细剖析我们在开源生态中打通全流程的技术细节，旨在为社区提供一份兼顾稳定性与性能的可落地参考。&lt;/p&gt;&lt;p&gt;核心收益概览：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;突破显存瓶颈&lt;/strong&gt;：通过权重压缩与低比特量化，使 1TB 级别的 K2 类模型能缩容至单机 H200 (141G) 显存内，避免了跨机通信瓶颈。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训推一致&lt;/strong&gt;：训练端利用 QAT 确保权重符合 INT4 分布，推理端执行 W4A16 (Weights INT4, activations BF16 ) 计算；二者均通过 BF16 Tensor Core 进行运算，实现了媲美 BF16 全精度的训推一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;单机效率倍增&lt;/strong&gt;：在超大模型场景下，INT4 策略大幅降低了显存与带宽压力，Rollout 效率显著超越 W8A8 (Weights FP8 , Activations FP8）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本项目由 SGLang RL 团队、 InfiXAI 团队、蚂蚁集团 Asystem &amp;amp; 阿福 Infra 团队， slime 团队与 RadixArk Miles 团队联合完成。相关功能与 recipe 已经同步到了&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fradixark%2Fmiles" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4ilayi-sc764c" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;slime&lt;/a&gt;&lt;/span&gt;与&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fradixark%2Fmiles" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4izwmd-3raw7y" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Miles&lt;/a&gt;&lt;/span&gt; 社区，欢迎大家试用与贡献。我们也在更进一步向 MXFP8 与 NVFP4 发起挑战。同时，由衷感谢&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fverda-cloud%2F" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n422r-lu83cm" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Verda Cloud&lt;/a&gt;为本工作提供的计算资源。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4j1is2qxn" data-pm-slice="0 0 []"&gt;&lt;strong&gt;1. 技术方案概览&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.1 总体流程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们实现了从训练到推理的完整 QAT INT4 闭环的方案，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeBhHLD0Rk4SlcF7zgmLUlFvOcXVDdHaKAJGOkdRQu30hA4ibrgLZN8rA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3287037037037037" data-type="png" data-w="1080" data-width="2256" data-height="742" data-imgfileid="503531142" data-aistatus="1" data-original-style="background-color: transparent;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3d5f74e4-6b19-4629-9288-bb9ece8ff3f3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4j33xz13up" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图1 QAT INT4 全流程&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在 &lt;strong&gt;QAT 训练阶段&lt;/strong&gt;，训练侧在维护 BF16 主权重（Master Weights）的基础上，前向传播通过&lt;strong&gt;伪量化（Fake Quantization）&lt;/strong&gt; 引入量化噪声。所谓 &amp;ldquo;伪&amp;rdquo;，是指该步骤并未真正将 BF16 数据类型转换为低精度的 INT4，而是保持浮点计算路径不变，通过插入 &lt;strong&gt;量化再反量化（Quant-Dequant）&lt;/strong&gt; 操作来模拟低精度的计算。&lt;/p&gt;&lt;p&gt;具体而言，高精度权重在经过 &amp;ldquo;离散化映射到 INT4&amp;rdquo; 后被立即还原，虽然其物理存储格式仍为浮点，但数值精度已实质性降低。这种原值与还原值之间的差异引入了量化误差，在数学上等效于向网络注入了噪声，迫使模型在训练阶段就通过梯度更新去适应这种精度损失。&lt;/p&gt;&lt;p&gt;反向传播则利用&lt;strong&gt; STE (Straight-Through Estimator) &lt;/strong&gt;技术跳过了量化算子的不可导特性。量化过程的核心操作是 &amp;ldquo;取整（Rounding）&amp;rdquo;，其数学形态为阶梯函数，导数在几乎所有位置均为 0。这意味着在标准反向传播过程中，梯度信号传导至此处会因 &amp;ldquo;&lt;strong&gt;梯度消失&lt;/strong&gt;&amp;rdquo; 而彻底中断，导致底层的主权重无法获得更新。&lt;/p&gt;&lt;p&gt;对此，STE 采用了 &amp;ldquo;梯度透传&amp;rdquo; 策略：在反向传播计算时，将取整函数的导数定义为 1（即视为恒等映射）。这一机制相当于在不可导的 &amp;ldquo;断崖&amp;rdquo; 上架设了一座桥梁，让梯度能够越过取整层，有效回传至高精度的浮点权重，确保 QAT 训练链路的闭环。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;权重转换阶段&lt;/strong&gt;，我们将训练收敛的 BF16 权重导出并执行真实量化（Real Quantization），将其转换为推理引擎适配的 INT4 格式（如 Marlin）。&lt;/p&gt;&lt;p&gt;进入 &lt;strong&gt;RL Rollout&lt;/strong&gt; 阶段，由 SGLang 加载 INT4 &amp;nbsp;Weights 并执行高效的 W4A16（INT4 权重 x BF16 激活）推理，生成的经验数据（Experience）将回流至第一阶段用于下一轮 RL 训练，从而构成一个自洽的迭代闭环。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.2 核心策略选择&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在量化格式上，我们参考 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fhuggingface.co%2Fmoonshotai%2FKimi-K2-Thinking" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4jabq0-h6clqx" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Kimi-K2-Thinking&lt;/a&gt; 选用了 &lt;strong&gt;INT4 (W4A16) &lt;/strong&gt;方案。这主要考虑到相比 FP4，INT4 在现有硬件（Pre-Blackwell 架构）上的支持更加广泛，并且业界已有成熟高效的 Marlin Kernel 实现。实验表明，在 1&amp;times;32 量化 Scale 粒度下，INT4 动态范围充足、精度稳定，其性能与生态链路均已高度优化。作为工业界 &amp;ldquo;足够好（Good Enough）&amp;rdquo; 的量化标准，INT4 在性能、风险与维护成本间实现理性平衡。当然，我们后续也计划在 NVIDIA Blackwell 系列硬件上进一步展开 FP4 RL 的探索。&lt;/p&gt;&lt;p&gt;在训练方法方面，我们采用了 &lt;strong&gt;Fake Quantization 配合 STE &lt;/strong&gt;的经典组合。通过维护 BF16 主权重，在前向计算中模拟量化噪声，并在反向传播时直通梯度，这种方式最大程度地保证了低精度训练的收敛性与稳定性。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jch4m19vb" data-pm-slice="0 0 []"&gt;&lt;strong&gt;2. 训练侧：Megatron-LM 的伪量化改造&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.1 &amp;nbsp;Fake Quantization 与 STE 实现&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyexwVVZGxzHFv9TVg4kiaNsR6O9RlZxb46ekChDhuo3x5NdCo9mnaiaAdA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6083333333333333" data-type="png" data-w="1080" data-width="2672" data-height="1626" data-imgfileid="503531144" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e6bd039c-3d4d-4661-a721-138e5c549363/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图2&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这一阶段的核心目标是在训练过程中实时模拟量化误差，迫使模型 &amp;ldquo;学会&amp;rdquo; 适应低精度表示。为此，我们采用了 &lt;strong&gt;Fake Quantization&lt;/strong&gt; 机制：尽管权重在存储和更新时仍保持高精度的 BF16 格式，但在前向传播的实际计算中，会被暂时映射到 INT4 的精度范围参与运算。&lt;/p&gt;&lt;p&gt;具体实现上，我们在 megatron/core/extensions/transformer_engine.py 中的 _FakeInt4QuantizationSTE 类构建了核心逻辑。基于分组最大绝对值进行动态量化（Dynamic Quantization），模拟 INT4 的 [-7, 7] 数值范围及截断操作，但在计算时仍使用 BF16 类型，仅引入量化误差。&lt;/p&gt;&lt;p&gt;而在关键的反向传播环节，我们引入了 &lt;strong&gt;STE &lt;/strong&gt;机制，确保梯度能够直接穿透量化层，不经修改地回传以更新主权重，从而保证训练的连续性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.2 Fake Quantization 对比实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了验证 QAT 方案的必要性，并探究训练与推理精度不匹配带来的具体影响，我们设计了一组消融实验，分别在 &amp;ldquo;&lt;strong&gt;开启 QAT INT4 训练，BF16 Rollout&lt;/strong&gt;&amp;rdquo; 和 &amp;ldquo;&lt;strong&gt;关闭 QAT 训练，直接进行 INT4 Rollout&lt;/strong&gt;&amp;rdquo; 两种非对称场景下进行了测试，并以对数概率绝对差值（Logprob Abs Diff）作为训推不一致的观测指标。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeE3LBfic8GhNlpTibJ1Htpzx25qoXUrhlzADsYtNJg9S2onhx0rxrCo1g/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531146" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/2aaf4c72-4e81-4f4f-adf5-30fd1cab718b/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4jjip1s9h" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图3 Rollout 侧 BF16，训练侧对比 QAT INT4 效果&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图3展示了 &amp;ldquo;开启 QAT INT4 训练，BF16 Rollout&amp;rdquo; 的场景&lt;/strong&gt;（即&lt;strong&gt;红线&lt;/strong&gt;部分）。可以看到，即使我们使用了高精度的 BF16 进行推理，误差依然显著偏高。这是因为在 QAT 过程中，模型权重已经针对 INT4 的量化噪声进行了 &amp;ldquo;适应性调整&amp;rdquo; 或补偿；推理时若移除量化步骤，这种补偿反而成为扰动，导致特性&lt;strong&gt;分布偏移（Distribution Shift）&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="2 2 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeqOfyWdnodEibsdAPydBM18dzLxEuO8GKe0fLZ3WoNoNiaicibwtAVl9zUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531145" data-aistatus="1" data-original-style="background-color: transparent;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/30dcbb70-cf24-478b-b32b-be2eda69d722/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4jk24cr6r" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图4 Rollout 侧 INT4 Weight Only，训练侧对比 QAT INT4 效果&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图4则展示了 &amp;ldquo;关闭 QAT 训练，直接进行 INT4 Rollout&amp;rdquo; 的场景&lt;/strong&gt;（即&lt;strong&gt;红线&lt;/strong&gt;部分）。这对应了传统的训练后量化（PTQ）模式。由于模型在训练阶段从未接触过量化噪声，直接将权重压缩至 INT4 不仅造成信息的剧烈丢失，更导致推理时的特征分布与训练时产生&lt;strong&gt;偏移&lt;/strong&gt;，致使误差随着训练步数呈现震荡上升的趋势。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：实验有力地证明，&lt;strong&gt;训练端的 Fake Quantization 与推理端的 Real Quantization 必须协同开启&lt;/strong&gt;。只有当训练时的模拟噪声与推理时的真实量化精度&lt;strong&gt;严格对齐&lt;/strong&gt;，才能有效抑制训推不一致，避免分布偏移，将误差控制在接近基线的水平，从而真正打通低精度 RL 训练的全流程。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jpfbl2le" data-pm-slice="0 0 []"&gt;&lt;strong&gt;3. 权重更新阶段&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.1 权重流转与动态格式适配&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeIZK8qWcb1xWbjrVDrticBBlFrSsrn73GNbibg0eqCkYV5v3ZyQP05asw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.31574074074074077" data-type="png" data-w="1080" data-width="3068" data-height="968" data-imgfileid="503531149" data-aistatus="1" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3fb2d364-a5f6-47d5-9df2-076217cdfc7a/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图5&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;为了复用 SGLang 在推理端已有的优化，我们直接采用了其内置的 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FIST-DASLab%2Fmarlin" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n57zz-h6x18j" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Marlin INT4&lt;/a&gt; 作为 INT4 的推理方案。然而，这在工程落地时我们遇到了显著的 &amp;ldquo;格式鸿沟&amp;rdquo;：QAT 训练产出的是类似 Hugging face 上的标准格式权重，而 SGLang 推理引擎的 Marlin Kernel 则强制要求权重必须经过特定的打包（Pack）与重排（Permute）处理，方能被 Kernel 高效读取。&lt;/p&gt;&lt;p&gt;面对 RL 训练中频繁的权重更新需求，首先需要解决格式兼容性问题。为此，我们设计了一套逆向的 `restore_weights_before_loading` &lt;strong&gt;保护机制&lt;/strong&gt;。该机制利用缓存的 `_original_shapes` 元数据，能够在权重更新动作发生前，强制将当前内存中的 Marlin 权重格式还原（Resize）回原始形状。这一设计有效防止了因维度不匹配导致的运行时错误，确保模型能够在标准权重格式与 Marlin 权重格式之间平滑切换。此外，我们还在系统层面新增了 `post_process_weights` API，允许控制平面根据训练节奏显式触发这一流程。&lt;/p&gt;&lt;p&gt;而针对权重加载完成后的格式适配挑战，我们在 `compressed_tensors_moe.py` 中实现了一套&lt;strong&gt;动态权重管理机制&lt;/strong&gt;。在模型权重加载结束阶段，系统会自动触发 `process_weights_after_loading` 流程，底层调用 `gptq_marlin_moe_repack` 与 `marlin_moe_permute_scales` 等算子，在内存中即时将标准权重转换为高度优化的 Marlin 权重格式，从而最大化推理时的访存与计算效率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.2 权重更新时的量化&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye3iazKfVkgaSKYXQBa8aiaBiaRU1HjSeN7l2UrwYiaezd4n55XozonkUCBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.10277777777777777" data-type="png" data-w="1080" data-width="3080" data-height="316" data-imgfileid="503531150" data-aistatus="1" data-original-style="background-color: transparent;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/933af1cd-c958-4107-bebd-a67814009001/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图6&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;进入核心的&lt;strong&gt; Real Quantization &lt;/strong&gt;环节。不同于训练时的 Fake Quantization，这一步通过代码中的 `int4_block_quantize` 函数执行不可逆的精度压缩操作：基于设定的 Group Size，计算每组权重的缩放因子（Scale），并将高精度浮点数映射到 `[-7, 7]` 的 INT4 整数域。&lt;/p&gt;&lt;p&gt;为了最大化显存利用率，接着执行 &lt;strong&gt;位宽打包（Packing）&lt;/strong&gt; 操作。由于 PyTorch 缺乏原生的 INT4 数据类型，我们通过 `pack_int4_to_int32` 函数利用位运算技巧，将 8 个 INT4 数值紧凑地 &amp;ldquo;压缩&amp;rdquo; 进 1 个 INT32 整数中（即 `8 &amp;times; 4 bits = 32 bits`）。最终，这些经过压缩的 Packed Weights 连同 Scale 因子被传输至推理引擎，完成了从 &amp;ldquo;训练格式&amp;rdquo; 到 &amp;ldquo;推理格式&amp;rdquo; 的转换。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4js46z10zu" data-pm-slice="0 0 []"&gt;&lt;strong&gt;4. 推理阶段&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeXYxQMBDq5MDSK0icJGuz14Mzm9ZtkZPA7APLqibS5Aod7JIC43EP88jg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.21296296296296297" data-type="png" data-w="1080" data-width="3078" data-height="656" data-imgfileid="503531151" data-aistatus="1" data-original-style="background-color: transparent;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/2c68dc09-1507-45dc-8996-a0cba9d2ff2d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图7&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;极简打包与零开销解包&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 RL 训练的 Rollout 阶段，我们直接复用了 SGLang 优化成熟的 W4A16 量化方案。SGLang 使用紧凑的 INT4 格式，将两个 4-bit 权重打包进一个字节，相比 BF16 节省了 75% 的内存。在推理时，Triton kernel 通过高效的位移和掩码操作（&amp;gt;&amp;gt; 4 和 &amp;amp; 0xF）快速解包，得益于计算与 IO 的并行覆盖，该过程几乎实现了零额外延迟。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MoE 算子深度融合&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;显存优化&lt;/strong&gt;：SGLang 引入动态的 moe_align_block_size，根据当前 Token 数量和 Expert 分布自动选择 block_size ，将同一 Expert 的 Token 聚集并对齐，提升显存带宽利用率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;计算融合&lt;/strong&gt;：SGLang 引擎除集成了高效的 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FIST-DASLab%2Fmarlin" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4jwvk7-izb4u3" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Marlin INT4&lt;/a&gt; 实现、还将 gating 部分 fuse 成一个高性能的 kernel，避免了反复启动 kernel 和读写中间结果。同时，该 INT4 推理方案兼容 GPTQ 和 AWQ 等主流量化格式，以及支持对称与非对称两种模式。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jtp21kkw" data-pm-slice="0 0 []"&gt;&lt;strong&gt;5. INT4 QAT RL 效果&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5.1 训练效果&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练侧&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeqTib4E0c7JibNBwofOZgeGXpfbN46cWJJHFbg9zldkS7WMfM6ygeuskA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531153" data-aistatus="1" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/f5b74b22-21b8-4a51-8094-b5c388e95936/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图8 Qwen3-235B-A22B Raw-Reward对比&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye1kl4dQgNF04naMX93RLKWBuPSibqzdrpJQ1iafe7doRrUEhrLgB21eJg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531152" data-aistatus="1" data-original-style="background-color: transparent;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/199a4263-02e3-4f07-9f5a-a5447bb98b46/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图9 Kimi-K2-Thinking Raw-Reward对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;上图展示了基于 slime 框架，Qwen3-235B-A22B 与 Kimi-K2-Thinking 模型在 dapo-math-17k 数据集上的训练表现。通过对比实验发现，相较于 &amp;ldquo;&lt;strong&gt;BF16 训 - BF16 推&lt;/strong&gt;&amp;rdquo; 及 &amp;ldquo;&lt;strong&gt;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo;，&amp;ldquo;&lt;strong&gt;BF16 训 - INT4 推&lt;/strong&gt;&amp;rdquo; 配置下的 Raw-Reward 仍能保持稳健增长，且其增长趋势与前两者基本一致，证明了该方案在训练过程中的有效性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;评估侧&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeIcicbobice5yfqmKsfXzIMsFiaMOUdanCBooFqb4SnprTZe4icQtktlccQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531157" data-aistatus="1" data-original-style="background-color: transparent;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/879e58dd-b4f0-4535-9b82-eb02b8e1b2ec/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图10 Qwen3-235B-A22B AIME数据集评估对比&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeme4ow83SQ19HRLHT0xFeKKGzwHgictO0vCcb5Cx42t3bI6gyH8FdA5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531156" data-aistatus="1" data-original-style="background-color: transparent;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/9a0c5ae5-0bad-4421-a99e-324088187457/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图11 Kimi-K2-Thinking AIME数据集评估对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了更加严谨地评估模型能力的演进，我们每隔 10 个训练步长就在 aime-2024 基准测试集上进行一次评估。上图给出了 Qwen3-235B-A22B 与 Kimi-K2-Thinking 在不同 RL 训练配置下的模型评分增长轨迹。&lt;/p&gt;&lt;p&gt;实验表明：&amp;ldquo;&lt;strong&gt;BF16 训 - INT4 推&lt;/strong&gt;&amp;rdquo; 方案不仅在评估分数上呈现出稳健的上升态势，且其性能提升的斜率与最终达到的峰值，均与 &lt;strong&gt;&amp;ldquo;BF16 训 - BF16 推&lt;/strong&gt;&amp;rdquo; 和 &lt;strong&gt;&amp;ldquo;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo; 方案保持了较高的重合度。这种高度的一致性有力地证明了模型在经过低比特量化后，其核心表示能力并未受损，保证了在大幅降低计算开销的同时，依然能够实现与全精度推理相媲美甚至完全看齐的泛化表现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5.2 训推差异&lt;/strong&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyer3hUV6arauxEYHCRhcgqsm3oRGd5ExJn2P559BTzBfGo3aficCBVM6g/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531159" data-aistatus="1" data-original-style="background-color: transparent;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/29edaee1-96bb-4ee7-ad33-21bd639b1c0a/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图12&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye8DSVA3kfCUjnPt6h1PTtkvPibl5MqjexHFmLr5BtUov415Gw6ysKAqA/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531158" data-aistatus="1" data-original-style="background-color: transparent;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/972ddd69-e290-4bf6-9870-d65cc19d8e25/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;图13&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了直观评估方案效果，我们在 Qwen3-30B 与 Qwen3-235B 模型上进行了的 QAT RL 训练验证。图中 Y 轴反映了训练侧与推理侧输出的 Logprob 绝对差值，数值越低意味一致性越强。实验结果显示，INT4&lt;strong&gt;（绿色虚线）&lt;/strong&gt;与 BF16 基准&lt;strong&gt;（红色实线）&lt;/strong&gt;呈现出惊人的重合度，且显著低于表现出较高误差水平的 FP8&lt;strong&gt;（蓝色虚线）&lt;/strong&gt;。这证实了 INT4 QAT 策略能有效规避 &amp;ldquo;&lt;strong&gt;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo; 模式下的精度损失，实现与全精度无异的训推表现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这种一致性背后的原因我们推测为两点：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;截断误差抑制&lt;/strong&gt;：训练侧的 Fake Quantization 将权重限制在 INT4 值域内。这种数值范围的约束，&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F1969558404759544488%2Fanswer%2F1970539327902679960" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n69wn-y5aasg" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;有效降低了矩阵乘法中 Accumulator 累加时因并行计算顺序不确定性引发的浮点舍入误差（Floating-point Rounding Error），即改善了所谓的&amp;ldquo;大数加小数&amp;rdquo;精度丢失问题。&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;高精度计算&lt;/strong&gt;：推理侧采用 W4A16 模式，其核心计算全程基于 &lt;strong&gt;BF16 Tensor Core&lt;/strong&gt; 进行，确保了运算精度与训练阶段的高度对齐。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4keaemhwb" data-pm-slice="0 0 []"&gt;&lt;strong&gt;5.3 Rollout 加速&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyer6Z0vALv0IRkNXCgkKXEsPLSsDd7Znr4LU8PmAFoRicNjTmxPplU56w/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531160" data-aistatus="1" data-original-style="background-color: transparent;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/9c7493bf-febe-4a02-bff5-5a0372de5053/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图14 Qwen3-235B-A22B Rollout 性能对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;从 Qwen3-235B 的 Rollout 性能对比图中可以直观看到，虽然 INT4&lt;strong&gt;（绿色点划线）&lt;/strong&gt;与 FP8&lt;strong&gt;（蓝色虚线）&lt;/strong&gt;均较 BF16 基线&lt;strong&gt;（红色实线）&lt;/strong&gt;实现了显著加速，但两者彼此之间并未拉开巨大的性能鸿沟。这一现象主要受限于当前的硬件特性：由于 NVIDIA H 系列 GPU 没有原生的 INT4 Tensor Core， W4A16 方案本质上利用的还是 BF16 Tensor Core 进行计算，虽然大幅降低了显存带宽压力，但在吞吐上无法像 W8A8 一样利用原生 FP8 Tensor Core 进行加速从而获得计算增益。因此，在单步推理耗时上，INT4 仅表现出微弱的优势，与 FP8 基本处于同一性能梯队。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyezmxjpLdfKvAiatib2rq58UaL5KZvOo07VhvCsUc9ia6TbPFn6Srmica8xg/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531162" data-aistatus="1" data-original-style="background-color: transparent;" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/69dfbafd-90a3-4952-8272-5f654d17f40c/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图15 Kimi-K2-Thinking Rollout 性能对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;对于 Kimi-K2-Thinking Rollout 性能的对比。首先观察双节点场景下的&lt;strong&gt;通信瓶颈&lt;/strong&gt;：图中 FP8&lt;strong&gt;（红线）&lt;/strong&gt;与 INT4&lt;strong&gt;（蓝线）&lt;/strong&gt;呈现出相似的水平。因为 H 系列 GPU 缺乏原生的 INT4 计算单元，INT4 无法在计算层面提供加速，因此整体性能依然受限于跨节点的通信带宽。&lt;/p&gt;&lt;p&gt;然而，&lt;strong&gt;绿线&lt;/strong&gt;所代表的单节点表现揭示了 INT4 的&lt;strong&gt;真正价值 &amp;mdash;&amp;mdash; 显存压缩&lt;/strong&gt;。通过将模型体积减半，我们成功将 1TB 级别的超大模型完整加载至单机显存中。这直接消除了昂贵的跨机通信开销，将 Rollout 耗时大幅缩减。这有力地证明，在当前硬件环境下，INT4 QAT 的核心收益在于通过压缩显存，解锁了高效的单机部署 Rollout 方案。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4k3fyfsqh" data-pm-slice="0 0 []"&gt;&lt;strong&gt;6. 总结与未来工作&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;slime 的这项工作不仅证明了在开源生态中复现工业界前沿方案的可行性，也为超大规模模型的低成本训练探索了新的路径。我们期望这套方案助力更多开发者深入理解 QAT 技术，并推动其在 RL 场景下的实际落地与广泛应用。&lt;/p&gt;&lt;p&gt;通过在开源框架上的复现，我们验证了 Kimi 团队所提出的 INT4 QAT 方案的有效性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;精度复现&lt;/strong&gt;：在 slime 的复现实验中，我们同样观察到了 INT4 QAT 的精度优势，实现了与 BF16 基线一致的效果。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;效率提升&lt;/strong&gt;：RL Rollout 阶段的吞吐提升显著，验证了低比特量化在 RL 场景下的巨大价值。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;未来工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练端效率优化：目前，由于在训练过程中引入了 QAT Fake Quantization 计算，带来了较大的额外性能开销，导致训练速度明显低于 BF16 模式。这在一定程度上折损了 Rollout 阶段带来的端到端性能收益。我们后续计划提出一套全新的优化方案，旨在解决这一训练侧的效率瓶颈，实现全链路的加速。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;推理侧 FP4&lt;/strong&gt;： 随着 NVIDIA Blackwell 架构的逐步普及，我们将积极探索 FP4 精度在 RL 训练与推理中的应用可行性，以期进一步挖掘硬件潜力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;slime 在 QAT INT4 的尝试不仅证明了在开源生态中复现工业界前沿方案的可行性，也为超大规模模型的低成本训练探索了新的路径。我们期望这套方案助力更多开发者深入理解 QAT 技术，并推动其在 RL 场景下的实际落地与广泛应用。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4k7c6d3pm" data-pm-slice="0 0 []"&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;SGLang RL Team: Ji Li, Yefei Chen, Xi Chen, BBuf&lt;/p&gt;&lt;p&gt;InfiXAI Team: Mingfa Feng, Congkai Xie, Shuo Cai&lt;/p&gt;&lt;p&gt;蚂蚁集团 Asystem &amp;amp; 阿福 Infra 团队：Yanan Gao, Zhiling Ye, Yuan Wang, Xingliang Shi&lt;/p&gt;&lt;p&gt;RadixArk Miles Team: Chenyang Zhao, Yueming Yuan, Jiajun Li, Yusheng Su, Mao Cheng, Tom, Banghua Zhu&lt;/p&gt;&lt;p&gt;slime Team: Zilin Zhu, Chengxing Xie, Lei Li, Haisha Zhao&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>将高质量内容融入AI生态，威立以科研智能塑造出版行业未来</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Tue, 03 Feb 2026 14:26:57 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;近期，威立执行副总裁兼总经理&lt;strong&gt;Jay Flynn&lt;/strong&gt;，威立高级副总裁兼学术出版全球负责人&lt;strong&gt;Liz Ferguson&lt;/strong&gt;及威立高级副总裁兼首席营销官&lt;strong&gt;Anna Reeves&lt;/strong&gt;到访中国市场，在威立北京办公室与威立全球副总裁兼中国区总裁张莫依女士深度对话，探讨了威立在科研出版及学术期刊方面的最新发展战略，与中国科研人员、机构和企业的合作方针，以及如何在AI时代向权威学术内容与科研信息服务合作伙伴转型。&lt;/p&gt;&lt;p&gt;&amp;ldquo;中国市场蕴藏着巨大机遇，且机遇远大于挑战。作为 40 多年前首批进入中国的国际出版机构之一，我们与中国各方都建立了深厚而持久的合作关系。现今，我们致力于利用人工智能技术和开放获取出版模式，不仅满足&amp;mdash;&amp;mdash;更要预判并超越合作伙伴不断变化的需求。&amp;rdquo;Jay 表示，&amp;ldquo;主动引领变革、持续创新并优化我们的策略，是威立与合作伙伴携手塑造未来的核心。&amp;rdquo;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 3%;"&gt;&lt;/strong&gt;作为全球领先的出版机构，威立如何看待人工智能目前对科研领域的变革作用？&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" target="_blank" rel="noopener noreferrer"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/c9e9ad12-c6b2-46fd-8f71-fead204939e8/1770099644235.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;strong&gt;以合作为基石，将高质量内容融入AI生态&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;2025年10月，威立推出业内首个&lt;strong&gt;人工智能网关（Wiley AI Gateway）&lt;/strong&gt;。不同于要求科研人员采用专有工具的封闭生态系统，该网关格外注重系统间的协同操作性，将学术内容与数据订阅服务无缝集成至当前主流人工智能平台。目前，Anthropic的Claude、AWS Marketplace、Mistral AI的Le Chat以及Perplexity均已实现与该网关的对接。&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;实际上，这一网关的打造初衷，正是帮助合作伙伴的内容在 AI 时代实现&amp;ldquo;数字化跃迁&amp;rdquo;，使其能够安全可控可信地融入 AI 生态。&lt;/p&gt;&lt;p&gt;&amp;ldquo;通过该网关，我们正将学术与专业内容转化为人工智能优化格式，同时保留引文完整性、研究方法背景，以及同行评审验证&amp;mdash;&amp;mdash;这些都是用户期待威立能够实现的核心价值。&amp;rdquo;Jay 表示，&amp;ldquo;我们正在打造一个全行业的解决方案，助力人工智能驱动型研究。未来，还会有更多国际出版机构参与其中。&amp;rdquo;&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.65%;"&gt;为满足科研与教育界在人工智能方面的特定需求，威立还采取了哪些措施？&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" target="_blank" rel="noopener noreferrer"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/41a51471-1394-4137-ac23-7847969159b0/1770099692579.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;倾听科研人员需求，助力负责任地使用AI&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;毫无疑问，人工智能正深刻变革行业格局与生态系统。&amp;ldquo;我们要确保在这一过程中充分关注科研人员的需求，并持续优化他们的使用体验。&amp;rdquo;Anna 表示。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;全球调研报告ExplanAItions&lt;/strong&gt;的基础上，威立于近期发布了科研人员人工智能使用现状报告&lt;strong&gt;ExplanAItions 2025&lt;/strong&gt;。&amp;ldquo;人工智能在科研人员日常工作中的使用率大幅增长。目前，全球 84% 的科研人员表示正在工作中使用人工智能工具。&amp;rdquo;Anna介绍道，&amp;ldquo;尽管科研人员对人工智能的应用前景持积极态度，他们也同时意识到需要负责任地使用这些工具。&amp;rdquo;&lt;/p&gt;&lt;p&gt;因此，威立在刚刚过去的11月推出了面向作者、编辑及审稿人的&lt;strong&gt;&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MjM5NTAxNzIzMg==&amp;mid=2650166838&amp;idx=2&amp;sn=0b8b17cf68d8a9e8697d4c923dc2582e&amp;scene=21#wechat_redirect" target="_blank"&gt;全新AI指南&lt;/a&gt;&lt;/strong&gt;，旨在为各个学科领域及工作流程中的学者提供支持，解决实际痛点问题。&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkuzw35n1l5k" data-pm-slice="0 0 []"&gt;&lt;strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;&lt;/strong&gt;威立品牌升级方案如何体现其在中国市场的未来方向？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ba254db6-d9c8-4d7c-a655-c66f6f0963af/1770099731558.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;持续扩展旗舰期刊系列，强化与中国伙伴合作&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&amp;ldquo;中国科研人员在国际期刊上发表了大量高质量研究成果&amp;mdash;&amp;mdash;无论是科研成果的数量，亦或影响力都令人瞩目。&amp;rdquo;作为威立学术出版全球负责人，Liz 介绍了威立在中国的合作及发展现状，&amp;ldquo;2025年，威立期刊发表的内容中约 25% 来自中国。我们同时与中国战略合作伙伴们共同推出了多种高影响力期刊。&amp;rdquo;&lt;/p&gt;&lt;p&gt;她同时介绍了威立在推动期刊发展方面的创新策略，&amp;ldquo;通过推进开放获取、优化同行评审流程，以及整合创新技术来更好地服务中国及全球科研人员。&amp;rdquo;&lt;/p&gt;&lt;p&gt;目前，威立正重点推进旗舰期刊&lt;em&gt;&lt;strong&gt;Advanced Science&lt;/strong&gt;&lt;/em&gt;从物质科学领域向生命与健康科学领域扩展，而该期刊目前发表的生命与健康科学领域的高质量研究成果数量已与物质科学领域持平。未来，威立还将在其他发展迅速且高影响力的科研领域创办期刊，包括肿瘤学、生物医学工程、生物技术、环境科学和数字健康等，同时继续深化威立在物质科学领域的优势地位。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;&lt;span data-mpa-action-id="mkw38emttw5" data-pm-slice="0 0 []"&gt;威立的最新期刊发展战略&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8c68aad9-5a97-450a-8fda-de8158959ca9/1770099775460.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkwe2sgyqma" data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;当威立从传统出版机构向权威学术内容与科研信息服务合作伙伴转型时，我们在中国市场的承诺与计划是什么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkwe2sgyqma" data-pm-slice="0 0 []"&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/73d1f4f3-1179-4198-a2de-096f71d62341/1770099790278.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>国产版Ollama来了，Clawdbot终于不只属于Mac和英伟达</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 11:49:24 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜+0&lt;/section&gt;&lt;p data-path-to-node="4" data-pm-slice="0 0 []"&gt;这几天，AI 圈的头号 C 位莫过于这只「龙虾」：&lt;strong&gt;Clawdbot&lt;/strong&gt;（现在得叫它 OpenClaw 了），它几乎把一群开发者折腾得彻夜难眠。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="1068" data-imgfileid="503531273" data-ratio="1.1986531986531987" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGTibSSVotZicsZhnHiaqw86b1gRTvWsZ3TZkpJrxLxPPAxevjQrke9XwGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="891" data-width="891" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1ea417bc-650c-48d6-99f8-08799be9ec64/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="5"&gt;为什么它这么火？因为和以前那些只会陪聊的 Chatbot 不同，Clawdbot 是个真正的「实干派」：它能接管你的电脑，在你睡觉时通宵写代码、修 Bug，甚至背着主人手搓出一套语音功能。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;更魔幻的是，随之诞生的 AI 社交平台 Moltbook 彻底刷屏了。在这个「AI 版 Reddit」上，150 万个 Agent 正通过自创语言和共谋进化，建立起背离人类掌控的独立机器社会与文化。&lt;/p&gt;&lt;p data-path-to-node="7"&gt;这听起来很酷，但随之而来的是「隐私的裸奔」与&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-path-to-node":"7","style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「钱包的哀嚎」&lt;/span&gt;。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;当 Clawdbot 这样的 Agent 全面读取你的屏幕、扫描你的文件，并在后台疯狂消耗昂贵的 API 额度时，很多开发者早就开始思考一个问题：Agent 虽好，难道我们以后的一举一动都要通过云端计费吗？&lt;/p&gt;&lt;p data-path-to-node="9"&gt;这催生了另一个巨大的需求：&lt;strong&gt;Local Agent（本地智能体）&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="10"&gt;但在这一波浪潮中，算力并不是唯一的门槛。以 Clawdbot 为例，当前社区主流方案主要围绕 macOS 与 NVIDIA GPU 生态展开，这与 Ollama、llama.cpp 以及相关 Agent 工具链的成熟度密切相关。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;相比之下，尽管华为昇腾、燧原等国产算力已经具备运行大模型的能力，但在通用 Agent 工具链与社区生态适配方面仍存在明显差距，这使得部分开发者难以直接参与到当前主流的 Agent 实验与应用中。&lt;/p&gt;&lt;p data-path-to-node="12"&gt;难道手握国产算力的开发者，只能眼巴巴看着这场狂欢吗？当然不是。&lt;/p&gt;&lt;p data-path-to-node="13"&gt;国产显卡其实从来不缺「肌肉」，缺的只是一把趁手的「兵器」。如果说 Clawdbot 解决了「AI 怎么干活」的问题，那么我们今天要聊的这个工具，就是来解决「AI 在哪干活」的问题。&lt;/p&gt;&lt;p data-path-to-node="14"&gt;&lt;strong&gt;2 月 2 日，清昴智能发布玄武 CLI 开源版本。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG2705pslwGfGCrZB0hD2S9vHSfvKpXzV34VZRWzMI2FnxMjNISCua1g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="0.5953703703703703" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503531275" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e1a73514-1ee8-4b77-ae18-e3d2474de33f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="15"&gt;你可以把它简单理解为「国产版 Ollama」，它旨在抹平硬件架构的差异，让基于国产卡的大模型部署进入「&lt;strong&gt;零门槛时代&lt;/strong&gt;」。不需要复杂的环境配置，&lt;strong&gt;5 分钟启动模型服务&lt;/strong&gt;，这不仅是企业降低部署成本的利器，更是每一位开发者激活手边国产算力的钥匙。&lt;/p&gt;&lt;p&gt;玄武 CLI 开源传送门：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;玄武 CLI GitHub 仓库：https://github.com/TsingmaoAI/xw-cli&lt;/li&gt;&lt;li&gt;玄武 CLI Gitcode 仓库：https://gitcode.com/tsingmao/xw-cli&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="18"&gt;别急着下单 Mac mini，你机箱里的「国货之光」其实早就准备好了。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG0iaZQNft3fO4V8SBVBouRB0oOwuttLiaMWVvno4rs7aI11qolHWlgeFQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.098148148148148" data-type="png" data-w="1080" data-width="1166" data-height="1280" data-imgfileid="503531274" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/5d972558-97d0-46c7-8d3b-d75c6fdac167/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="20"&gt;&lt;strong&gt;开发者到底在和什么战斗？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="21"&gt;进入 2026 年，随着 DeepSeek、Kimi 等高性能开源模型的成熟，AI 推理形态正在从以云端为中心，逐步向本地与边缘侧扩展。出于对数据隐私（金融代码、医疗数据）和低延迟 Agent 交互的需求，&lt;strong&gt;本地化推理&lt;/strong&gt;正在成为清晰可见的趋势。&lt;/p&gt;&lt;p data-path-to-node="22"&gt;在 NVIDIA 和 Apple Metal 生态中，Ollama 凭借「一个二进制文件、一行命令」的极致体验，成为最具代表性的本地推理工具之一。然而，这种统一而简洁的使用方式，并未真正惠及中国主流国产算力用户。&lt;/p&gt;&lt;p data-path-to-node="23"&gt;尽管国产芯片在硬件指标上已具备相当竞争力，但在软件生态层面仍存在明显断层：工具链割裂、算子覆盖不足、社区适配滞后，正让开发者陷入一种新的焦虑：&lt;strong&gt;算力在手，却用不起来&lt;/strong&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一张卡，一套世界观&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="25"&gt;与 CUDA 近乎统一的格局不同，国产芯片架构呈现出「百花齐放却互不相通」的态势。华为的 CANN、摩尔线程的 MUSA，以及各家自成体系的工具链彼此独立。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;对开发者而言，每更换一张卡，几乎意味着重新学习一套构建系统。由于上游社区难以维护如此多且杂的后端分支，国产卡用户往往只能依赖功能滞后、稳定性不足的非官方适配版本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从入门到放弃的「配置长征」&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="27"&gt;想在国产卡上跑通一个高性能模型？往往是一场耐心与运气的双重考验：&lt;/p&gt;&lt;p data-path-to-node="27"&gt;驱动、固件、Toolkit、算子包必须严格对齐，错一个版本号就报错；少配一个环境变量，程序就可能当场崩溃；即使使用 Docker，也无法像 NVIDIA 那样 &lt;code&gt;--gpus all&lt;/code&gt; 一键搞定，而是要手动透传多个复杂设备节点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新模型「水土不服」&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="29"&gt;更具挑战的是，新一代模型架构（如 MoE、FP8 量化）在国产环境中往往缺乏成熟的高性能算子支持，，容易触发非最优执行路径，导致推理性能大幅下降。当遭遇模糊错误码时，开发者往往无从查证。&lt;/p&gt;&lt;p data-path-to-node="29"&gt;这就是行业的真实切面：开发者想要的是「5 分钟启动服务」，现实给的却是「5 天还在配环境」。&lt;strong&gt;行业迫切需要一个能够抹平底层硬件差异、统一上层使用体验的中间层工具。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="31"&gt;&lt;strong&gt;玄武 CLI：国产算力的 Ollama 来了&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="32"&gt;如果说 Ollama 的成功来自「让 GPU 消失在用户视野中」，那么玄武 CLI 的目标则是「让国产 GPU 的差异性也消失」。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;它关注的重点并不是单纯「能否运行模型」，而是如何在复杂的国产芯片生态中，提供一种更统一、更稳定的部署与调用体验。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGo0dcPENRL8gEqWAxtj39rpXEDSNj220eoJUXoU5SO3kaQ5gqoEw1rQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.3291796469366564" data-type="png" data-w="963" data-width="963" data-height="1280" data-imgfileid="503531276" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a15feb86-939a-4c7a-91ec-753d762d4c4e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 玄武CLI的架构图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="33"&gt;&lt;strong&gt;国产原生适配：一键搞定，告别配置噩梦&lt;/strong&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;在国产算力生态中，最大的痛点来自芯片架构的高度碎片化。不同厂商、不同型号，对应不同驱动、不同推理引擎与参数组合，部署往往意味着反复查文档、改配置、踩坑调试。&lt;/p&gt;&lt;p data-path-to-node="34"&gt;玄武 CLI 的核心价值之一，就是把复杂性收敛到系统内部：它能够自动识别华为昇腾全系列、沐曦、燧原等多款国产芯片&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;对用户而言，不再需要理解底层架构差异，也无需手动调参调环境，真正实现「零调试部署」，从根本上降低国产芯片的使用门槛。&lt;/p&gt;&lt;p data-path-to-node="12" data-pm-slice="0 0 []"&gt;&lt;strong&gt;零门槛上手：1 分钟部署，无缝兼容无压力&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="13"&gt;在使用体验上，玄武 CLI 走的是与 Ollama 同一条路线：极简、快速、低学习成本。用户无需安装 Python 或复杂依赖，只要基础驱动就绪，解压即可运行，最快 1 分钟启动服务。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;服务启动&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="15"&gt;一切始于一行简洁的命令 &lt;code data-index-in-node="12" data-path-to-node="15"&gt;xw serve&lt;/code&gt;。无需复杂的环境变量配置，系统直接完成运行时配置初始化与全局端口分配，唤醒后台守护进程。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8c417f46-e6ca-4b03-a7fa-945e1a4e7ac0/1770090278305.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型交互&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="17"&gt;模型运行同样丝滑。通过 &lt;code data-index-in-node="12" data-path-to-node="17"&gt;xw run&lt;/code&gt; 命令，系统能直接检测实例状态。若模型已就绪，即可秒级进入 Chat 会话模式，直接开始问答交互。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0cd43644-f2dc-42a1-8e5f-eef3db58a775/1770090294379.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型下载&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="19"&gt;对于本地未获取的模型，告别繁琐的权重文件手动搬运与路径映射。通过 &lt;code data-index-in-node="33" data-path-to-node="19"&gt;xw pull&lt;/code&gt;，自动完成模型权重与配置文件的拉取，提供清晰的进度验证。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/465c39c0-88a5-43a6-b2f3-297db0364bd4/1770090306835.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="19"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG9Cn4kibZP18wNTKZFMCAmjKwvSvKHb1JiaEQY6ybSw8M6wAic1Cibn95ag/640?wx_fmt=jpeg#imgIndex=5" data-ratio="0.36666666666666664" data-type="jpeg" data-w="1080" data-width="3840" data-height="2088" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGCuvUthSCAL2wqo5cNO2CXzJ8WwGc607sNudGJXOWEnfy3n07XgHhLw/0?wx_fmt=jpeg&amp;from=appmsg" data-cropx2="1920" data-cropy2="704.2214532871973" data-backw="289" data-backh="106" data-imgfileid="503531301" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a0c113ad-2dfb-4a66-8109-8d7ac214eda7/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-path-to-node="20"&gt;玄武 CLI 目前已原生支持包括 &lt;strong&gt;DeepSeek、Qwen3、GLM-4.7、MiniMax 2.1&lt;/strong&gt; 等在内的数十款主流模型，并在今天已完成 GLM-OCR 的 Day0 适配，覆盖从端侧轻量级到千亿参数旗舰级模型。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;实例启动&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="22"&gt;得益于底层的极致优化，在执行 &lt;code data-index-in-node="15" data-path-to-node="22"&gt;xw start&lt;/code&gt; 启动实例时，系统能够自动调配 vLLM 等高性能后端。&lt;strong&gt;实测数据表明：即便是 32b 规模的模型，玄武 CLI 也能在 30 秒内完成启动。&lt;/strong&gt;这个时间内，系统会自动完成模型切分、显存加载，并成功启动推理引擎。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3081a90c-5c0b-40dd-8227-000d7e059cda/1770090391917.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="23"&gt;同时，&lt;strong&gt;玄武 CLI 在命令层面与 Ollama 高度一致&lt;/strong&gt;（如 &lt;code data-index-in-node="31" data-path-to-node="23"&gt;xw pull&lt;/code&gt; / &lt;code data-index-in-node="41" data-path-to-node="23"&gt;run&lt;/code&gt; / &lt;code data-index-in-node="47" data-path-to-node="23"&gt;ls&lt;/code&gt; / &lt;code data-index-in-node="52" data-path-to-node="23"&gt;stop&lt;/code&gt;），意味着会用 Ollama 就能直接上手玄武，几乎没有迁移成本。在应用层，它兼容 OpenAI API 接口，LangChain、LlamaIndex 以及各类 IDE 插件只需改一行 API 地址即可接入，无需重构原有应用栈。&lt;/p&gt;&lt;p data-path-to-node="24"&gt;在稳定性设计上，玄武 CLI 采用独立子进程架构，即使单个模型或任务出现异常，也不会影响整体服务，既适合个人开发者的轻量使用，也满足企业级稳定运行需求。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;&lt;strong&gt;高性能与全保障并行：多引擎覆盖，风险提前规避&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="26"&gt;玄武 CLI 内置自研的清昴核心推理引擎 MLGuider，在性能层面提供稳定保障，同时支持多种推理引擎并行兼容。这种设计一方面可以覆盖更广、更新的模型版本，另一方面也避免对单一引擎的过度依赖，从工程角度提前规避风险。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGeogmxpWlNyYXoCIrYN7fF5WAkehYc0y7LA0mQ5X2Ura8xS34ZnWv0A/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-ratio="0.37720111214087115" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503531305" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9bddccf8-5f3a-4317-8962-56c527d50fbb/640.gif" data-order="0" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="27"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 推理服务流程图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="28"&gt;多引擎并存，本质上是对兼容性与性能的双重极致优化。玄武 CLI 通过智能调度内置的 MLGuider 等引擎，能够深入芯片底层进行算子级调优，最大限度释放国产硬件算力。这种既保高性能推理、又顾模型多样性的策略，真正解决「国产卡能用但不好用」的核心问题。&lt;/p&gt;&lt;p data-path-to-node="29"&gt;同时，玄武 CLI 支持完全离线运行，不依赖云端服务，在国产芯片上即可完成模型管理与推理任务，适合对数据安全和稳定性要求较高的场景。&lt;/p&gt;&lt;p data-path-to-node="30"&gt;&lt;strong&gt;热门产品联动：拓展本地 AI 应用场景&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="31"&gt;在应用生态层面，玄武 CLI 并不只是一个「模型启动器」，而是一个本地 AI 能力的底座。它可以与 Clawdbot 等热门本地 AI 工具联动，为这些产品提供低门槛的模型部署与调用能力，使自动化任务与智能应用更容易落地。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d52862b3-7470-4e44-8f8b-bb2bff552a76/1770090427058.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="32"&gt;这种联动模式意味着，开发者不必重复解决模型部署问题，而可以把更多精力放在上层应用与业务逻辑上，从而放大本地 AI 工具的整体价值。&lt;/p&gt;&lt;p data-path-to-node="33"&gt;&lt;strong&gt;为什么是他们？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="34"&gt;玄武 CLI 的强大，源自其背后深厚的技术积淀。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;清昴智能是一家专注于&lt;strong&gt;芯片适配和模型-框架-算子联合调优&lt;/strong&gt;的全面领先 AI Infra 企业。创始团队来自清华大学计算机系，汇聚了来自斯坦福、新国立、爱丁堡大学以及华为、阿里、AMD 等全球顶尖机构的 AI 精英。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;创始人关超宇小学到大学 2 次跳级，15 岁进入本科，21 岁获得清华大学特奖、西贝尔学者等一系列殊荣，22 岁放弃华为天才少年、阿里星等大厂 offer，选择携手导师朱文武教授和前华为英雄个人和极客开发荣誉获得者姚航联合创业。他们不仅懂软件，更懂底层的芯片微架构以及如何攻克国产软件生态难题。&lt;/p&gt;&lt;p data-path-to-node="37"&gt;成立 3 年，即获得华为哈勃的战略注资，以及多家国内一线基金的上亿元财务投资。这不仅证明了其技术价值，更意味着其与国产芯片厂商有着深度的原厂级合作关系，能够第一时间获取底层驱动支持。&lt;/p&gt;&lt;p data-path-to-node="38"&gt;清昴智能并未止步于 CLI 工具。以自研的异构推理引擎 &lt;strong&gt;MLGuider &lt;/strong&gt;为核心，公司构建了从底层芯片到上层框架以及 Agentic AI 的全栈能力，致力于构建 AI 2.0 时代软件基础设施，为企业智能化转型和 AGI 实现打造坚实底座。&lt;/p&gt;&lt;p data-path-to-node="39"&gt;玄武 CLI 正是这一庞大技术愿景在开发者侧的「尖刀」产品，旨在通过极致的易用性打开市场缺口，构建生态护城河。&lt;/p&gt;&lt;p data-path-to-node="40"&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="41"&gt;技术，终究是要为人服务的。&lt;/p&gt;&lt;p data-path-to-node="42"&gt;过去几年，国产显卡用户面对的并非性能问题，而是生态问题：驱动、框架、工具链之间的割裂，使大量潜在算力长期处于「不可用状态」。&lt;/p&gt;&lt;p data-path-to-node="43"&gt;玄武 CLI 的出现，或许不能立刻让国产生态「拳打英伟达，脚踢苹果」，但它至少做到了一件事：把梯子递到了墙边。&lt;/p&gt;&lt;p data-path-to-node="44"&gt;它让开发者不必再充当「环境配置员」，而能重新回到创造本身；也让那些躺在机箱里吃灰的国产显卡，重新开始发热、计算，参与到真实的 AI 实践之中。&lt;/p&gt;&lt;p data-path-to-node="45"&gt;想要一起推动生态进步？赶快到 GitHub 给它一个 Star 吧！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="17,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="17,0,0"&gt;玄武 CLI GitHub 仓库：&lt;/b&gt;https://github.com/TsingmaoAI/xw-cli&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="17,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="17,1,0"&gt;玄武 CLI Gitcode 仓库：&lt;/b&gt;https://gitcode.com/tsingmao/xw-cli&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
