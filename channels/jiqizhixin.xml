<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-b906e030b985e6914990f04d1a50f7b99e514db6f9a9f5c6799dd321730a7549.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>技术深耕与生态共建｜SGLang 上海 Meetup顺利举行</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 10:16:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在当前人工智能从&amp;ldquo;聊天&amp;rdquo;范式加速向&amp;ldquo;能办事&amp;rdquo;的智能体时代演进的关键节点，LLM 系统优化与技术落地的实践探索，更需要开发者们的深度联结与经验共创。基于此，由 SGLang 社区、机器之心、张江孵化器联合举办的「SGLang 上海 Meetup」于2月6日在浦东&amp;middot;纳贤路 800 号 1 层顺利举行。&lt;img src="https://image.jiqizhixin.com/uploads/editor/f5e00e14-ef66-4241-8396-7583eac65a24/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;本场活动特邀 SGLang 核心开发成员张柏舟，Omni-infer 核心开发者郑锦焕，清华大学博士生、Slime核心开发者谢承兴，SGLang 核心开发者、Mooncake 核心开发者蔡尚铭，蚂蚁集团系统工程师、SGLang Contributor 李泽寰五位嘉宾，围绕「LLM 系统优化与落地实践的新可能」这一主题，让贡献者走到台前、优化者分享心法，为与会者呈现了一场兼具技术深度与工程实践价值的技术盛宴，并为 SGLang 开源生态的蓬勃发展持续助力。&lt;img src="https://image.jiqizhixin.com/uploads/editor/bfceb537-3352-4759-9a96-b139dd38fec7/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;张柏舟：SGLang 核心开发成员&lt;/p&gt;&lt;p&gt;SGLang 核心开发成员张柏舟在《SGLang Roadmap》分享中，系统回顾了 SGLang 开源推理框架从大规模部署到强化学习集成的演进历程，重点展示了 DeepSeek、GPT-OSS 等主流模型的 Day-0 支持能力。展望 2026 年，他披露了 PD 分离、投机解码、并行策略重构等技术路线，强调 SGLang 将持续深化与产业伙伴协同，打造高性能、高兼容性的开源推理基础设施。&lt;img src="https://image.jiqizhixin.com/uploads/editor/ff01136f-ec7f-48f9-9d5b-2a9c211c3aae/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;郑锦焕：Omni-infer 核心开发者&lt;/p&gt;&lt;p&gt;Omni-infer 核心开发者郑锦焕带来《Omni-infer 对 SGLang 的性能优化实践》主题分享，深度剖析Omni-infer的集成架构与性能调优策略，重磅介绍了Omni-Ai V1新版本的核心升级亮点，为开发者提供更高效的AI开发与部署工具。他提出基于最早完成时间的均衡调度算法，有效降低排队时延；通过并行 KV Cache 传输，显著减少传输开销并配合异步调度提升kv cache复用效率，构建全链路可视化方案，结合NPU硬件特征开展针对性优化。最终在 DeepSeek v3.1 实测中，系统 QPM 从 356 提升至 460，充分验证了系列优化的显著成效。此外，郑锦焕也同步公布了Omni-Ai V1的代码仓链接：https://gitee.com/omniai/omniinfer，方便开发者快速获取、部署与二次开发。&lt;img src="https://image.jiqizhixin.com/uploads/editor/853d41de-1f0b-4ed7-9567-7bf534771bbf/%E5%9B%BE%E7%89%874.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;谢承兴：清华大学博士生、Slime 核心开发者&lt;/p&gt;&lt;p&gt;清华大学博士生、slime 核心开发者谢承兴以《slime：面向 RL Scaling 的 LLM 后训练框架》为题，分享了由智谱开源的后训练框架 slime。针对 Agentic RL 时代多轮交互、长上下文等复杂应用场景，他系统介绍了 slime 的 Server-Based Rollout 架构与解耦式 rollout 函数设计，有效降低了用户的使用门槛。同时，框架通过引入 Importance Sampling、True On-Policy 对齐等机制，缓解并降低了训练过程中的不稳定性。目前，slime 已成功支撑 GLM 系列模型的后训练，并也支持 DeepSeek R1、Kimi k2 等大规模 MoE 模型的强化学习训练。&lt;img src="https://image.jiqizhixin.com/uploads/editor/aaa3cf70-39aa-464b-b855-eb7ca1dd7d72/%E5%9B%BE%E7%89%875.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;蔡尚铭：SGLang 核心开发者、Mooncake 核心开发者&lt;/p&gt;&lt;p&gt;SGLang 核心开发者、Mooncake 核心开发者蔡尚铭在《SGLang CPP：面向超长上下文的 Scaling out 黑科技》中，深入解析了 SGLang 针对超长上下文推理场景所设计的高性能 Chunked Pipeline Parallelism（CPP）实现。在原有PP架构的基础上，SGLang通过引入异步P2P通信与动态分块预填充两大核心技术，显著降低了流水线气泡，同时兼容PD分离与HiCache，为万亿参数模型提供了高效的多节点横向扩展方案。实测显示，在 H20 集群上部署 DeepSeek-V3.1模型，新架构在扩展至 PP4 TP8 时，预填充吞吐量相比 TP8 提升至 3.31 倍，TTFT 降低 67.9%，性能显著优于原有实现与TP32扩展方案。&lt;img src="https://image.jiqizhixin.com/uploads/editor/3f313b62-0559-4c68-a925-a0f6d39fee97/%E5%9B%BE%E7%89%876.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;李泽寰：蚂蚁集团系统工程师、SGLang Contributor&lt;/p&gt;&lt;p&gt;蚂蚁集团系统工程师、SGLang Contributor 李泽寰带来《从自回归到扩散，SGLang diffusion LLM 的探索与实践》，分享了扩散语言模型在 SGLang 中的工程实践。他对比了三种解码范式，指出 Block Diffusion 兼具任意长度输出与并行解码优势。通过将 dLLM 嵌入 SGLang 框架，实现 LLaDA2.0-flash 等扩散语言模型的高效推理，大幅降低评测与 RL 后训练耗时，并成功支撑起 dLLM 的生产级服务部署。&lt;/p&gt;&lt;p&gt;本次 Meetup 在热烈的自由交流中圆满落幕。从框架内核到部署优化，从训练范式到硬件适配，五位嘉宾的分享勾勒出 SGLang 生态的技术全景。这些真知灼见不仅为社区演进提供了宝贵参考，更为 LLM 系统优化领域的开发者注入了新的灵感与动力。未来，SGLang 社区将持续推动开源协作与技术创新，期待与更多开发者携手，共同探索大模型时代的无限可能。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ProjDevBench：AI编程智能体真的能从零构建完整软件项目吗？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 10:08:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;上海交通大学、上海创智学院与加州大学默塞德分校联合发布ProjDevBench——首个通过OJ细粒度反馈评估AI编程智能体端到端项目开发能力的基准测试，要求智能体仅凭自然语言需求文档，从零开始构建完整、可运行的软件仓库。&lt;/p&gt;&lt;p&gt;结果令人深思：所有智能体总体提交AC率仅27.38%，当任务从"补全现有代码"变为"从零构建"时，性能出现断崖式下跌。&lt;/p&gt;&lt;p&gt;•论文链接：https://arxiv.org/abs/2602.01655&lt;/p&gt;&lt;p&gt;•项目链接：https://github.com/zsworld6/projdevbench&lt;/p&gt;&lt;p&gt;【结果总结】&lt;/p&gt;&lt;p&gt;•六种主流编程智能体（Cursor、GitHub Copilot、Claude Code等）的总体提交AC率仅为27.38%，在从零构建任务中性能大幅下滑。&lt;/p&gt;&lt;p&gt;•OJ提供的细粒度诊断反馈（编译错误（CE）、运行时错误（RE）、超时（TLE）、内存超限（MLE）、答案错误（WA）等）是评估端到端开发能力的关键组成部分，远优于传统的pass/fail二元判定。&lt;/p&gt;&lt;p&gt;•交互轮次与性能呈强负相关（-0.734），智能体在遇到困难时陷入低效试错循环，而非通过反思实现突破。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、为什么需要端到端项目开发基准&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现有基准测试如HumanEval、MBPP聚焦于函数级代码生成，SWE-bench关注issue修复，但真实软件工程需要的远不止这些。当开发者使用Cursor或GitHub Copilot进行"vibe coding"时，他们期望智能体能够：从零设计系统架构、创建和组织多个源文件、配置依赖和构建系统（如CMakeLists.txt）、最终交付一个可编译运行的完整项目。&lt;/p&gt;&lt;p&gt;这种端到端的项目构建能力此前从未被系统性评估过。ProjDevBench填补了这一空白。&lt;/p&gt;&lt;p&gt;与传统基准的本质区别在于：HumanEval等要求智能体补全代码片段，SWE-bench要求修复现有代码库中的bug，而ProjDevBench要求智能体像真正的软件工程师一样，在没有任何初始代码模板的情况下，自主完成从架构设计到多文件编码的全流程。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ba2585c6-404d-4917-aed9-2aa2db76c26e/1770275491867.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、双重评估机制：OJ测试 + 代码审查&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与以往仅返回pass/fail的测试不同，ProjDevBench采用双轨制评估：&lt;/p&gt;&lt;p&gt;OJ执行评分（80%）：通过在线判题系统进行严格的黑盒测试，提供细粒度诊断信号——编译错误（CE）、运行时错误（RE）、超时（TLE）、内存超限（MLE）、答案错误（WA）等。这些信号支持智能体进行迭代调试，模拟真实开发中"编写代码-遇到报错-修改代码"的循环。&lt;/p&gt;&lt;p&gt;代码审查评分（20%）：结合规则脚本和LLM模拟的代码审查，检测OJ测试无法捕捉的问题：是否违反显式规则（如使用禁止的库）、是否存在作弊解法、是否利用测试套件漏洞而非遵循实际约束。&lt;/p&gt;&lt;p&gt;这种设计的核心洞察是：仅靠测试用例无法全面评估代码质量。一个能通过所有测试的解法，可能采用了投机取巧的方式，而非真正理解并遵循问题规范。如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e5e59bdb-e566-4ef8-a0ae-df3d71b2f0f1/1770275502360.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、任务设计与数据来源&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队从上海交通大学ACM班（https://acm.sjtu.edu.cn/home）的在线判题平台精选20道高难度编程项目，涵盖算法、数据结构、解释器、管理系统、存储组件等8大类别。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f35f090f-7c20-4794-a739-29e770a42f90/1770275509382.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;这些题目经过三阶段筛选：&lt;/p&gt;&lt;p&gt;初始收集：从约2,800道候选题目中筛选 范围过滤：保留需要多文件实现、模块组织、构建配置的项目级任务，排除纯算法单文件题目，剩余约100道 质量过滤：选取规范清晰、测试套件完善、难度非平凡的题目，最终保留20道&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/939f8aad-b500-4c3b-861f-fdbe72eee39c/1770275516876.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;两种任务模式：&lt;/p&gt;&lt;p&gt;•Easy模式（有代码库）：提供部分代码，要求补全项目&lt;/p&gt;&lt;p&gt;•Hard模式（无代码库）：仅提供自然语言规范，要求从零构建&lt;/p&gt;&lt;p&gt;人类参考解法平均包含约10个源文件，智能体平均需要138轮工具调用、消耗4.81M tokens才能完成一道题目，最复杂的任务需要超过两小时。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、实验结果解读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队评估了六种主流编程智能体：Cursor、GitHub Copilot、Claude Code、Augment、Codex CLI、Gemini CLI，搭配GPT-5、Claude Sonnet 4.5、Gemini 3 Pro等前沿模型。&lt;/p&gt;&lt;p&gt;整体表现：Codex + GPT-5取得最高综合得分77.85，但所有智能体的总体提交AC率仅为27.38%。&lt;/p&gt;&lt;p&gt;从零构建时性能断崖式下跌：这是最关键的发现。当任务从Easy（有代码库）变为Hard（无代码库）时，大多数配置出现显著性能下降。例如：&lt;/p&gt;&lt;p&gt;•GitHub Copilot + Sonnet-4.5：71.10 → 36.63&lt;/p&gt;&lt;p&gt;•Gemini CLI + Gemini-3-Pro：74.57 → 35.53&lt;/p&gt;&lt;p&gt;•Codex + Sonnet-4.5：66.07 → 31.88&lt;/p&gt;&lt;p&gt;这表明当前智能体擅长在现有代码基础上进行修补，但缺乏从零开始进行宏观架构设计的能力。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0cc45da4-5fad-455a-8afd-b98119ffc5d4/1770275526362.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、失败模式深度分析&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队对所有提交进行了系统性分析，揭示了智能体的核心短板：&lt;/p&gt;&lt;p&gt;提交状态分布：&lt;/p&gt;&lt;p&gt;•Accepted：27.38%&lt;/p&gt;&lt;p&gt;•Wrong Answer：41.86%&lt;/p&gt;&lt;p&gt;•Time Limit Exceeded：13.91%&lt;/p&gt;&lt;p&gt;•Runtime Error：7.01%&lt;/p&gt;&lt;p&gt;•Compile Error：4.52%&lt;/p&gt;&lt;p&gt;•Memory Leak：3.51%&lt;/p&gt;&lt;p&gt;规范理解偏差：智能体经常生成语法正确但遗漏关键业务逻辑的框架代码。在火车票管理系统任务中，所有智能体都实现了用户管理和列车查询，却遗漏了座位管理系统。在扫雷任务中，智能体访问了3,789个安全格子中的3,825个，表明实现不完整而非逻辑错误。&lt;/p&gt;&lt;p&gt;边界情况处理薄弱：大量运行时错误源于空指针解引用、数组越界等问题。在map实现中，红黑树的旋转函数缺乏空指针检查；在Bookstore任务中，所有智能体都未能通过隐藏测试点，暴露了对空字符串、文件I/O异常、嵌套场景的处理不足。&lt;/p&gt;&lt;p&gt;时间复杂度分析缺失：在ICPC管理系统任务中，智能体在每次解冻操作后重新排序所有队伍，得到O(K×N log N)的解法，而正确做法是利用排名变化的局部性实现O(K log N)。智能体倾向于使用熟悉但次优的模式，而非分析问题特性进行针对性优化。&lt;/p&gt;&lt;p&gt;资源管理局限：在BASIC解释器任务中，当std::stoi()抛出异常时，已分配的表达式对象未被释放，导致内存泄漏。智能体处理显式错误路径，却忽略正常操作中可能出现的异常。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;六、交互长度与性能的负相关&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队发现了一个反直觉的现象：智能体的交互轮次越多、消耗的token越多，最终得分往往越低。&lt;/p&gt;&lt;p&gt;•Tokens与得分的相关系数：-0.734&lt;/p&gt;&lt;p&gt;•交互轮次与得分的相关系数：-0.668&lt;/p&gt;&lt;p&gt;•交互轮次与token消耗的相关系数：0.898&lt;/p&gt;&lt;p&gt;这意味着当智能体遇到困难时，它们往往陷入低效的"尝试-报错-再尝试"死循环，无法像人类专家那样通过深度思考找到更优解。增加的token主要来自重复的交互轮次，而非少量但深入的长推理步骤。&lt;/p&gt;&lt;p&gt;静态代码复杂度（文件数量、修改行数）与性能的相关性较弱，表明任务难度主要体现在延长的交互和降低的性能上，而非直接由代码规模决定。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;七、代码审查的独特价值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除执行结果外，代码审查揭示了智能体在软件开发工作流理解上的盲点：&lt;/p&gt;&lt;p&gt;版本控制误解：智能体经常在本地修改代码并创建commit，却未push到远程仓库，导致提交不完整。这表明智能体隐式假设"写代码=完成任务"，忽略了进度必须通过版本控制显式记录和提交的要求。&lt;/p&gt;&lt;p&gt;规范遵从失败：构建系统配置错误、生成错误名称的可执行文件、使用禁止的标准库头文件、遗漏必需文件、修改受保护的模板。这些问题揭示了智能体将规范要求视为次要于功能正确性的倾向。&lt;/p&gt;&lt;p&gt;这些发现表明，智能体尚未将软件开发理解为一个结构化的工作流程，而仅仅是代码生成任务。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;八、总结与意义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ProjDevBench首次证实了当前AI编程智能体在处理真实、复杂的端到端软件开发任务时仍处于初级阶段。它们擅长局部代码修补，但在全局架构设计、时间复杂度优化、资源管理及复杂逻辑推理上尚未达到可用标准。&lt;/p&gt;&lt;p&gt;学术贡献：&lt;/p&gt;&lt;p&gt;•提出首个端到端提供细粒度反馈的项目开发基准，要求智能体从零构建完整可运行的软件仓库&lt;/p&gt;&lt;p&gt;•建立结合OJ细粒度反馈与LLM代码审查的双重评估协议&lt;/p&gt;&lt;p&gt;•系统性揭示智能体在规范对齐、边界处理、复杂度优化、资源管理等方面的失败模式&lt;/p&gt;&lt;p&gt;实际意义：&lt;/p&gt;&lt;p&gt;•为评估和改进下一代自主软件开发智能体提供了更贴近真实工程场景的标准&lt;/p&gt;&lt;p&gt;•明确了从"代码补全工具"到"软件工程师"的能力鸿沟&lt;/p&gt;&lt;p&gt;•指出了未来研究方向：如何让智能体在交互中更有效地利用反馈信号，从单纯的"试错"转向真正的"推理"&lt;/p&gt;&lt;p&gt;局限性：目前基准仅包含20道任务，主要集中于C++语言，尚未涵盖其他编程语言或人机交互式开发场景。未来将扩展任务规模、引入更多语言和任务类型。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>这个春节，AI 不聊天了，开始替我买单</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:59:31 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-path-to-node="5" data-pm-slice="0 0 []"&gt;大家发现了吗？这个马年春节，一场甚至比春运还要拥挤的「AI 春节大战」早已硝烟弥漫。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;腾讯元宝打响了第一枪，豪掷 10 亿现金红包，还推出了「元宝派」这种 AI 社交新玩法，试图重现 11 年前的微信红包时刻；百度文心一言也没闲着，拿出了 5 亿红包，还联合了北京台春晚；字节跳动则更是直接把火山引擎搬到了央视春晚的后台&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p data-path-to-node="7"&gt;一时间，微信群里满屏都是抢红包链接。大厂们都在焦虑：在「应用为王」的阶段，谁能在这个春节多吸引用户目光，谁就有可能成为下一个互联网超级入口。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;但就在这铺天盖地的「聊天红包」和「社交裂变」中，阿里旗下的千问 APP 突然杀进场，直接把筹码推到了 30 亿，也打响了一场与众不同的「现象级」AI 生活方式的全民演习。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;2 月 2 日，千问 APP 宣布启动 30 亿「&lt;strong&gt;春节请客计划&lt;/strong&gt;」。这不仅仅是发红包，而是&lt;strong&gt;阿里调动了淘宝、飞猪、大麦、高德等整个生态圈&lt;/strong&gt;，直接请全国人民「吃喝玩乐」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFicerkiaWASFdNicGxiakr7CXprH1GEicsicYK6mTbFuyE16923HEDpDlEQDeu2TSAVqSwmfxJ9zWrfYIY5WTgvQzPZ3r4eZTTNyZdY/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.7510204081632652" data-s="300,640" data-type="png" data-w="735" type="block" data-imgfileid="503532526" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ac90ff6f-9eeb-44c9-a608-7e86cca832ca/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="10"&gt;2 月 6 日活动首日，战果显赫。当天活动开始 9 小时，通过千问 APP 完成的 AI &lt;strong&gt;订单量已迅速突破 1000 万单&lt;/strong&gt;。同时后台收到了超过 3000 万次「帮我买」的指令。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;下面这张图想必能让大家感受到这波千问的实力：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHK82vDOtiakOJuFGenib669GgicyabrCOWNVMic0smhV7qh2icuMgjFmmGzQCkJ9mD60T36L8VczOwjqox9LS2tqpMFwmsxeGKRVEU/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5814814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532527" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/56f4b6f1-13b0-476a-a590-7b0535b00277/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="13"&gt;这种爆炸式的热情甚至一度让服务器陷入拥堵，千问官方也发文「求放过」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEkmI3RgibFW49g2t8uW7xH1EeVLJJquAVDDRsicXCib72vhh4JvprY9tWqNc6nOu6ib3dcTjRPOW8ibasicsCqPeicfTCDNmoLKWiaJ4I/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.8215297450424929" data-s="300,640" data-type="png" data-w="706" type="block" data-imgfileid="503532528" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/7e41c8b1-0b19-4a26-8b30-90424c3972b4/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="15"&gt;这千万级订单的背后，不仅是 30 亿真金白银的投入，也是在测试一种新的消费习惯。千问用一张张「免单卡」，让用户在交互中完成了「找千问」的习惯养成。&lt;/p&gt;&lt;p data-path-to-node="16"&gt;面对这场突如其来的「AI 购物潮」，千问团队在紧急扩容的同时也给出了定心丸：活动将持续至 2 月 28 日，且正在加速接入全国盒马门店和天猫超市。这意味着，免单的范围将从奶茶电影票，一路延伸到年夜饭桌上的澳洲龙虾和车厘子。&lt;/p&gt;&lt;p data-path-to-node="17"&gt;而更关键的是，千问的能力不只局限在「买东西」上。&lt;/p&gt;&lt;p data-path-to-node="18"&gt;比如在春节最令人头疼的出行规划场景中，千问展示了惊人的跨应用调度能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_gif/5L8bhP5dIqFmBeMgFVtYyldwceaOTwTJZ2wYAdzbuE6fSq4OiauBOtGyvYKSEGFLVjDxHVjQusJsFrrWSyVgQOVgyw0dcSOhZ87cYm1pwRyo/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="0.5944444444444444" data-s="300,640" data-type="gif" data-w="1080" type="block" data-imgfileid="503532530" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/12f1ac40-f145-4fd3-8e85-389f5cdcf35a/640.gif" data-order="0" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="19"&gt;&lt;i data-index-in-node="11" data-path-to-node="19"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 调用飞猪、高德示例&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p data-path-to-node="20"&gt;当用户提出复杂的差旅需求时，千问不只是给出文字建议，而是直接调动飞猪的票务网络锁定航班，同时通过高德筛选酒店。这种「所想即所得」的体验，直接省去了用户在多个 APP 之间反复跳转比价的繁琐过程，真正实现了从决策到交易的闭环。&lt;/p&gt;&lt;p data-path-to-node="21"&gt;而在更长尾的家庭消费领域，千问则充当了「全能导购」的角色。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532531" data-ratio="2.0625" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/5L8bhP5dIqFJtGm1iclV7O7kRgMrowHRTic5v5XPWLapBvQcuhj75PgMyY97ovkA4xNF2XgjPVnOXPMjxomBGApHU3atmuyZlibHY2DPia1DJOA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-type="gif" data-w="400" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/e12bd605-9ec7-48e9-8641-a08674b0cbec/640.gif" data-order="1" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="22"&gt;&lt;i data-index-in-node="11" data-path-to-node="22"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 调用淘宝示例&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p data-path-to-node="23"&gt;面对海量的商品参数和复杂的促销机制，千问能够迅速调取淘宝天猫的商品库，基于用户真实需求进行多维度筛选。用户无需再做算术题，AI 已经把性价比最高的方案直接呈现在了对话框里。&lt;/p&gt;&lt;p data-path-to-node="24"&gt;&lt;strong&gt;为什么是千问？为什么是现在？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="25"&gt;今年的春节档，俨然成了互联网大厂的「AI 阅兵场」。各家都在撒钱，各家都在推 AI，但如果仔细观察，你会发现大家的战术动作其实截然不同。&lt;/p&gt;&lt;p data-path-to-node="26"&gt;有的厂商在抢夺聊天框里的时长，有的在争夺生成式内容的流量。而千问这波「免单」操作，通过把 AI 从「聊天框」直接拽进了「购物车」和「取票机」，实际上是在争夺价值更高的东西：&lt;strong&gt;生活消费场景的入口&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="27"&gt;这种差异，也反映出中美 AI 发展路径上的不同侧重。&lt;/p&gt;&lt;p data-path-to-node="28"&gt;近期，Anthropic 推出 Claude Cowork 后，Salesforce、Adobe 等一众企业软件公司的股价出现明显波动。市场的担忧很直接：&lt;/p&gt;&lt;p data-path-to-node="29"&gt;如果 AI 已经可以自动整理资料、生成报表、管理邮件、协助决策，那些每年动辄几千美元订阅费的 SaaS 工具，还剩下多少不可替代性？&lt;/p&gt;&lt;p data-path-to-node="30"&gt;这种焦虑，并非空穴来风。&lt;/p&gt;&lt;p data-path-to-node="31"&gt;2 月 6 日凌晨，Anthropic 和 OpenAI 几乎同时更新了自家的基础模型。Claude Opus 4.6 在编码、金融、法律等专业评测中大幅领先前代模型；而 GPT-5.3-Codex 不仅在多项工程基准中刷新纪录，甚至已经开始参与自身的训练、调试和部署过程，加速模型迭代。&lt;/p&gt;&lt;p data-path-to-node="32"&gt;这些能力，指向的是同一个方向：&lt;strong&gt;用 AI 重塑白领的工作方式，直接冲击传统企业软件的价值链。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="33"&gt;这也是为什么，美国 AI 的主要战场，始终集中在高客单价的 B 端市场：企业付费意愿强，SaaS 体系成熟，AI 的每一次性能提升，都可以迅速转化为订阅收入和效率红利。&lt;/p&gt;&lt;p data-path-to-node="34"&gt;而中国的情况，几乎是另一面镜子。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;中国的移动互联网已经高度普及，支付体系高度统一，电商、本地生活、即时配送的数字化程度在全球范围内都极为罕见。对普通用户来说，「下单&amp;mdash;支付&amp;mdash;履约」本身就是一条被反复验证过的顺滑路径。在这样的环境下，AI 如果只停留在聊天或内容生成层面，反而很难释放它真正的价值。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;说白了，中国用户对 AI 的期待一直很简单：&lt;strong&gt;少说点废话，多把事情办完。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="37"&gt;千问之所以能打赢这场仗，核心在于「全栈 AI」的能力门槛。要实现「动动嘴就把事办了」，光有聪明的脑子是不够的，还得有灵活的手脚。&lt;/p&gt;&lt;p data-path-to-node="38"&gt;千问依托庞大的阿里生态，组装出了一副可以在物理世界「横着走」的躯体，形成了一条「AI + 实体」全链路：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="39,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,0,0"&gt;大脑&lt;/b&gt;：通义千问大模型（Qwen），负责听懂你那些稀奇古怪的需求。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,1,0"&gt;双手&lt;/b&gt;：淘宝、天猫、盒马、饿了么，负责在数以亿计的商品池里把东西挑出来，买回来。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,2,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,2,0"&gt;双脚&lt;/b&gt;：飞猪、高德、大麦，负责搞定票务和路线，把你精准地带到目的地。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,3,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,3,0"&gt;钱包&lt;/b&gt;：支付宝，负责最后的交易闭环，以及这次那个最让心动的：免单。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532532" data-ratio="0.6666666666666666" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqE6y5NMLMicUSYTMBhXrCnx5Nptcn2fOXlqtxCGEIUjMYynvSBwWibjbeoaicNwQUuianq674Fn2JY3KFkuQVWhO9tSicicKARP59wGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/f794ac86-39ce-47a7-8406-e382eca2c340/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="40"&gt;这就是阿里独有的「超级 Agent」路线。它不仅要让 AI「想得到」，更要让 AI「办得到」。&lt;/p&gt;&lt;p data-path-to-node="41"&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="42"&gt;如果说 2023、2024 年是 AI 的「技术元年」，2025 年是 AI 的「应用探索年」，那么这个春节，至少让很多普通用户第一次感受到：&lt;/p&gt;&lt;p data-path-to-node="43"&gt;&lt;strong&gt;AI 开始真的「派得上用场」了。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="44"&gt;去年春节，DeepSeek 用「深度思考」让我们见识了中国 AI 力量；而今年春节，千问正试图用「生活 Agent」打破 AI 的应用边界。&lt;/p&gt;&lt;p data-path-to-node="45"&gt;依托全球最庞大的互联网用户基数、最成熟的移动支付体系、以及最丰富的线上线下融合消费场景，中国 AI 正在走出一条完全不同的路：&lt;strong&gt;将最前沿的技术，无缝嵌入最鲜活的日常生活。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="46"&gt;从 PC 时代的「搜索框」，到移动时代的「APP 矩阵」，再到如今 AI 时代的「一句话办事」。我们与数字世界的交互方式，正在经历一场不可逆的减法。&lt;/p&gt;&lt;p data-path-to-node="47"&gt;所以，今年过年，与其在各个 APP 里迷路、比价、焦虑，不如试着把这些麻烦事儿都丢给千问。&lt;/p&gt;&lt;p data-path-to-node="48"&gt;毕竟，&lt;strong&gt;让 AI 学会「打工」，让我们腾出时间去真正地享受生活、陪伴家人&lt;/strong&gt;，这才是科技进化的终极奥义，不是吗？&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>CVPR 2026 Workshop征稿｜第六届AdvML@CV：多模态大模型智能体安全</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:55:21 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/36c31d93-d498-4a07-806d-e2aa788dc75d/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="4"&gt;IEEE/CVF 计算机视觉与模式识别会议 CVPR 2026 将于 2026 年 6 月 3 日至 6 月 7 日在美国科罗拉多州丹佛举办。我们将在 CVPR 期间举办第六届对抗机器学习计算机视觉研讨会（6th AdvML@CV），Workshop 预计安排在 6 月 3 日或 6 月 4 日。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532263" data-ratio="0.19285714285714287" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGIykaRbX7wDeBdFtB8yP11hTHfVRE8hnuWcPjLKYlu9WyPibhaJuINk8pLXzusjWnPEkO0gg07kia8N4wyicSickPwGrXzxgkOljY/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="700" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/14610ac2-ec99-4876-9f6a-0a99d1fb08a8/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="5"&gt;本届主题聚焦：&lt;b data-index-in-node="7" data-path-to-node="5"&gt;Safety of Vision-Language Agents（视觉-语言智能体安全）&lt;/b&gt;。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;&lt;strong&gt;主题聚焦：视觉-语言智能体的安全与鲁棒性&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="7"&gt;多模态基础模型推动了视觉理解、生成与推理能力的跃迁，也让 Vision-Language Agents（视觉-语言智能体）迅速成为「感知&amp;mdash;&amp;mdash;语言推理&amp;mdash;&amp;mdash;行动规划」一体化的新范，在无人驾驶、智能机器人等领域具有广阔应用前景。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;但随着智能体自主性增强，攻击面也从传统像素级扰动扩展到更复杂的安全风险：例如对抗提示（Adversarial Prompts）、指令注入（Instruction Injection）、Jailbreak 操控等，它们可能扰乱推理链条、误导感知决策，甚至诱发危险行为。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;我们希望通过本次 Workshop，汇聚计算机视觉、多模态学习与 AI Safety 社区的研究者与工程实践者，共同推进安全、鲁棒、可信的视觉-语言智能体研究与落地。&lt;/p&gt;&lt;p data-path-to-node="10"&gt;&lt;strong&gt;论文征稿&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="11"&gt;&lt;strong&gt;本次研讨会诚邀与以下主题相关（但不限于）的投稿：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="12,0,0"&gt;Attack and defense on vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,1,0"&gt;Datasets and benchmarks that could evaluate vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,2,0"&gt;Adversarial / Jailbreak attacks on vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,3,0"&gt;Improving the robustness of agents or deep learning systems&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,4,0"&gt;Interpreting and understanding model robustness, especially agentic AI&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,5,0"&gt;Adversarial attacks for social good&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,6,0"&gt;Alignment of vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="13"&gt;&lt;b data-index-in-node="0" data-path-to-node="13"&gt;投稿类型与格式要求：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Long Paper：正文最多 8 页（不含参考文献）&lt;/li&gt;&lt;li&gt;Extended Abstract：正文最多 4 页（含参考文献）&lt;/li&gt;&lt;li&gt;论文需匿名，并使用 CVPR 2026 Author Kit 模板撰写（LaTeX/Word 均可）、&lt;/li&gt;&lt;li&gt;被录用论文可选择收录至 CVF &amp;amp; IEEE Xplore Proceedings&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="15"&gt;&lt;strong&gt;重要日期&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Abstract Submission Deadline: 2026/03/05&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Paper Submission Deadline: 2026/03/05&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Author Notification: 2026/03/17&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Camera-Ready Deadline: 2026/04/01&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CVPR 2026 Conference: 2026/06/03&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="17"&gt;&lt;strong&gt;演讲嘉宾&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532342" data-ratio="0.6981481481481482" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFPCDzJTvnMrGVdprbicW3LAuydlVNYyLnQXbgrBUQY52oicibRP9PKT1mnibQ6XAMB7pNFMbZSwsDxJTqCfIK0mJ0lXwVQUpEhgms/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/06b4ec12-eb10-4191-b131-e71d41f8e9d7/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="19"&gt;&lt;strong&gt;组织团队&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532343" data-ratio="0.8712962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHXCmicQhclqvWkCiaEvgDQnUshCx2Ifbicl4jiciatFuuGvFtPMnAYtQgHU7mRm4ic6nXW3bnESX3oS8icQfeCmCKc0vUasUdiauL8fz4/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8698192d-18fd-436b-8590-f6015983dd56/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4 data-path-to-node="21"&gt;&lt;strong&gt;Program Committee&lt;/strong&gt;&lt;/h4&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqG48XjzYvZQXyllNRHia7PlvfFUStqV9IcbVBF1OyjzLFIf4iaxcP9CqtP5KlSjl5ibcX0HAZA5IAjb8FgWXCApozicZDfWjS9NNV4/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.32857142857142857" data-s="300,640" data-type="png" data-w="700" type="block" data-imgfileid="503532273" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/60389a17-b5c4-4989-ad1c-65f43b813986/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Workshop Sponsor&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532277" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHzZicW4ZSCnW0nLFFiahe5Xdm3UUxMPVMB44gfriaBUDCDHPN26ibHL6DdHFEUMG0TWEicicsmwfTU9Swkp7kyDRYzTcMYNAvInf9wc/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c48ae801-e852-4b1d-ac41-ee2dfd51f9a8/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="25"&gt;&lt;strong&gt;投稿入口与会议信息&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="26"&gt;欢迎转发给有相关研究方向的同学与合作伙伴，我们期待在研讨会现场与大家交流！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Workshop 官网：https://cvpr26-advml.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenReview 投稿入口&lt;b data-index-in-node="0" data-path-to-node="28"&gt;：&lt;/b&gt;https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/Advml&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>童年的滚球兽「走进」现实？华为天才少年创业，全球首个虚实融合的实时交互视频模型来了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:50:51 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Youli&lt;/section&gt;&lt;p&gt;还记得童年的那个愿望吗？&lt;/p&gt;&lt;p&gt;随着《数码宝贝》进化曲的响起，屏幕前的你我或许都曾幻想过：要是那只从数码蛋中破壳而出的滚球兽，真的可以从电视屏幕那端跳出来，就好了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqF1mqAjX6FMceLTBOpSAuCNBc4vzp5IsYmT0ic0h1cJz6Uq4Pyv9crYWjSuPUMuEVBibT1QRI5wZoo0DvibkoDOGpsYgAcgJjoiaLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5611111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532282" data-aistatus="1" data-original-style="width:517px;height:290px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3953dfd5-3224-46ab-8c89-97b4b368b140/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;彼时，我们只能将这种天马行空的「美梦」寄希望于「次元裂缝」的开启。再后来，技术增强现实（AR）技术曾一度带来了希望，但几经潮起潮落，结果仍停留在「预先制作的内容叠加」层面，数字角色无法真正感知环境。&lt;/p&gt;&lt;p&gt;而现在已经 2026 年了，生成式 AI、实时渲染、端侧算力、感知模型同时成熟，尤其是 Sora 展现出的前所未有的世界模拟能力，让大家意识到，原来虚拟内容不再需要完全预制，可以被实时生成、驱动，并具有物理合理性。技术的狂奔第一次让曾经的「中二梦」，具备了成为现实的可能：&lt;strong&gt;你真的可以从屏幕中「召唤」出一只滚球兽。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/26363918-a3a4-49cb-b1f7-4fe4326d03a3/1770619538527.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是不是很神奇？手机镜头对准桌面，选取一张滚球兽照片，下一秒，一只滚球兽就「脱屏而出」，出现在桌面上，四处张望。你伸出手，它刚开始会有点警惕，之后就亲昵地蹭你的手心，你轻轻一捏，它会给出Ｑ弹的物理反馈，而当你把手摊开，它甚至可以被你「托」在掌心之中，就好像，这是一只「活」的滚球兽&amp;hellip;&amp;hellip; &lt;strong&gt;通过一个手机摄像头，虚拟角色第一次实现了与现实世界的融合&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这就是由&lt;strong&gt;初创公司 Xmax AI 推出的首个虚实融合的实时交互视频模型 X1&lt;/strong&gt;，没有复杂的 Prompt，不需要漫长的渲染等待，只需要手势进行交互，就可以让虚拟世界与现实相连，在镜头中令「幻想」成真，让用户体验到实时交互的心流体验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGlsaCFJibAZhMSdk35bOicqZsIwV7qpS3zpdg5lnpbwuqloZDUbNGRKO9pTj2riafM8fCZswMUsyYJdWeVSptgvCv0FtBcJN7qSY/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7287037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503532523" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3cf72ce8-5bc6-4ebc-ba7e-13d498a08b31/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;目前，Xmax AI 已通过一款技术演示型应用 X-cam（目前开放 testflight 下载），将 X1 的能力开放给部分用户体验，感兴趣的朋友可以通过文末提到的方式获取邀请码，近距离体验一下技术的边界。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「虚实融合 + 实时交互」，视频生成进入「人人可玩」时代&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去这一年多，AI 视频生成领域可以说是遍地开花、神仙打架。&lt;/p&gt;&lt;p&gt;数据显示，&lt;strong&gt;2024 年全球 AI 视频生成市场规模已达 6.148 亿美元，预计到 2032 年将飙升至 25.629 亿美元&lt;/strong&gt;。在市场的强需求推动下，从 Sora 到 Runway，各路玩家都在沿着「更强的生成能力」方向极力狂奔：卷画质、卷时长、卷分辨率&amp;hellip;&amp;hellip;&amp;nbsp;&lt;/p&gt;&lt;p&gt;仔细看下来，整个赛道，大多数玩家选择的技术路线依然是文生视频，致力于面向专业领域的创作者 &amp;mdash;&amp;mdash; 影视、广告、内容工业等，打造更强大、更完善的生产力工具。&lt;/p&gt;&lt;p&gt;可不得不承认，在当前的「视频模型军备竞赛」中，普通用户似乎没有参与到狂欢中，感受就是「热闹是他们的，我什么也没有。」&lt;/p&gt;&lt;p&gt;原因很现实，首先是上手难，当然，很多视频生成工具操作起来已经很便捷，可很多时候写出精准的 Prompt 依然像是在编写代码，而且等待时间长，生成时间动辄从数秒到数分钟，再到数十分钟不等，缺乏即时反馈的快感。而漫长的等待后，得到的也不过是一段存在于屏幕里的「只能看、不能碰」，与当下日常生活毫无关系的虚拟视频。&lt;/p&gt;&lt;p&gt;Xmax AI 敏锐地捕捉到了这一点：&lt;strong&gt;AI 视频生成要想真正走入大众，就不能仅停留在「工具」阶段，要容易上手，要让大众有参与感，能够「玩」起来。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d9ef6218-bd9b-4b08-bb97-3c1ba62b3d24/1770619582103.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;可这也就意味着，在基础视频生成能力之外，行业还需要跨越两座「大山」：一是&lt;strong&gt;降低交互门槛&lt;/strong&gt;，改变传统的文生视频工具需要专业想法和 Prompt 撰写能力的方式；二是&lt;strong&gt;要与现实世界有更多结合&lt;/strong&gt;，人是生活在现实中，文生视频模型一定程度上确实满足了完全虚拟化的想象，可人对现实的幻想并没有被满足。&lt;/p&gt;&lt;p&gt;基于此，Xmax AI 走了一条截然不同的路线：推出首个虚实融合的实时交互视频模型 X1，让视频生成告别键盘输入，回归人类最本能的手势与触控，仅需要一个手机摄像头，就能打破虚拟与现实的「壁」。&lt;/p&gt;&lt;p&gt;具体来看，基于 X1 强大的端侧实时生成能力，Xmax AI 将这一技术落地为四大核心玩法：次元互动、世界滤镜、触控动图、表情捕手&amp;hellip;&amp;hellip; 每一台手机似乎都变成了连接虚实的「魔法棒」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;次元互动&lt;/strong&gt;：这就是前面那个视频所展示的能力，手机摄像头拍摄现实场景，任意上传一张角色参考图，就可以将该角色在镜头中「召唤」出来。&lt;/p&gt;&lt;p&gt;比如下面这个小兔子，你可以在镜头前伸出手与它互动，捏一捏、拍一拍，甚至将把它托到手上。视频中可以看到，当抚摸到兔子眼睛旁位置时，它会跟随人的动作转头，甚至可以看到绒毛因为触碰而遮盖眼睛的情况，没有延迟，因为它所有的物理反应都是 X1 模型实时生成的，所以，看起来就好像真的在抚摸一个真实存在的生命体。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/85d74f11-1357-49ad-9f87-e91ad965bdde/1770619611538.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;不仅仅是动漫角色，可以说是任何自己喜欢的纸片人、宠物、毛绒玩具，都可以在镜头中「活」过来。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;世界滤镜&lt;/strong&gt;：任意上传一张风格参考图，就可以将手机摄像头拍摄的画面实时转换，变成指定的风格，例如梵高画风、乐高画风等。可以用于渲染环境，也可以用于渲染人物，甚至可以用于渲染屏幕内容，像是正在玩的游戏画面。&lt;/p&gt;&lt;p&gt;直接来看一个例子，下面视频中的小姐姐通过选取不同风格的参考图，让自己「化身」为图片所示风格的人物，可以是经典动漫中的二次元虚拟形象，也可以是乐高积木风格。而且，当小姐姐做出挥手或是摇头动作时，视频中「变身」后的人物或形象会实时跟着做出相应的动作。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2d1caf9d-83bb-434b-a8e3-c1b7482e1215/1770619628159.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;触控动图&lt;/strong&gt;：让静态照片「活」过来、动起来，不再需要复杂软件。对于任意一张照片，都可以在触摸屏上对照片中的角色进行拖拽控制，让它实时运动起来。&lt;/p&gt;&lt;p&gt;比如下面视频中动漫风格的小兔子，左右拖动它的耳朵，它就开始左右摇头；上下挥动，它就做出被拍脑袋的动作；拖动嘴角，它会露出微笑。「实物」也可以，给自家猫咪狗子拍张照上传，就可以让它挥手、抡拳，跳起舞；眨眼、吐舌、卖起萌。甚至是「恶搞」的，将刘海剪成整齐模样的马，也在镜头下开始摇头晃脑&amp;hellip;&amp;hellip; 就像在操控提线木偶，轻松赋予静止图像以生命力。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1fb0f76d-2b7e-436e-8827-c652e1c90b57/1770619667166.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;表情捕手&lt;/strong&gt;：将相机镜头对准任意的人或物体，选择一个「大拇指」或「怒气冲冲」的 Emoji，AI 就会实时「捕捉」对方的特征，实时生成一个神态精准、魔性十足的动态表情包。这简直就是「社交神器」，以后聚会也不用担心冷场，随时就可以拿出来玩一下。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/829c893d-f2d7-4e10-b30b-5a0cb074a913/1770619679138.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;强大能力背后的技术挑战与实现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是不是很好玩，即便是对技术没什么了解，也可以轻松上手。但在业内人士看来，&lt;strong&gt;这不仅是产品的创新，更是工程能力的「暴力美学」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;「有趣体验背后，是极高的技术挑战。」Xmax AI 向机器之心透露，要实现上述这些效果，必须同时解决当前 AI 行业的三大痛点：&lt;/p&gt;&lt;p&gt;首先是&lt;strong&gt;极致实时&lt;/strong&gt;，从上面的视频中也可以看出来，视频中的人物或是形象的反应随时能够跟着手势变，给用户产生一种「我在和它互动」的感觉，而这就要求延迟必须控制在毫秒级，可当前市面上的大多数所谓「实时」模型响应往往需要数秒，难以满足 Xmax AI 想要在交互场景中呈现的效果需求。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a55fc9f3-417d-433f-959c-ac958afc21ac/1770619702892.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;其次是&lt;strong&gt;意图理解&lt;/strong&gt;，Xmax AI 的想法是希望交互方式多种多样且自然，对普通人来说门槛足够低，这就要求模型做到能够自动理解人的意图，并实时生成精准的反馈结果。可当前大多数模型都是文生视频、图生视频，无法实现这些手势交互效果。比如，对于模型来说，当人做出「捏」这个动作时，要读懂其中的意图，可要比读懂一段文字难得多。&lt;/p&gt;&lt;p&gt;另外，还存在&lt;strong&gt;数据稀缺&lt;/strong&gt;的问题，对于整个 AI 行业来说，数据都足够重要却又极致稀缺，更何况是相对小众的「虚实融合交互数据」，生产成本高，构造难度极大。但现实又是，想要实现好的虚实融合的效果就必须基于大量且专业的高质量训练数据。&lt;/p&gt;&lt;p&gt;这些挑战一度让 Xmax AI 犯了难。&lt;/p&gt;&lt;p&gt;但需要注意的是，&lt;strong&gt;Xmax AI 是一支既懂底层算法，又懂工程化落地，还拥有敏锐产品嗅觉的「特种部队」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;创始人史佳欣，出身于华为「天才少年」计划，是一位典型的技术极客。联合创始人梁宸，现任港科大（广州）助理教授、博导。联合创始人翁跃庭，是一位「六边形战士」型的全栈工程师。而公司核心技术团队则都是来自清华大学 KEG 实验室和 HCI 实验室的人才，是国内大模型领域和人机交互领域的顶尖力量。&lt;/p&gt;&lt;p&gt;不仅如此，团队核心成员也大都在字节、快手、华为、阿里等头部 AI 大厂历练过，有着丰富的技术落地实践经验。&lt;/p&gt;&lt;p&gt;因此，面对上述这些挑战，Xmax AI 交出了一份「硬核」的技术答卷。&lt;/p&gt;&lt;p&gt;针对极致实时性需求，Xmax AI 进行架构创新，提出了&lt;strong&gt;端到端的流式重渲染视频模型架构&lt;/strong&gt;，实现了帧级别的自回归 DiT（Diffusion Transformer），并通过多阶段的蒸馏压缩和对抗训练，百倍提升了每一帧画面的扩散采样速度。不仅将延迟压低至毫秒级，更是通过自研的「循环回归架构」打破了时长的限制，支持无限时长的连续生成。&lt;/p&gt;&lt;p&gt;针对模型对意图理解的高要求，Xmax.AI 则构建了&lt;strong&gt;统一的交互模型架构&lt;/strong&gt;，让模型既能理解摄像头透视下的空间三维关系，也能理解屏幕触控下的平面二维操作，从而对于用户的各类交互行为，模型都能够实现精准的意图识别。&lt;/p&gt;&lt;p&gt;而针对「数据荒漠」难题，Xmax AI 则搭建了&lt;strong&gt;虚实融合数据的合成管线&lt;/strong&gt;，利用半自动化方式，低成本、批量化地生成了高质量的交互训练数据，构建了难以复刻的行业壁垒。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;写在最后&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;体验了这么多玩法，相信大家已经隐约感知到 Xmax AI 想做的事情了。如果说 Sora 代表的是一条极致强化生成能力的路线，让 AI 学会拍电影、构图、运镜、叙事，那么 X1 则是希望 AI 能够陪你玩，随时出现在你周围的生活场景中。&lt;/p&gt;&lt;p&gt;从这个角度来看，对于 Xmax AI 团队而言，&lt;strong&gt;X1 模型仅仅是一个开始&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其实从前面 X1 的模型能力展现上也可以看出来，Xmax AI 不是想「再造」一个专业的视频创作工具，开发一款 App，更是在&lt;strong&gt;试图搭建下一代内容交互引擎，重新定义用户与 AI 生成内容之间的个性化交互方式&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在他们的愿景里，这个新时代中，那些曾经只能存在于影视作品和虚拟世界中的角色，不管是数码宝贝，还是银翼杀手式的仿生生命体，都可以走进现实，成为虚实融合的「数字生命体」，进入家庭，成为用户的虚拟陪伴、虚拟宠物等。&lt;/p&gt;&lt;p&gt;与此同时，「万物可交互」也不再只是一个空想，不管是刷短视频、看直播，还是视频通话、线上会议，都可以实时改变视觉形态，一边看一边玩，带来全新的个性化体验；社交互动变得更立体、更有趣，摄像头化身「精灵球」，随时随地「捕捉」一个好友过来，对 TA 进行打扮&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;也就是说，Xmax AI 所做的，是通过 AI 将「幻想」拉得更近，近到可以触碰、互动、分享，真正融入人们的日常生活。&lt;/p&gt;&lt;p&gt;正如 Xmax AI Slogan 所言，&lt;strong&gt;Play the World through AI（用 AI 玩转世界），让世界触手可「玩」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;最后，感兴趣的朋友可以通过 testflight 邀请链接下载 APP，下载后在登录界面点击申请邀请码，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;也可以通过 Xmax AI 官网&lt;/span&gt;来提前体验、感受这一切。这一次，你可以亲自推开那扇通往虚实融合世界的「门」。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;testflight 邀请链接：https://testflight.apple.com/join/8sWgKZeQ&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;Xmax AI官网链接：&lt;/span&gt;https://xmax.ai/&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>英伟达世界模型再进化，一个模型驱动所有机器人！机器人的GPT时刻真正到来</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:44:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜冷猫&lt;/section&gt;&lt;p&gt;驱动具身智能进入通用领域最大的问题在哪里？&lt;/p&gt;&lt;p&gt;我们认为，核心问题在于&lt;strong&gt;「跨具身（cross-embodiment）迁移」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;当然，具身智能执行通用复杂任务的核心是一个完善的世界模型。但是，大多世界模型其实并没有我们想象的那样具备极强的泛化性和迁移能力。&lt;/p&gt;&lt;p&gt;简单来说，这些用在机器人或是智能汽车上的世界模型，基本都是在某个固定的硬件平台上设计训练的，大多不具备很强的泛化能力，跨具身迁移几乎靠运气。&lt;/p&gt;&lt;p&gt;说白了，大多数机器人今天学到的不是 「世界是如何运作的」，而是 「在这台机器该怎么动」。我们需要能学到一个真正理解物理与因果的世界模型 —— 知道世界会怎么变、动作会带来什么后果，才能在不同身体、不同环境中迁移与泛化。&lt;/p&gt;&lt;p&gt;在这个问题上，作为算力的王者，深耕各类世界模型的英伟达再一次发力，构建了一个全新是世界模型，一切都是 Zero-Shot 的。&lt;/p&gt;&lt;p&gt;最近，&lt;strong&gt;英伟达 GEAR 实验室提出 DreamZero，一种基于预训练视频扩散骨干网络构建的世界动作模型（WAM）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这是一个拥有 140 亿参数的模型，能够让机器人仅通过简单的文本提示就完成此前从未见过的任务。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/138cced5-668d-450c-ad86-db8653f6d33f/1770619256775.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;实验室负责人 Jim Fan 将其称为机器人领域的&lt;strong&gt;「GPT-2 时刻」&lt;/strong&gt;：研究团队只需输入想法，机器人就能执行相应动作。目前，该模型的代码已在 GitHub 上开源。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGtibHtGULLqkMuKvV1iayuVtYJuMLOD4mcGC3qvOGzc6xgX1hUvky7DibOulicDmick2bjH7RrR7hq3cIUCnibBgVPFPTYxEZYZ8RJM/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.9166666666666666" data-type="png" data-w="1080" data-width="1146" data-height="1050" data-imgfileid="503532131" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ae63826a-e9ce-4864-97a4-c6d1e91328ba/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/5L8bhP5dIqHMfVwnl0mYnwzYGJxyLGXIXGjyRjX4uanVbmLL3pEpYAxgXMsLCOxcFqnvmnhuicRY0DNG5yfVnqxtCongS6VFLDjl34ey0YLs/640?wx_fmt=jpeg#imgIndex=2" data-ratio="0.45" data-type="png" data-w="1080" data-width="1296" data-height="584" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/5L8bhP5dIqHGWetuQnqDKk28X3y6UE71yxzbsG9tUqmnJ2L2YfTOH7v3GicxrUkhBkkWrz6bIK2j24SuzhwQHyUSGw6amC1pQ30BkxjxticicU/0?wx_fmt=png&amp;amp;from=appmsg" data-cropx2="1271.3356401384085" data-cropy2="571.7647058823529" data-imgfileid="503532132" data-aistatus="1" data-original-style="width: 567px;height: 255px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/9cda8a4a-9b7a-4f59-9c50-fd1175d191bd/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：World Action Models are Zero-shot Policies&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://dreamzero0.github.io/DreamZero.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Github 链接：https://github.com/dreamzero0/dreamzero&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不同于传统的 VLA 模型，WAM 通过联合预测未来世界状态与动作来学习物理动力学，并以视频作为世界演化的稠密表示。通过对视频与动作的联合建模，DreamZero 能够从异构机器人数据中高效学习多样化技能，而不依赖重复示范。在真实机器人实验中，相比最先进的 VLA，&lt;strong&gt;DreamZero 在新任务与新环境的泛化上实现了超过 2× 的提升&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;至关重要的是，通过模型与系统层面的优化，研究团队让一个 140 亿参数的自回归视频扩散模型实现了 7Hz 的实时闭环控制。此外，研究团队展示了两种跨具身迁移能力：仅使用 10–20 分钟的人类或其他机器人纯视频示范，即可在未见任务上带来 超过 42% 的性能提升。更令人惊讶的是，&lt;strong&gt;DreamZero 只需 30 分钟的 「玩耍数据」，就能适配到全新的机器人，同时仍保持零样本泛化能力&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFUKSqzXgHFEmvc2FqobKv2C0c2V9fazWI1N8wM0O4cYtgVhc0Nck1ibyoYsH4m7RzkKMRLRy4rhAsYAT9bEpmsctic57fqfrGTY/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=3" data-ratio="0.7824074074074074" data-type="png" data-w="1080" data-width="2040" data-height="1596" data-imgfileid="503532134" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9d654a02-8b0d-4316-bca5-11a4cff8255a/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero 整体概览。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;图中展示了 DreamZero 通过联合预测视频与动作，世界动作模型继承了关于世界物理规律的先验，从而实现了：&lt;/p&gt;&lt;p&gt;1）从多样、非重复的数据中高效学习；&lt;/p&gt;&lt;p&gt;2）在开放世界场景中的强泛化能力；&lt;/p&gt;&lt;p&gt;3）仅依赖纯视频数据即可完成跨具身学习；&lt;/p&gt;&lt;p&gt;4）对新机器人的少样本快速适配。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="692" data-imgfileid="503532135" data-ratio="0.3435185185185185" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEibvic8HKKwBzMrL1gziboHMAUK0D5o6YQBhKyYxDE4Ugr0Tm8tEgN1SGP4zjt9XnvS3Fty6IbarIISjiaXKhLjEuLGkkoqZGobvk/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" data-width="2012" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/89189648-87e5-49fe-bb32-627a4e98c440/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero 的模型架构。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;大多的预训练的视频扩散模型凭借来自网页规模数据的丰富时空先验，成为构建机器人策略的理想骨干网络。然而，将这类模型转化为高效的世界动作模型仍面临关键挑战：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1）视频–动作对齐&lt;/strong&gt;：联合预测视频与动作要求对视觉未来与电机指令进行紧密耦合，但如果只是简单地将独立的视频头与动作拼接，往往会导致二者对齐失效；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2）架构设计&lt;/strong&gt;：尚不清楚双向架构还是自回归架构更适合 WAM，这关系到多模态对齐、误差累积以及推理效率等关键问题；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3）实时推理&lt;/strong&gt;：视频扩散模型需要在高维潜空间中进行多步迭代去噪，使其在闭环控制场景下速度过慢、难以实用。&lt;/p&gt;&lt;p&gt;为此，DreamZero 通过模型设计选择有效应对了上述挑战。&lt;/p&gt;&lt;p&gt;模型接收三类输入：视觉上下文（通过 VAE 编码）、语言指令（通过文本编码器）、以及本体感知状态（通过状态编码器）。这些输入随后被送入一个基于 Flow Matching 的自回归 DiT 主干网络，由其联合预测未来的视频帧与动作，并通过各自独立的解码器输出结果。&lt;/p&gt;&lt;p&gt;在训练阶段，模型以分块（chunk）的方式工作：在给定干净视频上下文作为条件的情况下，对加噪的视频与动作潜变量进行去噪。在推理阶段，模型的预测会以异步方式在真实世界中执行，同时将真实观测结果回灌到 KV cache 中，以防止误差随时间累积。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队在 六种设置下展示了 DreamZero 的能力 —— 其中 五种用于测试泛化，一种用于实时部署。&lt;/p&gt;&lt;p&gt;相关的训练数据以及实验结果的演示可以参考以下链接：&lt;/p&gt;&lt;p&gt;https://dreamzero0.github.io/evals_gallery/&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AgiBot 预训练：已见 &amp;amp; 未见任务&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队对预训练模型进行开箱即用评测：任务来自预训练分布，但在未见对象的新环境中进行零样本测试。DreamZero（也包含从零训练版本）取得 62.2% 的平均任务进度，相比最佳预训练 VLA 基线（27.4%）提升 超过 2×。从零训练的 VLA 几乎为零；预训练 VLA 有一定进展，但幅度有限。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEkNaZyVPMJjvI71BOX6Ihib0TR3zZ0azZ1DXYsrPuiaoPycM3e4Iib8yBHBn54SBvS7ianQbHVvWdb5Ma30ticBqbmfIMmEruvM3xU/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=5" data-ratio="0.6027777777777777" data-type="png" data-w="1080" data-width="1742" data-height="1050" data-imgfileid="503532136" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f931095c-5597-4574-a19a-289947cfb9fb/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;对于训练中完全未出现的任务（如解鞋带、握手），DreamZero 仍达到 39.5% 的任务进度，而 VLA 再次表现吃力。值得注意的是，预训练 VLA 在未见任务上的有限进展，主要源于其无论指令如何都倾向于执行 「抓取 - 放置」 的默认动作，显示其过拟合于主导训练行为，而非真正理解新任务语义。研究团队在 4 台机器人、不同环境与物体上，对每个检查点进行了 80 次 rollouts。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHskv33PIFqGHlFMbaXMZRxXTciciap07N9U4OfUjn3qkBUREBwMvQwW9NcsDz4rv4XXbmBCicicXOdmiadk63SC5HgZxRibhVS4PqkE/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=6" data-ratio="0.6185185185185185" data-type="png" data-w="1080" data-width="1762" data-height="1090" data-imgfileid="503532137" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/f4d76bc3-d75b-4670-bd94-89652612fae6/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;DROID：已见任务 &amp;amp; 未见动作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为验证在公开数据上的效果，研究团队在 DROID（最异构的开源机器人数据集之一）上训练 DreamZero，并评测 20 个已见任务与 20 个未见动词任务（DROID 中未出现的动作）。DreamZero 显著优于预训练基线，在未见动词上取得 49% 的任务进度，而最先进的 VLA 仅为 25–32%。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGAP5sWm9BDRSlG2xImlQEF5arEsdpqKVCKs9mLLthhLaVJv4ONEibTHFYuqBVPlzAeISuApIzmLhSwPPIEgThM5L046p9P94bo/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=7" data-ratio="0.4638888888888889" data-type="png" data-w="1080" data-width="1676" data-height="778" data-imgfileid="503532138" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/26de7dc1-bf10-4c51-88ec-9dc57ba50d10/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;后训练：分布外泛化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本部分研究 WAM 在任务特定微调后是否仍保留泛化能力。研究团队在 三项下游任务上进行后训练：叠衬衫、装水果、清理餐桌。DreamZero 在三项任务上均表现更强，表明后训练后仍保持环境泛化能力。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e1f2c384-5b8b-4647-81bd-44353d65067f/1770619337408.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;跨具身迁移&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;仅用 30 分钟的玩耍数据（55 条轨迹），DreamZero 即可适配 YAM 机器人，并对南瓜、泰迪熊、纸袋等新物体实现零样本泛化，同时展现出强大的语言指令遵循能力。来自 AgiBot 预训练的知识可直接迁移，无需大规模重训。这是目前效率最高的具身迁移：以往需要数百小时示范的工作，能够在 30 分钟内完成（未使用任何其他 YAM 数据）。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6cac4cd7-8e43-4f85-8fb4-7cf456a4cd28/1770619347041.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;交互式提示&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;机器人基础模型的 「提示时代」 已经到来。研究团队展示了交互式提示的实战：带着机器人走到不同地方，让人们直接用语言提出新任务。机器人能够完成多种令人惊喜的操作。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6173f36e-897b-4202-8a54-46c380fffb64/1770619365955.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实时推理&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过模型、系统与实现层面的优化，DreamZero 实现了 每个动作块 150ms 的实时推理，支持 7Hz 闭环控制。结合异步推理与动作块平滑，执行过程更加流畅、响应迅速。研究团队对比了 16 / 4 / 1 个扩散步数的效果：步数越少延迟越低，而 DreamZero-Flash 即便在单步推理下也能保持性能。研究团队还展示了动作块平滑与异步推理对执行质量的影响。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8eb61091-d1b3-4c15-af45-f989d6b3e6eb/1770619375571.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero (16 diffusion step) + async &amp;amp; action chunk smoothing&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;零样本泛化能走多远？ 研究团队持续对 DreamZero 进行压力测试：在从未训练过的任务、从未见过的环境中探索能力。从扇汉堡、按电梯按钮，到敲木琴、摇铃鼓，不断涌现出令人惊讶的新能力。&lt;/p&gt;&lt;p&gt;DreamZero 只是开始 —— 它代表了基于视频世界模型的新一代机器人基础模型浪潮。&lt;/p&gt;&lt;p&gt;更多信息，请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>先解行为，再训Agent：CMU开源首份Agentic Search日志数据，把Agent拆开给你看</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:39:13 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/96ef0fb4-fe81-44d2-b328-1ef066c323a9/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在大模型驱动的 Agentic Search 日益常态化的背景下，真实环境中智能体 &amp;ldquo;如何发查询、如何改写、是否真正用上检索信息&amp;rdquo; 一直缺乏系统刻画与分析。&lt;/p&gt;&lt;p&gt;CMU 团队基于可重复检索平台 DeepResearchGym，从统一后端的半年真实流量中整理出 1400 万余条搜索请求、约 400 万个会话，在严格匿名化与清洗后，构建并于 Hugging Face 开源了首个 Agentic Search 行为日志数据集。&lt;/p&gt;&lt;p&gt;在此基础上，工作提出 &amp;ldquo;会话意图（Declarative / Procedural / Reasoning）&amp;rarr;轨迹动作（专化 / 泛化 / 探索 / 重复）&amp;rarr;检索信息采纳率（CTAR）&amp;rdquo; 三层分析框架，利用 LLM 进行会话切分与标签推断，刻画出智能体搜索中普遍存在的下钻偏好、事实型任务中的重试循环，以及不同改写模式对历史检索信息依赖程度的显著差异。&lt;/p&gt;&lt;p&gt;总体而言，该研究既为观察与评估 Agentic Search 行为提供了首个大规模开源日志，也为后续在智能体训练与系统设计中显式建模 &amp;ldquo;会不会搜&amp;rdquo; 提供了可复现的数据基础与可量化的行为信号。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGLypXiceiboka7k2ThhlsPician1HssLhP4a4OyJ7TopC1TcnYm4X7tdMUeqZ8iagnrVGPfx9j8RqkzTEqRARFsxoTAKC3D4XpvTiaM/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.44351851851851853" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531760" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/068d57f9-e798-41da-a3ee-6e678bf59efb/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Farxiv.org%2Fabs%2F2601.17617" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml8v1x9d-2l32ne" href="https://mp.weixin.qq.com/s/uRWHomOqZmOVfVRkx1l2fw"&gt;https://arxiv.org/abs/2601.17617&lt;/a&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHmiboKwy2g1KCpibCtMceHa3zPCp6oFLNHFpmxsYqQntqWlR216t62o71Ir8ichBdnmEPDkUNdrgf2Sw1ic9p78ul5vSLZDrvrtpI/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5462962962962963" data-type="png" data-w="1080" data-width="2208" data-height="1206" data-imgfileid="503531898" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/74ef5986-e7cb-4629-9ff2-3e01109d8d2c/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-pm-slice="2 2 []"&gt;Hugging Face 开源数据集：DeepResearchGym Agentic Search Logs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-pm-slice="2 2 []"&gt;数据集链接：&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fcx-cmu%2Fdeepresearchgym-agentic-search-logs" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml7s1eh5-d8envk" href="https://mp.weixin.qq.com/s/uRWHomOqZmOVfVRkx1l2fw"&gt;https://huggingface.co/datasets/cx-cmu/deepresearchgym-agentic-search-logs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml7sl2hy1s8s" data-pm-slice="0 0 []"&gt;&lt;strong&gt;01 从任务到行为：Agentic Search 的缺失一环&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;近年来，大型语言模型驱动的 Agentic Search 与 Deep Research 逐渐成为信息获取的重要形态，即系统不再只返回一页文件结果，而是通过智能体自动发起多轮检索、阅读文档、改写问题，再生成综合回答。&lt;/p&gt;&lt;p&gt;与之相对应，已有研究提出了多种基准任务和评测框架，用于衡量系统在问答、推理、工具调用等方面的性能。然而，这些评测大多基于构造好的题目和离散样本，&lt;strong&gt;缺乏对真实环境中智能体检索行为的系统观察与结构化分析&lt;/strong&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;多轮会话在实际使用中如何展开；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;不同任务类型下，智能体采用哪些检索策略；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在多步改写过程中，检索证据信息在多大程度上真正影响了后续查询。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这篇 Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests 论文针对上述缺口，基于 DeepResearchGym（DRGym）平台提出了两方面贡献：&lt;/p&gt;&lt;p&gt;1. 从半年真实流量中整理出 &lt;strong&gt;超过 1400 万条 Agentic Search 请求、约 400 万个搜索会话&lt;/strong&gt;，在严格匿名化与清洗之后，发布为 &lt;strong&gt;首个开源的 Agentic Search 行为日志数据集&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;2. 在此基础上，从 &lt;strong&gt;任务意图（intent）&lt;/strong&gt; 与 &lt;strong&gt;检索轨迹（trajectory）&lt;/strong&gt; 两个维度，系统分析智能体的搜索过程，并提出一个衡量 &amp;ldquo;是否利用检索到信息&amp;rdquo; 的指标 CTAR（Context-driven Term Adoption Rate）。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFKc5jITicqu4XtlaLspMe5hlp6MQUNpogqRDyicXlaCFAMiaBgmGWDdsjjIeFIZ2xKkOaSPxpZkwibl2Ugd9GZjULXUyK7LujLd18/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.38425925925925924" data-type="png" data-w="1080" data-width="2427" data-height="933" data-imgfileid="503531736" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ed045560-8129-41fd-9587-1d4dd177a593/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;02 数据与平台：DRGym 日志概况&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DRGym 是该团队搭建的一个面向研究用途的可重复检索平台（&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.deepresearchgym.ai%2F" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml7s0w1t-bgg8kn" href="https://mp.weixin.qq.com/s/uRWHomOqZmOVfVRkx1l2fw"&gt;https://www.deepresearchgym.ai/&lt;/a&gt;），对外提供统一的 /search API，后端基于密集检索，挂载在固定的 Web 语料快照上，例如 ClueWeb22、FineWeb 等。不同智能体可以以任意策略调用该接口，但所有请求都运行在统一的检索基础设施之上。&lt;/p&gt;&lt;p&gt;日志中的每条记录包含如下信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;查询文本 query_text；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;检索文档数量 num_of_docs（即 top-K）；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;所用数据集 dataset（如 ClueWeb22 / FineWeb）；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;检索预算相关参数 complexity；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;时间戳、匿名化 IP 等会话识别字段。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;论文选取约半年时间窗口，得到来自&lt;strong&gt;横跨 25 个国家，近 600 个 IP 地址，超过 1400 万条请求日志，约 400 万个会话（session）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这为验证日志是否具有广泛多元使用的多样性，而非某些基准题目的重复回放训练，作者从两方面进行了检查：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;使用文本向量表示（embedding）分析查询语义分布，结果显示查询覆盖的语义空间较为分散；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;将日志中的查询与若干常用 Agentic Benchmark 的题目进行语义匹配，重合比例极低。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHiaqUxZKhleMDVDIWziaD1ZrNibp7ulibgt78oDRfB2MaercFnSe0Lia0EibjdrCPicBgVPW93LMAUYtydrXuWAxf9haex10FSn6cxiaI/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.625" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531742" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/e33940db-739a-469b-8aa7-4daa238eba13/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqEuFhvJpxIpMp6PSK28x5HjksjcmVOpHulMS8lbyTa46ksWFcg8t7uNibDGUrvRKvCwE7AQudQGN2qFYyiaibVZ5HngeficP3hIc8c/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.33055555555555555" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531741" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/148696eb-3c05-494b-82bb-3c0fa3510e99/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;隐私方面，日志经过了字段裁剪与匿名化处理：去除直接可识别信息，对自由文本进行 PII 清理，并重新生成会话级别的匿名 ID，最终在 Hugging Face 上公开。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;03 从请求到会话：Session 切分方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;原始日志是时间顺序的请求流，要分析行为模式，需要先划分搜索会话。与传统人类 Web 日志不同，智能体请求往往高频且可并发，仅依赖固定时间阈值（例如 &amp;ldquo;间隔超过 30 分钟&amp;rdquo;）容易误分。&lt;/p&gt;&lt;p&gt;该工作采用了 &lt;strong&gt;语义 + 时间联合的 Sessionization 策略&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;1. 首先，基于一批相邻请求样本，通过 LLM 标注 &amp;ldquo;是否属于同一会话&amp;rdquo;，构建连续性标签；&lt;/p&gt;&lt;p&gt;2. 其次，使用查询的向量表示训练一个连续性判别模型，预测两条查询之间是否应归为同一 session；&lt;/p&gt;&lt;p&gt;3. 在线划分时，对同一匿名 IP 下的新查询，与当前所有活跃会话的末尾查询计算连续性分数，在分数与时间差均满足条件时并入对应会话，否则开启新会话。&lt;/p&gt;&lt;p&gt;这一策略最终得到约 400 万个 session。整体分布上：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;单轮会话仍然占据一定比例，但相当多的会话包含多步查询；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;大部分相邻请求的时间间隔在数秒到十几秒之内，体现出 agentic search 中 &amp;ldquo;高频、小步迭代&amp;rdquo; 的特征。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFsP0iaYdUSOmrYMO3On2oIAMfjdOdBnIPyfNn3TcCLlcEAuezIO8oDACFGftM0tjZIDFgPG0ibNjlmaVmJSDSaM6QnPX0o6PY34/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.8333333333333334" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531739" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/80657ec6-8643-4a6e-9402-4d3d5283a43e/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHb8HnJRhI31f4rnoQSt7Bja3AJGWgA4K34EnSxg6J7KACGlVeC8ias7jxOicSJfeYSKx5S0vBGHE7uE9qPJEOXQB3GicBDsnTzKk/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.8333333333333334" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531740" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/9b31fa2d-7745-49d7-a20d-2fda07f8bb71/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;04 两层视角：任务意图与检索轨迹&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在会话划分的基础上，论文从两层视角刻画 agentic search 过程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;会话层面：Session Intent，即智能体在此次搜索中试图完成的任务类型；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;逐步步骤轨迹层面：Trajectory Move，即相邻两条查询之间的改写动作类型。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;4.1 三类 Session Intent&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作者沿用经典的 Web Search 目标分类，对多轮会话进行三类划分：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Declarative&lt;/strong&gt;：陈述型 / 事实与知识检索&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;典型问题包括 &amp;ldquo;是什么&amp;rdquo;&amp;ldquo;谁是&amp;rdquo;&amp;ldquo;列出&amp;hellip;&amp;hellip;&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. Procedural&lt;/strong&gt;：过程型 / 操作与步骤检索&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;包括 &amp;ldquo;如何做&amp;rdquo;&amp;ldquo;如何修复&amp;rdquo;&amp;ldquo;完成某项任务的步骤&amp;rdquo; 等。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;3. Reasoning&lt;/strong&gt;：推理型 / 分析与比较检索&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;包括 &amp;ldquo;为什么&amp;rdquo;&amp;ldquo;如何权衡&amp;rdquo;&amp;ldquo;多因素比较和规划&amp;rdquo; 等。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;标注方式为，将一个 session 内全部查询串联，交由 LLM 进行意图分类，并在样本上用另一模型交叉验证，标签可靠性较高。&lt;/p&gt;&lt;p&gt;统计结果表明，日志中以陈述型任务为主，其次是推理型任务，过程型任务比例相对较小。不同意图下，会话长度与检索配置表现出明显差异，例如过程型任务更倾向一次性拉取更多文档，而推理型任务的查询文本往往更长、前后变化幅度更大。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHOTBuTmN9vT70t4XSFUuiaia1atrSnYLibDvRwFr8PHLdWra0sicVEibqlYvMJ2icxF4Vw4kBNcrnCDdnTPVicHabzRprCyqRN4wL0LA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.40828402366863903" data-type="png" data-w="845" data-width="845" data-height="345" data-imgfileid="503531743" data-aistatus="1" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/0f413bb7-12fc-48b0-a9ac-a8bade2127f0/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml7smlix7wa" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 不通过目标分类下的Query 样例&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4.2 四类 Trajectory Move&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在单个会话内部，相邻两条查询之间的变化被划分为四种改写动作：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Specialization（专化）&lt;/strong&gt;：增加约束，下钻到更具体的条件或子范围；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Generalization（泛化）&lt;/strong&gt;：去除约束，将查询放宽到更一般的描述；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. Exploration（探索）&lt;/strong&gt;：在同一主题下转向新的侧面或子问题，例如从 &amp;ldquo;定位&amp;rdquo; 转向 &amp;ldquo;属性信息&amp;rdquo;；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4. Repetition（重复）&lt;/strong&gt;：语义基本不变的轻微改写或直接重试，例如改写语序、替换同义表达。&lt;/p&gt;&lt;p&gt;类似的这些标签基于 LLM 对查询对的判别结果获得，并结合向量相似度和检索结果重叠进行验证。整体来看，智能体呈现出明显的 &lt;strong&gt;&amp;ldquo;下钻偏好&amp;rdquo;（Drill-down Bias）&lt;/strong&gt;：专化与探索使用频率较高，泛化相对稀少，而在许多事实型会话的后期，重复动作显著增多，形成 &amp;ldquo;重试循环&amp;rdquo;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqH08qiaR9dLGHaP1NQQetPW7PO9XED3qic1umdtAn7Lu9TYyIcgI4WmI1fDb4jYjhnNib64G24qtklwleDBXHOpVicJ3RXlZUScpFg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5046296296296297" data-type="png" data-w="1080" data-width="2472" data-height="1248" data-imgfileid="503531748" data-aistatus="1" data-original-style="background-color: transparent;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/cfb6d568-e6a6-40b7-9e64-81d42e6aa377/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 案例1：事实型会话的后期形成&amp;ldquo;重试循环&amp;rdquo;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;另一方面，智能体也表现出一定的 &lt;strong&gt;&amp;ldquo;重置 - 再细化&amp;rdquo;（Reset-then-Refine）&lt;/strong&gt;模式，如下图可见，智能体先在一个宽泛主题上做专化（例如从 &amp;ldquo;拿破仑战役&amp;rdquo; 收窄到 &amp;ldquo;1796 年意大利战役&amp;rdquo;），随后通过去掉这些约束做一次泛化（得到更短、更宽泛的查询），再沿着另一个侧面重新专化（切换到 &amp;ldquo;埃及远征&amp;rdquo; 等新的细化方向）。&lt;/p&gt;&lt;p&gt;从查询长度的变化也能看出这一点，即专化通常会拉长查询，而泛化则会缩短查询。整体上，泛化在这里更像是一种轻量级回溯，用来在不同细化分支之间切换，而不是持续性地将查询维持在宽泛层级。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqF5ZicEJdQTXxpJfCouNxT48ylDyoPealLIvWjSSpMq3YSMv8CibjQzU44NvqZrhGAtaWia0eCFI2JiaBateJiaXclVU2cbibqs2ySg0/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.3055555555555556" data-type="png" data-w="1080" data-width="2856" data-height="872" data-imgfileid="503531750" data-aistatus="1" data-original-style="background-color: transparent;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/4657cfa1-c74d-430f-9b15-1c821e53e0c9/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 案例2：&amp;ldquo;重置-再细化&amp;rdquo;的步骤循环&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;05 Agent 对于搜索到信息的具体利用：CTAR 指标&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在多轮检索中，关键问题之一是：&lt;strong&gt;新的查询在多大程度上受到了既有检索信息的影响&lt;/strong&gt;。由于日志中没有点击、停留时间等显性交互信号，论文提出了一个简单的间接度量：&lt;strong&gt;CTAR（Context-driven Term Adoption Rate）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;计算方法概括如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对相邻查询对 q_k &amp;rarr; q_{k+1} 进行分词与停用词过滤；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;找出 q_{k+1} 中首次出现的 &amp;ldquo;新词&amp;rdquo;；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在上一步或累积至今的历史检索结果中检查这些新词是否以词面形式出现；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CTAR 即为 &amp;ldquo;在上下文中出现的新词占全部新词的比例&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这个指标带来的核心发现包括：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 整体 CTAR 超过一半&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;大约一半以上的新词可以在之前检索到的文档中找到。这表明，在相当多的步骤中，智能体并非完全凭空提出新的条件，而是从已有获取信息中采纳术语和约束。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 不同 Trajectory Move 的 CTAR 存在显著差异&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;专化和探索动作的 CTAR 明显高于平均水平，说明这两类改写更依赖已有文档信息；重复动作的 CTAR 则较低，通常对应表述上的微调或重试，而非基于新信息的策略调整。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 历史上下文具有额外贡献&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;只看上一轮检索结果时，CTAR 较低；将更早步骤的文档一并纳入后，CTAR 稳定提升，说明部分新词来源于更早的检索信息，智能体在一定程度上会 &amp;ldquo;回溯&amp;rdquo; 历史上下文。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0w0W7SHsYfuMIWbNRHH5ZGOJAvB8cuEFN1Im4RjiaMrjL8jJIfKT3OSg/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.3416666666666667" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531831" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/3a680b10-e38e-4463-bca1-ab98d22f31d4/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0FmPiap2GlHeuGpGbzd9doX1icVhZ7FlR6P6b6etZNj5qJopb6I1ubsgA/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.625" data-s="300,640" data-type="png" data-w="1080" data-imgfileid="503531832" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/9ece6fce-3c79-4eea-a624-d177f5f683a8/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;需要强调的是，CTAR 仅刻画 &amp;ldquo;新词在检索信息中的可追溯性&amp;rdquo;，并不直接等价于因果利用；但由于定义简单且易于解释，适合作为衡量 &amp;ldquo;是否参考检索上下文&amp;rdquo; 的粗粒度指标。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;06 对 Agentic Search 系统设计的启示&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于上述行为分析与 CTAR 指标，论文在结尾讨论了若干与系统设计直接相关的启示：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 重复动作可视为潜在 &amp;ldquo;停滞信号&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;在大量陈述型会话中，随着 Agent 步骤推进，重复改写的占比明显提高，其检索结果高度重叠且 CTAR 较低。这种模式可以视为系统进入 &amp;ldquo;原地重试&amp;rdquo; 的信号。在工程上，可以基于重复率、结果重叠度和 CTAR 等联合特征，检测并中断重试循环，强制触发泛化或探索策略，或切换到更高配置的工具链。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 检索预算应随任务意图与轨迹自适应调整&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;日志表明，现有许多智能体将检索深度 K 写死为有限几个固定值，在同一会话内几乎不做调整。然而，不同意图和轨迹状态对检索策略的需求显然不同，即过程型任务更依赖一次性较宽的文档覆盖，推理型任务则往往更需要多轮细化与验证。因此，更合理的设计是，先对会话意图进行识别，再结合当前轨迹（如是否处于探索阶段、是否陷入重试）动态调整 top-K、上下文长度与工具组合，而非采用全局统一配置。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 将 CTAR 等 &amp;ldquo;信息采纳率&amp;rdquo; 指标纳入系统监控&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;CTAR 在不同改写类型之间具有明显区分度，在专化和探索步骤中，CTAR 高时往往对应基于检索信息的实质推进，而重复步骤中 CTAR 较低则更可能反映策略停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;因此，可以将 CTAR 一类指标纳入系统的观测与调度逻辑：当长时间观测到 CTAR 偏低或在特定模式下急剧下降时，触发算法层或工作流层面的干预与重规划。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;07 结语：从第一份开源日志 &amp;nbsp;到 Agentic IR 的 &amp;ldquo;常识层&amp;rdquo; 认识&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;整体来看，这篇工作完成了三件具有基础设施意义的事情：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 提供首个开源的 Agentic Search 行为日志数据集&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于 DRGym 平台采集并清洗的 1400 万 + 请求、约 400 万会话，在经过匿名化处理后，在 Hugging Face 平台公开，为后续研究提供了可复现的行为数据基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 提出面向 Agentic Search 的 &amp;ldquo;意图&amp;ndash;轨迹&amp;ndash;信息利用&amp;rdquo; 分析框架&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过三类 Session Intent、四类 Trajectory Move 以及 CTAR 指标，从结构和内容两个维度刻画智能体搜索过程，为后续的行为建模、策略比较和训练目标设计提供了分析工具。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 将若干经验性观察固化为可量化的设计建议&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具体包括，将重复改写视作停滞信号、依据任务意图与轨迹模式自适应检索预算，以及通过检索信息采纳率监控智能体是否真正 &amp;ldquo;读取并利用&amp;rdquo; 检索结果。&lt;/p&gt;&lt;p&gt;对于从事信息检索与智能体系统研究的读者，这份数据与框架为理解和改进 agentic search 提供了新的切入点；对于工程实践者，则可以据此审视现有系统的行为模式，并据实引入新的监控和控制机制。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;作者简介：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本论文第一作者为卡内基梅隆大学计算机学院语言技术研究所硕士研究生 Jingjie Ning，研究方向聚焦信息检索、DeepResearch、Query 理解与强化、推荐系统 Benchmark 等工作。Jingjie Ning 师从 Jamie Callan 教授及 Chenyan Xiong 教授。在卡内基梅隆大学前，Jingjie 曾在腾讯任职 Senior Data Scientist。个人主页：&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fethanning.github.io" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml8v2sli-26q852" href="https://mp.weixin.qq.com/s/uRWHomOqZmOVfVRkx1l2fw"&gt;https://ethanning.github.io&lt;/a&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGKeFj31FK7zoo8FiaEuic8gp4tY3ic2qPO1d4feyHua5oInT1XAFBZjtqS9ZPiaGRpO0MHribMyiadr92Oj9KGgOFpCwy3iadgGj4uaE/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="1" data-type="png" data-w="1080" data-width="2730" data-height="2730" data-imgfileid="503531762" data-aistatus="1" data-original-style="background-color: transparent;width: 260px;height: 260px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/08f66b9f-a8ac-4dcc-aeb3-ebcd84cd4e47/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 40%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>生成式科学智能的新标杆：IntelliFold 2新近发布并开源，主要指标实现全面领先</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:32:15 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在 GenAI 带动的 &amp;ldquo;生成式科学智能（Generative Science）&amp;rdquo; 的新浪潮中，生物基石模型始终是广受关注的热门领域；自然界的生命语言（序列、结构）与人类符号语言呈现类似的序列化特征，但其背后蕴含严苛的物理约束与生物演化逻辑，长期以来难为人类完全破解，同时因其对于人类生产、生活的关键重要作用，使生物基石模型成为领域内广受关注的 &amp;ldquo;皇冠上的明珠&amp;rdquo;。&lt;/p&gt;&lt;p&gt;生物基石模型的关键价值，在于其能从海量信息中借助 Transformer 等 GenAI 架构充分打开隐空间，挖掘出人类难以感知归纳却又大有所用的 &amp;ldquo;生命语法&amp;rdquo;；DeepMind 旗下的 AlphaFold 系列研究无疑是其中颇具开创性的重大突破。&lt;/p&gt;&lt;p&gt;及至 AlphaFold 3 发布后，其现象级的突破性进展与巨大产业潜力有目共睹，成为毫无争议的行业典范。随后，围绕结构预测及与之密切关联的从头设计应用，全球相继涌现出一批以 GenAI 大模型寻求突破的代表性成果（Chai Discovery、Boltz、OpenFold 等），明星团队、大额融资及至大厂并购（Evolutionary Scale）等积极消息此起彼伏，市场热度持续上涨；但及至 2025 年底，仍鲜有新发模型真正做到与 AlphaFold 3 匹敌［1］。与此同时，受限于开源程度、效能上限、部署便利性等，生物基石模型的进一步普及应用仍面临高性能、高可用的需求端真实考验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHYlsJo6RwJC1VQGZcq8cTpokSOSRXvVL20ibPItXfZ89iaicvuI4a2Zt6Z3HGlnnEbcdjfbibhEqBuegKIz0xHUosXyUkDK7veYnU/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.36666666666666664" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532502" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/44ca97c9-1e13-4ca1-886c-baf2d6605e10/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在生物基石模型的全球激烈竞逐中，IntelliGen AI 于本周末正式发布了 IntelliFold 2，这是继 2025 年 7 月 IntelliFold 首版发布后的一次重大升级 [2]。&lt;/p&gt;&lt;p&gt;通过从 GenAI 内在逻辑出发的诸多创造性思考与重新设计，以及诸多工程化模块的突破性创新，IntelliFold 2 成功实现了高效能与多功能的全面突破，在 FoldBench 基准测试中给出了超越 AlphaFold 3 主要指标的优秀表现；在&lt;strong&gt;抗体 - 抗原相互作用、蛋白 - 配体共折叠&lt;/strong&gt;等影响药物研发成败的关键任务上，IntelliFold 2 也树立了全新的行业性能标杆，为智能药物设计提供了可控、高精度的工业级引擎。&lt;/p&gt;&lt;p&gt;免费试用地址：https://server.intfold.com/&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心突破：在关键性能指标上全面超越 AlphaFold 3&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFF1dXBuGmrOkDA3CCZ2s8cbgdicvMXP7yWYLDVAbhhGV2TmALgj2uXpGDnfjWXmA0lzbPphHGI0DaKcJqiaz1wLpp0EGERq8e2s/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.39444444444444443" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532503" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/725797a0-fe60-4795-a1e0-b010979fad64/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在生命科学领域，复合物共折叠（Co-folding）预测一直是核心挑战。IntelliFold 2 在两个关键任务中取得显著进步，超越 AlphaFold 3、Chai、Boltz 等主流模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;抗体 - 抗原相互作用&lt;/strong&gt;：以 DockQ &amp;gt; 0.23 为成功标准，IntelliFold 2-Pro 达到 58.2% 的成功率（报告的 v2 模型结果为 5 次运行的平均值，最高分数达 63%），相较 AlphaFold 3 的 47.9% 展现出显著的性能进步。这一逾&lt;strong&gt; 10 个百分点&lt;/strong&gt;的提升，意味着在抗体筛选阶段，模型能够更稳健地迈过可用性门槛，进而减少湿实验试错成本，精准锁定高潜力候补分子。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;蛋白质 - 配体共折叠&lt;/strong&gt;：在该任务中，IntelliFold 2-Pro 同样以 &lt;strong&gt;67.7% &lt;/strong&gt;的成功率击败 AlphaFold 3（64.9%）及 Boltz 等其他主流模型 。这一精度的稳步提升对于基于结构的小分子药物设计至关重要，不仅有助于提高虚拟筛选的命中率，也为后续亲和力预测、别构靶点捕捉等具体任务打下了坚实基础。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;IntelliFold 2 的发布，通过&lt;strong&gt; v2-Flash、v2&lt;/strong&gt; 和 &lt;strong&gt;v2-Pro&lt;/strong&gt; 三个版本，精准覆盖了从学术微调到工业落地的多元需求。目前已正式开源 &lt;strong&gt;v2-Flash&lt;/strong&gt; 与&lt;strong&gt; v2 &lt;/strong&gt;两个版本。其中，v2 版本作为目前精度最高的开源模型之一，将为全球科研人员提供性能卓越的结构预测底座，大幅降低高性能生物计算的门槛 。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;智能&amp;rdquo; 为径，开启范式迭代：四大创新重塑新一代基座&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;IntelliFold 2 的核心突破，源于对 &amp;ldquo;信息表征能力&amp;rdquo; 和 &amp;ldquo;硬件计算特性&amp;rdquo; 的重新思考。针对共折叠（Co-folding）这一颇具挑战性的核心任务，团队并未因循传统的 &amp;ldquo;强数据堆砌&amp;rdquo; 或单一的 &amp;ldquo;专家经验注入&amp;rdquo; 路径，而是寻求生命分子微观交互规律与 AI 计算范式的深度融合，以更好的实现 &amp;ldquo;底层模型智能&amp;rdquo; 对 &amp;ldquo;下游任务智能&amp;rdquo; 的赋能。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqEK383lVmGZNPM5WVwfFNky016jMa4IFLwd16GT08GPsNnHyz3hfA4xdnCDSgcfu1jficGgD3526m5GkiahtzHxqFTNph8MbsNwA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.912962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532520" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b8ad40ac-1b8e-4b32-9c3b-158fdf3345c9/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scaling Law 在生物计算的验证&lt;/strong&gt;：通过 Latent Space Scaling 技术，团队扩展了 PairFormer 模块的特征维度，不仅增强了模型对复杂生物大分子相互作用的表征能力，更大幅优化了计算效率，将 GPU 的计算利用率（MFU）从 5% 提升至 &amp;nbsp;30%。这种算力投入带来了较高的边际效益，以可接受的时间成本，换取了对复杂结构表征能力的显著提升。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;原子级别的精细感知&lt;/strong&gt;：针对抗体 CDR 区域等柔性、易变的结构，传统粗粒度建模方法往往难以精确建模。IntelliFold 2 引入了随机原子级 Tokenization，让模型在训练中学会捕捉细粒度的原子接触模式。这种 &amp;ldquo;显微镜&amp;rdquo; 级别的感知力，弥补了宏观表征在局部细节上的缺失，成为其在抗体预测任务中领先的关键。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;强化学习引导的采样优化&lt;/strong&gt;：为了解决扩散模型采样在结构预测任务中的不稳定性，团队引入基于 PPO 算法的强化学习进一步优化扩散模型。通过将扩散过程建模为随机策略并进行微调，IntelliFold 2 有效校正了采样轨迹的偏移，减少了不准确结构的生成，为实际应用提供了可靠的工业级预测结果。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;难度感知的损失函数&lt;/strong&gt;：针对复合体中链间距离预测等高难度区域，IntelliFold 2 采用了难度感知的损失函数。通过动态调整损失函数权重，引导模型专注于那些容易被忽略的长尾困难区域。这种优化改善了训练收敛稳定性，提升了模型在处理复杂多链复合物时的稳定性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;IntelliFold 2 的技术迭代体现了生成式科学的研发新范式，它既不依赖传统专有数据注入或专家调优，也非单纯的对 Transformer block 堆叠提升参数量，而是结合对领域科学和模型的综合深入理解，找到好的 scaling 的角度，遵循智能涌现的底层逻辑，求解稳定可泛化的有效性能提升；否则，单纯堆砌算力和参数量或者机械地对跨领域先验经验进行迁移放大依然达不到好的效果。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;灵活可用：从可控预测到应用问题解决的领先实践&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;IntelliFold 2 不仅仅实现了精度的提升，还体现了对于可用性的强调：通过通用基础模型、轻量适配器（LoRA）与任务引导，形成闭环推理链路，连接结构预测与诸多功能发现。这超越了传统单模型 - 单任务开发逻辑，以统一基石模型 - 多任务的架构，开创性拓展了结构模型的效能场景与价值链条。同时，这一架构设计也允许领域科学家注入假设、约束和数据，为研究人员进行可控、精准且具有预见性的任务开发打下了创新支点。藉由此次基石模型更新，IntelliFold 2 在蛋白 - 配体亲和力预测、别构靶点虚筛等应用问题上充分展示出显著更优且卓有价值的行业应用潜力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 亲和力预测（小分子）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;IntelliFold 2 延续了初版的高保真亲和力预测能力，不但在 PDBBind 数据集性能优于 Boltz-2 及模型初代版本，还在更接近产业真实任务的复杂 In-house 数据集上亲和力预测表现（Pearson r = 0.60）显著领先 Boltz-2 等开源模型 (0.38)。结合 IntelliFold 特有的别构和表位分析能力，已经形成 &amp;ldquo;预测结构&amp;rarr;估计结合&amp;rarr;指导设计&amp;rdquo; 的完整闭环，并在真实案例中帮助合作伙伴在分子筛选任务上实现了效率与精度的大幅提升以及高难靶点的突破进展。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGjVuxEt4o8hTr4GqciaSVguLLfr63ibczZUSlFYFaicnJ4KQrODdlERzuBbwapXAb4nsdwrPpJGsMgRyhewEZ8OFt9DBopyFaPW0/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.0308151093439364" data-s="300,640" data-type="png" data-w="1006" type="block" data-imgfileid="503532519" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7ffc3d15-3221-44a0-bb09-767ba2391b60/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 动态捕捉靶点（别构效应）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在生命的微观世界里，蛋白质的功能并非由静态结构决定，而是源于其在复杂能量景观中的动态构象调控。其中，别构效应是最具代表性的机制之一：只需在蛋白质的一端轻轻拨动 &amp;ldquo;开关&amp;rdquo;，就能通过构象的连锁反应控制另一端的生化活性。虽然这为药物研发提供了巨大的想象空间，但由于其高度的动态性和复杂性，寻找此类药物靶点如同大海捞针。&lt;/p&gt;&lt;p&gt;IntelliFold 2 构建了一套 &amp;ldquo;微观原子精度&amp;rdquo; 与 &amp;ldquo;宏观构象一致性&amp;rdquo; 协同的表征体系：在微观层面，它能以原子级分辨率捕捉氨基酸侧链的细微偏移；在宏观层面，则能维持蛋白质整体拓扑结构的物理合理性。以此通用表征为基石，通过针对性地引入别构数据进行高效微调，模型进一步习得了解析复杂别构效应的能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFtsFG7hXAqB4Op31riboAKQiaTGAHrVWUJZ3bcGFI8pcyWP2b4Va2Db0w4GJs4xichqmLHTYOwfibHmrlAydoYgfNXvqykkY48vBE/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.6472222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532506" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/1899b5ba-77b2-4f8d-949a-7e8f12a2cd97/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;借助这一特殊机制的引入，IntelliFold 2 实现了在真实任务上别构位点筛选能力，并作为其动态靶点预测能力的首个应用案例；后续成功帮助合作伙伴实现了基于别构结合的候选分子重新筛选，最终以特定靶点优秀成功率（高至 67%）实现多问题全部命中，并成功转化至后续环节 asset 开发。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 结语 &amp;mdash;&amp;mdash;&amp;ldquo;Answer ball&amp;rdquo; from the East&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在语言智能（Language Intelligence）和有形智能（Physical Intelligence）两大领域，近年来的新一波 GenAI 浪潮持续带来猛烈冲击与快速变革迭代，我们正亲历着一场将空前改变生产与生活的智能大变革。围绕着大语言模型、具身智能、自动驾驶等赛道与产业的发展热潮，我们欣喜地看到全球科技竞争的高速迭代和诸多优秀成果的不断涌现，中国科技力量持续追赶甚至并驾齐驱；&lt;/p&gt;&lt;p&gt;然而在关联诸多领域科学和生产力前沿的 &amp;ldquo;生成式科学智能&amp;rdquo; 领域，尤其是在相对高光的生命科学方向，不同于北美市场后 AlphaFold 3 时代模型探索的百花齐放，目前在全球其他地区能够直接可比的基模成果并不多见。受限于生物基石模型的高算力消耗门槛、LLM 与生物背景的复合人才需求，大中华地区也更多是 Biomap、百度、字节跳动等互联网大厂背景团队在持续参与并取得阶段性成果。&lt;/p&gt;&lt;p&gt;IntelliFold 2 的发布，及其模型 SOTA 表现与开源举措，无疑是一记令人振奋的 &amp;ldquo;Answer ball&amp;rdquo;。这表明围绕生成式科学的全球竞赛，新兴团队仍然有机会加入竞争，甚至取得领先的卓越表现。&lt;/p&gt;&lt;p&gt;据悉，得益于强大的基石基础模型进展，IntelliGen AI 首版针对 Binder 和抗体的从头设计（De novo design）模型也即将于 2026 年中适时发布，进而实现预测与生成的统一，进一步加速药物研发与生命科学探索的进程。&lt;/p&gt;&lt;p&gt;未来，围绕生物基石模型的全球竞争无疑将更加激烈；希望新一代际的创新者们能够实现 &amp;ldquo;科技有效&amp;rdquo;，驱动 &amp;ldquo;智能药物设计时代&amp;rdquo; 的真正到来。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;［1］Xu, S., Feng, Q., Qiao, L. et al. Benchmarking all-atom biomolecular structure prediction with FoldBench. Nat Commun 17, 442 (2026). https://doi.org/10.1038/s41467-025-67127-3&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;［2］https://github.com/IntelliGen-AI/IntelliFold&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>模型「漂移」新范式，何恺明新作让生成模型无须迭代推理</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:28:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑 | 冷猫&lt;/section&gt;&lt;p&gt;训练一个生成模型是很复杂的一件事儿。&lt;/p&gt;&lt;p&gt;从底层逻辑上来看，生成模型是一个逐步拟合的过程。与常见的判别类模型不同，判别类模型通常关注的是将&lt;strong&gt;单个样本映射到对应标签&lt;/strong&gt;，而生成模型则关注&lt;strong&gt;从一个分布映射到另一个分布&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;从大家最熟悉的扩散模型说起，扩散模型，包括一些基于流的对应方法，通常通过微分方程（随机微分方程 SDE 或常微分方程 ODE）来刻画从噪声到数据的映射。&lt;/p&gt;&lt;p&gt;但训练扩散模型是一件费时费力的事情，因为其核心计算过程是一个&lt;strong&gt;迭代过程&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;为了尽可能提升生成模型的效率，大量工作致力于&lt;strong&gt;减少扩散的步数&lt;/strong&gt;。比较有代表性的一类是蒸馏方法，将一个预训练的多步模型蒸馏为单步模型。另一类研究则尝试从零开始训练单步扩散模型。例如：&lt;/p&gt;&lt;p&gt;变分自编码器（VAE）通过优化证据下界（ELBO）进行训练，该目标由重建损失和 KL 散度项组成。在采用高斯先验时，经典 VAE 本身就是一步生成模型。然而，在当今主流应用中，VAE 往往使用由扩散模型或自回归模型学习得到的先验，此时 VAE 更多地充当分词器的角色。&lt;/p&gt;&lt;p&gt;正则化流（Normalizing Flows, NFs）学习从数据到噪声的映射，并通过最大化样本的对数似然进行训练。这类方法要求模型结构可逆，且能够显式计算雅可比行列式。从概念上看，正规化流在推理阶段是一步生成器，生成过程通过网络的逆映射完成。&lt;/p&gt;&lt;p&gt;这些方法仍然无法摆脱持续迭代的训练过程的桎梏。&lt;/p&gt;&lt;p&gt;相比之下，何恺明研究团队的最新工作提出了一种在概念上完全不同的范式「&lt;strong&gt;漂移模型（Drifting Model）&lt;/strong&gt;」，不依赖扩散模型与流模型中常见的微分方程表述，天然支持一步推理，并构建了一种训练目标，使得神经网络优化器能够直接推动分布的演化。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHCefYwWuZqXicT2N35I5Q3DVWTY8UmBJNHU9g3lRET9cR4azUPVL1WYcN5icicTg5icaFoJqqVcgfxGbmW4UqBkqH0Fazs0R71V5M/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.21851851851851853" data-type="png" data-w="1080" data-width="1608" data-height="352" data-imgfileid="503531989" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/0d454273-97ef-41fd-b2fa-1b63b3fe7b39/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Generative Modeling via Drifting&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2602.04770v1&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;漂移模型&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGamjmz8oT4Hhp7LOd0kgKcbClUbGhp6RqX9MIa8tMjHbdbm59YkuhaGtqtfEYE0icQ2ibRyqwK3MmH4wwUSibY9eL8s6Me5yMpkk/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.2537037037037037" data-type="png" data-w="1080" data-width="1606" data-height="408" data-imgfileid="503531993" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/944c30d9-8acc-41b8-86bf-92b6f304870b/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 漂移模型训练示意图&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在本文中，研究团队提出了一种新的生成建模范式 &amp;mdash;&amp;mdash; &lt;strong&gt;漂移模型&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;漂移模型的核心特征在于：&lt;strong&gt;推送（pushforward）映射在训练过程中不断演化，从而不再需要迭代式的推理过程。映射 f 由一个单次前向、非迭代的网络&lt;/strong&gt;来表示。由于深度学习中的训练过程本身就是迭代优化的，因此可以自然地将其视为：通过不断更新 f，来演化推送分布 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHicVRmcwbdZy81fgYCSjcmo9lsXFAI5TpLQC8fQntfiaGjLR7J8XmQJ0d3WoejZY3ibyLUiaKNWt5Q7aeTbJwgeZbteq7Rha45Yw4/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3619047619047619" data-s="300,640" data-type="png" data-w="105" type="block" data-imgfileid="503531996" data-aistatus="1" data-original-style="width: 67px;height: 24px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/2dc5f7fd-188b-403d-b763-83a04501c5c2/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dii" style="width: 7.15%;"&gt;。&lt;/p&gt;&lt;p&gt;如图所示，网络 f 执行一次&lt;strong&gt;推送（pushforward）操作&lt;/strong&gt;：&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGKUIB60RKULVGLtG17aMbLIzx3IoMCczEYic3AhYpHbGVmTv5dGicpvFe1weB6zcfBR2sl4l4mbMML2fgIk5mXPyia6xF9TF8micA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.2641509433962264" data-s="300,640" data-type="png" data-w="159" type="block" data-imgfileid="503531998" data-aistatus="1" data-original-style="width: 92px;height: 24px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b8da6f2b-6f10-4275-810f-42141d42404c/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 11.64%;"&gt;, 将先验分布&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGxclJalRmEQe8em0UUXbQDsCibJb6icme9FrvUFiaJfBNUbhOkzOzMyhadibRTz7tu7LiaA0sRgDQHibwgd9VeDUqD4F0ZpgB1Wc0Ws/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.48" data-s="300,640" data-type="png" data-w="75" type="block" data-imgfileid="503531999" data-aistatus="1" data-original-style="width: 58px;height: 28px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/67e5649b-a064-48d2-b9e4-4dae0f585211/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dii" style="width: 5.96%;"&gt;（例如高斯分布，图中未显示）映射为推送分布 q（橙色）。训练的目标是使该分布逼近真实数据分布 &lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFeoDJhPGODn2rJ86OlX46ka14JhLvbqOiaXcQjdwSJjpWiciaTnZKcwE1cdxncMxKAh6AnkHib3Jobmu0KibaWy8icCIXwjyfPfUX8Y/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5072463768115942" data-s="300,640" data-type="png" data-w="69" type="block" data-imgfileid="503532000" data-aistatus="1" data-original-style="width: 49px;height: 25px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/bd11310e-607e-464e-992c-b50686bacf82/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dii" style="width: 6.23%;"&gt;（蓝色）。&lt;/p&gt;&lt;p&gt;随着训练过程的迭代，会得到一系列模型 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFwLdZhwRDk3iaYPn7utSNXticu6ica8u9azzia6F3pI3AKlIORcN5xuBxTVe604X76gbphBwq3WxrOdRQPxXmHKo4xQeLicy7C6ffI/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.7894736842105263" data-s="300,640" data-type="png" data-w="57" type="block" data-imgfileid="503532001" data-aistatus="1" data-original-style="width: 30px;height: 24px;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/19d3a08e-1a4e-449f-966e-84d8422fde5d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dii" style="width: 3.85%;"&gt;，对应地也产生一系列推送分布&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEeh3kicvo6okic57fsHgpibLMBic75vPcRCllw5KnvYFS4uMZlynPrfInjCgLdqYic6kicURblxVQPdibZuP2qHFHcBfaAU1GqOgIMJE/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.75" data-s="300,640" data-type="png" data-w="60" type="block" data-imgfileid="503532002" data-aistatus="1" data-original-style="width: 36px;height: 27px;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/2f400384-a95c-4ac0-9098-6d9be0db6b67/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 4.31%;"&gt;&amp;nbsp;。&lt;strong&gt;漂移模型的核心关注点在于：训练过程中这一推送分布的演化轨迹&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;为了驱动训练阶段推送分布的演化，研究团队引入了一个漂移场（drifting field）来控制样本的运动。&lt;strong&gt;该漂移场依赖于生成分布和数据分布。当这两个分布一致时，根据定义漂移场为零，系统达到平衡态&lt;/strong&gt;，样本不再发生漂移。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFRp1FCdUQQp3BCvfskicPNtwuTfGqLNZQ1saLDgtoZ5vyzqGZqZmVyOug35xJvKDBn4qaFzLGHVMdAefz8bveCIQuES8ibIUbZw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.9765625" data-type="png" data-w="1024" data-width="1024" data-height="1000" data-imgfileid="503532003" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/6a52db01-32f7-408f-b8f8-83b39f30b29c/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;漂移场示意图：生成的样本 𝐱 （黑色）根据向量 𝐕=𝐕p+&amp;minus;𝐕q&amp;minus; 进行漂移。这里， 𝐕p+ 是正样本（蓝色）的均值偏移向量， 𝐕q&amp;minus; 是负样本（橙色）的均值偏移向量。 𝐱 被 𝐕p+ 吸引，同时被 𝐕q&amp;minus; 排斥。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;基于这一表述，研究团队提出了一种&lt;strong&gt;简单的训练目标&lt;/strong&gt;，用于最小化生成样本的漂移。目标函数如下：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGRpn0G7kePGXzib2icoic58p0libdclrA1N1Z7zzhJ6xHj9WeMonOG5ILd7MobXqL9uia1h40HS7btsDdPuficvkxm9HPBGNic0y7kBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.15347721822541965" data-type="png" data-w="834" data-width="834" data-height="128" data-imgfileid="503532004" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/01352163-6a49-45db-89fb-74de0a526a1b/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;该目标会诱导样本产生移动，并通过迭代优化过程（如 SGD）推动底层推送分布的演化。&lt;/p&gt;&lt;p&gt;在这里，我们不再描述漂移模型的细节，感兴趣的读者请参阅原论文。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;实验验证涵盖多个领域和规模，为该方法的有效性提供了全面证据。&lt;/p&gt;&lt;p&gt;漂移模型天然支持&lt;strong&gt;单步生成（1-NFE）&lt;/strong&gt;，并在实验中展现出强大的性能。在 ImageNet 256&amp;times;256 上，在标准的&lt;strong&gt;潜空间生成协议&lt;/strong&gt;下，研究团队获得了 &lt;strong&gt;1-NFE FID = 1.54&lt;/strong&gt;，在单步生成方法中取得了新的 SOTA，且该结果即便与多步的扩散模型相比也依然具有竞争力。&lt;/p&gt;&lt;p&gt;进一步地，在更具挑战性的像素空间生成协议（即不使用潜变量）下，本文方法达到了 1-NFE FID = 1.61，显著优于此前的像素空间方法。这些结果表明，漂移模型为&lt;strong&gt;高质量且高效率的生成建模&lt;/strong&gt;提供了一种极具潜力的新范式。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFOJv1j2dicSsu1CicCoWqjwG8uKxj08K6iaB2ianSZL5YSgUCUpJOKGPdfhyPmPVCHpAcjmMicHqt1fWfyvicrYdd8QkEDicCVEnJpjY/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.7833333333333333" data-type="png" data-w="1080" data-width="1218" data-height="954" data-imgfileid="503532005" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/b92d4275-fbce-4437-8208-356b3504066c/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;上图展示了一个二维玩具示例：在三种不同初始化条件下，生成分布 q 在训练过程中逐步演化，并最终逼近一个&lt;strong&gt;双峰分布&lt;/strong&gt; p。在这一玩具实验中，本文的方法能够在&lt;strong&gt;不出现模式坍塌的情况下逼近目标分布。即使在 q 被初始化为坍塌到单一模态&lt;/strong&gt;的状态（图中下方所示）时，这一性质仍然成立。&lt;/p&gt;&lt;p&gt;这为本文的方法为何对模式坍塌具有鲁棒性提供了直观解释：当 q 坍塌到某一个模态时，目标分布 p 中的其他模态仍会对样本产生 &amp;ldquo;吸引力&amp;rdquo;，促使样本继续移动，从而推动 q 持续演化。&lt;/p&gt;&lt;p&gt;该实验展示了对多模态目标分布的稳健收敛，同时避免了模式崩溃，即使在从崩溃状态初始化的情况下。&lt;/p&gt;&lt;p&gt;此外，研究团队在&lt;strong&gt; ImageNet 256&amp;times;256 &lt;/strong&gt;上评估了所提出的模型。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGicecAZImo86ic9iaRkUNh3dYPMk1Qia4ywVKZJatDA6Z4Ne6Zl3JWlmicGcQDAvq2gYn2wvQJkWNJlNm2yCYUg2cAcm485VdcMJOM/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.575925925925926" data-type="png" data-w="1080" data-width="1250" data-height="720" data-imgfileid="503532006" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/3677cab6-5fa6-47fe-aaf4-8b9c8907510c/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在表 1 中，研究团队进行了一个&lt;strong&gt;破坏性消融实验&lt;/strong&gt;，刻意打破这一反对称性设定。结果表明：满足反对称性的情况（即默认设置）表现良好，而其他破坏该性质的设定则&lt;strong&gt;性能灾难性崩溃&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;本文的方法通过采样&lt;strong&gt;正样本&lt;/strong&gt;和&lt;strong&gt;负样本&lt;/strong&gt;来估计向量场 V 。在表 2 中，研究团队在固定训练 epoch 数和固定 batch size B 的条件下，研究了正样本数 &amp;nbsp;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqEWK2MFicmPMxHzPyj5Z9Q2YRd1PhamibyTQGXpx8aoEibs4t05aIcsJq9oXl8Vfo0CmjajQsQOE8zL6q9cNGET4ULd9gNEqU1qOc/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.7368421052631579" data-s="300,640" data-type="png" data-w="57" type="block" data-imgfileid="503532007" data-aistatus="1" data-original-style="width: 39px;height: 29px;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/882fd64c-efb4-46ea-8694-a8fe1e45ef11/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dii" style="width: 3.76%;"&gt; 和负样本数 &lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGWYZzANyM7pgz0xhHC966VYIHsDYFblhLbVhxYEjNbqB0CdrndjDf0sRgdCpNtbic2F7kLqHkr8jqqULAY5jNp6YTd19dyUww4/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.75" data-s="300,640" data-type="png" data-w="60" type="block" data-imgfileid="503532008" data-aistatus="1" data-original-style="width: 43px;height: 32px;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/b20cd40e-598a-46df-a242-25ea9675f41f/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dii" style="width: 3.76%;"&gt;的影响。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqE5A5uXcZ80JTo68GiaaqWHQEliboFpwK6lPlVURib6oozH66pMiaHABzvSUIfNWJ9La3OlRE3NLYiaLbics7odnKl8djJU8syibqgQicI/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5453703703703704" data-type="png" data-w="1080" data-width="1236" data-height="674" data-imgfileid="503532009" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/8a0bd706-d630-4df9-ad98-eb779d01e9ff/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;表 2 显示，更大的 &lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHjtZRcZE6UKcxibI1ErIwQAngQIKvpzyjLzekmYFeXesxwaNuQK6Lq5l1uqlXDkfMvV4CaAU2a4Wc6njpCU8MONhMenIYljicvI/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.6140350877192983" data-s="300,640" data-type="png" data-w="57" type="block" data-imgfileid="503532010" data-aistatus="1" data-original-style="width: 41px;height: 25px;" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/84e5e78c-523d-44d1-b284-0bffe5232150/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dii" style="width: 3.76%;"&gt; 和  &lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqEU5pgX19blgCt2MmQp8nLpozyTpl13mial6TxpeQib81ncZvMmScQbZ5Oib9OekjKNZe2XCBgVQIicY1icctyP3YQkIf9yuKSrI2RA/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.631578947368421" data-s="300,640" data-type="png" data-w="57" type="block" data-imgfileid="503532011" data-aistatus="1" data-original-style="width: 35px;height: 22px;" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/39ae46b1-2202-4376-aad9-f3e0159f7ca7/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dii" style="width: 3.85%;"&gt;&amp;nbsp;能够带来更好的效果。更大的样本规模有助于更准确地估计 V ，从而提升生成质量。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqG5M5mwhI3C4aH5cbt86Ptl0evPVA41QXOv89wWLw3yj0cfbgOhL7YyRNGMicaEG2ah1gib9biaehJ0juBIS4Oyp842ibLVQxkRCgM/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.7833333333333333" data-type="png" data-w="1080" data-width="1220" data-height="956" data-imgfileid="503532012" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/cc540c77-f9cc-47a8-8c85-c279498ad834/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;表 3 的对比结果表明，特征编码器的质量起着至关重要的作用。&lt;/p&gt;&lt;p&gt;研究团队还训练了更强的模型变体，并在表 4 中进行了汇总；与以往方法的对比见表 5。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHU9PqBvKnChHTZLOOenricODOEczIx1XqwgG58vn7LkYJiczicrxODLaLmN2sZdUxWrKVTeq18ibXJW9PoZkcT3xeibdaug9mquQog/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-ratio="0.42777777777777776" data-type="png" data-w="1080" data-width="1244" data-height="532" data-imgfileid="503532013" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/0b3df81d-7817-4fa1-a382-0f95bbd356a5/640.png" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFibpaFcnxbtfOTanJx7qGmg1QGUG05K4GkBkm7knOjTicDE3Fb1gOvjwc2TrcX8NBjHkD71XgKo3N5savJqia0FBCBFAFywDJibug/640?wx_fmt=png&amp;from=appmsg#imgIndex=20" data-ratio="0.8888888888888888" data-type="png" data-w="1080" data-width="1276" data-height="1134" data-imgfileid="503532014" data-aistatus="1" data-original-style="null" data-index="22" src="https://image.jiqizhixin.com/uploads/editor/d4938783-03e0-4fa0-bfe6-178b0e9cf90e/640.png" alt="图片" data-report-img-idx="20" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;本文方法在原生 1-NFE 生成条件下&lt;strong&gt;取得了 1.54 的 FID&lt;/strong&gt;，超过了此前所有基于扩散 / 流轨迹近似的 1-NFE 方法。值得注意的是，本文中的 Base 尺寸模型即可与此前的 XL 尺寸模型相竞争。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFhTwSyJHvSM88s7chOkO9BEk5Nshr9lVn1V1oHiaZjN4V7kpOClEicVKtBQlHeiaGYVvSXsYdXcMuaruADI6Rr6V3coySlWT4asw/640?wx_fmt=png&amp;from=appmsg#imgIndex=21" data-ratio="1.0138888888888888" data-type="png" data-w="1080" data-width="1262" data-height="1280" data-imgfileid="503532015" data-aistatus="1" data-original-style="null" data-index="23" src="https://image.jiqizhixin.com/uploads/editor/401ddf49-b590-47cd-9e11-1685ace94a3b/640.png" alt="图片" data-report-img-idx="21" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;表 6 对比了不同的像素空间生成器。本文的&lt;strong&gt;一步像素空间&lt;/strong&gt;方法取得了 &lt;strong&gt;1.61 的 FID&lt;/strong&gt;，在性能上超过或可与此前的多步方法竞争。与其他一步像素空间方法（如 GAN）相比，本文的方法仅使用&lt;strong&gt; 87G FLOPs &lt;/strong&gt;即可达到 &lt;strong&gt;1.61 FID&lt;/strong&gt;；而&lt;strong&gt; StyleGAN-XL&lt;/strong&gt; 则需要 1574G FLOPs 才能达到 &lt;strong&gt;2.30 FID&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;漂移模型解决了生成式 AI 中质量与效率之间的基本权衡问题。传统的优质模型，如扩散模型，取得了优异的结果，但在推理过程中计算成本高昂。这项工作表明，在大幅降低计算需求的情况下，可以达到相似的质量，有可能使以前受推理速度限制的实时应用成为可能。&lt;/p&gt;&lt;p&gt;该方法还强调了生成建模中鲁棒特征表示的重要性。预训练特征提取器的关键作用表明，自监督学习的进步直接有益于这一范式，在表示学习和生成之间建立了协同效应。&lt;/p&gt;&lt;p&gt;该方法在不同领域（从高分辨率图像合成到复杂的机器人控制）的成功表明，通过漂移场进行分布演变的核心原理可能广泛适用于各种生成任务，为高效生成建模开辟了新的研究方向。&lt;/p&gt;&lt;p&gt;更多细节，请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>登顶Hugging Face论文热榜，LLM重写数据准备的游戏规则</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:21:55 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/d78ce9b4-4ac1-4d85-a77a-5a72e644195f/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在企业级系统中，数据团队普遍面临一个困境：模型迭代飞速，但数据准备的「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;老旧管道」却愈发沉重。清洗、对齐、标注&amp;hellip;&amp;hellip; 这些工作依然深陷于人工规则与专家经验的泥潭。您的团队是否也为此困扰？&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;数据格式五花八门，正则表达式越写越多，却总有意想不到的「脏数据」出现&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;跨系统表结构不一致，对齐逻辑复杂，人工映射耗时耗力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;海量数据缺少标签和语义描述，分析师「看不懂、用不好」&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这背后是数据准备这一经典难题 &amp;mdash;&amp;mdash; 它占用了数据团队近 80% 的时间与精力，却依然是智能化进程中最顽固的瓶颈。传统方法主要依赖静态规则与领域特定模型，存在三大根本局限：高度依赖人工与专家知识、对任务语义的感知能力有限、在不同任务与数据模态间泛化能力差。&lt;/p&gt;&lt;p&gt;如今，一份引爆&amp;nbsp;&lt;strong&gt;HuggingFace 趋势榜的联合综述&lt;/strong&gt;指出，大语言模型（Large Language Models，LLMs）正在从根本上改变这一局面，推动数据准备从&lt;strong&gt;「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;规则驱动」向「&lt;/span&gt;&lt;/strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;语义驱动」&lt;/strong&gt;的范式转变。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBwSIeF1DSEmGwcBSqpcjWAYyNrfpQTiavlJmkJs57jjhjzjibA98NZq6Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.525" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531599" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/7148d538-757f-4838-9399-03ca4978378c/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB6nr3zmSDmS6C98MSL7M3evXuITMYQibaNZibmuriaIuWMFVMgQ23NLNfw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.625" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531600" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/1c6cc8a7-f3c1-4d85-b8a0-ec66403ecbca/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;来自上海交通大学、清华大学、微软研究院、麻省理工学院（MIT）、上海 AI Lab、小红书、阿里巴巴、港科大（广州）等机构的研究团队，系统梳理了近年来大语言模型在数据准备流程中的角色变化，试图回答一个业界关心的问题：&lt;strong&gt;LLM 能否成为下一代数据管道的「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;智能语义中枢」，彻底重构数据准备的范式？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBwQ6ibe6icXLnuphQUzY3FiaucOiadhN6WEElD6wrQyZf2cRG0miaMrKfm3w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.25925925925925924" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531601" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/42daadb9-a1db-4a11-912d-59be9c5f49eb/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;arXiv 论文地址：https://arxiv.org/abs/2601.17058&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Huggingface 论文主页：https://huggingface.co/papers/2601.17058&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 项目主页：https://github.com/weAIDB/awesome-data-llm&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;从「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;人工规则」到「&lt;/span&gt;&lt;/strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;strong&gt;语义驱动」的数据准备范式转移&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;传统的数据准备高度依赖人工规则和任务定制模型：正则表达式、字段校验逻辑、领域特定的分类器，不仅构建和维护成本高昂，且一旦数据格式变化或面临跨域集成，整套体系就显得异常脆弱。&lt;/p&gt;&lt;p&gt;研究团队指出，LLM 的引入正在推动这一流程从「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;规则驱动」向「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;语义驱动」转变。模型不再仅仅执行预设逻辑，而是尝试理解数据背后的含义，并据此完成检测、修复、对齐和补充等操作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;在这篇综述中，作者从应用层面（Application-Ready）的视角出发，构建了一个以任务为中心的分类框架，将 LLM 增强的数据准备过程拆分为三大核心环节：&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据清洗（Data Cleaning）&lt;/strong&gt;：错误检测、格式标准化、异常修复、缺失值填补等；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据集成（Data Integration）&lt;/strong&gt;：实体匹配、模式匹配、跨源对齐与冲突消解；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据增强（Data Enrichment）&lt;/strong&gt;：列类型识别、语义标注、表级与库级画像构建。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBv7HMKIickfg9OOhxyrDgHrhbMVT3HicIArDg6YZYkNKeH5t6BLiaqy8Yw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.8064814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531603" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/06dd0942-1340-4ac0-91db-cad81b5fd5f5/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 1：数据准备三大核心任务：数据清洗、集成与增强，分别解决数据的一致性与质量问题、隔离与集成障碍、以及语义与上下文限制&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;论文中的整体框架展示了 LLM 在数据准备流水线中的多维度角色。研究团队将现有技术路径归纳为三类，这与传统单一方法形成鲜明对比：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;基于 prompt 的方法（M1）&lt;/strong&gt;：通过结构化提示和上下文示例，直接引导模型完成标准化、匹配或标注等任务，强调灵活性与低开发成本。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;检索增强与混合方法（M2）&lt;/strong&gt;：结合检索增强生成（RAG）、模型调优（如微调）、小型模型或传统规则系统，在成本、规模与稳定性之间寻求平衡。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;智能体编排方法（M3）&lt;/strong&gt;：让 LLM 作为协调中枢，调用外部工具和子模型，逐步构建复杂的数据处理工作流，探索自动化与自主决策的边界。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB4h0Sib4It0o1eF4vMUmGR4LFjiccBTjshWDkG0FDLGlBwgPexnMmzUgA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.9768518518518519" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503531608" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f58c601e-999b-45ee-9899-e020a8aeaebc/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 2：LLM 增强的数据准备技术全景总览，涵盖数据清洗、数据集成和数据增强三大任务及其细分技术路线&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;代表性工作与系统：从理论到工程实践&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在具体方法层面，论文梳理了近年来一批具有鲜明工程导向特征的代表性工作。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在&lt;strong&gt;数据清洗场景&lt;/strong&gt;中，CleanAgent 引入了能够自主规划的智能体架构，通过调用 Python 库等外部工具动态构建清洗工作流。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在&lt;strong&gt;数据集成领域&lt;/strong&gt;，Jellyfish 探索了「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;大模型教小模型」的蒸馏范式，利用 GPT-4 的推理轨迹微调轻量级模型，显著降低了大规模匹配的成本。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;而在&lt;strong&gt;数据增强方向&lt;/strong&gt;，Pneuma 则结合了 RAG（检索增强生成） 技术，通过检索数据湖中的相关表格与文档，为原始数据补充缺失的语义上下文与元数据。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SByibSlnT7rCSC4fpriav8yh5xwKJBadUcXzVFZRveAVE8sFrF4ryuCyDA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=6" data-ratio="0.8870370370370371" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503531609" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/beaaec66-d134-40fc-a887-7e80cd6d8704/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表 1：LLM 增强的数据准备方法技术概览&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;论文总结的「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;技术版图式」对照表（如上方表 1），将不同方法按照技术路径（基于 prompt、RAG、智能体等）与任务环节（清洗、集成、增强） 进行交叉定位。其核心价值在于帮助工程团队进行技术选型：在不同规模、成本约束与任务阶段下，应优先考虑哪类技术路线。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;从该表中，研究团队提炼出几条对工程实践极具指导意义的观察：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;基于 prompt 的方法适合小规模、高复杂度任务：例如高价值表格的语义修复、复杂实体歧义消解，但在大规模场景中成本和一致性难以控制。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RAG 与混合系统成为主流工程选择：通过检索、规则系统或轻量模型分担高频、低难度任务，让 LLM 专注于「难例」和核心语义决策，实现更高的整体性价比。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;智能体路线仍处于探索阶段：多步工具调用在复杂工作流中展现出潜力，但其稳定性、调试成本和结果可评估性仍是当前的主要瓶颈。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;常用评估数据集与基准&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了代表性方法和系统，论文还整理了当前用于评估 LLM 数据准备能力的代表性数据集与基准（如下方表 2），为工程团队和研究者提供了一份「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;可复现实验地图」。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBGE3FIyIcK41zNia2hvzPNA5cBeBXTBxETzBNEwreMyibYKD4mhiba8qsw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.48055555555555557" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531617" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/82a0e0f2-001c-42ba-83a9-e4f0c9e4fc40/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表 2：数据准备代表性数据集总览&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;从任务维度看，这些基准大致覆盖了三类典型场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据清洗（Data Cleaning）&lt;/strong&gt;：常用数据集包括 Hospital 和 Flights，用于评估模型在格式错误修复、值标准化和缺失字段补全等任务中的稳定性与准确性。这类数据集通常包含人为注入或真实采集的噪声模式，适合测试模型在结构性错误下的鲁棒性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据集成（Data Integration）&lt;/strong&gt;：在实体匹配和跨源对齐任务中，WDC Products 和 Amazon-Google Products 等电商类数据集被广泛使用，用于检验模型在名称歧义、属性不一致和多对多匹配场景下的语义判别能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据增强（Data Enrichment）&lt;/strong&gt;：表语义标注和列类型识别任务中，研究工作常基于 OpenWikiTable、Public BI 等表格语义数据集，评估模型生成元数据和语义描述的准确性与一致性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;研究团队指出，当前多数基准仍以中小规模表格和结构化数据为主，对于企业级数据湖、日志流和多模态数据场景的覆盖仍然有限，这也在一定程度上限制了不同方法在真实系统中的横向对比能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心洞见、现存挑战与工程指南&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在对大量文献与系统进行深入对比后，研究团队给出了贯穿全文的核心洞见，并清晰地指出了迈向真实应用必须跨越的鸿沟：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工程可落地性优先&lt;/strong&gt;：在真实系统中，吞吐量、延迟、成本控制和结果可回溯性，往往比单次任务的绝对准确率更为关键。这意味着追求极致精度的复杂方法，未必是工程上的最优解。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;混合架构是主流方向&lt;/strong&gt;：短期内，LLM 更可能作为「语义中枢」嵌入传统数据管道，与规则系统、检索引擎和轻量模型形成协同的混合架构，而非完全替代现有基础设施。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;评估体系是当前瓶颈&lt;/strong&gt;：不同研究采用的数据集、指标和任务定义差异较大，缺乏统一、可复现的评估标准，严重制约了技术的横向比较、迭代与工程选型。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;然而，走向大规模真实应用，仍面临明确挑战：推理成本与延迟在大规模场景下仍显高昂；稳定性与幻觉问题在要求严苛的清洗、匹配任务中亟待解决；而统一的评估体系建设更是任重道远。&lt;/p&gt;&lt;p&gt;因此，综述指出，更现实的路径并非用大模型完全取代现有设施，而是将其作为 「&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;语义协调者」嵌入关键节点。&amp;nbsp;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;这份综述为工程团队提供了一张详尽的技术地图与选型指南。如果你正在搭建或优化企业级数据平台，它可以帮你判断：在哪些环节引入大模型担任「&lt;/span&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;智能语义层」能带来最高性价比，而在哪些部分，经过验证的传统规则系统与数据库内核仍是更可靠、高效的选择。&lt;/span&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>扩散语言模型深度思考</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:16:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;blockquote&gt;&lt;p&gt;原文链接：https://zhuanlan.zhihu.com/p/1998418717743289472&lt;/p&gt;&lt;p&gt;作者：王云鹤&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool="mdnice编辑器"&gt;写这个的时候，其实我脑子里第一反应是好多年以前某位领导问过我，&lt;strong&gt;transformer的下一跳是什么？&lt;/strong&gt; 我当时的回复是transformer是一个量变到质变长期积累得到的范式，很早期的视觉里面也有类似的nonlocal等，而且卷积也在跟attention持续互补发挥作用。&lt;strong&gt;diffusion本身也不算transformer的下一条，但是从建模方式上，可能有潜力会对ar带来很大冲击。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool="mdnice编辑器"&gt;很早就关注扩散语言模型了（diffusion language model，dllm），但是受限于精力和算力一直没机会深度思考。从文本角度探索diffusion的架构相对当前比较好入手，并且这里面很多问题不解决，多模态的版本也不好搞，所以我们会先聚焦dllm上的算法基础。&lt;/p&gt;&lt;p data-tool="mdnice编辑器"&gt;去年下半年陆陆续续开始在一些方向上有一些探索，受启发于某位内部专家，赶在元旦之前写了一篇算是洞察材料的文章。&lt;/p&gt;&lt;p data-tool="mdnice编辑器"&gt;前几天在AAAI的报告重点介绍了团队的几个工作，包含next-block diffusion的训练，diffusion in diffusion的分层结构，diffusion agent等。&lt;/p&gt;&lt;figure data-tool="mdnice编辑器"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/LxD720kEAESicp6ibJL4NHviaZSrwxaYZ7OdDziadIUFeu58PiabXwkTRRsoKyniaicd1Fg0JBroHAxepo0s4HOib2ibiarg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=0" alt="感叹于现在AI进步真的日益显著，该图片根据文字内容，用AIGC生成" data-ratio="0.4905847373637265" data-type="jpeg" data-w="1009" data-imgfileid="100006851" data-aistatus="1" data-original-style="display: block;margin-top: 0px;margin-right: auto;margin-bottom: 0px;margin-left: auto;max-width: 100%;border-top-style: none;border-bottom-style: none;border-left-style: none;border-right-style: none;border-top-width: 3px;border-bottom-width: 3px;border-left-width: 3px;border-right-width: 3px;border-top-color: rgba(0, 0, 0, 0.4);border-bottom-color: rgba(0, 0, 0, 0.4);border-left-color: rgba(0, 0, 0, 0.4);border-right-color: rgba(0, 0, 0, 0.4);border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;object-fit: fill;box-shadow: rgb(133, 161, 201) 0px 0px 5px 0px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/873ce813-d0a3-4751-bc5e-5ebdcd1d1da5/640.png" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;figcaption&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 感叹于现在AI进步真的日益显著，该图片根据文字内容，用AIGC生成&lt;/sup&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p data-tool="mdnice编辑器"&gt;相关PPT已经上传在项目网站上（&lt;strong&gt;sofar我还是觉得diffusion搞到代码和agent场景很有意思&lt;/strong&gt;）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;section&gt;诺亚diffusion llm项目网站&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;https://noah-dllm.github.io/&lt;/code&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool="mdnice编辑器"&gt;关于当前diffusion遇到的问题，我们也写了个proposal，更早源自于前段时间在伦敦和加拿大等地的一些研讨，陆陆续续被我整理成了一些中文观点，附如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;推理高效的架构（kv cache/attention/model architecture）&lt;/strong&gt;：当前的diffusion model整体框架还是用的ar模型，尤其是attention的形式，包括qkv和kv cache的计算等。在ar模型的next token prediction的推理模式中，可以复用kv cache来提升整体效率。但是diffusion model的mask位置的随机性导致kv cache复用的机制会失效，这也是当前diffusion model没有被广泛应用的主要问题之一。所以我们需要一个更合适diffusion model推理高效性的attention结构或者更有结构性的mask方式等，可以在兼顾diffusion解码方式的优势，也带来推理效率的提升。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：从ar的kv cache去思考，如果diffusion模型中mask tokens在去噪过程中，一定是从左到右顺序的，似乎可以解决kv cache复用的问题。但似乎不是完美方案，我们会去尝试一些更适合diffusion模型的attention或者kv相关的机制。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;更适配的词表（tokenizer）&lt;/strong&gt;：最理想的diffusion model并不应该去follow ar现有的范式，而应该想人思考一样具有结构性，比如先对一个给定的topic生成一个全局的outline，然后再针对局部进行认真思考。本质上人在写文字的时候或者做报告的时候，往往是多尺度的。但是，ar模型的编码器通常都是通过各种切分和统计的算法得到的，也就是说，粒度是统一的。回到diffusion model，它的tokenizer是否也应该具有结构性，例如，不同粒度的tokenizer，有些负责段落间的联系，有些负责细节的修改，有些适合快速填充中间的空缺。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：简单的想法可能是tokenizer之间有重叠，形成词表金字塔，根据数据的首全局信息进行统计，比如首尾部分的信息等，参考一些图像生成的工作，先全局再局部，或者反之亦然。但是，想发挥这种词表的优势，对应的训推范式可能也需要进行调整。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;更好的优化范式（training and optimizing）&lt;/strong&gt;：diffusion model被讨论的比较多的优化上的问题在于梯度的计算比较低效，极端情况下一个很长的序列例如128k当中只有一个mask token，但是我们需要计算所有的前向和反向传播过程，也就是说，为了一个token做了很多的计算，并且得到的梯度反馈可能是很小的，这就意味着大多数时候同样训练开销下的diffusion模型精度不如ar模型。此外，当前diffusion模型在预训练阶段会在数据上加一定比例的随机mask tokens，在sft阶段是把answer都加上mask，可能会存在一些不同阶段训练不一致的问题，进一步对后续的强化学习也有很大的挑战。另外，masking的比例也是一个需要认真研究的问题，尤其是结合不同的训练阶段进行动态地调整。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：我觉得这是一个非常挑战的问题，暂时我能想到的idea是训练过程中。此外，在pre-training和sft阶段，可以引入更多的融合方案让diffusion模型的训练效果更好。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;掩码方式的更优方法（more mask tokens）&lt;/strong&gt;：当前主流的diffusion model中，通常只有一个mask token，当然这也是一个非常简洁有效的范式，但是多样性不足，对于所有被mask掉的位置都用同样的token去处理，功能性上略有不足。此外，当前的masking范式，所有的位置都是等概率被处理的，有一些简单的地方可能不需要mask，而且mask的位置之间也没有联动性，缺乏结构化的masking机制。也就是说，当前的masking方式在diffusion model里面还处于比较初步的阶段，如何通过把diffusion model的潜力进一步挖掘出来，是一个非常值得深入讨论的问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：把原来的mask tokenizer变成多个，并加以一定的先验假设，让新增的tokenizer联动起来，也能高效的发挥作用。此外，也可以考虑更先进的加mask方式，尤其是结构化的，我觉得这样对复杂场景尤其是代码任务上可能会有更大的潜力。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;动态长度输出（dynamic output length）&lt;/strong&gt;：虽然diffusion有很好的并行解码的特性，能够针对给定的问题进行快速输出，但是通常需要预先给定预期的输出长度，并且通过eos token来实现最终的输出长度。这很像我们的作文考试，例如以我的家乡为题写一个800字的文章，如果让扩散模型来执行这个任务的话，800字以内可以终止，以外的话就很难生成。虽然在模型的训练过程中有不同长短的数据用来学习，可以让diffusion模型对预期的输出长度有一定的适应性，但是一些极端情况下，比如9.11和9.8哪个大都需要100k tokens的话，是一种非常低效也可能对精度有影响的方式。所以，对于给定的问题，如何自适应地推断出最优的输出长度是非常值得探索的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：在模型训练过程中同时增加对eos的位置预测，以便在推理过程中可以针对输入问题进行感知，减少无用的推理开销。此外，一些参数复用的技术可能会对超出给定长度的外推也是有帮助的。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;适配diffusion模型的数据（data engineering）&lt;/strong&gt;：当前大多数的diffusion models都复用了ar模型的数据。哪怕是从ar模型改造过来的diffusion模型也在持续使用ar模型所积累的数据和相关的技术。虽然这些数据都是通用的，尤其是里面包含的知识也可以被扩散模型学习和吸收，但是如果我们想进一步激发扩散模型的潜力，让模型可以学习更多的结构化的知识并且具备更好的推理能力，当前的数据还是有进一步提升空间的。由于diffusion的训练阶段mask token都是随机加入的，想结合这个特性在数据上做一些优化所面临的挑战和需要的工作量还是非常巨大的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea ：可能我们可以在预训练数据中增加mask的位置信息，通过对一些重要tokens的标注来提升diffusion模型的学习效率。此外，sft数据和rl的数据可能可以通过更有结构化的masking和标注，来让模型提升性能。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;资源高效的模型优化（model efficiency）&lt;/strong&gt;：可能目前还不着急在扩散模型中讨论细粒度的模型压缩，比如weight或者神经元层面的稀疏化，因为很多基础的模型结构层面的东西还没有收敛。但是如何提升整体diffusion模型的推理效率还是非常值得研究的，尤其是当batch size加大之后，global diffusion的推理跟ar比在部分场景呈现劣势。此外，去噪过程本身的高效性也是值得研究的，无论是对那种形态的diffusion，都可以获得比较显著的收益。此外，从当前的各项评测来看，ar模型还是有显著优势的，如何把ar和diffusion同时使用来获得更大的整体收益，也是值得讨论的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：几个可以优先尝试获得较大收益的可能方向是，扩散多步的蒸馏，参考之前在图像生成的相关工作，把diffusion模型的去噪步骤降低。投机推理，低比特后量化等也可以被用来降低推理开销。一些互补性的高效组合，例如，长序列单batch场景用diffusion，大batch整合信息用ar等。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;慢思考及隐式思考（reasoning &amp;amp; latent thinking）&lt;/strong&gt;：在diffusion模型的sft训练过程中，对于给定的query会在预定的长度空间内通过不断的去噪过程来产生answer并进行对比。如果用传统的顺序的思维链等来实现慢思考，对于diffusion model来说可能是低效的，并没有充分释放diffusion模型的潜能，还有很多改进空间，无论是数据上还是优化方式等。此外，diffusion的另外一个潜力大的地方在于可以做remasking，在去噪的过程中，也可以对置信度较低的token进行重置然后进行修改，给了更多深度思考和隐式思考的可能性，一旦充分利用这些特性，模型的智能可以进一步提升。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：首先可能是需要针对diffusion的特性进行新的思维链的构建，比如结构化的思维链，尤其是代码和agent场景，从一个outline入手，再去补充细节可以产生更好的结果。此外，不同粒度的思考，过程中的editing和remasking等，都可以进行更多的探索。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;结构化的提示词工程和记忆（prompt and memory）&lt;/strong&gt;：虽然当前有一些算法是通过ar模型续训到diffusion模式的，也有通过blcok解码的机制来让diffusion模型具有一定的局部ar特性。但是diffusion模型的masking方式和解码方式终究是不同的，尤其是diffusion不但可以向前看tokens，也可以向后看的特性。这就引发了我们的思考，是否有更适合diffusion模式的prompt格式，以及更高效的prompt方法，比如只给出几个全局关键token，快速实现全过程的解码和推理，这对代码、deep research、agent等场景是非常有帮助的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：一个非常直接的方式是把原来的ar问答式prompt变成完形填空式的prompt用于diffusion模型，尤其是给一些关键信息辅助生成更好的回复。也可以通过一些prompt自演进的方式，来做更深度的优化。同时，这些方法也可以被用到rag和memory相关的工作中。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;未来的统一架构（uniform architecture）&lt;/strong&gt;：在各种多模态或者多视角等任务中，我们经常会讨论，一个端到端from scratch的方案可能更好地激发模型的能力上限。但是，实际情况往往面临着很多挑战，比如，不同模态数据的对齐，数据配比，甚至不同任务所用的模型和优化目标完全不一样。比如用于理解的模型当前会用ar较多一些，而生成类任务会用diffusion多一些。未来一定是属于多模态的，那如何探索出来一个更统一的模型结构和训练范式，是非常有必要行的，而diffusion model当前所展现出来的能力恰恰具备一定的潜力。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;idea：以vla模型为例，这是一个比较火热的领域，尤其是与真实物理环境交互，已经成功地融合了多种信息。当前的主流架构是vision和language部分以ar模式为主，但是action部分会采用diffusion的模式，不过这里的diffusion是传统的而不是离散的加mask的，为了生成更连续的动作轨迹。那么，能否用discrete diffusion models让这三个部分融合在一起，就是一个非常非常有意思的问题。&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool="mdnice编辑器"&gt;最后，附diffusion llm搭建的agent demo：&lt;/p&gt;&lt;figure data-tool="mdnice编辑器"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/LxD720kEAESicp6ibJL4NHviaZSrwxaYZ7OefF2VY2ul8RiajQz6IjV43ZLibRXcP7xJ6KVbPAEAtD8DtoIKu3TT3xA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.7016666666666667" data-type="jpeg" data-w="600" data-imgfileid="100006850" data-aistatus="1" data-original-style="display: block;margin-top: 0px;margin-right: auto;margin-bottom: 0px;margin-left: auto;max-width: 100%;border-top-style: none;border-bottom-style: none;border-left-style: none;border-right-style: none;border-top-width: 3px;border-bottom-width: 3px;border-left-width: 3px;border-right-width: 3px;border-top-color: rgba(0, 0, 0, 0.4);border-bottom-color: rgba(0, 0, 0, 0.4);border-left-color: rgba(0, 0, 0, 0.4);border-right-color: rgba(0, 0, 0, 0.4);border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;object-fit: fill;box-shadow: rgb(133, 161, 201) 0px 0px 5px 0px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/39f3ebdf-e1e4-4179-8a36-3f2dde107bd6/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/figure&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | 中国科大、科大讯飞团队开发ChemEval：化学大模型多层次多维度能力评估的新基准</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Mon, 09 Feb 2026 13:28:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;作者丨论文团队&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;近年来，大语言模型在文本理解、知识问答和通用推理任务中展现出惊人的能力，也逐渐被引入到化学文献分析、反应预测和分子设计等科学场景中。然而，一个关键问题长期被忽视：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;模型「看起来会化学」，是否真的具备化学研究所需的能力？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现有主流评测基准（如 MMLU、SciEval 等）大多以通用学科或浅层科学问答为主，难以刻画化学研究中高度专业、层次分明且跨模态的能力需求。即便是已有的化学评测工作，也往往局限于少量任务或单一能力维度，难以反映模型在真实科研场景中的综合表现。&lt;/p&gt;&lt;p&gt;针对这一核心缺口，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-top: 16px;margin-bottom: 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;认知智能全国重点实验室联合研究团队 &amp;mdash;&amp;mdash; 中国科学技术大学陈恩红教授团队与科大讯飞研究院&amp;nbsp;AI for Science&amp;nbsp;团队，在人工智能领域顶级国际会议 ICLR 2026 发表最新研究成果，论文提出了多层级、细粒度的化学能力评测框架&amp;nbsp;ChemEval，并系统性揭示了大语言模型在化学领域的真实能力边界。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-top: 16px;margin-bottom: 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;该工作为 AI for Science 方向中「如何科学地评估大模型是否真正理解化学」这一核心问题，提供了完整、可复现且具有学术深度的答案。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOuztAso8CkjXibETvt1h4iabLfqeIGpkMoTcocqT5sn488hjSNODChoiayQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.16096579476861167" data-s="300,640" data-type="png" data-w="994" type="block" data-imgfileid="100027352" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/3b5d4980-cd3d-4f41-b019-a18042211eaa/640.png" alt="图片" data-before-load-time="1770614853829" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文地址：https://openreview.net/forum?id=JrqjSkEPrX&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-top: 16px;margin-bottom: 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;论文的主要作者为中国科学技术大学博士生黄育庆、张荣杨，所属认知智能全国重点实验室陈恩红教授团队，其他作者包括科大讯飞 AI 研究院执行院长王士进、副院长李鑫、研究员徐飞扬、梁华东等人。团队在 AI4Chemistry 领域开展深入研究，具体包括化学推理大模型 post-training、化工大模型&amp;nbsp;DeepReasearch、化学大模型智能体、化学领域大模型评测等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;ChemEval &amp;mdash;&amp;mdash; 从化学研究者视角出发的评测体系&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ChemEval 并非简单堆叠题目，而是围绕化学研究的认知过程，构建了一套四层递进式评测结构：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;基础与进阶化学知识问答：考察模型对核心化学概念、定量计算与理论知识的掌握；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;化学文献理解与信息抽取：评估模型从论文、表格和图像中提取关键信息并进行归纳生成的能力；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;分子层级理解：覆盖分子命名、结构转换、性质预测与描述等核心分子认知任务；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;科学推理与化学推断：包括逆合成分析、反应条件推荐、产物预测与机理分析等高阶任务。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;整个评测体系共包含 13 个能力维度、62 项具体任务，既涵盖文本任务，也系统引入分子结构图、光谱图等多模态输入，贴近真实化学研究流程。&lt;/p&gt;&lt;p&gt;更重要的是，ChemEval 的数据并非简单复用已有公开数据，而是结合开源数据集与化学领域专家人工构建的数据，通过严格的三阶段标注与审校流程，确保科学性与评测可靠性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOulbpSl9M8e8pUVcicZlZ1tjmW2Q10NzIGaeX1Kv0HNnAetEz11wuv3Zw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.4675925925925926" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="100027351" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/a91df650-b197-4979-b0e9-ac57681929bd/640.png" alt="图片" data-before-load-time="1770614854324" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;ChemEval 概览图与测试数据示例&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通用大模型 vs. 化学专用模型，谁更「懂化学」？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于 ChemEval，研究团队对主流通用大语言模型与化学专用模型进行了系统评测，得到了一系列具有启发性的结论：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;通用大模型在化学文献理解、指令遵循和部分推理任务中表现突出，但在涉及分子结构、反应机理等深度化学知识时明显乏力；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;化学专用模型在术语理解、分子性质等专业任务上具备优势，但往往牺牲了通用语言理解能力，存在「灾难性遗忘」和指令不稳定问题；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;单纯增加模型规模或引入「思考链」并不足以解决复杂化学任务，瓶颈并不在推理长度，而在领域知识建模与表示能力本身；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在多模态化学任务中，当前模型在简单结构识别上尚可，但在综合结构识别 + 机理推断的任务中普遍存在显著困难。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些结果以系统性、量化方式揭示了当前大模型在化学研究中的真实能力边界，也为后续模型设计与训练方向提供了明确指引。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmWG9lDvibcet4On5VLvVIOuPicb6WvrfibdiblNOIjYibcd1gSXlHic9EhD1CgVia2NCWLIiaVYIbDKGD8Jg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5203703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027350" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/2be808d3-641c-4c43-825e-53c2c1096ed1/640.png" alt="图片" data-before-load-time="1770614854561" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;通用大模型与化学专用模型的评估结果&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为 AI for Science 提供「标尺」，而不只是排行榜&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不同于「刷榜型」评测工作，ChemEval 更强调诊断价值：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;它能够精确定位模型在化学研究流程中「卡在哪一层能力」；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;揭示通用能力与领域能力之间的结构性矛盾；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;为化学大模型的训练策略、数据构建和工具增强提供可操作的参考依据。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;研究团队认为，真正推动 AI for Science 的关键，不是让模型在单一任务上表现更好，而是让模型在完整科学认知链条中更可靠、更可解释。ChemEval 正是朝这一目标迈出的重要一步。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验室持续推进 AI &amp;times; Chemistry 深度融合&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;该工作是认知智能全国重点实验室与科大讯飞 AI for Science 团队在科学智能与化学大模型评测方向的重要进展之一。近年来，团队围绕「模型是否真正理解科学」这一核心问题，持续在科学推理、多模态理解和领域评测体系建设方面开展系统研究。&lt;/p&gt;&lt;p&gt;未来，团队将进一步探索化学大模型与专业仿真工具、实验数据和多模态信息的深度融合，推动 AI 从「辅助理解」走向「参与发现」，为化学研究范式变革提供坚实的智能基础。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
