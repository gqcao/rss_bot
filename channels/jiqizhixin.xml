<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>千问30亿免单引爆春节AI大战，奶茶免单开启AI购物时代</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Fri, 06 Feb 2026 10:39:10 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-06-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-06-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;春节AI大战杀疯了！2月6日一早，千问APP&amp;ldquo;春节30亿大免单&amp;rdquo;正式上线，发动奶茶攻势，邀请全国人民用AI一句话免费点奶茶。千问APP人士表示，&amp;ldquo;我们希望通过春节大免单活动，邀请全国人民体验AI时代的全新生活方式，让AI融入到人们真实的生活消费之中。&amp;rdquo;&lt;/p&gt;&lt;p&gt;今年春节的AI大战硝烟弥漫，此次千问春节30亿大免单，在阿里历史上的春节活动中投入最大，在春节AI大战中投入金额也最高。&lt;/p&gt;&lt;p&gt;此前的1月15日，千问APP已接入淘宝闪购、支付宝、淘宝、飞猪、高德等阿里生态场景，上线AI购物功能。&lt;/p&gt;&lt;p&gt;有网友对比各家AI应用的红包活动，千问的玩法简单直接、下载就给25元免单卡，门槛最低、金额最大。&lt;/p&gt;&lt;p&gt;千问APP活动页面显示，第一波免单活动时间为2月6日-2月12日。所有用户更新千问APP后，都能白拿一张25元无门槛免单卡，不仅能免单喝奶茶，也能通过淘宝闪购买年货、点外卖。&lt;/p&gt;&lt;p&gt;通过千问APP一句话下单，免单卡可立即在全国30多万家奶茶店使用，蜜雪冰城、瑞幸咖啡、霸王茶姬、奈雪的茶、沪上阿姨、茶百道、库迪咖啡等茶饮咖啡品牌都可使用。&lt;/p&gt;&lt;p&gt;此外，每邀请一名新朋友下载千问APP，双方可各得一张25元免单卡，每人最多可得21张，相当于525块钱。当日累计成功邀请3位新朋友，则可获得机会，抽取价值万元的千问AI生活卡。&lt;/p&gt;&lt;p&gt;有网友算了一笔账，如果一家6口人参与千问免单活动，5分钟就可获得275元的无门槛免单卡，如果用来点蜜雪冰城柠檬茶，可以免费喝84杯。&lt;/p&gt;&lt;p&gt;活动页面显示，春节30亿大免单的第二波将从2月13日开始，用户可领取现金红包，最高可得2888元。&lt;/p&gt;&lt;p&gt;去年春节，是&amp;ldquo;深度思考&amp;rdquo;出圈的DeepSeek时刻；今年春节，将是&amp;ldquo;AI超级Agent&amp;rdquo;出圈的千问时刻。千问APP有望通过真金白银的投入，培养用户&amp;ldquo;有事找AI&amp;rdquo;的习惯。用户不再需要在多个APP间反复跳转，只需向AI表达意图，即可完成从消费决策、交易到履约服务的全过程，带来AI时代的全新消费体验，彻底引爆AI购物。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Agentic Memory开年就卷起来了？刚刚，华人团队MemBrain拿下多项SOTA！</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 06 Feb 2026 10:29:04 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-06-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-06-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2026 刚来到 2 月，无论是底层模型大厂还是初创公司统统加速开卷，其中 Agentic Memory 方向的快速进化更是把大模型的能力上限推向了 NEXT LEVEL!&lt;/p&gt;&lt;p&gt;OpenAI 和 Anthropic 持续推高上下文窗口的上限，Clawdbot 小虾凭借记忆能力住进了更多用户心中，AI 行业共识正在发生微妙但剧烈的转向 &amp;mdash;&amp;mdash; 没有记忆的 Agent 只是一个高级的自动补全工具。要让 AI 真正处理复杂项目或长期任务，它必须具备一种跨会话的、结构化的长期记忆机制。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 技术圈和资本押注的新风口&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如果说大模型提供了 &amp;ldquo;高度运转的大脑&amp;rdquo;，那么记忆层就是 Agent 真正迈向好用的 &amp;ldquo;关键能力&amp;rdquo;。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Sequoia Capital 的合伙人在近期的一次闭门会上谈到，未来的 Agent 核心挑战之一是实现 &amp;quot;持久化身份&amp;quot;(Persistent Identity)&amp;mdash;&amp;mdash; 让 AI 不仅能记住用户的历史交互，更要在长时间运行中保持一致的理解和上下文记忆。&lt;strong&gt;在 Agentic AI 的进化竞赛中，一个长期被忽视的瓶颈正成为顶级风投追逐的新风口：持久化记忆（Persistent Memory）。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;刚刚，Feeling AI 团队在深夜发布 MemBrain1.0，在 LoCoMo / LongMemEval / PersonaMem-v2 等多项主流记忆基准评测中拿下全新 SOTA，反超 MemOS、Zep 和 EverMemOS 等记忆系统和全上下文模型；在 KnowMeBench Level III 两个难度等级最高的评测中更是比现有评测结果大幅提升超 300%。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Feeling AI 是 2025 年刚浮出水面不久的 AI 初创团队，创始人戴勃是生成式 AI 领域的青年科学家，他曾在 NTU 和上海 AI 实验室任职，担任生成式 AI 团队的负责人。据媒体此前报道，他们还低调的完成了两轮超亿元的融资，是国内最早在世界模型和 3D 动态交互方向进行尝试的团队之一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Memory for Agentic AI 新 SOTA&amp;mdash;&amp;mdash;MemBrain1.0&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Feeling AI 公布的评测结果，通过在 4 个业内公认的记忆基准测试来一场 &amp;quot;实战演练&amp;quot; 以验证算法的 &amp;quot;真功夫&amp;quot;。基于 EverMemOS 开源仓库搭建的一个 &amp;quot;公平擂台&amp;quot;&amp;mdash;&amp;mdash; 所有 &amp;ldquo;参赛选手&amp;rdquo; 都使用同一个基础大模型（GPT-4.1-mini）作为 &amp;quot;底座&amp;quot;，MemBrain1.0 与其他大模型记忆增强方法在同一起跑线上的能力比拼&lt;strong&gt;结果先看为敬&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532034" data-ratio="1.3333333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqF4NV5rN52cSLkyibiangg6sn4snsRibjgcI3jIbeoiaKtrRNLUvQuthEeJWUibmicwkZOk02X2V04EzZkzPeKwvmktMyibNkcDMmibevo/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/9b391852-32b1-4bde-9520-65ea761c4a8e/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在主流测试基准 LoCoMo 和 LongMemEval 上，MemBrain1.0 分别以 93.25% 和 84.6% 的准确率斩获新的 SOTA。这部分得益于&lt;strong&gt; MemBrain1.0 精细的实体 - 时间上下文管理设计， 在时序任务以及多会话场景任务下取得显著提升&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHTwaOTLgMFYRtia22xpoUsNZq1e0OYuaguFwEHTONDXfDu732fZ3icL6g2FWyS4WCOE23GSQZlhuiaMyiarG5sTnV7OBpxY7ic81HI/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.35833333333333334" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532035" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4fa3d699-cd1f-4de8-b293-3b13ebd108d4/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFzpsa9au6JRiaceVA6PmPzVFnGb3s8cKsRqAZiaBGkOIwWZpDLYWMWRAZTKAsTWSDSb6BmTibSWhE0QgH6zAQfwYZtyTUUZia8WKk/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.30277777777777776" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532036" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/34bd65a4-6599-4bda-a2f3-f6d6f3fc2acf/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在另一个&lt;strong&gt;考察隐性画像捕捉能力的 PersonaMem-v2 &lt;/strong&gt;测试基准上，MemBrain1.0 以 51.50% 的准确率超越现有公开的方法，&lt;strong&gt;精准实现了对用户长期偏好的深度洞察&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEyxfKZ3PBGl7HFMJ3NZ64hZ7Lob0c6QcCFWZEfPBticF1ZoyY9icdcKZGeodQriaS5NLpRqeBAghE5SQRz6xuogia312LaQubibRe0/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.40370370370370373" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532037" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/d93af870-4794-4ce9-9dac-a1429f6a76df/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，在 Hugging Face 广受关注的 Knowme-Bench 基准中，MemBrain 的实验结果表明，在 KnowMeBench Level III: Psychoanalytic Depth:Mind-BodyInteraction (T6),Expert-Annotated Psychoanalysis (T7) 难度等级最高的两个评测中更是比现有评测结果大幅提升超 300%，&lt;strong&gt;显示出 MemBrain 在高阶认知理解任务中显著的性能优势&lt;/strong&gt;。Knowme-Bench 的核心挑战在于要求模型超越基础的精确记忆抽取，实现基于记忆内容的深层分析与复杂推理。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqEL904pwKBD4F9Rh923cLdVeT4ibaibxcSYeAosVLhJ7Ogu1sQWT7zgicOYvCRQ7aLqDYq4Ojn86yzJWQjzY6Sg4g7fSJKAxHgicAE/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3212962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532038" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/324d97b9-7e74-4a2c-b8ea-e738d351e102/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;MemBrain1.0 的算法强在哪？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;让记忆系统学会 &amp;quot;主动思考&amp;quot;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现有记忆系统在检索机制上普遍采用多路召回与重排架构（如全文检索、向量检索与图数据库的混合检索）&amp;mdash;&amp;mdash; 虽然配备了一整套 &amp;quot;豪华工具箱&amp;quot;，但性能很大程度上取决于预设的分支参数，系统只能按预设轨道上 &amp;quot;照章办事&amp;quot;。尽管 EverMemOS 等前沿工作尝试引入查询改写（Query Rewriting）来适配多维度的查询需求，但这种单轮触发机制本质上还是 &amp;quot;单次触发&amp;quot; 的被动响应，难以实现真正的自适应检索。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MemBrain 的破局点在于用 Agentic 思路重构整个记忆系统&lt;/strong&gt;。它将实体提取、会话摘要生成、记忆合并、冲突消解、分层记忆压缩等核心环节，拆解为独立且能协同作战的子 Agent&amp;mdash;&amp;mdash; 每个 Agent 专注自己的领域，但能根据任务动态配合。传统检索手段被 &amp;quot;降维&amp;quot; 为可调用的工具，真正的决策权交给了 Agent 之间的协作调度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这种设计让部署灵活度直接拉满，还为异步记忆更新等日常工程需求预留了充足的扩展空间。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实体与时间：记忆管理的 &amp;quot;细节功夫&amp;quot;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;精确提取历史文本中的实体与时间戳信息，是记忆系统实现关联分析与逻辑推理等高阶任务的基石。然而，现有方案在&lt;strong&gt;长时记忆的数据组织模式与细粒度上下文管理上&lt;/strong&gt;仍显粗放 &amp;mdash;&amp;mdash; 时间信息捕捉不够完整、实体关联不够清晰、时间与事件的组织方式不够规范，这些细节上的不足限制了系统在复杂下游场景中的性能上限。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MemBrain 在前沿探索的基础上，针对长时上下文进行了深度的结构化工程优化。&lt;/strong&gt;通过精细化的字段设计与上下文对齐机制，在实体提取完整性、时间戳规范化、数据结构清晰度等多个细节层面持续打磨，确保了记忆数据的高保真度与检索时的高度相关性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;适配大模型原生能力，深度参与推理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;很多研究者喜欢用图结构来描绘记忆网络 &amp;mdash;&amp;mdash; 毕竟现实世界中，实体之间的关系确实错综复杂。&lt;/p&gt;&lt;p&gt;但这里有个有趣的矛盾：当下主流基座模型的底层架构，更适配线性或树状的信息排列方式。这种架构范式与图结构表征之间的天然差异，带来的后果是：现有的图数据库记忆系统在查询时，还是主要依赖传统图算法在唱主角，LLM 只能在一旁 &amp;quot;看戏&amp;quot;，无法深度参与图推理。结果就是经常发生明显的语义转化损耗 &amp;mdash;&amp;mdash; 就像信息在传递过程中不断 &amp;quot;失真&amp;quot;。这无疑限制了 LLM 在复杂记忆推理场景中的真正实力。&lt;/p&gt;&lt;p&gt;最近备受关注的 Anthropic 推出的 Skill 机制，采用 &amp;quot;一切皆文件&amp;quot; 的设计哲学，把文件按需加载到上下文中。&lt;strong&gt;MemBrain 通过优化这一思路：虽然不用图结构，但把相关信息组织成可按需加载的 &amp;quot;语义单元&amp;quot;&amp;mdash;&amp;mdash; 就像把散落的知识点打包成一个个 &amp;quot;信息包&amp;quot;，LLM 可以直接打开阅读，而不需要经过复杂的图算法转换。这样既保留了信息之间的关联关系，又让 LLM 能够深度参与推理，在推理时随用随取，灵活检索和组装知识。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;为什么是创业团队 Feeling AI？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是什么让这样一支成立不久的初创团队半路杀了出来，一举拿下多项基准的 SOTA？&lt;/p&gt;&lt;p&gt;Feeling AI 的创始人戴勃，是生成式 AI 领域的青年科学家、香港大学的助理教授，他曾在 NTU 和上海 AI 实验室任职，担任生成式 AI 团队的负责人。戴勃博士毕业于香港中文大学，在开源社区爆火的视频生成模型 AnimateDiff，以及 CityNeRF、Scaffold-GS 这几项工作都是他的代表作，AnimateDiff 可以说引领视频生成走向商业化的关键工作，据传多家大厂曾开出天价吸引其加入。&lt;/p&gt;&lt;p&gt;但低调创业的戴勃并没有选择在视频生成领域下钻，而是在 2024 年就押注了世界模型。戴勃带领的核心团队来自清华、港中文、NTU 和米哈游、英伟达、商汤等，团队更是不乏毕业于清华姚班的天才少年。&lt;/p&gt;&lt;p&gt;但作为国内最早在世界模型和 3D 动态交互方向进行尝试的团队，为什么要做 Agentic Memory？答案或许藏在其官方发布的一张海报中。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532053" data-ratio="1.3333333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGymlhZEug8jknoKYFSPNACbbYibY5nDajR7eu1CPP2g8mtjL0SGsSNa4S0nia5M8Gv4p6BksIickkNSYudsvSibCvulWBvD7U67A8/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/fba486f6-b851-4c3a-897a-373033f547f7/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;他们把世界模型的实现分成了 InteractBrain（理解、记忆与规划），InteractSkill（能力与执行）和 InteractRender（渲染与呈现）三层，MemBrain 所代表的记忆能力则是 InteractBrain 的关键组成部分之一。当 OpenAI-o1、DeepSeek-R1 为 AI 带来的推理能力还不足以为物理世界的智能系统达成闭环时，机器像人类一样与动态变化的物理世界共存的能力就至关重要。&lt;/p&gt;&lt;p&gt;也许 Feeling AI 试图回答的正是 &amp;mdash;&amp;mdash; 如何让世界模型真正走向动态世界的智能交互，而与世界动态交互的核心也将由 &amp;ldquo;人&amp;rdquo; 变为 &amp;ldquo;人和 AI&amp;rdquo;。如此看来，在 Agentic Memory 的能力上为世界模型构建护城河也十分合理。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;随着 Mem0 等项目在 GitHub 上迅速走红，DeepSeek 的论文剑指大模型记忆，以及 OpenAI 频繁更新其 &amp;ldquo;Memory&amp;rdquo; 功能接口，一个明确的信号已经释放：在 2026 年的 AI 版图中，谁能解决 Agent 的 &amp;ldquo;随时失忆症&amp;rdquo;，谁就能掌握通往 AGI 的下一把钥匙。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如果说算力是 Agent 的心脏，那么 Memory 正被公认为它的灵魂，智能大脑正在走向卓越记忆能力的比拼。&lt;/strong&gt;正如 Nvidia 科学家 Jim Fan 所言，Agent 的下一步演进不在于参数量的无限堆砌，而在于通过高效的技能库索引与自我反思机制，让 AI 在面对全新、更复杂任务时，能够直接检索并复用此前探索中积累的技能与经验教训。&lt;strong&gt;随着 Memory for Agentic AI 成为基础设施层的核心标配，我们正在见证 AI 从 &amp;ldquo;无状态&amp;rdquo; 的单次调用，向 &amp;ldquo;有意识&amp;rdquo; 的持续进化跨越。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;强大的记忆能力以及适配模型原生的层级化记忆系统&lt;/strong&gt;，意味着 Agentic AI 正从模型能力逐步走向用户体验层面的全面升级，这也将成为 AI 与用户共生、共创的新起点。&lt;/p&gt;&lt;p&gt;MemBrain1.0 Github https://github.com/feelingai-team/MemBrain&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Stable-DiffCoder超越自回归模型！扩散模型在代码生成取得新突破</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 06 Feb 2026 10:16:53 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-06-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-06-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;扩散语言模型（Diffusion Language Models, DLLMs）因其多种潜在的特性而备受关注，如能加速的非自回归并行生成特性，能直接起草编辑的特性，能数据增强的特性。然而，其模型能力往往落后于同等规模的强力自回归（AR）模型。&lt;/p&gt;&lt;p&gt;近日，&lt;strong&gt;华中科技大学和字节跳动&lt;/strong&gt;联合推出了 &lt;strong&gt;Stable-DiffCoder&lt;/strong&gt;。这不仅仅是一个新的扩散代码模型，更是一次关于 「扩散训练能否提升模型能力上限」 的深度探索。&lt;/p&gt;&lt;p&gt;Stable-DiffCoder 在完全复用 Seed-Coder 架构、数据的条件下，通过引入 &lt;strong&gt;Block Diffusion 持续预训练（CPT）及一系列稳定性优化策略，成功实现了性能反超&lt;/strong&gt;。在 多个 Code 主流榜单上（如 MBPP，BigCodeBench 等），它不仅击败了其 AR 原型，更在 8B 规模下超越了 Qwen2.5-Coder ，Qwen3，DeepSeek-Coder 等一众强力开源模型，证明了&lt;strong&gt;扩散训练范式本身就是一种强大的数据增强手段&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531499" data-ratio="0.3819444444444444" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBxZSEU3OC8nlchvGGEricA2J8q6PpNZClu93bVuvTgaG1icMHico4l0RKw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="864" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/5c547812-6950-4828-963d-087f6dd40c9b/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接: https://arxiv.org/pdf/2601.15892&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Github 链接: https://github.com/ByteDance-Seed/Stable-DiffCoder&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;模型链接: https://huggingface.co/collections/ByteDance-Seed/stable-diffcoder&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531500" data-ratio="0.2824074074074074" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBR6sw9z3EFISCtkdpYbg98cuKTcje4IiaVqfkEicSgJBPPAofZZKpqiadQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/f05f84c2-7cb6-4c94-acb8-295204824667/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;扩散过程难以高效学习样本知识&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;扩散过程虽然表面上可以扩充很多数据，可以作为一个数据增强的手段，但是实际上会引入很多噪声甚至错误知识的学习。&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如下面的例子：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531501" data-ratio="0.050397877984084884" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBXhuPcA1m0NQ6yeeiaosgQoU812XZHPpXQXOBca4eO6rBSR2L94R5NGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="754" type="block" data-original-style="width: 408px;height: 21px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/c875ace9-6231-486b-bf7e-b6fc6eecaf58/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;将其 mask 成&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB48p32Tkx2fjWSZeLFZeu6hE225vqDuLXITBMhdu4ggULxo7vFMaW9Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.06230529595015576" data-s="300,640" data-type="png" data-w="642" type="block" data-imgfileid="503531502" data-aistatus="1" data-original-style="width: 400px;height: 25px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/20477662-97e9-4fcc-b809-2e0ea6f08cd5/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;可以发现对于最后一个 mask_n，其只能在看见 a=1，b=2 的情况下去学习 a+b=7，会形成错误的知识映射。最后充其量也只能学到，a=3，b=4 在 a+b = 这个语境下的共现概率更大一点，不能学到明确的加法规则。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;token 推理的知识和流程设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;论文通过建模这个知识的学习来解释这个现象：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531506" data-ratio="0.075" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBzroic0XXqlkTDsUPfYgiabCoFdianOITYDLNQicYdEnTplyQODdU4udIzw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="720" type="block" data-original-style="width: 447px;height: 34px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/75ab220a-cd0c-4786-b262-3e38b4bbe7a0/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;假设 c 是当前可见的样本，根据真实分布通过这些样本在当前位置能够推理出的 token 集合为 C (c)，大小为 K (c)（这里多个 token 同时推理的情景一致，因此只简单的考虑单个 token 推理）。由于使用的真实分布来定义的，所以 c 越多越干净的时候，K (c) 越小。&lt;/p&gt;&lt;p&gt;可以知道模型最后希望学习的分布是 &lt;img data-aistatus="1" data-imgfileid="503531507" data-ratio="0.10843373493975904" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBzWrw71BG0kOhYibDWWGExvTe7TtPHHLicdzibxWJYOFeNDD5NzuEowwaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="332" type="block" data-original-style="width: 221px;height: 24px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9d6c6190-1184-4457-b56d-d2f860e6d2bc/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 27.52%;"&gt;，而要学好这个过程需要满足两个条件：（1）K (c) 比较小；（2）从数据中采样的 c 要尽可能多。&lt;/p&gt;&lt;p&gt;因此，如果用纯双向的扩散过程，在 mask 比例较大的时候，当前 token 见到的 c 变小，不干净的概率变大，导致 K (c) 变大，难以映射到清晰的规则。同时其会产生会产生各种各样的 c，平均每个 c 的学习量会减小。另外，还要保证训练采样的 c 跟推理用的 c 是一致的，才能更好的使用训练学习的知识。&lt;/p&gt;&lt;p&gt;接下来论文通过在 2.5B 的模型设计实验来进一步阐释并证明这个结论。论文从一个 AR model 初始化，然后训练一段新的知识。论文设计了 3 个训练方式来探索：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531509" data-ratio="1.2685370741482966" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBDeMDpdckvdMZpLJEaICbD2BRuVLhRKrJ3D997aGudWs3Zpib6wVfPIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="998" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/f332324e-1e45-4223-bc8d-6a054940763d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;（1）AR-&amp;gt;BiDLLM: 用 AR 的方式继续训练，在 100k step 的时候 CPT 成双向的 DLLM。&lt;/p&gt;&lt;p&gt;（2）ARDLLM-&amp;gt;BiDLLM: 用 AR 的结构，但是使用纯双向的采样模式来训练。然后 100k step CPT 成 BiDLLM。&lt;/p&gt;&lt;p&gt;（3）BiDLLM：使用纯双向的 DLLM 训练。&lt;/p&gt;&lt;p&gt;可以发现，最后效果是（1）&amp;gt;（2）&amp;gt;（3），这也符合前面的理论。不用随机 [MASK] 的（1）方案对于知识有更快的压缩速度，并且转换成 BiDLLM 也保持着最佳性能，这可以证明在要高效的学好一个 DLLM，可以用 AR 或者小 block size 的 block diffusion 来进行知识压缩。另外有趣的是，在 block=32 时（1）和（2）的表现比（3）差，但是在 100k 之后表现比（3）好。100k 之前可以说明，AR 采样的 c 跟 block size=32 推理过程的 c 不太匹配，但是由于 AR 压缩了大量有用的知识，稍微 CPT 一下就能适配这种推理过程。同时也可以说明，AR 这种结构的先验，可能更适合 prompt+response 这种从左侧开始推理的过程。&lt;/p&gt;&lt;p&gt;因此我们将训练流程设计为，先用 AR 压缩一遍知识，然后用 AR 退火的前一个 checkpoint 继续 CPT 成小 block 的 block diffusion，来探索 diffusion 过程的数据增强能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;稳定的 DLLM warmup 策略持续预训练设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;扩散模型的持续预训练通常对超参数的设计（如学习率）非常敏感，容易出现 grad norm 的异常变高，这也会受到各种训练架构的影响。为了保持各种训练架构的学习稳定，以及繁杂的调参过程，团队设计了一种适配的 warmup 策略。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531513" data-ratio="0.4722222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBjSicCBzOeFaqDlVVLCTASLraSVGoicoibY1SPzB0MjbZ5hNTBUNVo5MIA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/6d3be5fb-6f29-426d-b8a8-10eb94633cc3/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;DLLM 的 CPT 过程不稳定主要受到下面 3 个原因影响：&lt;/p&gt;&lt;p&gt;（1）Attention 从单向变成双向&lt;/p&gt;&lt;p&gt;（2）Mask 变多导致任务变得很难&lt;/p&gt;&lt;p&gt;（3）为了对齐 ELBO，会在交叉熵前面乘上加权系数。比如只 mask 了一个 token，会等价于只计算了这个 token 的 loss，会大幅增大这个 token 对于梯度的影响，进而影响 grad norm 和 loss。&lt;/p&gt;&lt;p&gt;由于退火 attention 的方式难以灵活适配 flash attention 等架构，该团队针对（2）（3）来设计 warmup 过程。具体的，在 warmup 阶段将 mask 比例上界逐渐 warmup 到最大值，从而使得一开始任务从易变难。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBIR5ibXxKgrFaYVqpcxto2OmKVpVN2XZXyrDCD0Ch7sdnQu9wF6OXV3g/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.08564814814814815" data-s="300,640" data-type="png" data-w="864" type="block" data-imgfileid="503531514" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/22581437-dabc-4e0c-9dcb-1b75e70fc598/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;其次，在 warmup 阶段去掉交叉熵中加权的系数，从而让每个 token 对 loss 的影响更平稳：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBfeC2vviaDpJFdnJxN6iaGGQ5JWiboxjBIqF0y42vaicunOM7juw99j3ESQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.08101851851851852" data-s="300,640" data-type="png" data-w="864" type="block" data-imgfileid="503531515" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/543292a1-c02a-4b64-bed1-be5cca8c73f7/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Block-wise 截断的噪声调度&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在使用 block diffusion 时，由于通过 cross attention 拼接了干净的前缀，可以使得每个 token 都产生有用的 loss。然而如果使用传统的 noise schedule 会使得有些块不产生 loss 信号，通过求解积分可以算出 block 不产生信号的概率如下，这在小 block 时会特别明显：&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531516" data-ratio="0.15901060070671377" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB7FvAc9fAD2tfa3DGROa7CYjHMIyttibOqtp93ZzuTDrShhZpN1IscAw/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-type="png" data-w="566" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/4e8beb3c-0d06-4021-be9b-204dae721342/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;因此团队做了两个设计：（1）强制每个块都采样一个 token（2）将 noise 采样下界设置为 1/B，这样可以使得至少期望采样一个 token。同时可以避免强制采样 1 个 token 之后，原本对应的 t 过小，从而使得交叉熵加权过大的问题。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531517" data-ratio="0.09523809523809523" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBPWbUlUVsxg6VpDiazwWPrPjCITv9oUZYQW3Ozx7JAmib73AfdaNUrvtQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-type="png" data-w="546" type="block" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/17d45373-734a-46a1-bddb-e50f1d06fb7c/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;实验结果：多个代码 benchmark 在 8B 左右的模型保持领先&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对于 Base 模型&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531520" data-ratio="0.7752808988764045" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB3pr54gNJSJy2LKyRXSzHrK4BOfI9VJUv1xlVuqnHoJlreTKkGH6S8A/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-type="png" data-w="1068" type="block" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/d51b214c-33d9-450d-b189-722a33708718/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB8rLYaxdqYoqnHhakGh7tLV69UmaHrIqgAibI4ibPJNsw2ibrMqVpf81MQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.4305555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531521" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/bdc3c37b-93db-41f8-b830-ce85ee94e9f8/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531523" data-ratio="0.6423220973782772" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBBBE3SfiaxeMPvHacicUiaiaACtNgJdcWIbJ9Xia4YzDKbm8WMO9QZibk6xeA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-type="png" data-w="1068" type="block" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/f84c26fd-f0a9-46b2-b0d8-b3239c9cd454/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Stable-DiffCoder-8B-Base 在代码生成，多代码语言生成，代码推理上表现出色。超过一系列 AR 和 diffusion-based 的模型。另外可以发现模型在稀疏代码语言上（如 C#，PHP 等，预训练中数据较少），相比于 AR baseline 得到了大幅增强，可以证明 DLLM 的训练过程起到了一定的数据增强的效果。同时在代码推理能力上也得到了增强。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对于 Instruct 模型&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Stable-DiffCoder-8B-Instruct 在代码生成，代码编辑，代码推理等任务上做了综合评测，并有着优越的表现。其中在常用的任务（humaneval，mbpp）上大幅超过原有 AR baseline 和其他 8B 左右的 DLLM model。在测试集闭源的 MHPP 达到 qwen32B 的水平，BigCodeBench 上更是超过一系列模型并仅次于 DeepSeek236B 的模型。同时在代码编辑 CanItEdit 任务上更是有着惊艳的效果。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531527" data-ratio="1.1583333333333334" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB7ibZMicA8QwWMaU9ZgCbktTC2zhwia5pF3hdZR5g2nOTvOhQvURlWibbBg/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/c42e4b69-57dd-4d2b-bc8d-4d725666ad0a/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB9rxNBKWokNpJyEVW6HNYXkcupuzFAWbCxTAOZTv8ELYDOdANrq4PLQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.425" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531529" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/2361af61-ba2c-4b4d-b424-404aa2fa7a2a/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531540" data-ratio="0.8231481481481482" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBZ2CoatEo1hryYYB58nt8n7Mr1D1f46W1Z2BeuicgDicp2XChCkNyCaWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/9a7c7300-7672-47a4-ade3-9715d21d7b9b/640.png" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531541" data-ratio="0.7527777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBsqV90WjWzxT0YRea5bsNXGB1C7gqF2WcbltYJ2nYzjVCkLIUPicm15A/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/ce77c83b-2094-47ca-95ec-2b6eee5ce0c5/640.png" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBB5I9ibeicl41DVlRF8UOibMbfzFEdo5VePajnAXgOgRPL70ibp9VpBLABw/640?wx_fmt=png&amp;from=appmsg#imgIndex=20" data-ratio="0.8937007874015748" data-s="300,640" data-type="png" data-w="1016" type="block" data-imgfileid="503531542" data-aistatus="1" data-original-style="null" data-index="22" src="https://image.jiqizhixin.com/uploads/editor/ff333f9d-ab99-4de8-a47e-af82c7205485/640.png" alt="图片" data-report-img-idx="20" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;总结与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Stable-DiffCoder 的发布，打破了 「扩散模型只能做并行加速」 的刻板印象。它证明了：&lt;strong&gt;扩散训练范式本身就是一种极佳的表征学习手段&lt;/strong&gt;。通过合理的课程设计及稳定性优化，扩散模型完全可以在代码理解和生成质量上超越传统的 AR 模型。&lt;/p&gt;&lt;p&gt;对于未来的大模型演进，Stable-DiffCoder 提示了一条新路径：也许我们不需要抛弃 AR，而是将 AR 作为高效的知识压缩器，再利用 Diffusion 作为 「强化剂」，进一步推高模型的智能上限。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>硬碰硬！刚刚，Claude Opus 4.6与GPT-5.3-Codex同时发布</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Fri, 06 Feb 2026 09:39:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-06</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-06</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在春节来临之前，海外大模型先来了一波硬碰硬的发布。&lt;/p&gt;&lt;p&gt;北京时间 2 月 6 日凌晨，Anthropic 与 OpenAI 相继推出了新版本基础大模型，分别是 Claude Opus 4.6 与 GPT-5.3-Codex。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqH8XTFqWohxCt0EPqY2ibHZnSb2AHyIHsBqkjodicfnRdfWusvS0WIzkbYiaEhDDsDXy1a05VMBdVrBUGVVGa5RRJyLJrXSdv8ODE/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3747645951035782" data-s="300,640" data-type="png" data-w="1062" type="block" data-imgfileid="503532059" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/7b9fc45f-4069-4bab-a207-bef23a6b2775/640.png" alt="Image" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFrhwDickSGkp6XAlGZEyhN8b0BOunhmzqKf0A981M8WPTHzwV9Whdwicfb7AkEzJRSOqjoTgTjnMw8kiblqCdV7AiamHBicVdhUNM4/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.8886827458256029" data-s="300,640" data-type="png" data-w="1078" type="block" data-imgfileid="503532061" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c3679b89-96f9-4983-b2c1-29b747aa8947/640.png" alt="Image" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;昨天两家还在因为 AI 里面的广告而论战，今天在大模型发布上又撞车了。话不多说，直接看他们的模型能力如何。&lt;/p&gt;&lt;h3&gt;Claude Opus 4.6&lt;/h3&gt;&lt;p&gt;Claude Opus 4.6 是 Anthropic 对其旗舰人工智能模型的一次重大升级。在这代模型上，规划更加谨慎，能够维持更长时间的自主工作流程，并在关键的企业基准测试中超越了包括 GPT-5.2 在内的竞争对手。&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/VDYp_--scQFIR6r1fR-3pA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/56d3bec8-ba27-4573-9543-3207883a7a7a/1770341765430.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2F5L8bhP5dIqEmDTCHKk4C9kap4IAPXMQPfiaiaInfdkWk2FEuNstuB99QWrzxoKGo5yXyKFg5W2A9qpmq2oOANGtvLcbuHCbRppteiceUtZD44o%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4374467935285657611" data-ratio="1.7777777777777777" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4374467935285657611" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="1920" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4374467935285657611"&gt;&lt;div data-v-2cc14175=""&gt;&lt;div data-v-2cc14175="" data-v-a62cf6ba=""&gt;&lt;div data-v-a62cf6ba=""&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;新模型首次拥有 100 万 token 的上下文窗口，使 AI 能够处理和推理比以往版本多得多的信息。Anthropic 还在 Claude Code 中引入了类似于 Kimi K2.5 的「智能体团队」功能 &amp;mdash;&amp;mdash; 一项研究预览功能，它允许多个 AI 智能体同时处理编码项目的不同方面，并进行自主协调。&lt;/p&gt;&lt;p&gt;Anthropic 强调，Opus 4.6 可将其增强的功能应用于一系列日常工作任务，包括运行财务分析、进行研究以及使用和创建文档、电子表格和演示文稿。现在在 Cowork 环境中，Claude 可以自主地执行多任务，Opus 4.6 可以代表人类运用所有这些技能。&lt;/p&gt;&lt;p&gt;Opus 4.6 在多项评估中均表现出色。例如，它在智能体编码评估工具 Terminal-Bench 2.0 中取得了最高分，并在「人类最后的考试」（一项复杂的多学科推理测试）中领先于所有其他前沿模型。在 GDPval-AA（一项评估模型在金融、法律和其他领域中具有经济价值的知识工作任务上的表现的测试）中， Opus 4.6 的表现比业界次优模型（OpenAI 的 GPT-5.2）高出约 144 个 Elo 分数，比其前身（Claude Opus 4.5）高出 190 分。此外，Opus 4.6 在 BrowseComp 测试中也优于其他所有模型，该测试用于衡量模型在线查找难寻信息的能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqH9xkdsaRO47a9OEElYqMXNqExcX4zKHUZqibzr6GYc8uKtvGKicEI9K92u6TmNhx8IFHZic3fSGnJ6DKQbgxpTK5h6icf3bZxDHz8/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.1416666666666666" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532062" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f952235b-25cc-4a1a-b164-d651cb95943e/640.png" alt="Image" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;Claude Opus 4.6 现已在 claude.ai、API 以及所有主流云平台上线，定价保持不变，每百万 token 5 美元 / 25 美元。&lt;/p&gt;&lt;p&gt;目前大模型的一个常见问题是「上下文腐烂」，即当对话 token 数量超过一定阈值时，模型性能会下降。Opus 4.6 的性能显著优于其前代产品：在 MRCR v2 的 8 针 1M 变体测试中（该测试如同大海捞针），Opus 4.6 的得分为 76%，而 Sonnet 4.5 的得分仅为 18.5%。这标志着模型在保持最佳性能的同时，能够利用的上下文信息量发生了质的飞跃。&lt;/p&gt;&lt;p&gt;为了证明 Opus 4.6 的强大智能体能力，Anthropic 的一名研究员使用 16 个智能体从零开始构建了一个基于 Rust 的 C 语言编译器，设定任务后就基本放手不管了。最后 AI 输出的代码长达 10 万行，可以编译 Linux 内核，耗资 2 万美元，超过 2000 次 Claude Code 会话，历时两周。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHsG4iaTFDic72Pt32aia14Gs0XgK220KrjzgzT0aqRXm0XSy4dD5NM3MWibHPAInejctTJ0CMfXpqRZAb907J7t2pQTqK7FQjeRMA/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5333333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532063" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7810e0db-6f5e-4383-be5e-8cec8a89b5f3/640.png" alt="Image" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;该编译器可以在 x86、ARM 和 RISC-V 上构建可启动的 Linux 6.9，它通过了 GCC 99% 的压力测试，可以编译 FFmpeg、Redis、PostgreSQL、QEMU，还通过了开发者的终极考验：编译并运行了 Doom 游戏。&lt;/p&gt;&lt;p&gt;该编译器的代码：https://github.com/anthropics/claudes-c-compiler&lt;/p&gt;&lt;section&gt;&lt;span data-cover="http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F5L8bhP5dIqGR6vjmfic8KPUETOK4z0JicbWXIoW2GyEib96ZQ70zeQXwjcK31YZict1kDibuULsLmicUUPiaxgbm2Bgb1EdZ4T0F82tr8STEUS1a64%2F0%3Fwx_fmt%3Djpeg" data-mpvid="wxv_4374485449742958593" data-ratio="1.7777777777777777" data-src="https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;auto=0&amp;vid=wxv_4374485449742958593" data-vh="371.8125" data-vidtype="2" data-vw="661" data-w="1920" height="384" scrolling="no" width="661"&gt;&lt;div data-key="wxv_4374485449742958593"&gt;&lt;div data-v-2cc14175=""&gt;&lt;div data-v-2cc14175="" data-v-a62cf6ba=""&gt;&lt;div data-v-a62cf6ba=""&gt;&lt;div data-v-a62cf6ba=""&gt;&lt;div data-v-a62cf6ba=""&gt;&lt;a href="https://mp.weixin.qq.com/s/VDYp_--scQFIR6r1fR-3pA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6933d3f9-2aba-4158-83d3-3b1997706642/1770341818798.png" style="width: 700px;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;虽然没有人类参与编写代码，但研究人员不断重新设计测试，在智能体程序互相干扰时构建 CI 管道，并在所有 16 个智能体程序都卡在同一个 bug 时创建变通方法。&lt;/p&gt;&lt;p&gt;看起来，在未来加入 AI 的工作流程中，人的角色已经从编写代码转变为构建让 AI 能够编写代码的环境。&lt;/p&gt;&lt;h3&gt;GPT-5.3-Codex&lt;/h3&gt;&lt;p&gt;在 OpenAI 这边，新一代模型 GPT-5.3-Codex 的发布紧随其后。奥特曼称其拥有目前最佳的编码性能，进一步释放了 Codex 的潜能。&lt;/p&gt;&lt;p&gt;GPT-5.3-Codex 在多项基准上刷新纪录：在 SWE-Bench Pro 上达到 56.8%，在 Terminal-Bench 2.0 上达到 77.3%，同时相比此前版本运行更快、消耗的 token 更少。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqE79zXoyqFj0f31fjy8RYlF4hJCQpMOCMdKwpRBYfdaJZrxoYApnMibSnibJVc3UmNfyiaultXcbJGOL1YHib0y98TfKmsyoLFdqw8/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.725" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532064" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a7650f97-f0e7-4074-928b-26ec2bd11d41/640.png" alt="Image" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqH8RNMbK9UnSDbS3lTBGygydJ1kd1VniaarvyydQKZgpXks6pC9GjxBS9f3n1Fgbh9XVcPjZeRmp8EaeMz66tunyW4o9CrIVqqU/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="1.174342105263158" data-s="300,640" data-type="png" data-w="608" type="block" data-imgfileid="503532065" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/dd3592b5-675d-4d09-83e2-29c8bb2b1f99/640.png" alt="Image" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEK5iaQOv094chBgEejuibqVbhHfWicGFyasSSsMnstic2MKPe9yfY4y0kzUpqibLTSW6sFjSqPuXQ7LfV2lGey7jRdZb2ABgI5RWWc/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.662962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532066" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/63e56109-78cc-4600-9622-ab1b27ba8689/640.png" alt="Image" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;OpenAI 表示，该模型融合了 GPT-5.2-Codex 的前沿编码性能和 GPT-5.2 的推理及专业知识能力，速度提升了 25%。这使其能够胜任需要研究、工具使用和复杂执行的长时间任务。&lt;/p&gt;&lt;p&gt;它就像一位真正的同事一样，你可以在 GPT-5.3-Codex 工作时对其进行指导和交互，而不会丢失上下文信息。借助 GPT-5.3-Codex，Codex 从一个能够编写和审查代码的代理，变成了一个几乎可以执行开发人员和专业人士在计算机上的任何操作的代理。&lt;/p&gt;&lt;p&gt;除了更加强大的编码能力外，GPT-5.2-Codex 在 OpenAI 长期关注的美学方面又一次有了长足的进步。&lt;/p&gt;&lt;p&gt;在这次发布中，OpenAI 让 GPT-5.3-Codex 构建了两款游戏：一款是 Codex 应用发布时推出的赛车游戏的第二版，另一款是潜水游戏。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/5L8bhP5dIqGRN2QE8Z8oNd7W4rvg0FtldbkGSQG9qsfR3UQMFeU18wOB0qU73WNB22CzZKiacQVibr5D1TLMdSn2KwcSaqlwibR90CAVibh5AFc/640?wx_fmt=gif&amp;from=appmsg#imgIndex=8" data-ratio="0.55125" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503532067" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/6a5ad3fc-bfcd-4231-aebc-8610ad87d4a5/640.gif" data-order="0" alt="Image" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/5L8bhP5dIqGCsajBJnE0RaafibgaVl8J3wySxNe2SXJU0R98F3CtYb9SPFjzNRVTDanJIEZkZRq99vuvquOZIxCRlYj7xzQSZM11Q3iapYVGo/640?wx_fmt=gif&amp;from=appmsg#imgIndex=9" data-ratio="0.58" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503532068" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/23befd2a-2de5-4991-9bd7-d7a88cc8da9a/640.gif" data-order="1" alt="Image" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700px;"&gt;&lt;/section&gt;&lt;p&gt;OpenAI 表示，GPT-5.3-Codex 利用其网页游戏开发技能以及预先设定的通用后续提示（例如「修复错误」或「改进游戏」），自主地迭代开发了数百万个 token。&lt;/p&gt;&lt;p&gt;这次发布的 GPT-5.3-Codex ，OpenAI 对其的期望远不止步于一个智能编码模型，而是一个能够「Beyond coding」，实现工作助理的智能体。&lt;/p&gt;&lt;p&gt;GPT-5.3-Codex 能够支持软件生命周期中的所有工作 &amp;mdash;&amp;mdash; 调试、部署、监控、编写产品需求文档、编辑文案、用户研究、测试、指标分析等等。&lt;/p&gt;&lt;section&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 700px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqF7B1wMJjdnb5cuR5aswmPymT8Z6Viblye44bUmNNUv7gIt8WwvV8TgxuHg1MthTR1ctmJCKUuRFI1LtXU3kgVmnTLqsWZGHicPA/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532069" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/e7e12a58-1dfe-4382-acd3-66f134fdb2db/640.png" alt="Image" data-report-img-idx="8" data-fail="0"&gt;&lt;span class="fr-inner"&gt;GPT-5.3-Codex 输出净值分析表格示例&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;OpenAI 认为，随着模型能力的不断增强，差距不再仅仅在于智能体能够做什么，而是在于人类如何轻松地与多个并行工作的智能体进行交互、指导和监督。鉴于此，Codex 应用可以让管理和指导智能体变得更加便捷，而 GPT-5.3-Codex 的加入更使其交互性更强。&lt;/p&gt;&lt;p&gt;借助新模型，Codex 会频繁更新，让你随时了解关键决策和进展。人们无需等待最终输出，即可实时互动 &amp;mdash;&amp;mdash; 提出问题、讨论方法，并共同探索解决方案。GPT-5.3-Codex 会语音播报其运行过程，响应反馈，并让你从始至终掌握整个流程。&lt;/p&gt;&lt;p&gt;最后，OpenAI 表示，GPT-5.3-Codex 的训练和部署使用了 Codex，OpenAI 的许多研究人员和工程师都表示，他们现在的工作与两个月前相比发生了根本性的变化。&lt;/p&gt;&lt;p&gt;例如，研究团队使用 Codex 来监控和调试本次版本的训练运行。它不仅加速了基础设施问题的调试，还帮助追踪整个训练过程中的模式，对交互质量进行深入分析，提出修复方案，并构建了丰富的应用程序，使研究人员能够精确地了解模型行为与先前模型之间的差异。&lt;/p&gt;&lt;p&gt;工程团队使用 Codex 对 GPT-5.3-Codex 框架进行了优化和适配。当出现影响用户的异常极端情况时，团队成员利用 Codex 识别上下文渲染错误，并找出缓存命中率低的根本原因。在整个发布过程中，GPT-5.3-Codex 通过动态扩展 GPU 集群来应对流量高峰并保持延迟稳定，持续为团队提供支持。&lt;/p&gt;&lt;p&gt;在 Alpha 测试期间，一位研究人员想要了解 GPT-5.3-Codex 每回合能完成多少额外工作，以及由此带来的生产力提升。GPT-5.3-Codex 生成了几个简单的正则表达式分类器，用于估算用户澄清请求的频率、正面和负面反馈以及任务进度，然后将这些分类器可扩展地应用于所有会话日志，并生成一份包含结论的报告。&lt;/p&gt;&lt;p&gt;GPT-5.3-Codex 已包含在 ChatGPT 的付费套餐中，但 API 还需要等待一段时间。&lt;/p&gt;&lt;p&gt;OpenAI 报告说，由于基础设施和推理堆栈的改进，Codex 用户现在运行 GPT-5.3-Codex 的速度也提高了 25%，从而实现了更快的交互和更快的结果。&lt;/p&gt;&lt;h3&gt;结语&lt;/h3&gt;&lt;p&gt;海外的大模型已经轮番上阵，在春节前的最后这几天，国内大模型也必然会卷起来，包括 DeepSeek v4 也许即将到来。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532070" data-ratio="0.2544731610337972" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/5L8bhP5dIqH3LgQdfaYbATVaW4UBQbAmhXLuAlzk5tVzZibwZFLym9RN04h9dwWvXs5MAyzdqWBZRG6FKVLFcNiaG3a7kZI3DOW5r8Yg4N8LU/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=11" data-type="jpeg" data-w="1006" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/24e02c0d-849e-4541-acff-aa63cd4882ac/640.png" alt="Image" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;你期待住了吗？&lt;/p&gt;&lt;p&gt;参考内容：&lt;/p&gt;&lt;p&gt;https://www.anthropic.com/news/claude-opus-4-6&lt;/p&gt;&lt;p&gt;https://www.anthropic.com/engineering/building-c-compiler&lt;/p&gt;&lt;p&gt;https://openai.com/index/introducing-gpt-5-3-codex/&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>众智FlagOS实现面壁新模型MiniCPM-o 4.5：“发布即适配”性能全面反超原生</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Thu, 05 Feb 2026 21:26:30 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2026年2月3日，面壁智能正式发布并开源了集语言、视觉、语音于一体的全模态大模型 MiniCPM-o 4.5，&lt;strong&gt;众智FlagOS系统软件栈，成功助力该模型在发布当日即完成对六大主流AI芯片的适配与优化，并实现端到端推理性能全面超越各芯片原生方案&lt;/strong&gt;，这标志着国产基础软件在破解&amp;ldquo;跨芯适配难&amp;rdquo;行业痛点上取得里程碑式突破。&lt;/p&gt;&lt;p&gt;作为首个&lt;strong&gt;全双工全模态&lt;/strong&gt;大模型，面壁MiniCPM-o 4.5 首次实现&amp;ldquo;类人&amp;rdquo;感知交互，能够根据环境&amp;ldquo;边看、边听、边说&amp;rdquo;，保证输入输出实时同步。这就&lt;strong&gt;对底层推理系统的计算效率、资源调度与多模态数据流的低延迟处理能力提出了极高要求&lt;/strong&gt;。对此，&lt;strong&gt;众智 FlagOS 凭借其统一、高性能的跨芯片系统软件栈，提供了从算子优化到编译调度的全链路加速方案&lt;/strong&gt;，有效解决了大模型在多元硬件上保持高实时性、高吞吐推理的关键难题，&lt;strong&gt;实现了&amp;ldquo;一次开发，跨芯运行&amp;rdquo;的效果&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在确保模型精度无损失的前提下，基于 FlagOS 版本的MiniCPM-o 4.5在全部六款芯片上均实现了端到端推理效率的显著提升，不同场景下平均加速比为7.76%&amp;mdash;22.4%。&lt;strong&gt;在统一硬件条件下&lt;/strong&gt;，FlagOS 版本相比 CUDA 版本提升端到端推理效率 6.10%，&lt;strong&gt;与各芯片自身的原生系统软件栈相比，&lt;/strong&gt;FlagOS 带来的性能提升更为显著，例如在 Nvidia 硬件上提升 6.10%，在 Hygon 硬件上提升 4.57%，整体平均提升幅度突出。&lt;strong&gt;而在长负载任务的平均测试中&lt;/strong&gt;，FlagOS 版本的端到端性能比例达到 106.10%，全面验证了其优化效果。这一系列数据强有力地证明，FlagOS不仅解决了&amp;ldquo;有没有&amp;rdquo;的适配问题，更实现了&amp;ldquo;好不好&amp;rdquo;的性能超越，为应用方提供了更具性价比的多元算力选择。&lt;/p&gt;&lt;p&gt;此次合作的成功实践，为面临硬件适配困境的模型厂商提供了明确路径，通过集成FlagOS这类统一软件栈，能够以较低成本快速实现模型在多芯片平台的高性能部署，从而将研发重心回归模型创新本身。随着FlagOS生态的持续发展，其&amp;ldquo;一次开发、多芯运行&amp;rdquo;的能力有望成为AI应用生态的重要基础，推动大模型技术更高效、更经济地服务于各行各业。随着FlagOS生态的持续发展，有望成为驱动AI应用生态繁荣的关键基础设施，最终推动大模型技术以更低的部署成本、更灵活的硬件选择，加速赋能千行百业。&lt;/p&gt;&lt;p&gt;FlagOS 是北京智源人工智能研究院联合众多科研机构、芯片企业、系统厂商等国内外机构共同发起并创立的面向多种 AI 芯片的统一、开源系统软件栈，旨在解决不同 AI 芯片大规模落地应用的问题，构建「模型 - 系统 - 芯片」三层贯通的开放技术生态，实现 &amp;ldquo;一次开发，跨芯运行&amp;rdquo; 的效果。&lt;/p&gt;&lt;p&gt;在此次适配中，FlagOS 提供了一套&lt;strong&gt;&amp;ldquo;嵌入优化、自动加速&amp;rdquo;的便捷方案&lt;/strong&gt;。它通过智能插件让模型能直接被主流推理框架识别调用，同时将深度优化的核心算子库内置至模型中。运行时，系统会自动将关键计算切换为针对不同芯片优化的版本，而开发者无需修改任何代码。最后，统一编译工具会确保这些优化指令精准高效地在各类芯片上执行。该方案的核心价值在于将复杂的芯片适配与性能优化工作&lt;strong&gt;封装在系统底层&lt;/strong&gt;，使开发者和用户能够以&amp;ldquo;拿来即用&amp;rdquo;的方式，便捷地在多种硬件上获得更流畅、更快速的体验，这有效降低了前沿模型落地应用的技术门槛与适配成本，为AI技术在不同计算设备上的广泛部署提供了更加可行的路径。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>强化学习远不是最优，CMU刚刚提出最大似然强化学习</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 05 Feb 2026 16:05:16 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在大模型时代，从代码生成到数学推理，再到自主规划的 Agent 系统，强化学习几乎成了「最后一公里」的标准配置。&lt;/p&gt;&lt;p&gt;直觉上，开发者真正想要的其实很简单：&lt;strong&gt;让模型更有可能生成「正确轨迹」&lt;/strong&gt;。从概率角度看，这等价于最大化正确输出的概率，也就是经典的最大似然（Maximum Likelihood）目标。&lt;/p&gt;&lt;p&gt;然而，一项来自 CMU、清华大学、浙江大学等研究机构的最新工作指出了一个颇具颠覆性的事实：&lt;/p&gt;&lt;p&gt;现实中广泛使用的强化学习，并没有真正在做最大似然优化。严格的理论分析显示，&lt;strong&gt;强化学习只是在优化最大似然目标的一阶近似&lt;/strong&gt; &amp;mdash;&amp;mdash; 距离我们以为的最优训练目标，其实还差得很远。&lt;/p&gt;&lt;p&gt;正是基于这一观察，研究团队对强化学习的目标函数进行了重新审视，提出了最大似然强化学习（Maximum Likelihood Reinforcement Learning）：将基于正确性的强化学习重新刻画为一个潜变量生成的最大似然问题，进一步引入一族&lt;strong&gt;以计算量为索引的目标函数，使训练目标能够逐步逼近真正的最大似然优化&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGXuoIQIL9DDQbHQhbEgTCEibXicmHlOs3jcWbicTKpO9BE3WCR4WJsvlOG9ntMNWnxQvib5rGaC0ZF7fnaOU99sZAhnEpQlUFJD7M/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.37777777777777777" data-type="png" data-w="1080" data-width="1560" data-height="590" data-imgfileid="503531857" data-aistatus="1" data-original-style="background-color: transparent;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ac6327ab-34b3-48e0-866b-2a4c7cc76acd/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Maximum Likelihood Reinforcement Learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2602.02710&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目地址：https://zanette-labs.github.io/MaxRL/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-pm-slice="0 0 []"&gt;Github 地址：https://github.com/tajwarfahim/maxrl&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml8vy9n9qg" data-pm-slice="0 0 []"&gt;&lt;strong&gt;传统强化学习的「卡脖子」问题&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在代码生成、数学推理、多步决策这些任务中，我们已经形成了一种几乎默认的共识：&lt;strong&gt;只要反馈是二值的、过程是不可微的，就用强化学习。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;强化学习这套范式，支撑了从 AlphaGo 到大语言模型推理能力提升的一系列关键进展。&lt;/p&gt;&lt;p&gt;从端到端的角度看，强化学习就是给定一个输入，模型隐式地诱导出一个「成功概率」. 如果不考虑可微性约束，最自然、也最原则性的目标，就是&lt;strong&gt;最大似然&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;但论文研究团队发现：&lt;strong&gt;基于期望奖励的强化学习，其实只是在优化最大似然目标的一阶近似&lt;/strong&gt;。更具体地说，最大似然目标在总体层面可以展开为一系列以 pass@k 事件为基的项，而标准强化学习只优化了其中的一阶项。&lt;/p&gt;&lt;p&gt;简单来说，强化学习并没有真正最大化「模型生成正确答案的概率」，而是在优化一个与真实似然存在系统性偏差的替代目标。&lt;/p&gt;&lt;p&gt;这也解释了一个广泛存在却难以言说的现象：&lt;strong&gt;强化学习早期进展迅速，但越到后期，性能提升越困难&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;研究团队针对这一新发现，对「基于正确性反馈的强化学习」进行了重新刻画，论文的主要贡献如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;将基于正确性的强化学习形式化为一个&lt;strong&gt;潜变量生成的最大似然问题&lt;/strong&gt;，并证明标准强化学习仅优化了最大似然目标的一阶近似。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;提出了一族&lt;strong&gt;以计算量为索引的目标函数&lt;/strong&gt;，通过对 pass@k 事件进行 Maclaurin 展开，在期望回报与精确最大似然之间实现连续插值。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;推导出一种简单的 &lt;strong&gt;on-policy 估计器&lt;/strong&gt;，其期望梯度与该计算量索引的似然近似目标完全一致，这意味着增加采样真正改善了被优化的目标本身。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml8w0t7x10l1" data-pm-slice="0 0 []"&gt;&lt;strong&gt;最大似然：真正改进优化目标&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;研究团队认为，最大似然估计在有监督学习中表现卓越，为什么不直接在强化学习中实现它？&lt;/p&gt;&lt;p&gt;上一节中的观察启示我们：可以构造一个随计算量变化的目标函数族，逐步引入更高阶项；随着可用计算资源的增加，该目标函数族将逐渐收敛到完整的最大似然目标。&lt;/p&gt;&lt;p&gt;论文通过一系列推导，将最大似然目标在失败事件方面进行麦克劳林展开：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0sKMpgeBCegn17CxhOpButkD0TxibuRZmxCT8qSsTAYlSgr9mC0ic1pWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.13984168865435356" data-type="png" data-w="758" data-width="758" data-height="106" data-imgfileid="503531860" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c62445d5-3730-4f5f-aa85-5e82e193972d/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;展开式中的最大似然梯度很难用有限样本进行估计。&lt;/p&gt;&lt;p&gt;特别是，估计大 k 值的 pass@k 梯度需要越来越多的样本，尤其是在通过率 p 很小的情况下。这种有限样本的困难正是提出最大似然强化学习（MaxRL）的动机所在。&lt;/p&gt;&lt;p&gt;研究团队将 MaxRL 定义为一类强化学习方法，它们显式地以最大似然为目标，而不是以通过率为目标，同时在有限采样和不可微生成的条件下仍然可实现。下面我们考虑一种实现该目标的原则性方法。&lt;/p&gt;&lt;p&gt;考虑通过将麦克劳林展开式截断为有限阶来近似最大似然目标，然后估计该目标。对于截断级别 T &amp;isin;N，我们将固定输入 x 的截断最大似然目标定义为：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0Ujo4WlNL7PmxclCBVcAibXpgOhMoU1tibExbbraj3Wgsyj9npAsQDMXw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.25" data-type="png" data-w="440" data-width="440" data-height="110" data-imgfileid="503531863" data-aistatus="1" data-original-style="background-color:transparent;width:325px;height:81px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/615970f3-0745-4385-871c-9d9fa8816b6d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 40%;"&gt;&lt;/section&gt;&lt;p&gt;对其求导得到截断的总体梯度：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0NKlibHsEUD57jqkgDb5swhe4JNxficxEVlaS47ich0g6GibiaSxMZlsX4PQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.20072992700729927" data-type="png" data-w="548" data-width="548" data-height="110" data-imgfileid="503531864" data-aistatus="1" data-original-style="background-color:transparent;width:410px;height:82px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/33355cb6-1387-47c6-af8e-29f11c384479/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 40%;"&gt;&lt;/section&gt;&lt;p&gt;这定义了一族目标函数：T = 1 还原为强化学习，T &amp;rarr; &amp;infin; 还原为最大似然，中间的 T 值则在两者之间插值。因此，截断级别 T 直接控制了有助于学习的正确性事件的阶数。随着在 rollout 方面消耗更多的计算量，对更高阶梯度的估计变得可行。&lt;/p&gt;&lt;p&gt;换句话说： MaxRL 提供了一个原则性框架，用于通过增加计算量来换取对最大似然目标更高保真度的近似。&lt;/p&gt;&lt;p&gt;上述公式已经给出了一种可行的无偏估计思路：利用&lt;strong&gt; pass@k 梯度估计器&lt;/strong&gt;，对有限级数中的每一项分别进行近似。在这一策略下，&lt;strong&gt;任何对 pass@k 估计器的改进，都会直接转化为对截断最大似然目标的更优梯度估计&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;不过，在本篇论文中，研究者采取了一条不同的路径，将带来&lt;strong&gt;更为简洁的估计器形式&lt;/strong&gt;，同时也提供了一个&lt;strong&gt;新的理解视角&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;最大似然目标的梯度可以写成如下的条件期望形式：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGHVnJCjTc9kzDZ0gLhXFic9ZFIqucgDLuV7ibmBc4Ric7EgiavQm2iaQJSp5P6NquZZClxOCVDw8PcdgQDSRicJIAT75t5iczNjHB9PI/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.0784313725490196" data-type="png" data-w="918" data-width="918" data-height="72" data-imgfileid="503531866" data-aistatus="1" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/0a9ab980-9075-4a12-ab6a-cd6f773e5d65/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;该定理表明，&lt;strong&gt;最大似然梯度等价于仅对成功轨迹的梯度进行平均&lt;/strong&gt;。这一解释为构造具体的梯度估计器提供了直接途径：只需用采样得到的成功轨迹，对上述条件期望进行样本平均即可。&lt;/p&gt;&lt;p&gt;其核心洞见在于：&lt;strong&gt;最大似然目标的梯度可以表示为在「成功条件分布」下的期望&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;因此，本文采用了一种简单的策略：从非条件化的策略分布进行采样，但只对成功轨迹进行平均，得到了强化学习风格的估计器，其具备随着 rollout 数的增加，对最大似然梯度的近似将不断改善的特性。&lt;/p&gt;&lt;p&gt;换言之，在 MaxRL 框架下，额外的计算资源不仅改善了估计质量，更直接改进了被优化的目标本身。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml8wev0dip9" data-pm-slice="0 0 []"&gt;&lt;strong&gt;令人惊讶的效率进步&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在实验中，这一改变带来了远超预期的收益。研究团队在多个模型规模和多类任务上，对 MaxRL 进行了系统评估，结果显示：MaxRL 在性能与计算效率的权衡上均稳定地优于现有强化学习方法。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFTkiccibNjCgswdCE66Mg9fiaWlUIJhSKsX8BeedtQV1icUajoWXgZ3gwzfayQYqibm35PicycvXqSEcGkE2iamliaTrSJkdYR5ZJiaKxQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.4509283819628647" data-type="png" data-w="754" data-width="754" data-height="340" data-imgfileid="503531869" data-aistatus="1" data-original-style="background-color: transparent;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e7b7650e-73c6-4f47-a36a-7056b7fb26fb/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;实验结果直观展示了 MaxRL 在训练效率上的优势。在相同训练步数下，MaxRL 性能提升明显更快，并且随着 rollout 数的增加，MaxRL 持续受益。&lt;/p&gt;&lt;p&gt;这种优势并不只体现在训练阶段，相较于使用 GRPO 训练的模型，MaxRL 测试时的 scaling 效率最高可提升 &lt;strong&gt;20 倍&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0icJpHqwQbic4GRzVNibJFbhWMMBu3nmxqoyNuwDQrqbvLy3lCG1qdD42Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.36666666666666664" data-type="png" data-w="1080" data-width="1103" data-height="404" data-imgfileid="503531872" data-aistatus="1" data-original-style="background-color: transparent;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/b5c1667c-5faf-44fa-80f7-74986eeae78f/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在迷宫任务上，无论测试时的采样预算 k 取何值，随着训练 rollouts 的增加，MaxRL 都能持续降低 &amp;minus;log (Pass@k)，而 GRPO 与 RLOO 的改进幅度则明显更早趋于平缓。这一结果直观地展示了 MaxRL 在训练阶段更优的性能&amp;ndash;效率权衡。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0U9JQ5tnNbC6xSzG0ngUAJ2F8POGe8B2WicX1ojOriaR6HA7ZjbfSj0Yg/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.2962962962962963" data-type="png" data-w="1080" data-width="1103" data-height="327" data-imgfileid="503531871" data-aistatus="1" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/60495a1f-2195-4d9f-984f-44c0ee01372c/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;比较在不同 pass@k 设置下各方法随训练中采样计算增加时的优化趋势，可以看到，对于 GRPO 与 RLOO，曲线在早期下降后迅速变平，说明额外采样主要用于降低噪声；而 MaxRL 在不同 k 值下均保持持续下降，推动模型不断逼近一个更接近最大似然的优化目标。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFMC8zEw4nXe8BqT3U2Spu3byibRk9BaUH4PSHfufU3QdWpORQgJt0Bia8ks0B7zbgpCZb8HEg27yWhdyNoJjW8NvIvicwt5gCTQk/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.6314814814814815" data-type="png" data-w="1080" data-width="1085" data-height="685" data-imgfileid="503531873" data-aistatus="1" data-original-style="background-color: transparent;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/3c75cffa-529f-43dd-845c-ae69b0cca7c5/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在更大规模设置下，MaxRL 的优势依然保持稳定。这表明，MaxRL 所带来的改进并非依赖于特定规模或超参数设置，当训练规模扩大时，MaxRL 并未出现收益递减过快或优势消失的现象。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFRp9SvrfUoAKwQDAREIQUxtT8PURSc2umYjeZLUQtTy0RmDqyBDibo7keKrEwicZdy21mBPZRTdtdCnfKXynibluK652Tl7HKE2E/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.33796296296296297" data-type="png" data-w="1080" data-width="1111" data-height="375" data-imgfileid="503531874" data-aistatus="1" data-original-style="background-color: transparent;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/1afd92f9-2f0f-4b4b-976e-9ed298905f37/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;进一步的实验结果表明，MaxRL 的优势并不依赖于过于理想化的实验条件，即使在反馈存在噪声或验证信号并非完全可靠的设置下，MaxRL 仍然能够保持相对稳定的性能优势。&lt;/p&gt;&lt;p&gt;总体来看，MaxRL 为不可微、基于采样的学习问题提供了一种更为深入的解法。它通过一个随计算量自然扩展的目标框架，系统性地逼近真正的似然优化。&lt;/p&gt;&lt;p&gt;当优化目标本身可以随算力演进、逐步逼近最大似然，强化学习究竟会成为通往通用智能的长期答案，还是只是通往下一个训练范式的过渡方案？&lt;/p&gt;&lt;p&gt;更多信息，请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 Workshop二轮征稿开启：聚焦终身智能体的学习、对齐、演化</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 05 Feb 2026 15:59:36 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/cf9051f7-7424-4baa-9f8d-540de28cad89/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;人工智能正在进入一个新的转折点。&lt;/p&gt;&lt;p&gt;以大语言模型（LLM）、强化学习（RL）和具身智能（Embodied AI）为核心的 &lt;strong&gt;AI Agent &lt;/strong&gt;迅速崛起，展现出规划、推理、工具调用、自主决策等多维能力。然而，当前主流的范式仍然存在关键瓶颈：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;面对动态任务和 OOD 任务的迁移，模型&lt;strong&gt;灾难性遗忘&lt;/strong&gt;仍然难以避免。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;用户目标、环境反馈、上下文约束随时间变化时，Agent&amp;nbsp;&lt;strong&gt;对齐一致性&lt;/strong&gt;下降。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;真实世界长期运行带来的&lt;strong&gt;算力、token、能源、交互成本&lt;/strong&gt;约束，使系统&lt;strong&gt;可持续性不足&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果我们希望 AI Agent 真正走进开放世界，成为可靠的长期助手，我们必须迈向 &lt;strong&gt;Lifelong Agent（终身智能体）&lt;/strong&gt;，让 Agent &lt;strong&gt;持续学习、长期对齐、自主进化、资源可感知、可持续部署&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在本届 ICLR 2026 会议期间，来自 UIUC, Edinburgh, Oxford, Princeton 等机构共同发起的 Lifelong Agent Workshop 中，便将会对以上所有问题进行深入探讨。&lt;/p&gt;&lt;p&gt;本次 Workshop 旨在打造首个&lt;strong&gt;跨领域统一论坛&lt;/strong&gt;，系统性推动 Lifelong Agent 研究范式，打通语言智能、强化学习、具身系统、多智能体协作、AI4Science 等方向，共同定义 Agent 发展的&lt;strong&gt;下一座技术里程碑&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531806" data-ratio="0.512962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu0v3gBicPWXp8FHZslsDgBuB9r6Hn96oicRsgj4zayib7dX5cUMIoWfzFoQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/dc19d4c3-2b0d-4046-8684-2dc2fe3cabeb/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;🗓️&lt;strong&gt; Workshop 时间地点&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;📅&amp;nbsp;2026 年 4 月 26 日（或 27 日）&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;📍 Rio de Janeiro（里约热内卢），ICLR 2026 期间&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;🔎 形式：&lt;strong&gt;全日制 Hybrid（线下 + 线上实时参与）&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;🌍 官网：https://lifelongagent.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;🎯 规模：预计 &lt;strong&gt;200&amp;ndash;400 现场参会，500&amp;ndash;600 线上覆盖&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Workshop 官网已上线，Poster / 录播 / Q&amp;amp;A 资源会持续开放。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;🧠 征稿方向（包括但不限于）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Workshop 鼓励跨领域、面向长期运行的 Agent 研究，并特别关注以下主题：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1 Lifelong Learning（持续学习）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;memory-augmented RL、continual exploration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;多模态 / 具身数据流整合、长短期记忆融合&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;终身学习 Benchmarks 与评估方法&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2 Lifelong Alignment（长期对齐）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;用户目标变化建模、个性化与公平性权衡&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;监督与安全保障机制&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;drift detection &amp;amp; correction、长期价值学习&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;3 Self-Evolving Agent（自主进化）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;推理策略自优化、模块 / 技能自主扩展&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;多智能体终身协作生态、LLM + 小模型专精协同&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Emergent behaviors、open-ended self-improvement&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;4 Embodied &amp;amp; Real-World Lifelong Agents（具身终身智能）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;机器人终身学习、感知 - 行动长期闭环&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;不确定性建模、复杂环境下持续运行&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;5 Efficient &amp;amp; Sustainable Agents（高效与可持续）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Token/Compute/Energy 受限下的学习与推理&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;资源感知调度、长期部署系统设计&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;6 Multi-Agent Lifelong Systems（多智能体终身系统）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;持续多智能体协作 / 竞争 / 谈判机制&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;群体行为监测 Benchmarks 与持久群体智能&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;7 AI Agents for Science（科学智能体）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;自主假设生成、实验设计、科学知识发现&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;具身实验室 Agent、AI4Science 长期生态&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;8 Evaluation &amp;amp; Benchmarks（终身评估与基准）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;long-horizon adaptability、alignment drift metrics&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;终身智能体持久性、可信度、可扩展增长评估&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;⏰ 投稿截止与论文类型&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;📌 投稿截止：2026/2/15 UTC（以 OpenReview 提交系统为准）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;共支持两类论文投稿：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Full Paper（完整论文）&lt;/strong&gt;：最多 9 页，适合已完成工作&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Short Paper（短论文）&lt;/strong&gt;：2&amp;ndash;5 页，鼓励最新突破、轻量方法、Follow-up 实验、开源实现、理论洞察、案例分析&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本次的投稿为非 Arxiv 性质，&lt;strong&gt;欢迎同时将投稿到 ACL 以及 ICML 的优秀工作同时投来本次 Workshop !&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;🔗 Workshop 详情与投稿入口&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Workshop 官网： https://lifelongagent.github.io/（下方海报二维码可直达）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文提交入口： https://openreview.net/group?id=ICLR.cc/2026/Workshop/LLA（OpenReview Group）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Lifelong Agent 不是某个单点任务的提升，而是&lt;strong&gt;智能范式的升级&lt;/strong&gt;：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;让 AI Agent 成为长期稳定、自主对齐、可持续成长、面向科学发现、跨模态交互、可复现部署的真实世界系统&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这是 Agent 研究的 &lt;strong&gt;Next Frontier&lt;/strong&gt;，也是 &lt;strong&gt;2026 年最值得关注的 Workshop 方向之一&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;让我们一起推动 &lt;strong&gt;Lifelong Agent&lt;/strong&gt; 走向下一座里程碑，让它成为 Agent 时代的&lt;strong&gt; Next Big Thing&lt;/strong&gt;！&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8E2ZhNe9jdxvsscgGEBSu07u7sdsGGviaa1aSjeojFn5uTSBPZPTibOZBMNru9ic7qs1R98aeqlFQtg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="2.2574074074074075" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503531807" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/49587a2b-faf9-4d9e-a0a6-2a8f355ed96c/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>中国第一，全球第二，视频大模型领军者生数科技完成超 6 亿元A+轮融资</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Thu, 05 Feb 2026 14:09:08 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2月5日，&lt;strong&gt;生数科技宣布完成超6亿元人民币A+轮融资&lt;/strong&gt;。本轮融资由中关村科学城公司和星连资本领投，上市公司万兴科技、视觉中国、拓尔思进行战略投资，原有股东启明创投、北京市人工智能产业投资基金、卓源亚洲、建发新兴投资、淮海投资等投资人加码跟投。&lt;/p&gt;&lt;p&gt;旗下多模态大模型 Vidu ，在国际权威AI基准测试机构&lt;strong&gt;Artificial Analysis&amp;nbsp;&lt;/strong&gt;最新公布的榜单中，&lt;strong&gt;Vidu Q3 排名中国第一&lt;/strong&gt;，全球第二，&lt;strong&gt;比肩马斯克xAI Grok，超越 Runway Gen-4.5 ，Google Veo3.1和 OpenAI Sora 2。&lt;img src="https://image.jiqizhixin.com/uploads/editor/9230b1d0-620e-4774-8c2c-5c5646e62ae3/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;2024年7月Vidu全球上线，全球首创&amp;ldquo;参考生视频&amp;rdquo;，率先解决了商业视频需求中的多主体连续一致性难题。Vidu在全球商业内容生成模型中保持全球最快生成速度，据Artificial Analysis榜单显示，Vidu 生成速度较 OpenAI Sora2快 10 倍，比 Google Veo 3 Fast 和 Grok-imagine-video 快 2 倍。生数科技还于2025 年 12 月开源 TurboDiffusion 框架，在单张 RTX 5090 显卡上仅需 1.9 秒即可生成 5 秒视频，将视频生成效率提升100-200倍。&lt;/p&gt;&lt;p&gt;生数科技已构建起Vidu MaaS、Vidu SaaS、Vidu Agent的应用矩阵，赋能全球范围的内容创作者以及广告、动画、影视、教育、游戏、硬件、文旅、广电等行业企业。目前，&lt;strong&gt;Vidu 已成为全球创作者、内容机构和企业首选用于商业内容创作的模型之一，2025年实现用户和收入超过10倍增长。&lt;/strong&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>智能必须基于世界模型？我们和蚂蚁灵波团队聊了聊</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 05 Feb 2026 13:10:54 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜泽南&lt;/section&gt;&lt;p&gt;大模型的革命行将结束，即将开启的会是物理 AI 时代？&lt;/p&gt;&lt;p&gt;上周，图灵奖得主、深度学习先驱 Yann LeCun 对通用人工智能（AGI）发表了自己的最新观点。他认为语言并不等同于智能，预测文本并不意味着理解现实。真实世界纷繁复杂、充满物理性和因果关系，而如今的大语言模型（LLM）几乎无法触及这些。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHhuLUyvk0k3qxx6fqQIrdHUOvnibSeH7CITKTuic2SllJ1OTwG0vXicQqcHQ7AIkjcpbRVPm6f5SEhicg3AvGlaQhicdjEIjSWDWwI/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.42962962962962964" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531708" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6a7eff01-12f9-4547-8ee4-a55a5fa0359f/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;LeCun 认为，真正的智能必须能像人类一样，在脑海中进行推演，只有具备了这种「预测未来」的能力，AI 才能进行复杂的规划。&lt;/p&gt;&lt;p&gt;虽然关于 AI 技术理论的争鸣多发生在大洋彼岸，但令人出乎预料的是，在 2026 年开年，率先把物理 AI 这一最前沿的方向推进一步的，却是一家中国公司。&lt;/p&gt;&lt;p&gt;在刚刚过去的一周，蚂蚁集团旗下的蚂蚁灵波科技（Robbyant）以一种近乎「饱和式攻击」的节奏，&lt;strong&gt;连续四天开源发布了四款具身智能模型：高精度空间感知模型 LingBot-Depth、具身大模型 LingBot-VLA、世界模型 LingBot-World 到具身世界模型 LingBot-VA。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531711" data-ratio="0.6055555555555555" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFd0e0F3uZ4crORZkxP1yFHVZtPdbej9rcq4vYakwCNSk94ac41lu7DvubyLC3rjib0gzpo2NfiaSc7sA13wRjcjfbGUuViaDtOJY/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/2328b813-888f-4b7d-9960-0865ed0e9c55/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在机器之心与蚂蚁灵波 CEO 朱兴及首席科学家沈宇军的对话中，我们发现，蚂蚁正在通过一套独特的「逆向思维」，试图探索具身智能（Embodied AI）新路径 &amp;mdash;&amp;mdash; 从物理交互出发，在真实世界中构建智能。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;蚂蚁的 AI First，不止于数字世界&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;下一个 AI 技术的突破将会是物理世界的 AI：世界模型、因果关系、真正的规划。蚂蚁灵波正在以行动验证这一重要趋势。&lt;/p&gt;&lt;p&gt;朱兴在采访中表示，蚂蚁的 AGI 版图包括数字智能与物理智能，在设立灵波科技前，蚂蚁已布局多家具身智能（Embodied AI）及机器人相关企业，覆盖整机、核心零部件、灵巧手、具身大模型等多个关键环节。2025 年，蚂蚁灵波科技正式成立，承担在具身领域探索 AGI 的使命。经过一年的研发，团队端出了四款具身模型，在一周内集中开源。&lt;/p&gt;&lt;p&gt;朱兴介绍，&lt;strong&gt;灵波的工作「从真实硬件出发」，希望从数字世界迈向物理世界，为机器人打造更聪明的大脑&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;「我们笃定数字世界的智能还远没有达到上限，语言模型、多模态模型、视频生成模型还会进一步发展」，朱兴说，「蚂蚁的百灵团队负责数字智能的技术演进，灵波也积极参与其中，因为很多基础技术在具身模型的训练中可以复用。同时灵波还负责另一条路径的探索。」&lt;/p&gt;&lt;p&gt;他表示，「物理世界智能跟数字世界智能最大的不同，就是前者可以拿到真实世界的反馈。从真实反馈中学习往往是『智能』产生的必要条件。」&lt;/p&gt;&lt;p&gt;因此，灵波过去一年核心聚焦在具身基模的训练。「我们希望具身智能领域能和大语言模型一样，随着基模能力的提升让物理世界整体智能水涨船高。」&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术路线：真实数据优先&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本次发布中最值得玩味的，是蚂蚁灵波对具身智能技术路线的「非主流」选择。&lt;/p&gt;&lt;p&gt;目前，具身智能领域的流行路径之一便是「Sim-to-Real」（从仿真到现实）：其核心思路是，为了解决机器人训练数据稀缺、试错成本高等问题，先在仿真的虚拟环境中海量、安全地训练机器人（或 AI 智能体），再将习得的策略「迁移」到现实世界的机器人身上。&lt;/p&gt;&lt;p&gt;然而，蚂蚁灵波对此路径给出了不一样的观点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「Sim-to-Real 不是我们选择的主技术路线，」沈宇军在采访中表示。「我们坚定认为基模的训练应该更多地使用互联网数据和真实数据。所谓的『真实数据成本高』也只是阶段性的，随着产业发展会有序解决，比如可以通过更低成本更加高效的数采方式等等。」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;同时，沈宇军认为「仿真数据很多场景还无法模拟」的挑战是切实存在的 &amp;mdash;&amp;mdash; 流体、柔性物体、传感器误差，这些仿真很难搞定，解决周期可能比降低真实数据的采集成本更久。&lt;/p&gt;&lt;p&gt;相比于在虚拟温室里「造梦」，蚂蚁灵波选择了一条更艰难但可能更正确的路：互联网数据 + 真实数据。&lt;/p&gt;&lt;p&gt;这一思路在 LingBot-VLA 上得到了验证。基于九种主流构型的超两万小时高质量真机数据的预训练，该模型在权威评测中超越了一系列国际顶尖基线。这项技术引发了 AI 社区的关注，人们认为这是现实世界机器人技术的一大进步。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531712" data-ratio="0.8194444444444444" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqESOccAsdJCIv4Vb1A69eOBMInkcqUiaQkibZicWdfvvibtEK6UQiaQxydosaFRYeJicwvIsib2rtufkVECCoRACMcI3OsXy7C9tukBwQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/cb21fa94-3b1c-4dde-9e92-6a1d80777200/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;而作为本次发布的「压轴」，LingBot-VA 则彻底展现了灵波的技术野心。这是全球首个用于通用机器人控制的因果视频 - 动作世界模型。它学会了利用视频生成模型来实现「想象」，结合多模态模型的逻辑推理，再叠加真实环境的反馈。&lt;/p&gt;&lt;p&gt;蚂蚁灵波正在试图构建视频预测与现实世界行动之间的闭环。现在具身智能的 AI 已经可以基于单一模型预测未来的景象，并生成实现该视频所需的操作，仅通过 30-50 次真实世界的演示就能学习新技能，其成功率还要比常见的基准模型（如 &amp;pi;0.5）高出约 20%。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531713" data-ratio="0.5475" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_gif/5L8bhP5dIqGVjaV00Rg9zb6H7bOIRgqHCceqPicCZdzs89Mq6wQQRjfqAoEg3uDoF7WPJVbMrXEBwozxT9LUeEux60gibNg1SPru35xYoaUcc/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-type="gif" data-w="800" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/cfb4fb12-3516-42f0-99de-91f02c9b6d78/640.gif" data-order="0" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;「我们发现，利用物理世界的数据叠加一层预训练，对具身模型能力的提升非常有帮助，」沈宇军表示。这解释了为什么 LingBot-VA 能在业界第一个实现「边推演、边行动」&amp;mdash;&amp;mdash; 它不是在死记硬背仿真数据，而是在试图理解物理规律。这似乎刚好回应了 Yann LeCun 对于 AI 在物理世界里实现预测的呼吁。&lt;/p&gt;&lt;p&gt;除此之外，在上周发布的深度视觉模型 LingBot-Depth 上，蚂蚁灵波探索了通过深度传感器误差作为掩码来优化深度图的深度补全模型，大幅降低了当前主流视觉深度相机的误差，让机器人&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;看的更清楚&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;。&lt;/p&gt;&lt;p&gt;而在 LingBot-World 上，该团队开源了视觉效果堪比谷歌 Genie 3 的实时可交互世界模型，其生成的世界严格遵循物理规律，也为具身智能的模拟打好了基础。&lt;/p&gt;&lt;p&gt;这些技术在全球机器学习社区吸引了大量关注，人们期待来自中国的开源技术可以改变业界现状。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531714" data-ratio="1.0725" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/5L8bhP5dIqGxwFqf9rYRK811V1nMAH6g0MWzcVNMWTJ0ySEw6L4QJKBcnEwnMbjx3LoxGv4bomia1iaThTnNtEVG2k0lMljoJrTCvvY6rm4Lw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-type="gif" data-w="800" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/b239b996-9824-4535-85cc-1af84c592d64/640.gif" data-order="1" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;不过在朱兴看来，蚂蚁灵波目前所做的还是打好基础：「具身智能总体技术阶段目前还处于早期，且技术路线也没有收敛，从这点来说（蚂蚁灵波的技术）没有什么是其他家一定做不到的。我们反而更关注模型本身能力的上限探索以及如何让生态伙伴用的更好。我们之所以做基模，很大的考量反而就是为了降低生态伙伴后训练的成本。而我们这次发布，也同步开源了高效的后训练代码，也是这一想法的落地。」&lt;/p&gt;&lt;p&gt;&lt;strong&gt;机器人的「DeepSeek 时刻」还在路上&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 2025 年的 1 月，DeepSeek R1 横空出世，用开源证明了低成本 + 强推理的可行性。如今随着灵波等公司的模型开源，具身智能领域是否也会迎来它的 R1 时刻？&lt;/p&gt;&lt;p&gt;对此，朱兴表示：「DeepSeek 时刻对具身智能来说还为时尚早，应该说 ChatGPT 时刻都还没有到来。面向下一步，我们会持续加强对具身世界模型的投入，探索具身智能的新上限。」&lt;/p&gt;&lt;p&gt;但也正是因为如此，蚂蚁灵波可以成为那个「点火者」。通过 InclusionAI 社区，灵波将这四款核心模型全部开源。朱兴的逻辑非常清晰：&lt;strong&gt;在路线尚未收敛的早期阶段，开源是推进行业进步的最优解，因此未来蚂蚁灵波的技术还会继续全面开放&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;更深层的野心在于生态位。&lt;/p&gt;&lt;p&gt;不同于特斯拉 Optimus「造脑也造驱干」的封闭模式，蚂蚁灵波希望构建起机器人领域的「安卓系统」。「我们更侧重基模研发，初期就坚定选择了跨构型的路径，通过跟行业内相关数据提供商深入合作来满足模型训练数据多样性的需要，」朱兴解释道。&lt;/p&gt;&lt;p&gt;当然，机器人的本体千差万别，基于统一的基础模型，任务执行的成功率还会受到影响。&lt;strong&gt;蚂蚁灵波的策略是提供高效的「后训练工具链」，让硬件厂商能用更低的数据量和 GPU 成本，将灵波的「大脑」适配到自己的「身体」上。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这或许才是开源背后的真正商业护城河。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;终局猜想&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;蚂蚁造的具身智能，最终会去哪？&lt;/p&gt;&lt;p&gt;虽然商业模式会「自然而来」，但蚂蚁基因中的服务业属性，或许可以让我们猜测一下灵波&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;大脑&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"line-height: 1.75em;margin-bottom: 0px;margin-left: 8px;margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;加持的机器人会是什么样子。从民生服务到普惠金融，蚂蚁的优势在于连接人与服务。&lt;/p&gt;&lt;p&gt;蚂蚁灵波期待随着技术成熟，以具身智能形式呈现的服务能够更好地走入物理世界，更好的服务于人。&lt;/p&gt;&lt;p&gt;当然，眼前的挑战依然巨大。沈宇军表示，从技术角度上看，强化学习（RL）的具体落地范式尚未收敛，AI 推理中至关重要的 System 2（慢思考）的能力仍在探索中，这些都可能是制约下一步技术大规模落地的瓶颈。&lt;/p&gt;&lt;p&gt;但背靠蚂蚁集团 AGI 整体战略，业界一梯队的 AI Infra 支持，以及坚定的资金投入，灵波显然已经做好了打持久战的准备。&lt;/p&gt;&lt;p&gt;随着蚂蚁灵波最近四个模型的连续发布和开源，蚂蚁的 AI 战略实现了从数字世界到物理世界的关键延伸，这标志着其「基础模型 - 通用应用 - 实体交互」的全栈路径已经逐渐清晰。下一步，蚂蚁灵波计划持续探索模型能力的提升，尤其是世界模型跟具身智能的深度结合，并积极拓展生态，协助生态合作伙伴实现落地，让机器人真正走入商业应用。&lt;/p&gt;&lt;p&gt;一个深度融合、开源开放并服务于真实场景的 AGI 生态，正在加速成型。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>谷歌做了个论文专用版nano banana！顶会级Figure直出</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 05 Feb 2026 13:07:35 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜SIA&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;section&gt;你负责写方法，AI负责画 Figure。 科研打工人，终于等来&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;画图解放日&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;。&lt;/section&gt;&lt;/blockquote&gt;&lt;p data-pm-slice="0 0 []"&gt;还在为论文里的方法框图熬夜画 PPT、拉箭头、对齐字体吗？&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;一张 Figure 2，动辄几个小时，严重的甚至能耗上几天，科研人的&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;隐藏副本&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;不是实验，而是画图。&lt;/p&gt;&lt;p&gt;既要忠于论文原意，又得暗暗符合顶会那套心照不宣的&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;学术审美&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;：颜色不能土，布局不能乱，箭头更不能连错。&lt;/p&gt;&lt;p&gt;看起来只是一张图，实际上是美学、逻辑和耐心的三重折磨。&lt;/p&gt;&lt;p&gt;那么，问题来了：现在的大模型已经能写论文、跑实验、改代码，为什么偏偏搞不定这些学术插图？有人可能会问：DALL&amp;middot;E、基础 VLM 不行吗？&lt;/p&gt;&lt;p&gt;答案是：真不行。&lt;/p&gt;&lt;p&gt;它们画出来的图往往是：模块和文字对不上、字体直接乱码、箭头逻辑错误。图是&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;/span&gt;好看&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-pm-slice":"0 0 []","style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;&lt;/span&gt;，但不中用啊。&lt;/p&gt;&lt;p&gt;于是，一个狠角色出现了：PaperBanana 🍌&lt;/p&gt;&lt;p&gt;来自北大 + Google Cloud AI Research 的团队，目标很简单也很狂：&lt;strong&gt;你写方法，AI 画 Figure，水准呢？直接投顶会的那种。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;科研打工人，终于等到了&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;/span&gt;画图解放日&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBxQx0GqjoyibSEH41ahv4eU5nR72JGIlcoOpticFexXwgeiag5azTGfv4Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.08936170212765958" data-type="png" data-w="705" data-width="705" data-height="63" data-backw="562" data-backh="50" data-imgfileid="503531648" data-aistatus="1" data-original-style="width:100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/bab333c8-520f-4889-a22d-54d458cfab93/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBBjsVwDqTVtWtSp78E0EnK7wss7wiaHD4tGnxfjSIQUZ9wBAcNlCc0fw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.11864406779661017" data-type="png" data-w="708" data-width="708" data-height="84" data-backw="562" data-backh="67" data-imgfileid="503531649" data-aistatus="1" data-original-style="width:100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/8c7e9e56-fd0c-4036-9751-24fed628a97e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBshe1koWdgpDEjnOZgbuyaxGIGh1RxQIvFDFQib7MUIoH5fVpam6L5dQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.15211267605633802" data-type="png" data-w="710" data-width="710" data-height="108" data-backw="562" data-backh="85" data-imgfileid="503531647" data-aistatus="1" data-original-style="width:100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/ab48d380-3311-48b3-8a24-d6e80122291d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBjd2f8XC6GVDUrtMbaOpuGOoic18ksZTtQeKVMQvfOyibRYcBo1LQiagNg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.1271186440677966" data-type="png" data-w="708" data-width="708" data-height="90" data-backw="562" data-backh="71" data-imgfileid="503531650" data-aistatus="1" data-original-style="width:100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/8e469466-1cbb-48e6-bc93-3919bbc2c06e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;来看效果成色。&lt;/p&gt;&lt;p&gt;PaperBanana 展示了解决两类学术插图的能力：&lt;/p&gt;&lt;p&gt;第一类，是论文方法流程图与模型结构示意图，用来说明算法如何运作（左）；第二类，是统计图表，用来表达实验结果与数据对比（右边）。&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBsF73NXddicseEroyoxQ02cx8icgSRrib04vIbnhrIyicAm0XicaQUjJz5Rw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" alt="飞书文档 - 图片" data-ratio="0.5796296296296296" data-type="png" data-w="1080" data-backw="562" data-backh="326" data-imgfileid="503531652" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/85a89758-8308-4b8a-9b88-e4b7e2ada040/640.png" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp;左边是方法框图（Methodology Diagrams），右边是统计图（Statistical Plots）&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;与以往&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;只会画图像&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;&lt;/span&gt;的生成模型不同，PaperBanana 强调两点：不是只要&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;画得好看&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-pm-slice":"0 0 []","style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;&lt;/span&gt;，而是必须&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;画得正确&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;。&lt;/p&gt;&lt;p&gt;它要保证：模块之间的逻辑关系不出错、数据表达符合科研规范、图可以直接服务论文叙事，而不是装饰。&lt;/p&gt;&lt;p&gt;研究指出，PaperBanana 可以覆盖多种常见学术插图类型，包括方法流程图、模型结构示意图、概念性框架图，以及通过代码驱动生成的高精度统计图。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PaperBanana 不仅能从零生成，还能润色你现有的丑图。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;给它一张草图或初版框图，它负责自动美化、重排布局、统一风格，让它更像顶会论文里的标准图形&lt;/p&gt;&lt;p&gt;更直观的对比&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;左侧是手工绘制的插图，右侧是 PaperBanana 风格增强（Style Enhanced）后的版本。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBoRkrlPxaPzYuMrNd3bmX5iawOmyiahg8mZFe2oc3bibXaJTRGG0OPbavA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" alt="飞书文档 - 图片" data-ratio="0.9111111111111111" data-type="png" data-w="1080" data-backw="562" data-backh="512" data-imgfileid="503531653" data-aistatus="1" data-original-style="width: 100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/7d339656-ab8c-4100-bb97-ebc32b8290e4/640.png" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;这些示例覆盖了多个典型科研场景，包括 Transformer 与不同 LayerNorm 变体的对比示意、工程流程与三维建模管线的系统框架，以及强化学习和表示学习中抽象几何关系的表达。它们的共同特点在于逻辑复杂、元素密集，对人工排版提出了极高要求，也正是科研人员最容易在&amp;ldquo;画图&amp;rdquo;上消耗大量时间与精力的部分。&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;语义结构上一致，但视觉呈现，完全不同。&lt;/p&gt;&lt;p&gt;原始图信息完整，却给人一种&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;/span&gt;能看懂，但不好看&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-pm-slice":"0 0 []","style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;&lt;/span&gt;的感觉：布局略显松散，配色偏向单一，不同模块之间的层级关系也不够清晰。&lt;/p&gt;&lt;p&gt;PaperBanana 润色增后，图中的逻辑被重新梳理进一套更规范的视觉体系之中。&lt;/p&gt;&lt;p&gt;不同功能模块通过颜色进行区分，虚线和分区框用来强化层次结构，箭头的走向也更加明确，整体观感明显更接近顶会论文中常见的标准范式。&lt;/p&gt;&lt;p&gt;再看下面的图例，同一张图对比，高低立判。&lt;/p&gt;&lt;p&gt;人类画的图，对，但不一定好看。&lt;/p&gt;&lt;p&gt;未经调教的原始模型生成（Nano-Banana-Pro），画出来但难读。&lt;/p&gt;&lt;p&gt;PaperBanana 真正做到了&lt;span data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"span","attributes":{"style":"color: rgb(71, 71, 71);font-family: Arial, sans-serif;font-size: 14px;font-style: normal;font-variant-ligatures: normal;font-variant-caps: normal;font-weight: 400;letter-spacing: normal;orphans: 2;text-align: start;text-indent: 0px;text-transform: none;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);text-decoration-thickness: initial;text-decoration-style: initial;text-decoration-color: initial;display: inline !important;float: none;","data-pm-slice":"0 0 []"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;/span&gt;画清楚、讲明白&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;，也更符合顶会审美的论文级插图：配色更现代统一，信息更精炼，模块分区更清晰。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBIicKdibkKfCyZmgTecdNGkJNA0ZicmBzGjFoYUANNP5jgD90iaJNJeDSog/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" alt="飞书文档 - 图片" data-ratio="0.674074074074074" data-type="png" data-w="1080" data-backw="562" data-backh="379" data-imgfileid="503531654" data-aistatus="1" data-original-style="width:100%;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/1f8e6a22-f928-40b0-acb8-4715c30dfc5b/640.png" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;那么，它是如何做到这一点的？&lt;/p&gt;&lt;p&gt;PaperBanana &lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;画论文图&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;变成了一条由多智能体协作完成的流水线。&lt;/p&gt;&lt;p&gt;系统先检索参考范例，再规划结构化描述，并在审美规范约束下生成初稿；&lt;/p&gt;&lt;p&gt;随后由视觉代理将文本描述转化为图像或代码绘图，评论代理不断对照原始论文内容进行纠错与打磨。&lt;/p&gt;&lt;p&gt;经过多轮迭代后，输出的不再是普通示意图，而是一张同时满足语义正确性与顶会审美标准的论文级插图。&lt;/p&gt;&lt;p&gt;这不是简单的作图自动化，而是一种&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;科研表达方式&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;的自动规范化。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBFlnn1MGKFqyJTfuahGVq5YS96ALI7mAo2WrDzGOeUJWoulm0wPNF3Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" alt="飞书文档 - 图片" data-ratio="0.42407407407407405" data-type="png" data-w="1080" data-backw="562" data-backh="238" data-imgfileid="503531655" data-aistatus="1" data-original-style="width:100%;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/1992b9b0-be14-44fe-b369-1d798987abc9/640.png" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;研究人员还顺带对比了两种路线：直接让模型&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;画图&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt; VS 让模型&lt;span data-pm-slice="0 0 []"&gt;「&lt;/span&gt;写代码画图&lt;span data-pm-slice="0 0 []"&gt;」&lt;/span&gt;。&lt;/p&gt;&lt;p&gt;结论很扎心：AI 直接画出来的图虽然精美，但经常在数字上胡说八道。&lt;/p&gt;&lt;p&gt;目前最靠谱的方式还是：AI 写绘图代码（基于 Gemini-3-Pro），再生成统计图。&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBeAK7XkW2amNibfBD1V7GRnNoZkLGykrY9d3825T8HM1KTK45xuavYkw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" alt="飞书文档 - 图片" data-ratio="1.3564814814814814" data-type="png" data-w="1080" data-backw="562" data-backh="762" data-imgfileid="503531656" data-aistatus="1" data-original-style="width:100%;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/dd428e7b-9ff7-48fa-a979-fa2a2a3760d7/640.png" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;这只是开始。类似工具已经开始出现，比如：Claude Scientific Writer，集成论文写作 + 插图 + 图表生成。&lt;/p&gt;&lt;p&gt;未来科研可能变成这样：你不用再在 PPT 里对齐箭头、调颜色、拖文本框到凌晨三点，而是把更多时间留给真正重要的事情。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://dwzhu-pku.github.io/PaperBanana/&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://github.com/K-Dense-AI/claude-scientific-writer&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 | 这道题是否需要用图思考？模型来告诉你！自适应思考模式切换助力通用视觉推理提升</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Thu, 05 Feb 2026 13:03:52 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/59a468c7-c629-4147-9f07-ba73f349fb2c/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;本文来自复旦大学和阿里巴巴未来生活实验室，已中稿 ICLR 2026。&lt;/p&gt;&lt;p&gt;目前的视觉推理方法衍生出了多种思考模式，主要有和 LLM 一致的纯文本思考模式以及更加贴近图片的用图思考。两种推理模式在不同的领域各有所长，&lt;strong&gt;但现有的工作聚焦于单个思考模式，无法充分利用两个模式之间的互补性&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;因此，本文提出了 mixture-of-visual-thoughts，一种自适应的推理范式：目标是&lt;strong&gt;将不同推理模式整合到一个模型内部并引导其进行自适应的模式选择&lt;/strong&gt;。为了让模型学习这样的推理范式，研究者引入了一个两阶段的学习框架 AdaVaR，通过 SFT 学习不同的推理模式，并设计了一个专门的 AdaGRPO 算法来在强化学习设定下引导模型学习如何根据问题选择合适的推理模式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBBPQeWGrp4CoZ5hf4aXEa1GseTUbS4NvYURyhEIq02zFUQ55kAhU85Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3522727272727273" data-s="300,640" data-type="png" data-w="968" type="block" data-imgfileid="503531482" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/9f3e0a97-5091-49df-be3a-ea633b98cf1e/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：Mixture-of-Visual-Thoughts:Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://arxiv.org/pdf/2509.22746&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://github.com/Future-Living-Lab/mixture-of-visual-thoughts&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;3B &amp;amp; 7B模型：https://huggingface.co/collections/ZejunLi/adavar-models&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml7f52eo15o9" data-pm-slice="0 0 []"&gt;&lt;strong&gt;背景：视觉推理的不同思考模式&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;目前对于 LVLM (large vision-language model) 的视觉推理方法已经有了大量的探索，其中主流推理范式包括以下两种：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBXxK6Gibpg3Hgec572FgRUV1RHk46pJRJAMAwqbrAcNdaKicqoFeI1GXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6287037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531481" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/baa9e2ef-5dbe-4278-9027-6af911e99a9d/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml7f6oyn6k0" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 1: 两种推理模式的直观对比。&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;1. 纯文本思考模式：和 LLM 一样，直接用自然语言描述推理过程；&lt;/p&gt;&lt;p&gt;2.Visually-Grounded 思考模式：通过结构化的信息（主要是 bounding box 等坐标）将推理路径中的关键概念与图片中的区域对应起来，进一步还可以将对应局部区域进一步裁剪缩放后输入给模型，帮助其利用局部的信息，即 GPT-o3 中提到的 thinking with images 的概念。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这两种思考模式不同的设计也让它们在不同领域上有不同的优劣表现&lt;/strong&gt;，以下图几个基于 Qwen2.5-VL-7B 的推理模型为例（正数 / 负数代表相对基座模型有提升 / 下降）：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBAFj1SdMR6zxbk1IKGzFZGSGWZkAJdeReYxN78qcA7wvYIKiaQaqplKQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.5833333333333334" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531484" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/5855eab9-e937-4448-94e6-ea32a054ebf5/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 2: 基于 Qwen2.5-VL-7B 的不同推理模式模型相对于基座的提升 / 下降。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;1. 文本思考模式更善于抽象的视觉问题，比如数学几何题，但可能会带来幻觉，也在视觉搜索的 V * 上表现不佳（存在&lt;strong&gt;过度思考&lt;/strong&gt;和&lt;strong&gt; language bias &lt;/strong&gt;的问题）；&lt;/p&gt;&lt;p&gt;2.Grounded 模式则更善于定位和利用视觉信息，抑制幻觉，&lt;strong&gt;但是在抽象的数学问题上提升不明显&lt;/strong&gt;（对于抽象的概念，比如角度，大小等，模型对其进行 grounding 来提供有效的信息）&lt;/p&gt;&lt;p&gt;受启发于此，本文希望探索这样的一个问题：&amp;ldquo;&lt;strong&gt;我们是否可以博采众长，将不同思考模式在不同领域上互补的优势整合起来，来帮助提升通用的视觉推理能力呢？&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mixture-of-Visual-Thoughts：自适应的视觉推理范式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于这样的想法，本文提出了 Mixture-of-Visual-Thoughts（简称 MoVT），一种自适应的视觉推理范式：我们希望一个统一个推理模型（1）&lt;strong&gt;能够具有不同思考模式&lt;/strong&gt;；（2）&lt;strong&gt;同时能够根据问题自适应地选择合适的模式&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;本文也基于 MoVT 的范式进行了初步的探索，我们提出了 AdaVaR 学习框架，通过两个阶段的训练来构建出具有 MoVT 自适应推理能力的模型：&lt;/p&gt;&lt;p&gt;1. 我们在推理序列的开始&lt;strong&gt;给不同模式引入了对应的特殊 prefix token&lt;/strong&gt;，比如 &amp;lt; text&amp;gt;, &amp;lt;ground&amp;gt;，作为指示符帮助模型区分不同的推理模式，然后通过 SFT 整合数据帮助模型学习不同的思考模式；&lt;/p&gt;&lt;p&gt;2. 我们设计了一个 AdaGRPO 的强化学习算法来引导模型进行模式的选择。i. &lt;strong&gt;通过固定模式 prefix token&lt;/strong&gt;，我们引导模型对同一个问题使用不同的思考模式生成推理 rollout，ii. 并设计了特殊的 advantage 计算方法：同时用 rollout-level advantage 增强模型的推理能力，并计算思考模式之间相对的 &lt;strong&gt;mode-wise advantage 来引导模型选择更优的思考模式&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;具体方法的介绍和细节请感兴趣的读者参阅下面一节。&lt;/p&gt;&lt;p&gt;我们在多个场景的多个数据集上进行了评测，如图 2 所示，不同于单模式的模型只是在特定场景表现突出，我们的 AdaVaR 模型&lt;strong&gt;在多个任务上都有一致的提升&lt;/strong&gt;。从 8 个数据集的平均性能来看，我们的 AdaVaR-3B 能够媲美 Qwen2.5-VL-7B，AdaVaR-7B 甚至要超越 GPT-4o 的性能。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AdaVaR：思维模式的整合和训练方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通过 Prefix Token 统一不同思考模式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;首先我们希望让多个思考模式在同一个模型内共存。为此，我们设计了一种统一的推理序列形式，通过&lt;strong&gt;特殊的 mode prefix token 来区分不同模式&lt;/strong&gt;：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBhk1wrriahg33u0LkZK7hAfMv1JZn6z5j529iciahhjhVsSawktn36CnwQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.19907407407407407" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531487" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/570ed588-27ca-496b-8aae-b772316bbdc3/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;蓝色部分就是&lt;strong&gt; mode prefix token&lt;/strong&gt;，红色部分则是对应的&lt;strong&gt;思考过程&lt;/strong&gt;。基于自回归的生成设定，我们发现生成这样的推理序列相当于在一次生成中先后完成了（1）根据问题生成 prefix token，完成模式选择；（2）根据选择的模式进行对应的思考。&lt;/p&gt;&lt;p&gt;mode prefix token 的引入帮助模型区分了不同的模式，也支持了后续 RL 算法中对于思考模式的干预。&lt;/p&gt;&lt;p&gt;基于这样统一的形式，我们对两种模式收集了对应的数据来进行 SFT，这样模型就具有了以两种模式思考的能力。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml7fera31jjx" data-pm-slice="0 0 []"&gt;&lt;strong&gt;AdaGRPO：引导模型进行模式选择&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;之后我们希望模型能够自适应地根据问题选择合适的推理模式。我们在强化学习的环境下进行这样的学习，其核心想法是：&lt;strong&gt;对同一个问题，模型会分别用两种模式推理 n 次&lt;/strong&gt;，与其他方法类似，我们以结果的正确性为导向，基于规则判断答案的正确与否作为奖励函数。然后基于 rollout 之间，模式之间的比较，&lt;strong&gt;设计双层次的 advantage 计算方式鼓励模型生成更好的推理路径，同时选择更优的思考模式&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBRfKwvcWgibacYEIEs8ArjviczhCCVhgEvmhIkw7qYwcLr6A6D4ibiaPRfg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.3851851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531488" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/834085ed-41ee-41a4-bf18-f22a31dfda76/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 3: AdaGRPO 和 GRPO 的比较。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为此我们设计了 AdaGRPO，对 GRPO 做出了如下的优化：&lt;/p&gt;&lt;p&gt;1.Prefix-guided Exploration：&lt;strong&gt;GRPO 中的 rollout 生成过程是自由的，可能导致模式之间的不均匀探索&lt;/strong&gt;，比如对同一个问题生成的 2n 条思考过程都是同一个模式。所以 AdaGRPO 中，我们通过固定 mode prefix，让模型前 n 条 和 后 n 条 rollout 分别来自文本思考和 grounded 思考模式；&lt;/p&gt;&lt;p&gt;2.Adaptive Advantage：GRPO 中只计算了 rollout-level advantage &lt;b data-index-in-node="56" data-path-to-node="4,0" data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml7folaqwc4" data-pm-slice="0 0 []"&gt;A_i = (r_i - Mean) / Std&lt;/span&gt;&lt;/b&gt; 来提升推理能力，而且给 rollout 里所有的 token 都赋予同样的 advantage。为了显式地引导模式的选择：&lt;/p&gt;&lt;p&gt;a.AdaGRPO 中&lt;strong&gt;以相对胜率刻画了两个模式之间相对的优势 &lt;/strong&gt;A_t 和 A_v（A_t = 对于该问题，用文本推理模式得到的 reward 要高于 Grounded 模式的概率，反之亦然）;&amp;nbsp;&lt;/p&gt;&lt;p&gt;b. 如上一节中设计的推理序列形式，mode prefix token 和思考过程承担了不同的作用，于是我们也将不同的 advantage &lt;strong&gt;赋予不同的 token&lt;/strong&gt;，将 A_t 和 A_v 赋予 mode prefix token 来鼓励模式的选择，将 A_i 赋予思维过程的 token 来提升模型的推理能力&lt;/p&gt;&lt;p&gt;除此之外我们还设计了一个课程学习的数据构造方法。开始时模型在简单混合的数据上学习（包括几何题和物体计数任务）；后续则在多个任务混合的数据上学习。题目也会从简单到难，让模型逐步学习从简单到难的推理以及模式选择。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们基于 Qwen2.5-VL-3B/7B 构造了我们的 AdaVaR-3B/7B，在 8 个数据集上与其他基于 Qwen2.5-VL 的推理模型进行了比较：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBPSqjhnfSm9kYoehHOBl42XfCRmk2YcodqACc7DuFicWNnJPHsBP4Hxw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5277777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531489" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9eb3a68d-2328-4575-8264-5c5d1c4e87bc/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表 1: 不同模型之间的性能。黄色底代表文本推理模型，绿色底为 Grounded 推理，蓝色底为本文的 AdaVaR 模型。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;1. 首先评测的结果也支持了之前背景里提到的论述 -- &lt;strong&gt;只基于单思考模式的推理模型通常是特定领域的专家，很难有通用的提升&lt;/strong&gt;，具体来说：i. 文本推理模型，比如 VLAA-Thinker-3B 和 OVR-7B，主要在数学任务上表现好，但是物体相关问题回答不好；ii. Grounded 推理模型则在 V * 和 POPE 上都表现不错，但数学任务上不理想，只有 DeepEyes 有提升，其他都很难保持基座模型的数学推理能力；&lt;/p&gt;&lt;p&gt;2. 而 AdaVaR-3B 和 AdaVaR-7B 是&lt;strong&gt;仅有的在所有任务上都优于 Qwen2.5-VL 基座的模型&lt;/strong&gt;，甚至在 MathVista，WeMath，POPE 上都是最优，MMStar 和 MathVision 也是最优 / 次优的表现。&lt;/p&gt;&lt;p&gt;3. 从平均准确率刻画的总体性能来看，AdaVaR-3B/7B 都是在对应组别最优的模型。&lt;strong&gt;AdaVaR-3B 是唯一一个达到了 Qwen2.5-VL-7B 水平的 3B 模型，而 AdaVaR-7B 甚至要比 GPT-4o 还好。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;深入分析自适应推理的机制&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;进一步我们还深入探究了自适应推理中的机制，希望能回答：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SB4xSAUWHLgSJrbIJiaqIE7kWt2YsTHME97ZC0rx2ibuS0fwvwBG3PBXtg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.4842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531490" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/a7c69763-6b1c-4e4f-a148-744a63fefaf8/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表 2: 对于不同模式，不同阶段模型的性能。下标 T 和 G 分别代表固定使用 text 和 grounded 模式。GRD% 代表自适应的模型选择 grounded 模式的比例。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q1: 在一个模型内是否可以，以及如何学习到不同的模式？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;首先我们从表 2 可以看到 AdaVaR 模型在 SFT 和 RL 阶段上，从性能来看，两个模式就展现出了截然不同的表现，类似之前提到的文本模式数学能力强，Grounded 模式善于处理物体导向的问题。&lt;/p&gt;&lt;p&gt;Q1.1: 只用一个模式，提高数据的 diversity 是不是就够了？&lt;/p&gt;&lt;p&gt;A1.1: No。我们用 AdaVaR 相同的数据训练了两个单思考模式的 baselines，Grounded-SFT-RL 和 Text-SFT-RL。两个模型的性能都没有 AdaVaR 好，&lt;strong&gt;说明 MoVT 整合两种模式的想法是非常有效的&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;Q1.2: 两个模式在同一个模型内部是否会互斥？&lt;/p&gt;&lt;p&gt;A1.2: No。同样比较 Grounded-SFT-RL 和 Text-SFT-RL 和 AdaVaR 的两个模式，我们发现整体上差异并不大，也都相较 SFT 阶段有明显的提升，&lt;strong&gt;说明整合到一个模型内并不会抑制单个思考模式的提升&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;Q1.3: 是否需要 Mode prefix 来区分不同模式？&lt;/p&gt;&lt;p&gt;A1.3: Yes。参见表 2 的 Mix-SFT-RL baseline，我们去掉了 mode prefix，直接混合两个模式的数据。这样做的性能甚至要比单模式推理的模型还差，说明&lt;strong&gt;显式的模式区分是必要的&lt;/strong&gt;，而且&lt;strong&gt;显式的区分支持了后续在 AdaGRPO 里 prefix-guided exploration 对于模式探索的干预&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;Q1.4: 两个模式是否是互补的？&lt;/p&gt;&lt;p&gt;A1.4: Yes。表 2 中还计算了 AdaVaR 的 Upper Bound，即任意一个模式做对的准确率，我们可以看到对于所有数据集，即使是数学题这种文本模式明显占优的，Upper Bound 也要明显高于文本模式，这也展示了后续 MoVT 范式的巨大潜力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q2: 模型能学到合理的模式选择能力吗，怎么学习到的？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对比表 2 中的单模式和自适应模式的表现，我们发现：&lt;/p&gt;&lt;p&gt;1.SFT 以后的模型对于模式的选择并不是最优，比如 MathVista 上文本模式明显更好，但是 AdaVaR-SFT 还是选择了 31% 的 grounded 模式，说明 &lt;strong&gt;SFT 阶段很难控制模式的选择&lt;/strong&gt;；&lt;/p&gt;&lt;p&gt;2. 但是 RL 以后，AdaVaR 的模式选择就比较合理，在数学问题上选择文本模式，V * 和 POPE 上选择 grounded 模式，同时 AdaVaR 在所有任务上，自适应模式都要优于文本 /grounded 的单模式推理，说明了 &lt;strong&gt;RL 阶段模型学到了合理的模式选择能力&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;Q2.1: 模型是怎么学习到这样的能力的？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBhPISOhp7xsYfvXQO3PL6LaCkzkZ0MibibEmRhoQOWlLd2PzaJ4AQlGMQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.32685185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531491" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/9780aaf5-1fc1-41e7-a9e1-1254cce3e984/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 4: 数学问题上的 (a) 训练 reward 曲线；(b) MathVista 上的表现。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;A2.1: 如图 4 所示，以数学问题为例，我们可以看到，&lt;strong&gt;图 (a) 中两个模式相对的 reward 高低（表现好坏）引导了图 (b) 中 GRD% 曲线（模式选择）的变化&lt;/strong&gt;。大概可以分为三个学习阶段：&lt;/p&gt;&lt;p&gt;1. 初期探索阶段：刚开始，因为两个模式之间的相对好坏还不稳定，此时 GRD 模式甚至在一段时间内更优，导致一开始&lt;strong&gt;存在一定的波动&lt;/strong&gt;；&lt;/p&gt;&lt;p&gt;2. 稳定阶段：在 500 步开始，&lt;strong&gt;TXT 模式要明显优于 GRD 模式&lt;/strong&gt;，模型的选择也逐步稳定到以 TXT 模式为主，但此时自适应的 ADA 模式要比文本模式还弱，说明模型还没有学习到具体哪些题目应该使用 GRD 模式&lt;/p&gt;&lt;p&gt;3. 微调阶段：在 1500 步后，因为我们使用了分布更广的数据，帮助模型学习到了更佳精细的推理和模式选择能力，&lt;strong&gt;两个模式都在提高&lt;/strong&gt;，并且 ADA 模式最终要优于两个单模式。&lt;/p&gt;&lt;p&gt;Q2.2: 对于模式选择的关键因素？&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;A2.2: 我们还发现，AdaGRPO 中的关键机制，包括 prefix-guided exploration，Adaptive advantage，以及数据的 diversity 和课程学习都非常重要，具体的实验比较可以参见原论文的 Table 3。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结论与未来展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本文说明了 MoVT 这样，整合多个推理模式的方式能够是一种构建通用推理能力的可行思路，而 AdaGRPO 则是能够有效学习模式选择的算法。更一般来看，MoVT 相当于从思考模式的角度提升了模型生成推理 Rollout 的丰富度，促进了 RL 过程中的 exploration。本文对于自适应推理的探索也存在一定的局限性，希望未来能有相关工作进行研究：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;为了保持两种模式 Rollout 的一致性，本文中探索的 Grounded 思考模式&lt;strong&gt;并未像一些现有的工作额外引入局部的视觉特征&lt;/strong&gt;，后续如何在统一的框架里整合差异更明显的思考模式同样值得探索；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;本文目前仅考虑了&lt;strong&gt;两种推理模式&lt;/strong&gt;，但 MoVT 的框架也可以容纳更多的模式，也可以用于学习目前主流关心的思考 / 不思考的自适应切换能力，甚至区分长思考，短思考，是否使用工具等等；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对于未来更多种类的思考模式，势必会面对更加&lt;strong&gt;严峻的 exploration-exploitation tradeoff&lt;/strong&gt;：模式越多，为了平衡模式之间的探索，单个模式的推理数量势必更少，也会进一步抑制各个模式内部的提升；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;目前 MoVT 采取的是&lt;strong&gt;并行的模式选择范式&lt;/strong&gt;，进一步还可以结合搜索机制，考虑线性的模式切换，比如先进行短思考，再考虑是否进行长思考等等更复杂的逻辑，来提升推理模型的上限。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>中国科学院研发驱动材料开发的多智能体，通过自主协作与机器人执行实现闭环科研</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Thu, 05 Feb 2026 11:57:57 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;编辑丨%&lt;/p&gt;&lt;p&gt;材料科学正面临着空前的挑战。随着新材料需求的激增，科学家们亟需快速筛选、设计和验证新的材料体系，尤其是在纳米技术、能源存储以及药物传递等领域。然而，传统的研究方法，尤其是在材料发现的初期阶段，仍然依赖于手动的实验设计和试错。&lt;/p&gt;&lt;p&gt;在 LLM 逐渐发展且进化的当下，如何使用这种工具实现高效智能的开发，成为了相关研究者需要探讨的核心问题之一。&lt;/p&gt;&lt;p&gt;来自中国科学院深圳先进技术研究院（SIAT）的团队开发了一种基于知识的多智能体与机器人系统（MARS），用于端到端自主材料发现。该系统通过结合多智能体与机器人技术，&lt;strong&gt;首次实现了从问题定义到实验执行，再到结果反馈的全自动闭环&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;相关研究内容以「Knowledge-driven autonomous materials research via collaborative multi-agent and robotic system」为题，于 2026 年 1 月 21 日发布在《Matter》。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-backh="128" data-backw="546" data-height="260" data-imgfileid="100027361" data-ratio="0.23425925925925925" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnA6AroNE959MTXRTunZ61XOPWhZ6xRUkyRRMb8cvD9MjNcjFadWia6ic66ibGGFSIjLcSVjTmOVeIiag/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" data-width="1109" data-original-style="width:100%;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/db1169e7-7b98-4075-a16e-1da5e5d95862/640.png" alt="图片" data-before-load-time="1770263831277" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：&lt;em&gt;https://www.cell.com/matter/abstract/S2590-2385(25)00620-4&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;分层多智能体架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MARS 系统的核心亮点在于其&lt;strong&gt;高度模块化与协同工作&lt;/strong&gt;的智能体架构。系统内包含&amp;nbsp;&lt;strong&gt;19 个智能体&lt;/strong&gt;，这些智能体被分为五大类，各自承担不同的任务：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ORCHESTRATOR&lt;/strong&gt; 负责全局协调，确保任务在所有智能体之间平稳流转；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;SCIENTIST&lt;/strong&gt; 负责科学知识的推理与检索，定义研究问题并提出实验假设；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ENGINEER&lt;/strong&gt; 负责将科学家的研究假设转化为实验可执行的具体步骤和操作方案；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;EXECUTOR&lt;/strong&gt; 负责实际执行实验，控制机器人平台进行物理实验操作；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;ANALYST&lt;/strong&gt; 负责对实验数据进行分析，并将结果反馈给 ORCHESTRATOR，推动进一步实验的设计与优化。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-aistatus="1" data-backh="321" data-backw="546" data-height="675" data-imgfileid="100027364" data-ratio="0.5879629629629629" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnA6AroNE959MTXRTunZ61X1sMzpvxlqKEePEOROmka44644TxYWFbuxGJxlhM7qP1hbSEkYRSlfQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" data-width="1148" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1d814fd4-35ff-485d-94cf-1792480ad6c8/640.png" alt="图片" data-before-load-time="1770263831302" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 1：MARS 的架构。&lt;/p&gt;&lt;p&gt;这一系统设计的一个显著优势是其能够完全实现实验闭环，即每一个实验的执行和结果分析都在同一个平台上进行，消除了传统方法中实验阶段与分析阶段之间的时间和数据损失。&lt;/p&gt;&lt;p&gt;每个智能体并非通用型，而是结合了特定的工具与数据库，围绕任务完成特定工作，确保了系统的高度专精性与高效协作。最重要的是，这一设计仿效了真实科研团队的分工，通过任务的模块化协作，将复杂的科研过程自动化。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnA6AroNE959MTXRTunZ61XmyQWFiciawtiafb2G7OlCIiaoZdu6Au9NRLpVg6icLf1y9kMibBIzJY0Nl2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7494929006085193" data-type="png" data-w="986" data-width="986" data-height="739" data-backw="546" data-backh="409" data-imgfileid="100027362" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/ccfd2c5c-bf16-4978-baef-ea089940c218/640.png" alt="图片" data-before-load-time="1770263831583" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 2：MARS 的优化能力。&lt;/p&gt;&lt;p&gt;智能体所使用的相关数据库共包含 1348 篇钙钛矿高影响力文献的知识库，生成向量数据库（7818 个片段）与知识图谱（11.7 万节点、23.2 万条边），融合多维度检索方式，确保知识获取的全面性与准确性。&lt;/p&gt;&lt;p&gt;该系统无需依赖大量实验数据，通过 LLM 结合材料领域知识推理实验参数，实现小样本高效优化。它打通了&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;知识检索 - 方案设计 - 代码生成 - 机器人实验 - 数据分析&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;全流程，即使没有人工干预，也可以自行完成材料研发。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;材料开发中的表现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;据报道，实验验证中，系统在 10 次迭代内优化了钙钛矿纳米晶体合成。钙钛矿这种材料，因其在光电和催化领域的潜力而被广泛关注，但其合成与性能调控非常复杂，通常需要大量的实验验证与优化。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-backh="361" data-backw="546" data-height="655" data-imgfileid="100027363" data-ratio="0.6609485368314834" data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLnA6AroNE959MTXRTunZ61XTtyIKHwuZUibic5jdibnBsEsQaFfdKcrU9DvJW7Og0pKqiaDbxNEDDZvuA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="991" data-width="991" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/083d1af0-7694-4bcc-9ec9-ce7c266e3dbb/640.png" alt="图片" data-before-load-time="1770263831668" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 3：MARS 辅助的钙钛矿纳米复合材料的设计与合成。&lt;/p&gt;&lt;p&gt;通过 MARS，研究团队在&lt;strong&gt;仅 3.5 小时内&lt;/strong&gt;，完成了对钙钛矿复合材料的优化，生成了具有高发光效率且稳定性更强的水稳定钙钛矿复合材料。在整个过程中，&lt;strong&gt;系统在 10 轮实验内&lt;/strong&gt;就完成了性能的显著优化，这相较于传统的人工方法，不仅在时间上缩短了&amp;nbsp;&lt;strong&gt;60 倍&lt;/strong&gt;，还大大提高了数据的一致性和可靠性。&lt;/p&gt;&lt;p&gt;而相较于其他单智能体 LLM（GPT-4o），MARS 的胜率高达 60.02%，对比 ChatGPT-4o 网页版胜率也达到了 61.89%。且 MARS 还适配开源模型，具有良好的框架扩展性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;小结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队明确指出，推理系统的智能性和实验执行的精准性仍有进一步优化的空间。例如，复杂反应条件的适应性、机器人平台的实时反馈机制以及模型训练时的知识更新机制等，都需要在实际应用中进行持续的改进。&lt;/p&gt;&lt;p&gt;此外，随着跨学科的深入发展，MARS 系统的应用将不仅限于材料科学，还可以扩展到生物医学、环境科学等多个领域。跨模态、多领域的知识整合将成为 MARS 未来拓展的关键。&lt;/p&gt;&lt;p&gt;相关报道：&lt;em&gt;https://phys.org/news/2026-01-multi-agent-ai-robots-automate.html&lt;/em&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
