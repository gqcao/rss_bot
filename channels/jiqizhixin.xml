<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>刚刚，腾讯姚顺雨署名首篇论文发布，「下半场」先搞上下文学习</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 19:01:33 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;不久前在 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651011350&amp;idx=1&amp;sn=ce3087a30d2765b6f1b15a48cb85d1d6&amp;scene=21#wechat_redirect" target="_blank"&gt;AGI-Next&amp;nbsp;&lt;/a&gt;前沿峰会上，姚顺雨曾分享过一个核心观点：模型想要迈向高价值应用，核心瓶颈就在于能否「用好上下文（Context）」。&lt;/p&gt;&lt;p&gt;这与最近 OpenAI &amp;nbsp;Jiayi Weng 在访谈中的看法不谋而合。Jiayi Weng 认为，上下文决定了模型和人类认知的边界。只要信息足够对等，普通人大概也能在 OpenAI 胜任工作，人和人的差距往往只是源于信息的不对称。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;而近日，混元团队和复旦联合团队发布了首篇论文《CL-bench》，在「重视上下文」的基础上又往前推了一大步。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;值得一提的是，这也是姚顺雨加入腾讯后首次署名的研究论文。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGpRfm74d9f3aluoliaHicia1YLj4RcBpKj6nvnlTw9wWMSUcFiaxvFKpDgA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531368" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e3dda214-84ad-40fa-a83e-c7d613ebdfcd/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文题目：CL-bench: A Benchmark for Context Learning&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：www.clbench.com&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;论文证实了一个更棘手的问题：即便抹平了上下文带来的信息差，模型也未必能解决问题。这说明模型在上下文利用上，依然存在显著的能力短板。&lt;/p&gt;&lt;p&gt;具体来说，作者认为上下文「给到位了」并不等同于任务就能「做对」。这中间考验的是模型的学习能力：就像两个学习天赋不同的人，读的是同一本武功秘籍，有人能瞬息间领悟招式精髓，有人却始终不得要领。&lt;/p&gt;&lt;p&gt;这种差异的本质在于模型的上下文学习能力不同。 如果模型缺乏从上下文中学习新知识、掌握新技能的能力，哪怕解决任务所需的逻辑和范例都近在咫尺，它也依然无从下手。&lt;/p&gt;&lt;p&gt;本文将结合腾讯混元官网首次发表的技术博客《Learning from context is harder than we thought》的中文版，聊聊在上下文学习这件事上，模型面对的真实困境。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;博客链接：https://hy.tencent.com/research&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;从上下文中学习，远比我们想象的要难&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;我们需要 AI 成为「上下文学习者」（Context learners）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去几年，大语言模型的进化速度快得令人惊叹。如今的前沿模型，已经是顶级的「做题家」：它们能解开奥数级别的难题，能推演复杂的编程逻辑，甚至能通过那些人类需要苦读数年才能拿下的专业资格考试。&lt;/p&gt;&lt;p&gt;然而，这些耀眼的成绩单可能掩盖了一个真相：&lt;strong&gt;能在考场拿满分的学生，未必能胜任真实世界的工作。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;回看我们人类的日常工作：开发者扫过从未见过的工具文档，就能立刻开始调试代码；玩家拿起新游戏的规则书，在实战中边玩边学；科学家从复杂的实验日志中筛选数据，推导出新的结论和定律。我们发现在这些场景中，人类并不只依赖多年前学到的「死知识」，而是在实时地从眼前的上下文中学习。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGabXDVaw7j9MKgq5ewJdf1Xh9ZNrkqAmw4XJFnuHrdFObEWzYbI0dwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.37962962962962965" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531369" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/36539c54-8eb1-4096-9222-7d5e3d7015ed/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;三个人类日常生活和工作场景的例子。这三个例子分别为：（1）面对 SkyNet 无人机 SDK 文档 (~70K 字)，将自然语言所表达的飞行请求转成安全、合规的 SDK 伪代码； （2）直接上手玩一款游戏：给定一款新游戏的完整规则 (~15K 字)，分析隐藏房间场景并给出可能结果；（3）分析 300 份原始实验日志，验证数据、推导关系式并估计共振常数。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;然而，今天的语言模型并非如此。它们主要依赖「参数化知识」&amp;mdash;&amp;mdash; 即在预训练阶段被压缩进模型权重里的静态记忆。在推理时，模型更多是在调用这些封存的内部知识，而不是主动从当前输入的新信息中汲取营养。&lt;/p&gt;&lt;p&gt;这揭示了当前模型的训练范式和在真实场景中应用之间是不匹配的：我们优化出的模型擅长对自己「已知」的事物进行推理，但用户需要的，却是让模型解决那些依赖于杂乱、动态变化的上下文的任务。&lt;/p&gt;&lt;p&gt;简而言之：&lt;strong&gt;我们造出了依赖「过去」的参数推理者，但世界需要的是能吸收「当下」环境的上下文学习者&lt;/strong&gt;。要弥合这一差距，我们必须从根本上改变模型的优化方向。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyet0lcIkZDzTDD3XzBgm8iaTaJElOZSlFibCBKl2oNgpgl7tGECya1n8Eg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531248" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4a974c7a-abf8-4804-8063-f31a3a27289d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;语言模型的范式转变。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;CL-bench: 衡量模型的上下文学习能力&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了衡量现有模型距离真正的「上下文学习者」还有多远，我们构建了 &lt;strong&gt;CL-bench&lt;/strong&gt;。这是一个专门评测语言模型能否&lt;strong&gt;从上下文中学习新知识并正确应用&lt;/strong&gt;的基准。&lt;/p&gt;&lt;p&gt;CL-bench 包含由资深领域专家精心制作的 500 个复杂上下文、1899 个任务和 31607 个验证标准。CL-bench 只包含一个简单但苛刻的要求：&lt;strong&gt;解决每个任务要求模型必须从上下文中学习到模型预训练中不存在的新知识，并正确应用。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;模型需要学习的知识非常广泛。它包括新的领域知识、不熟悉的规则系统、复杂的产品工作流，甚至是必须从实验数据中推导归纳出的定律或结论。&lt;/p&gt;&lt;p&gt;所有这些知识要么是由领域专家完全新构建的，要么是取自那些不太可能出现在当前前沿模型训练数据中的小众、长尾来源。因此，模型无法通过回忆静态的参数化知识来解决任务，都要求模型从提供的上下文进行学习并应用。&lt;/p&gt;&lt;p&gt;具体来说，CL-bench 涵盖了四种广泛的现实世界上下文学习场景：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye1PKLxS0BCTJZgSCvJR4v410muIq8Micl2tRsuvvRwH6YzdIFYZ5ks2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4009259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531257" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/0d77320d-b9a0-4885-8b12-7c34d9d3c23b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; CL-bench 的上下文分类体系。&lt;/sup&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;领域知识推理&lt;/strong&gt;：&amp;nbsp;上下文提供特定的领域知识（例如 虚构的法律体系、创新的金融工具或小众专业知识）。模型需要利用这些知识来推理并解决具体问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;规则系统应用&lt;/strong&gt;：&amp;nbsp;上下文提供新定义的正式系统（例如 新的游戏机制、数学形式体系、编程语法或技术标准）。模型必须理解并应用这些规则来执行任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;程序性任务执行&lt;/strong&gt;：&amp;nbsp;上下文提供复杂的过程系统（例如 工作流、产品手册和操作指南）。模型必须理解并应用这些程序性信息来完成任务。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;经验发现与模拟&lt;/strong&gt;：&amp;nbsp;上下文提供复杂系统内的实验数据、观测记录或模拟环境。与前几类涉及演绎推理不同，这一类专注于归纳推理，也是最具挑战性的。模型必须从数据中发现潜在的定律或结论，并应用它们来解决任务。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGKPuYjN7w5dUnZZaiaBwyyia1nrapGBqgibKYxicw7UlMx9belAQPljF45g/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.4759259259259259" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531308" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c28d4a69-c396-464f-9318-deedcfd9ba79/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;CL-bench 示例。解决这些任务要求语言模型从提供的上下文中学习。图中这四个案例分别是：（1）在一部长达 2.3 万字、刚刚生效的新法律下判一起真实纠纷；（2）基于一门新设计的教育编程语言规范，实现一个带有时间条件终止的周期性程序；（3）在一套从未见过的编程框架中执行代码；（4）在给定技术规格和长期环境政策情景的条件下，模拟关键技术金属的可持续全球供应。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这些类别包含了大部分现实世界工作中常见的演绎推理和归纳推理任务，能充分衡量模型的上下文学习能力。关于 CL-bench 的更多细节，请参阅我们的论文 [1]。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;CL-bench 的设计原则和特性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;CL-bench 围绕一个简单但严格的设计原则构建：&lt;strong&gt;每个任务都必须要求从上下文中学习新知识&lt;/strong&gt;。 CL-bench 中的每个上下文都是&lt;strong&gt;完全自包含（Self-contained）&lt;/strong&gt;的。解决任务所需的所有信息都显式地提供在上下文本身之中：不需要外部检索，也不允许隐藏假设。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531372" data-ratio="0.4101851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGegrsicWiaqYrK5gGZuOXlHia7aETTiciaIzciarPGAfqfpKjCjyrMvJcqw1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/5da9aa61-d63c-4775-bfbc-ca6fe2f848d9/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 解决CL-bench 中的任务需要模型从相应的 context 中学习新知识。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了确保性能真正反映上下文学习，而不是记忆或数据泄露，CL-bench 采用了无污染（Contamination-free）设计：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;虚构创作&lt;/strong&gt;：&amp;nbsp;专家创作完全虚构的内容，例如为虚构国家设计一套完整的法律体系（包括新颖的判例和法律原则），或创建具有独特语法和语义的新编程语言。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;现有内容的修改&lt;/strong&gt;：&amp;nbsp;专家修改现实世界的内容以创建变体，例如更改历史事件、改变科学和数学定义，或修改技术文档和标准。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;整合小众和新兴内容&lt;/strong&gt;： 专家纳入了在预训练数据集中代表性极低的小众或近期新兴内容，如前沿研究发现、新发布的产品手册或技术文档，以及来自专门领域的特定知识。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在不提供任何上下文的情况下，最先进的模型 &lt;strong&gt;GPT-5.1 (High) &lt;/strong&gt;仅能解决&lt;strong&gt;不到 1%&lt;/strong&gt; 的任务。这有力地证明了数据是无污染的，模型若不从上下文中学习，几乎完全无法解决这些任务。&lt;/p&gt;&lt;p&gt;此外，CL-bench 的设计具有高复杂性和序列依赖性。&lt;strong&gt;51.1% 的任务&lt;/strong&gt;需要序列依赖，意味着后续任务的解决方案取决于早期交互的结果。这种多轮次设计显著增加了任务难度。平均而言，领域专家花费约 &lt;strong&gt;20 小时&lt;/strong&gt;标注每个上下文，以确保任务构建的质量和深度。&lt;/p&gt;&lt;p&gt;CL-bench 中的每个任务都是完全可验证的。平均而言，每个上下文关联&lt;strong&gt; 63.2 &lt;/strong&gt;个验证标准，每个任务包含 16.6 个评估标准。每个任务的正确性都从多个角度进行评估，确保了评估的全面性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;部分实验发现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们在 CL-bench 上评估了十个最先进的语言模型。结果揭示了清晰且一致的差距。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyePxuWssOuS3FUCeSZ7AgdMfcicDgtoicqgvO6cUqvPiaJ5CWiaRuhPoXXFw/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.387037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531258" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/747f34fa-d1cd-45ce-850a-1990f2e52643/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表：十个前沿模型在 CL-bench 上的任务解决率。所有模型均在推理模式下进行评估，结果报告为三次运行的平均值 &amp;plusmn; 标准差 (%)。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;平均而言，模型仅解决了 &lt;strong&gt;17.2% &lt;/strong&gt;的任务。即便是表现最好的模型 &lt;strong&gt;GPT-5.1 (High)&lt;/strong&gt;，也仅达到了&lt;strong&gt; 23.7%&lt;/strong&gt;。换句话说，尽管上下文中拥有解决每个任务所需的全部信息，模型在绝大多数任务上都失败了。这表明&lt;strong&gt;当前的 SOTA 模型几乎不会从上下文中学习&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;还有几个额外的现象值得注意：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;忽略或误用上下文是导致失败的主要原因。&lt;/strong&gt; 许多错误并非源于信息缺失，而是源于模型忽视了上下文中的关键细节，或错误地应用了它们。在许多情况下，模型只会利用预训练学习到的静态知识来解决任务，即使上下文明确定义了新的规则、概念或程序，模型也不会学习和利用。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyebKvU6O4gYfibfbQ1WQmUroQr5gvhR6pjAWMJpWrVuhUicZ7icIFCUvicaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.38744075829383884" data-s="300,640" data-type="png" data-w="844" type="block" data-imgfileid="503531252" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b88fc5c2-a006-4101-bf03-499b65e09466/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;表：各模型错误类型的分布（因为一个 solutions 可能有多种错误原因，所以每行错误率总和大于 100%）。&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;长上下文推理和指令遵循是必要的，但不是充分条件。&lt;/strong&gt; 案例分析表明，那些难以跨长上下文追踪依赖关系或难以精确遵循约束的模型，往往表现得更差。然而，即使是能够处理长输入并可靠遵循指令的模型，仍然在许多任务上失败。上下文学习需要的能力，远不止长上下文理解和指令遵循能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;从实验数据和环境模拟中进行归纳推理比演绎应用更困难。&lt;/strong&gt; 演绎任务让模型根据 context 中明确给出的规则和流程进行应用，而经验发现和环境模拟类任务则要求 归纳推理 &amp;mdash;&amp;mdash; 从数据中总结规律或在虚拟环境中探索。模型在这类任务上的表现明显较差，任务解决率通常低于 10%，且结果波动大。这表明发现规律远比应用规则更具挑战性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGq2sEiauf3miac6OQVuRsBj6WpeNFWk3yybWkxtsOPnwKX6QiaO953tAuQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.44814814814814813" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531370" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/6ff8d689-beca-49cd-9f6a-41d449dc786a/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; GPT-5.1 在高 / 低推理强度设置下，各子类别表现对比。&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;更高的推理强度通常能提升 context 学习效果。 &lt;/strong&gt;对部分模型来说，增加 推理强度 可以改善表现，使模型更深入地理解复杂 context 。例如，GPT-5.1 在管理类和实验数据类任务上的表现提升约 6%。但其他模型提升有限甚至可能下降，说明单靠更多推理并不足够，模型还必须能够正确吸收和组织 context 信息。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGrZib1oiaeW7941ddR1jLfkSTHLsHlDqNSN2f6PxTybGnEib9Qwicu8MHXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5481481481481482" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531371" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/beccea51-148b-4432-a717-365c82f50ce2/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;不同输入长度下模型上下文学习表现的变化趋势。（不同 context 下模型的表现变化呈现相似趋势。）&lt;/sup&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Context 学习的难度与 context 长度相关，但短 context 也可能很复杂。&lt;/strong&gt; 较长的 context 通常让所有模型的任务更难，这验证了长 context 处理仍是关键瓶颈。然而，即使是短 context ，如果包含信息密集、规则隐含、依赖复杂或约束严格的内容，也依然很具挑战性，说明 context 学习的难度不仅仅来源于长度，也来自于其复杂度。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;更多发现请参见我们的论文 [1]。综上所述，CL-bench 揭示了一个不能被忽视的现象：&lt;strong&gt;当今的前沿语言模型还仍然不会利用上下文，从上下文中学习。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;CL-bench 充分解释了语言模型在真实场景中为什么经常出错：即使有了上下文工程，给模型准备好了所需的上下文，模型也会失败。如果模型不能真正从中学习，仅仅提供上下文是不够的。上下文学习作为一项模型基础的学习能力，很大程度上被忽视了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;展望未来&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果上下文学习显著提升，人类在 AI 系统中的角色将发生变化：我们不再是主要的数据提供者（training data provider），而变成了 context 提供者（context provider）。竞争的焦点将从 &amp;ldquo;谁能把模型训练得更好&amp;rdquo;，转向 &amp;ldquo;谁能为任务提供最丰富、最相关的 context &amp;rdquo;。&lt;/p&gt;&lt;p&gt;但其实这里还有一个挑战。即便上下文学习足够强大，它目前依然是&lt;strong&gt;临时性的（Ephemeral）&lt;/strong&gt;：模型的上下文窗口一旦清空，学到的知识随之消失。因此，我们还要关注如何让从上下文中习得的知识&lt;strong&gt;持久化&lt;/strong&gt;？这种知识不仅是事实，还包括能帮助模型跨任务迁移的技能、经验和模式等。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeaYmkNKNZeffvicB68xX6CMRxb9O4qeGwOPCW1MjicQU1wCqGp0VzJx2g/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.5583333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531253" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/c9b09d0e-7bb9-4e22-865f-1c8b6b31b1ac/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 记忆巩固是语言模型通过上下文学习经验的关键&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;因此，如何记忆很可能成为 2026 年的另一个核心主题。 要充分发挥语言模型的潜力，可能需要新的架构、新的优化方式来决定「该保留什么」。&lt;/p&gt;&lt;p&gt;一旦上下文学习与记忆变得可靠，模型或许就能实现自主学习：它们将自主准备上下文，从中学习并自我巩固。&lt;/p&gt;&lt;p&gt;这听上去多么令人兴奋！但当下我们的目标很明确：&lt;strong&gt;让「上下文学习」真正走向现实！&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;写于 2026 年 1 月，正值新年来临之际。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;[1] CL-bench: A Benchmark for Context Learning&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>致敬Kimi K2：基于slime的全流程INT4量化感知RL训练</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 18:54:12 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/d1756c60-302a-42b1-88e5-83ca358ffdb4/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;受 Kimi K2 团队启发，SGLang RL 团队成功落地了 INT4 量化感知训练（QAT） 流程方案。通过 &amp;ldquo;训练端伪量化 + 推理端真实量化（W4A16）&amp;rdquo; 的方案组合，我们实现了媲美 BF16 全精度训练的稳定性与训推一致性，同时 INT4 极致压缩也将 1TB 级超大模型的采样任务容纳于单机 H200 (141G) 显存内，消除了跨机通信瓶颈，显著提高了 Rollout 效率，为社区提供了兼顾高性能与低成本的开源参考。&lt;/p&gt;&lt;p&gt;近期，SGLang RL 团队在强化学习的训练稳定性，训练效率与适用场景方面取得了重要进展，具体包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Unified multi-turn VLM/LLM 多轮采样范式：我们提供了 VLM 多轮采样范式的实现&lt;span data-mpa-action-id="ml4n7xgd20kt" data-pm-slice="0 0 []"&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fzhaochenyang20%2FAwesome-ML-SYS-Tutorial%2Fblob%2Fmain%2Frlhf%2Fslime%2Fvlm-multi-turn%2Freadme.md" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4muh7b-2pezt0" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;blog&lt;/a&gt;&lt;/span&gt;，开发者只需编写一套定制化的 rollout 函数，即可像训练 LLM 一样，轻松开启 VLM 的多轮强化学习。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;稳定性提升：我们实现了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FTHUDM%2Fslime%2Fblob%2Fmain%2Fdocs%2Fzh%2Fadvanced%2Fslime-router.md%2322-rollout-routing-replay-r3-for-moe" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4mx7pv-r18f3o" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Rollout Router Replay&lt;/a&gt; 机制，显著提升了 MoE 模型在 RL 训练过程中的稳定性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;低精度训练：我们在 RL 场景中成功实现了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Flmsys.org%2Fblog%2F2025-11-25-fp8-rl" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4mz7bu-gq35el" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;全流程 FP8 训练与采样&lt;/a&gt; ，进一步释放了硬件性能。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;投机采样&lt;/strong&gt;：我们在 RL 场景中成功实践了 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fthudm.github.io%2Fslime%2Fadvanced%2Fspeculative-decoding.html" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n0v6w-u09i45" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;投机采样&lt;/a&gt;，实现了大规模训练的无损加速。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此基础上，我们更进一步，在 slime 框架上成功复现并落地了&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FTHUDM%2Fslime%2Fblob%2F58525eb986c66a271aa31077e17b8afebe704b4f%2Fscripts%2Flow_precision%2Frun-kimi-k2-Thinking-int4.sh" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n1wmw-1zr50z" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;INT4 量化感知训练（QAT）&lt;/a&gt;全流程方案。该方案深受 Kimi 团队 K2-Thinking 技术报告中关于 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F1969558404759544488%2Fanswer%2F1970539327902679960" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n31mf-ib1f6c" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;W4A16 QAT (Quantization-Aware Training)&lt;/a&gt;实践的启发。为了致敬先行者并回馈社区，本文将详细剖析我们在开源生态中打通全流程的技术细节，旨在为社区提供一份兼顾稳定性与性能的可落地参考。&lt;/p&gt;&lt;p&gt;核心收益概览：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;突破显存瓶颈&lt;/strong&gt;：通过权重压缩与低比特量化，使 1TB 级别的 K2 类模型能缩容至单机 H200 (141G) 显存内，避免了跨机通信瓶颈。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训推一致&lt;/strong&gt;：训练端利用 QAT 确保权重符合 INT4 分布，推理端执行 W4A16 (Weights INT4, activations BF16 ) 计算；二者均通过 BF16 Tensor Core 进行运算，实现了媲美 BF16 全精度的训推一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;单机效率倍增&lt;/strong&gt;：在超大模型场景下，INT4 策略大幅降低了显存与带宽压力，Rollout 效率显著超越 W8A8 (Weights FP8 , Activations FP8）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本项目由 SGLang RL 团队、 InfiXAI 团队、蚂蚁集团 Asystem &amp;amp; 阿福 Infra 团队， slime 团队与 RadixArk Miles 团队联合完成。相关功能与 recipe 已经同步到了&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fradixark%2Fmiles" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4ilayi-sc764c" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;slime&lt;/a&gt;&lt;/span&gt;与&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2Fradixark%2Fmiles" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4izwmd-3raw7y" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Miles&lt;/a&gt;&lt;/span&gt; 社区，欢迎大家试用与贡献。我们也在更进一步向 MXFP8 与 NVFP4 发起挑战。同时，由衷感谢&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fverda-cloud%2F" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n422r-lu83cm" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Verda Cloud&lt;/a&gt;为本工作提供的计算资源。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4j1is2qxn" data-pm-slice="0 0 []"&gt;&lt;strong&gt;1. 技术方案概览&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.1 总体流程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们实现了从训练到推理的完整 QAT INT4 闭环的方案，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeBhHLD0Rk4SlcF7zgmLUlFvOcXVDdHaKAJGOkdRQu30hA4ibrgLZN8rA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3287037037037037" data-type="png" data-w="1080" data-width="2256" data-height="742" data-imgfileid="503531142" data-aistatus="1" data-original-style="background-color: transparent;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3d5f74e4-6b19-4629-9288-bb9ece8ff3f3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4j33xz13up" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图1 QAT INT4 全流程&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;在 &lt;strong&gt;QAT 训练阶段&lt;/strong&gt;，训练侧在维护 BF16 主权重（Master Weights）的基础上，前向传播通过&lt;strong&gt;伪量化（Fake Quantization）&lt;/strong&gt; 引入量化噪声。所谓 &amp;ldquo;伪&amp;rdquo;，是指该步骤并未真正将 BF16 数据类型转换为低精度的 INT4，而是保持浮点计算路径不变，通过插入 &lt;strong&gt;量化再反量化（Quant-Dequant）&lt;/strong&gt; 操作来模拟低精度的计算。&lt;/p&gt;&lt;p&gt;具体而言，高精度权重在经过 &amp;ldquo;离散化映射到 INT4&amp;rdquo; 后被立即还原，虽然其物理存储格式仍为浮点，但数值精度已实质性降低。这种原值与还原值之间的差异引入了量化误差，在数学上等效于向网络注入了噪声，迫使模型在训练阶段就通过梯度更新去适应这种精度损失。&lt;/p&gt;&lt;p&gt;反向传播则利用&lt;strong&gt; STE (Straight-Through Estimator) &lt;/strong&gt;技术跳过了量化算子的不可导特性。量化过程的核心操作是 &amp;ldquo;取整（Rounding）&amp;rdquo;，其数学形态为阶梯函数，导数在几乎所有位置均为 0。这意味着在标准反向传播过程中，梯度信号传导至此处会因 &amp;ldquo;&lt;strong&gt;梯度消失&lt;/strong&gt;&amp;rdquo; 而彻底中断，导致底层的主权重无法获得更新。&lt;/p&gt;&lt;p&gt;对此，STE 采用了 &amp;ldquo;梯度透传&amp;rdquo; 策略：在反向传播计算时，将取整函数的导数定义为 1（即视为恒等映射）。这一机制相当于在不可导的 &amp;ldquo;断崖&amp;rdquo; 上架设了一座桥梁，让梯度能够越过取整层，有效回传至高精度的浮点权重，确保 QAT 训练链路的闭环。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;权重转换阶段&lt;/strong&gt;，我们将训练收敛的 BF16 权重导出并执行真实量化（Real Quantization），将其转换为推理引擎适配的 INT4 格式（如 Marlin）。&lt;/p&gt;&lt;p&gt;进入 &lt;strong&gt;RL Rollout&lt;/strong&gt; 阶段，由 SGLang 加载 INT4 &amp;nbsp;Weights 并执行高效的 W4A16（INT4 权重 x BF16 激活）推理，生成的经验数据（Experience）将回流至第一阶段用于下一轮 RL 训练，从而构成一个自洽的迭代闭环。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.2 核心策略选择&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在量化格式上，我们参考 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fhuggingface.co%2Fmoonshotai%2FKimi-K2-Thinking" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4jabq0-h6clqx" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Kimi-K2-Thinking&lt;/a&gt; 选用了 &lt;strong&gt;INT4 (W4A16) &lt;/strong&gt;方案。这主要考虑到相比 FP4，INT4 在现有硬件（Pre-Blackwell 架构）上的支持更加广泛，并且业界已有成熟高效的 Marlin Kernel 实现。实验表明，在 1&amp;times;32 量化 Scale 粒度下，INT4 动态范围充足、精度稳定，其性能与生态链路均已高度优化。作为工业界 &amp;ldquo;足够好（Good Enough）&amp;rdquo; 的量化标准，INT4 在性能、风险与维护成本间实现理性平衡。当然，我们后续也计划在 NVIDIA Blackwell 系列硬件上进一步展开 FP4 RL 的探索。&lt;/p&gt;&lt;p&gt;在训练方法方面，我们采用了 &lt;strong&gt;Fake Quantization 配合 STE &lt;/strong&gt;的经典组合。通过维护 BF16 主权重，在前向计算中模拟量化噪声，并在反向传播时直通梯度，这种方式最大程度地保证了低精度训练的收敛性与稳定性。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jch4m19vb" data-pm-slice="0 0 []"&gt;&lt;strong&gt;2. 训练侧：Megatron-LM 的伪量化改造&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.1 &amp;nbsp;Fake Quantization 与 STE 实现&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyexwVVZGxzHFv9TVg4kiaNsR6O9RlZxb46ekChDhuo3x5NdCo9mnaiaAdA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6083333333333333" data-type="png" data-w="1080" data-width="2672" data-height="1626" data-imgfileid="503531144" data-aistatus="1" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e6bd039c-3d4d-4661-a721-138e5c549363/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图2&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这一阶段的核心目标是在训练过程中实时模拟量化误差，迫使模型 &amp;ldquo;学会&amp;rdquo; 适应低精度表示。为此，我们采用了 &lt;strong&gt;Fake Quantization&lt;/strong&gt; 机制：尽管权重在存储和更新时仍保持高精度的 BF16 格式，但在前向传播的实际计算中，会被暂时映射到 INT4 的精度范围参与运算。&lt;/p&gt;&lt;p&gt;具体实现上，我们在 megatron/core/extensions/transformer_engine.py 中的 _FakeInt4QuantizationSTE 类构建了核心逻辑。基于分组最大绝对值进行动态量化（Dynamic Quantization），模拟 INT4 的 [-7, 7] 数值范围及截断操作，但在计算时仍使用 BF16 类型，仅引入量化误差。&lt;/p&gt;&lt;p&gt;而在关键的反向传播环节，我们引入了 &lt;strong&gt;STE &lt;/strong&gt;机制，确保梯度能够直接穿透量化层，不经修改地回传以更新主权重，从而保证训练的连续性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.2 Fake Quantization 对比实验&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了验证 QAT 方案的必要性，并探究训练与推理精度不匹配带来的具体影响，我们设计了一组消融实验，分别在 &amp;ldquo;&lt;strong&gt;开启 QAT INT4 训练，BF16 Rollout&lt;/strong&gt;&amp;rdquo; 和 &amp;ldquo;&lt;strong&gt;关闭 QAT 训练，直接进行 INT4 Rollout&lt;/strong&gt;&amp;rdquo; 两种非对称场景下进行了测试，并以对数概率绝对差值（Logprob Abs Diff）作为训推不一致的观测指标。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeE3LBfic8GhNlpTibJ1Htpzx25qoXUrhlzADsYtNJg9S2onhx0rxrCo1g/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531146" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/2aaf4c72-4e81-4f4f-adf5-30fd1cab718b/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4jjip1s9h" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图3 Rollout 侧 BF16，训练侧对比 QAT INT4 效果&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图3展示了 &amp;ldquo;开启 QAT INT4 训练，BF16 Rollout&amp;rdquo; 的场景&lt;/strong&gt;（即&lt;strong&gt;红线&lt;/strong&gt;部分）。可以看到，即使我们使用了高精度的 BF16 进行推理，误差依然显著偏高。这是因为在 QAT 过程中，模型权重已经针对 INT4 的量化噪声进行了 &amp;ldquo;适应性调整&amp;rdquo; 或补偿；推理时若移除量化步骤，这种补偿反而成为扰动，导致特性&lt;strong&gt;分布偏移（Distribution Shift）&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="2 2 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeqOfyWdnodEibsdAPydBM18dzLxEuO8GKe0fLZ3WoNoNiaicibwtAVl9zUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531145" data-aistatus="1" data-original-style="background-color: transparent;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/30dcbb70-cf24-478b-b32b-be2eda69d722/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-mpa-action-id="ml4jk24cr6r" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图4 Rollout 侧 INT4 Weight Only，训练侧对比 QAT INT4 效果&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图4则展示了 &amp;ldquo;关闭 QAT 训练，直接进行 INT4 Rollout&amp;rdquo; 的场景&lt;/strong&gt;（即&lt;strong&gt;红线&lt;/strong&gt;部分）。这对应了传统的训练后量化（PTQ）模式。由于模型在训练阶段从未接触过量化噪声，直接将权重压缩至 INT4 不仅造成信息的剧烈丢失，更导致推理时的特征分布与训练时产生&lt;strong&gt;偏移&lt;/strong&gt;，致使误差随着训练步数呈现震荡上升的趋势。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：实验有力地证明，&lt;strong&gt;训练端的 Fake Quantization 与推理端的 Real Quantization 必须协同开启&lt;/strong&gt;。只有当训练时的模拟噪声与推理时的真实量化精度&lt;strong&gt;严格对齐&lt;/strong&gt;，才能有效抑制训推不一致，避免分布偏移，将误差控制在接近基线的水平，从而真正打通低精度 RL 训练的全流程。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jpfbl2le" data-pm-slice="0 0 []"&gt;&lt;strong&gt;3. 权重更新阶段&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.1 权重流转与动态格式适配&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeIZK8qWcb1xWbjrVDrticBBlFrSsrn73GNbibg0eqCkYV5v3ZyQP05asw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.31574074074074077" data-type="png" data-w="1080" data-width="3068" data-height="968" data-imgfileid="503531149" data-aistatus="1" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3fb2d364-a5f6-47d5-9df2-076217cdfc7a/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图5&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;为了复用 SGLang 在推理端已有的优化，我们直接采用了其内置的 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FIST-DASLab%2Fmarlin" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n57zz-h6x18j" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Marlin INT4&lt;/a&gt; 作为 INT4 的推理方案。然而，这在工程落地时我们遇到了显著的 &amp;ldquo;格式鸿沟&amp;rdquo;：QAT 训练产出的是类似 Hugging face 上的标准格式权重，而 SGLang 推理引擎的 Marlin Kernel 则强制要求权重必须经过特定的打包（Pack）与重排（Permute）处理，方能被 Kernel 高效读取。&lt;/p&gt;&lt;p&gt;面对 RL 训练中频繁的权重更新需求，首先需要解决格式兼容性问题。为此，我们设计了一套逆向的 `restore_weights_before_loading` &lt;strong&gt;保护机制&lt;/strong&gt;。该机制利用缓存的 `_original_shapes` 元数据，能够在权重更新动作发生前，强制将当前内存中的 Marlin 权重格式还原（Resize）回原始形状。这一设计有效防止了因维度不匹配导致的运行时错误，确保模型能够在标准权重格式与 Marlin 权重格式之间平滑切换。此外，我们还在系统层面新增了 `post_process_weights` API，允许控制平面根据训练节奏显式触发这一流程。&lt;/p&gt;&lt;p&gt;而针对权重加载完成后的格式适配挑战，我们在 `compressed_tensors_moe.py` 中实现了一套&lt;strong&gt;动态权重管理机制&lt;/strong&gt;。在模型权重加载结束阶段，系统会自动触发 `process_weights_after_loading` 流程，底层调用 `gptq_marlin_moe_repack` 与 `marlin_moe_permute_scales` 等算子，在内存中即时将标准权重转换为高度优化的 Marlin 权重格式，从而最大化推理时的访存与计算效率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.2 权重更新时的量化&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye3iazKfVkgaSKYXQBa8aiaBiaRU1HjSeN7l2UrwYiaezd4n55XozonkUCBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.10277777777777777" data-type="png" data-w="1080" data-width="3080" data-height="316" data-imgfileid="503531150" data-aistatus="1" data-original-style="background-color: transparent;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/933af1cd-c958-4107-bebd-a67814009001/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: justify; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图6&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;进入核心的&lt;strong&gt; Real Quantization &lt;/strong&gt;环节。不同于训练时的 Fake Quantization，这一步通过代码中的 `int4_block_quantize` 函数执行不可逆的精度压缩操作：基于设定的 Group Size，计算每组权重的缩放因子（Scale），并将高精度浮点数映射到 `[-7, 7]` 的 INT4 整数域。&lt;/p&gt;&lt;p&gt;为了最大化显存利用率，接着执行 &lt;strong&gt;位宽打包（Packing）&lt;/strong&gt; 操作。由于 PyTorch 缺乏原生的 INT4 数据类型，我们通过 `pack_int4_to_int32` 函数利用位运算技巧，将 8 个 INT4 数值紧凑地 &amp;ldquo;压缩&amp;rdquo; 进 1 个 INT32 整数中（即 `8 &amp;times; 4 bits = 32 bits`）。最终，这些经过压缩的 Packed Weights 连同 Scale 因子被传输至推理引擎，完成了从 &amp;ldquo;训练格式&amp;rdquo; 到 &amp;ldquo;推理格式&amp;rdquo; 的转换。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4js46z10zu" data-pm-slice="0 0 []"&gt;&lt;strong&gt;4. 推理阶段&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeXYxQMBDq5MDSK0icJGuz14Mzm9ZtkZPA7APLqibS5Aod7JIC43EP88jg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.21296296296296297" data-type="png" data-w="1080" data-width="3078" data-height="656" data-imgfileid="503531151" data-aistatus="1" data-original-style="background-color: transparent;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/2c68dc09-1507-45dc-8996-a0cba9d2ff2d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jkrp421ph" data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 0px; margin-right: 0px; text-align: center; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图7&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;极简打包与零开销解包&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 RL 训练的 Rollout 阶段，我们直接复用了 SGLang 优化成熟的 W4A16 量化方案。SGLang 使用紧凑的 INT4 格式，将两个 4-bit 权重打包进一个字节，相比 BF16 节省了 75% 的内存。在推理时，Triton kernel 通过高效的位移和掩码操作（&amp;gt;&amp;gt; 4 和 &amp;amp; 0xF）快速解包，得益于计算与 IO 的并行覆盖，该过程几乎实现了零额外延迟。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MoE 算子深度融合&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;显存优化&lt;/strong&gt;：SGLang 引入动态的 moe_align_block_size，根据当前 Token 数量和 Expert 分布自动选择 block_size ，将同一 Expert 的 Token 聚集并对齐，提升显存带宽利用率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;计算融合&lt;/strong&gt;：SGLang 引擎除集成了高效的 &lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fgithub.com%2FIST-DASLab%2Fmarlin" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4jwvk7-izb4u3" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;Marlin INT4&lt;/a&gt; 实现、还将 gating 部分 fuse 成一个高性能的 kernel，避免了反复启动 kernel 和读写中间结果。同时，该 INT4 推理方案兼容 GPTQ 和 AWQ 等主流量化格式，以及支持对称与非对称两种模式。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4jtp21kkw" data-pm-slice="0 0 []"&gt;&lt;strong&gt;5. INT4 QAT RL 效果&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5.1 训练效果&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练侧&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeqTib4E0c7JibNBwofOZgeGXpfbN46cWJJHFbg9zldkS7WMfM6ygeuskA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531153" data-aistatus="1" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/f5b74b22-21b8-4a51-8094-b5c388e95936/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图8 Qwen3-235B-A22B Raw-Reward对比&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye1kl4dQgNF04naMX93RLKWBuPSibqzdrpJQ1iafe7doRrUEhrLgB21eJg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531152" data-aistatus="1" data-original-style="background-color: transparent;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/199a4263-02e3-4f07-9f5a-a5447bb98b46/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图9 Kimi-K2-Thinking Raw-Reward对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;上图展示了基于 slime 框架，Qwen3-235B-A22B 与 Kimi-K2-Thinking 模型在 dapo-math-17k 数据集上的训练表现。通过对比实验发现，相较于 &amp;ldquo;&lt;strong&gt;BF16 训 - BF16 推&lt;/strong&gt;&amp;rdquo; 及 &amp;ldquo;&lt;strong&gt;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo;，&amp;ldquo;&lt;strong&gt;BF16 训 - INT4 推&lt;/strong&gt;&amp;rdquo; 配置下的 Raw-Reward 仍能保持稳健增长，且其增长趋势与前两者基本一致，证明了该方案在训练过程中的有效性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;评估侧&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeIcicbobice5yfqmKsfXzIMsFiaMOUdanCBooFqb4SnprTZe4icQtktlccQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531157" data-aistatus="1" data-original-style="background-color: transparent;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/879e58dd-b4f0-4535-9b82-eb02b8e1b2ec/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图10 Qwen3-235B-A22B AIME数据集评估对比&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeme4ow83SQ19HRLHT0xFeKKGzwHgictO0vCcb5Cx42t3bI6gyH8FdA5Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.75" data-type="png" data-w="1080" data-width="1800" data-height="1350" data-imgfileid="503531156" data-aistatus="1" data-original-style="background-color: transparent;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/9a0c5ae5-0bad-4421-a99e-324088187457/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图11 Kimi-K2-Thinking AIME数据集评估对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了更加严谨地评估模型能力的演进，我们每隔 10 个训练步长就在 aime-2024 基准测试集上进行一次评估。上图给出了 Qwen3-235B-A22B 与 Kimi-K2-Thinking 在不同 RL 训练配置下的模型评分增长轨迹。&lt;/p&gt;&lt;p&gt;实验表明：&amp;ldquo;&lt;strong&gt;BF16 训 - INT4 推&lt;/strong&gt;&amp;rdquo; 方案不仅在评估分数上呈现出稳健的上升态势，且其性能提升的斜率与最终达到的峰值，均与 &lt;strong&gt;&amp;ldquo;BF16 训 - BF16 推&lt;/strong&gt;&amp;rdquo; 和 &lt;strong&gt;&amp;ldquo;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo; 方案保持了较高的重合度。这种高度的一致性有力地证明了模型在经过低比特量化后，其核心表示能力并未受损，保证了在大幅降低计算开销的同时，依然能够实现与全精度推理相媲美甚至完全看齐的泛化表现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5.2 训推差异&lt;/strong&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyer3hUV6arauxEYHCRhcgqsm3oRGd5ExJn2P559BTzBfGo3aficCBVM6g/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531159" data-aistatus="1" data-original-style="background-color: transparent;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/29edaee1-96bb-4ee7-ad33-21bd639b1c0a/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图12&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye8DSVA3kfCUjnPt6h1PTtkvPibl5MqjexHFmLr5BtUov415Gw6ysKAqA/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531158" data-aistatus="1" data-original-style="background-color: transparent;" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/972ddd69-e290-4bf6-9870-d65cc19d8e25/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;图13&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了直观评估方案效果，我们在 Qwen3-30B 与 Qwen3-235B 模型上进行了的 QAT RL 训练验证。图中 Y 轴反映了训练侧与推理侧输出的 Logprob 绝对差值，数值越低意味一致性越强。实验结果显示，INT4&lt;strong&gt;（绿色虚线）&lt;/strong&gt;与 BF16 基准&lt;strong&gt;（红色实线）&lt;/strong&gt;呈现出惊人的重合度，且显著低于表现出较高误差水平的 FP8&lt;strong&gt;（蓝色虚线）&lt;/strong&gt;。这证实了 INT4 QAT 策略能有效规避 &amp;ldquo;&lt;strong&gt;BF16 训 - FP8 推&lt;/strong&gt;&amp;rdquo; 模式下的精度损失，实现与全精度无异的训推表现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;这种一致性背后的原因我们推测为两点：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;截断误差抑制&lt;/strong&gt;：训练侧的 Fake Quantization 将权重限制在 INT4 值域内。这种数值范围的约束，&lt;a data-miniprogram-appid="wxe81de4a47ea1ab33" data-miniprogram-applink="" data-miniprogram-nickname="小外链" data-miniprogram-path="go?to=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F1969558404759544488%2Fanswer%2F1970539327902679960" data-miniprogram-servicetype="0" data-miniprogram-type="text" data-unique-id="ml4n69wn-y5aasg" href="https://mp.weixin.qq.com/s/oFyyuwl_hTJPe3RrZnk2fw"&gt;有效降低了矩阵乘法中 Accumulator 累加时因并行计算顺序不确定性引发的浮点舍入误差（Floating-point Rounding Error），即改善了所谓的&amp;ldquo;大数加小数&amp;rdquo;精度丢失问题。&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;高精度计算&lt;/strong&gt;：推理侧采用 W4A16 模式，其核心计算全程基于 &lt;strong&gt;BF16 Tensor Core&lt;/strong&gt; 进行，确保了运算精度与训练阶段的高度对齐。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4keaemhwb" data-pm-slice="0 0 []"&gt;&lt;strong&gt;5.3 Rollout 加速&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyer6Z0vALv0IRkNXCgkKXEsPLSsDd7Znr4LU8PmAFoRicNjTmxPplU56w/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531160" data-aistatus="1" data-original-style="background-color: transparent;" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/9c7493bf-febe-4a02-bff5-5a0372de5053/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图14 Qwen3-235B-A22B Rollout 性能对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;从 Qwen3-235B 的 Rollout 性能对比图中可以直观看到，虽然 INT4&lt;strong&gt;（绿色点划线）&lt;/strong&gt;与 FP8&lt;strong&gt;（蓝色虚线）&lt;/strong&gt;均较 BF16 基线&lt;strong&gt;（红色实线）&lt;/strong&gt;实现了显著加速，但两者彼此之间并未拉开巨大的性能鸿沟。这一现象主要受限于当前的硬件特性：由于 NVIDIA H 系列 GPU 没有原生的 INT4 Tensor Core， W4A16 方案本质上利用的还是 BF16 Tensor Core 进行计算，虽然大幅降低了显存带宽压力，但在吞吐上无法像 W8A8 一样利用原生 FP8 Tensor Core 进行加速从而获得计算增益。因此，在单步推理耗时上，INT4 仅表现出微弱的优势，与 FP8 基本处于同一性能梯队。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyezmxjpLdfKvAiatib2rq58UaL5KZvOo07VhvCsUc9ia6TbPFn6Srmica8xg/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.6666666666666666" data-type="png" data-w="1080" data-width="1800" data-height="1200" data-imgfileid="503531162" data-aistatus="1" data-original-style="background-color: transparent;" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/69dfbafd-90a3-4952-8272-5f654d17f40c/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图15 Kimi-K2-Thinking Rollout 性能对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;对于 Kimi-K2-Thinking Rollout 性能的对比。首先观察双节点场景下的&lt;strong&gt;通信瓶颈&lt;/strong&gt;：图中 FP8&lt;strong&gt;（红线）&lt;/strong&gt;与 INT4&lt;strong&gt;（蓝线）&lt;/strong&gt;呈现出相似的水平。因为 H 系列 GPU 缺乏原生的 INT4 计算单元，INT4 无法在计算层面提供加速，因此整体性能依然受限于跨节点的通信带宽。&lt;/p&gt;&lt;p&gt;然而，&lt;strong&gt;绿线&lt;/strong&gt;所代表的单节点表现揭示了 INT4 的&lt;strong&gt;真正价值 &amp;mdash;&amp;mdash; 显存压缩&lt;/strong&gt;。通过将模型体积减半，我们成功将 1TB 级别的超大模型完整加载至单机显存中。这直接消除了昂贵的跨机通信开销，将 Rollout 耗时大幅缩减。这有力地证明，在当前硬件环境下，INT4 QAT 的核心收益在于通过压缩显存，解锁了高效的单机部署 Rollout 方案。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4k3fyfsqh" data-pm-slice="0 0 []"&gt;&lt;strong&gt;6. 总结与未来工作&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;slime 的这项工作不仅证明了在开源生态中复现工业界前沿方案的可行性，也为超大规模模型的低成本训练探索了新的路径。我们期望这套方案助力更多开发者深入理解 QAT 技术，并推动其在 RL 场景下的实际落地与广泛应用。&lt;/p&gt;&lt;p&gt;通过在开源框架上的复现，我们验证了 Kimi 团队所提出的 INT4 QAT 方案的有效性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;精度复现&lt;/strong&gt;：在 slime 的复现实验中，我们同样观察到了 INT4 QAT 的精度优势，实现了与 BF16 基线一致的效果。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;效率提升&lt;/strong&gt;：RL Rollout 阶段的吞吐提升显著，验证了低比特量化在 RL 场景下的巨大价值。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;未来工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;训练端效率优化：目前，由于在训练过程中引入了 QAT Fake Quantization 计算，带来了较大的额外性能开销，导致训练速度明显低于 BF16 模式。这在一定程度上折损了 Rollout 阶段带来的端到端性能收益。我们后续计划提出一套全新的优化方案，旨在解决这一训练侧的效率瓶颈，实现全链路的加速。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;推理侧 FP4&lt;/strong&gt;： 随着 NVIDIA Blackwell 架构的逐步普及，我们将积极探索 FP4 精度在 RL 训练与推理中的应用可行性，以期进一步挖掘硬件潜力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;slime 在 QAT INT4 的尝试不仅证明了在开源生态中复现工业界前沿方案的可行性，也为超大规模模型的低成本训练探索了新的路径。我们期望这套方案助力更多开发者深入理解 QAT 技术，并推动其在 RL 场景下的实际落地与广泛应用。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml4k7c6d3pm" data-pm-slice="0 0 []"&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;SGLang RL Team: Ji Li, Yefei Chen, Xi Chen, BBuf&lt;/p&gt;&lt;p&gt;InfiXAI Team: Mingfa Feng, Congkai Xie, Shuo Cai&lt;/p&gt;&lt;p&gt;蚂蚁集团 Asystem &amp;amp; 阿福 Infra 团队：Yanan Gao, Zhiling Ye, Yuan Wang, Xingliang Shi&lt;/p&gt;&lt;p&gt;RadixArk Miles Team: Chenyang Zhao, Yueming Yuan, Jiajun Li, Yusheng Su, Mao Cheng, Tom, Banghua Zhu&lt;/p&gt;&lt;p&gt;slime Team: Zilin Zhu, Chengxing Xie, Lei Li, Haisha Zhao&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>将高质量内容融入AI生态，威立以科研智能塑造出版行业未来</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Tue, 03 Feb 2026 14:26:57 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;近期，威立执行副总裁兼总经理&lt;strong&gt;Jay Flynn&lt;/strong&gt;，威立高级副总裁兼学术出版全球负责人&lt;strong&gt;Liz Ferguson&lt;/strong&gt;及威立高级副总裁兼首席营销官&lt;strong&gt;Anna Reeves&lt;/strong&gt;到访中国市场，在威立北京办公室与威立全球副总裁兼中国区总裁张莫依女士深度对话，探讨了威立在科研出版及学术期刊方面的最新发展战略，与中国科研人员、机构和企业的合作方针，以及如何在AI时代向权威学术内容与科研信息服务合作伙伴转型。&lt;/p&gt;&lt;p&gt;&amp;ldquo;中国市场蕴藏着巨大机遇，且机遇远大于挑战。作为 40 多年前首批进入中国的国际出版机构之一，我们与中国各方都建立了深厚而持久的合作关系。现今，我们致力于利用人工智能技术和开放获取出版模式，不仅满足&amp;mdash;&amp;mdash;更要预判并超越合作伙伴不断变化的需求。&amp;rdquo;Jay 表示，&amp;ldquo;主动引领变革、持续创新并优化我们的策略，是威立与合作伙伴携手塑造未来的核心。&amp;rdquo;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 3%;"&gt;&lt;/strong&gt;作为全球领先的出版机构，威立如何看待人工智能目前对科研领域的变革作用？&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" target="_blank" rel="noopener noreferrer"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/c9e9ad12-c6b2-46fd-8f71-fead204939e8/1770099644235.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;strong&gt;以合作为基石，将高质量内容融入AI生态&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;2025年10月，威立推出业内首个&lt;strong&gt;人工智能网关（Wiley AI Gateway）&lt;/strong&gt;。不同于要求科研人员采用专有工具的封闭生态系统，该网关格外注重系统间的协同操作性，将学术内容与数据订阅服务无缝集成至当前主流人工智能平台。目前，Anthropic的Claude、AWS Marketplace、Mistral AI的Le Chat以及Perplexity均已实现与该网关的对接。&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;实际上，这一网关的打造初衷，正是帮助合作伙伴的内容在 AI 时代实现&amp;ldquo;数字化跃迁&amp;rdquo;，使其能够安全可控可信地融入 AI 生态。&lt;/p&gt;&lt;p&gt;&amp;ldquo;通过该网关，我们正将学术与专业内容转化为人工智能优化格式，同时保留引文完整性、研究方法背景，以及同行评审验证&amp;mdash;&amp;mdash;这些都是用户期待威立能够实现的核心价值。&amp;rdquo;Jay 表示，&amp;ldquo;我们正在打造一个全行业的解决方案，助力人工智能驱动型研究。未来，还会有更多国际出版机构参与其中。&amp;rdquo;&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.65%;"&gt;为满足科研与教育界在人工智能方面的特定需求，威立还采取了哪些措施？&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" target="_blank" rel="noopener noreferrer"&gt;&lt;/a&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/41a51471-1394-4137-ac23-7847969159b0/1770099692579.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;倾听科研人员需求，助力负责任地使用AI&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;毫无疑问，人工智能正深刻变革行业格局与生态系统。&amp;ldquo;我们要确保在这一过程中充分关注科研人员的需求，并持续优化他们的使用体验。&amp;rdquo;Anna 表示。&lt;/p&gt;&lt;p&gt;在&lt;strong&gt;全球调研报告ExplanAItions&lt;/strong&gt;的基础上，威立于近期发布了科研人员人工智能使用现状报告&lt;strong&gt;ExplanAItions 2025&lt;/strong&gt;。&amp;ldquo;人工智能在科研人员日常工作中的使用率大幅增长。目前，全球 84% 的科研人员表示正在工作中使用人工智能工具。&amp;rdquo;Anna介绍道，&amp;ldquo;尽管科研人员对人工智能的应用前景持积极态度，他们也同时意识到需要负责任地使用这些工具。&amp;rdquo;&lt;/p&gt;&lt;p&gt;因此，威立在刚刚过去的11月推出了面向作者、编辑及审稿人的&lt;strong&gt;&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MjM5NTAxNzIzMg==&amp;mid=2650166838&amp;idx=2&amp;sn=0b8b17cf68d8a9e8697d4c923dc2582e&amp;scene=21#wechat_redirect" target="_blank"&gt;全新AI指南&lt;/a&gt;&lt;/strong&gt;，旨在为各个学科领域及工作流程中的学者提供支持，解决实际痛点问题。&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkuzw35n1l5k" data-pm-slice="0 0 []"&gt;&lt;strong&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;&lt;/strong&gt;威立品牌升级方案如何体现其在中国市场的未来方向？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ba254db6-d9c8-4d7c-a655-c66f6f0963af/1770099731558.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;持续扩展旗舰期刊系列，强化与中国伙伴合作&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&amp;ldquo;中国科研人员在国际期刊上发表了大量高质量研究成果&amp;mdash;&amp;mdash;无论是科研成果的数量，亦或影响力都令人瞩目。&amp;rdquo;作为威立学术出版全球负责人，Liz 介绍了威立在中国的合作及发展现状，&amp;ldquo;2025年，威立期刊发表的内容中约 25% 来自中国。我们同时与中国战略合作伙伴们共同推出了多种高影响力期刊。&amp;rdquo;&lt;/p&gt;&lt;p&gt;她同时介绍了威立在推动期刊发展方面的创新策略，&amp;ldquo;通过推进开放获取、优化同行评审流程，以及整合创新技术来更好地服务中国及全球科研人员。&amp;rdquo;&lt;/p&gt;&lt;p&gt;目前，威立正重点推进旗舰期刊&lt;em&gt;&lt;strong&gt;Advanced Science&lt;/strong&gt;&lt;/em&gt;从物质科学领域向生命与健康科学领域扩展，而该期刊目前发表的生命与健康科学领域的高质量研究成果数量已与物质科学领域持平。未来，威立还将在其他发展迅速且高影响力的科研领域创办期刊，包括肿瘤学、生物医学工程、生物技术、环境科学和数字健康等，同时继续深化威立在物质科学领域的优势地位。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;&lt;span data-mpa-action-id="mkw38emttw5" data-pm-slice="0 0 []"&gt;威立的最新期刊发展战略&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8c68aad9-5a97-450a-8fda-de8158959ca9/1770099775460.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkwe2sgyqma" data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/QTPaID1kkvgk6Hb58ZtWhDVU8jx5Qpp76Tg6K4NpibVao9C52o95qbFrDT4WEO9F9AnR52y9gia0ziavAeicBOgchg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.0733333333333333" data-s="300,640" data-type="png" data-w="150" data-imgfileid="502686094" data-aistatus="1" data-original-style="vertical-align: middle;max-width: 100%;width: 100%;box-sizing: border-box;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b202541a-b47f-4f54-ac60-fa1358565bf5/640.png" alt="图片" data-before-load-time="1770099589811" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 2.74%;"&gt;当威立从传统出版机构向权威学术内容与科研信息服务合作伙伴转型时，我们在中国市场的承诺与计划是什么？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="mkwe2sgyqma" data-pm-slice="0 0 []"&gt;&lt;a href="https://mp.weixin.qq.com/s/T1oONeXegiY1FuIti4SUDQ" rel="noopener noreferrer" target="_blank"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/73d1f4f3-1179-4198-a2de-096f71d62341/1770099790278.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>国产版Ollama来了，Clawdbot终于不只属于Mac和英伟达</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 11:49:24 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜+0&lt;/section&gt;&lt;p data-path-to-node="4" data-pm-slice="0 0 []"&gt;这几天，AI 圈的头号 C 位莫过于这只「龙虾」：&lt;strong&gt;Clawdbot&lt;/strong&gt;（现在得叫它 OpenClaw 了），它几乎把一群开发者折腾得彻夜难眠。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="1068" data-imgfileid="503531273" data-ratio="1.1986531986531987" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGTibSSVotZicsZhnHiaqw86b1gRTvWsZ3TZkpJrxLxPPAxevjQrke9XwGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="891" data-width="891" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1ea417bc-650c-48d6-99f8-08799be9ec64/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="5"&gt;为什么它这么火？因为和以前那些只会陪聊的 Chatbot 不同，Clawdbot 是个真正的「实干派」：它能接管你的电脑，在你睡觉时通宵写代码、修 Bug，甚至背着主人手搓出一套语音功能。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;更魔幻的是，随之诞生的 AI 社交平台 Moltbook 彻底刷屏了。在这个「AI 版 Reddit」上，150 万个 Agent 正通过自创语言和共谋进化，建立起背离人类掌控的独立机器社会与文化。&lt;/p&gt;&lt;p data-path-to-node="7"&gt;这听起来很酷，但随之而来的是「隐私的裸奔」与&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"data-path-to-node":"7","style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「钱包的哀嚎」&lt;/span&gt;。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;当 Clawdbot 这样的 Agent 全面读取你的屏幕、扫描你的文件，并在后台疯狂消耗昂贵的 API 额度时，很多开发者早就开始思考一个问题：Agent 虽好，难道我们以后的一举一动都要通过云端计费吗？&lt;/p&gt;&lt;p data-path-to-node="9"&gt;这催生了另一个巨大的需求：&lt;strong&gt;Local Agent（本地智能体）&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="10"&gt;但在这一波浪潮中，算力并不是唯一的门槛。以 Clawdbot 为例，当前社区主流方案主要围绕 macOS 与 NVIDIA GPU 生态展开，这与 Ollama、llama.cpp 以及相关 Agent 工具链的成熟度密切相关。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;相比之下，尽管华为昇腾、燧原等国产算力已经具备运行大模型的能力，但在通用 Agent 工具链与社区生态适配方面仍存在明显差距，这使得部分开发者难以直接参与到当前主流的 Agent 实验与应用中。&lt;/p&gt;&lt;p data-path-to-node="12"&gt;难道手握国产算力的开发者，只能眼巴巴看着这场狂欢吗？当然不是。&lt;/p&gt;&lt;p data-path-to-node="13"&gt;国产显卡其实从来不缺「肌肉」，缺的只是一把趁手的「兵器」。如果说 Clawdbot 解决了「AI 怎么干活」的问题，那么我们今天要聊的这个工具，就是来解决「AI 在哪干活」的问题。&lt;/p&gt;&lt;p data-path-to-node="14"&gt;&lt;strong&gt;2 月 2 日，清昴智能发布玄武 CLI 开源版本。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG2705pslwGfGCrZB0hD2S9vHSfvKpXzV34VZRWzMI2FnxMjNISCua1g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="0.5953703703703703" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503531275" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/e1a73514-1ee8-4b77-ae18-e3d2474de33f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="15"&gt;你可以把它简单理解为「国产版 Ollama」，它旨在抹平硬件架构的差异，让基于国产卡的大模型部署进入「&lt;strong&gt;零门槛时代&lt;/strong&gt;」。不需要复杂的环境配置，&lt;strong&gt;5 分钟启动模型服务&lt;/strong&gt;，这不仅是企业降低部署成本的利器，更是每一位开发者激活手边国产算力的钥匙。&lt;/p&gt;&lt;p&gt;玄武 CLI 开源传送门：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;玄武 CLI GitHub 仓库：https://github.com/TsingmaoAI/xw-cli&lt;/li&gt;&lt;li&gt;玄武 CLI Gitcode 仓库：https://gitcode.com/tsingmao/xw-cli&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="18"&gt;别急着下单 Mac mini，你机箱里的「国货之光」其实早就准备好了。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG0iaZQNft3fO4V8SBVBouRB0oOwuttLiaMWVvno4rs7aI11qolHWlgeFQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.098148148148148" data-type="png" data-w="1080" data-width="1166" data-height="1280" data-imgfileid="503531274" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/5d972558-97d0-46c7-8d3b-d75c6fdac167/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="20"&gt;&lt;strong&gt;开发者到底在和什么战斗？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="21"&gt;进入 2026 年，随着 DeepSeek、Kimi 等高性能开源模型的成熟，AI 推理形态正在从以云端为中心，逐步向本地与边缘侧扩展。出于对数据隐私（金融代码、医疗数据）和低延迟 Agent 交互的需求，&lt;strong&gt;本地化推理&lt;/strong&gt;正在成为清晰可见的趋势。&lt;/p&gt;&lt;p data-path-to-node="22"&gt;在 NVIDIA 和 Apple Metal 生态中，Ollama 凭借「一个二进制文件、一行命令」的极致体验，成为最具代表性的本地推理工具之一。然而，这种统一而简洁的使用方式，并未真正惠及中国主流国产算力用户。&lt;/p&gt;&lt;p data-path-to-node="23"&gt;尽管国产芯片在硬件指标上已具备相当竞争力，但在软件生态层面仍存在明显断层：工具链割裂、算子覆盖不足、社区适配滞后，正让开发者陷入一种新的焦虑：&lt;strong&gt;算力在手，却用不起来&lt;/strong&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;一张卡，一套世界观&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="25"&gt;与 CUDA 近乎统一的格局不同，国产芯片架构呈现出「百花齐放却互不相通」的态势。华为的 CANN、摩尔线程的 MUSA，以及各家自成体系的工具链彼此独立。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;对开发者而言，每更换一张卡，几乎意味着重新学习一套构建系统。由于上游社区难以维护如此多且杂的后端分支，国产卡用户往往只能依赖功能滞后、稳定性不足的非官方适配版本。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从入门到放弃的「配置长征」&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="27"&gt;想在国产卡上跑通一个高性能模型？往往是一场耐心与运气的双重考验：&lt;/p&gt;&lt;p data-path-to-node="27"&gt;驱动、固件、Toolkit、算子包必须严格对齐，错一个版本号就报错；少配一个环境变量，程序就可能当场崩溃；即使使用 Docker，也无法像 NVIDIA 那样 &lt;code&gt;--gpus all&lt;/code&gt; 一键搞定，而是要手动透传多个复杂设备节点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新模型「水土不服」&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="29"&gt;更具挑战的是，新一代模型架构（如 MoE、FP8 量化）在国产环境中往往缺乏成熟的高性能算子支持，，容易触发非最优执行路径，导致推理性能大幅下降。当遭遇模糊错误码时，开发者往往无从查证。&lt;/p&gt;&lt;p data-path-to-node="29"&gt;这就是行业的真实切面：开发者想要的是「5 分钟启动服务」，现实给的却是「5 天还在配环境」。&lt;strong&gt;行业迫切需要一个能够抹平底层硬件差异、统一上层使用体验的中间层工具。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="31"&gt;&lt;strong&gt;玄武 CLI：国产算力的 Ollama 来了&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="32"&gt;如果说 Ollama 的成功来自「让 GPU 消失在用户视野中」，那么玄武 CLI 的目标则是「让国产 GPU 的差异性也消失」。&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;它关注的重点并不是单纯「能否运行模型」，而是如何在复杂的国产芯片生态中，提供一种更统一、更稳定的部署与调用体验。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGo0dcPENRL8gEqWAxtj39rpXEDSNj220eoJUXoU5SO3kaQ5gqoEw1rQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.3291796469366564" data-type="png" data-w="963" data-width="963" data-height="1280" data-imgfileid="503531276" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a15feb86-939a-4c7a-91ec-753d762d4c4e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 玄武CLI的架构图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="33"&gt;&lt;strong&gt;国产原生适配：一键搞定，告别配置噩梦&lt;/strong&gt;&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;在国产算力生态中，最大的痛点来自芯片架构的高度碎片化。不同厂商、不同型号，对应不同驱动、不同推理引擎与参数组合，部署往往意味着反复查文档、改配置、踩坑调试。&lt;/p&gt;&lt;p data-path-to-node="34"&gt;玄武 CLI 的核心价值之一，就是把复杂性收敛到系统内部：它能够自动识别华为昇腾全系列、沐曦、燧原等多款国产芯片&lt;/p&gt;&lt;p data-pm-slice="0 0 []"&gt;对用户而言，不再需要理解底层架构差异，也无需手动调参调环境，真正实现「零调试部署」，从根本上降低国产芯片的使用门槛。&lt;/p&gt;&lt;p data-path-to-node="12" data-pm-slice="0 0 []"&gt;&lt;strong&gt;零门槛上手：1 分钟部署，无缝兼容无压力&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="13"&gt;在使用体验上，玄武 CLI 走的是与 Ollama 同一条路线：极简、快速、低学习成本。用户无需安装 Python 或复杂依赖，只要基础驱动就绪，解压即可运行，最快 1 分钟启动服务。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;服务启动&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="15"&gt;一切始于一行简洁的命令 &lt;code data-index-in-node="12" data-path-to-node="15"&gt;xw serve&lt;/code&gt;。无需复杂的环境变量配置，系统直接完成运行时配置初始化与全局端口分配，唤醒后台守护进程。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8c417f46-e6ca-4b03-a7fa-945e1a4e7ac0/1770090278305.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型交互&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="17"&gt;模型运行同样丝滑。通过 &lt;code data-index-in-node="12" data-path-to-node="17"&gt;xw run&lt;/code&gt; 命令，系统能直接检测实例状态。若模型已就绪，即可秒级进入 Chat 会话模式，直接开始问答交互。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0cd43644-f2dc-42a1-8e5f-eef3db58a775/1770090294379.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型下载&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="19"&gt;对于本地未获取的模型，告别繁琐的权重文件手动搬运与路径映射。通过 &lt;code data-index-in-node="33" data-path-to-node="19"&gt;xw pull&lt;/code&gt;，自动完成模型权重与配置文件的拉取，提供清晰的进度验证。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/465c39c0-88a5-43a6-b2f3-297db0364bd4/1770090306835.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="19"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG9Cn4kibZP18wNTKZFMCAmjKwvSvKHb1JiaEQY6ybSw8M6wAic1Cibn95ag/640?wx_fmt=jpeg#imgIndex=5" data-ratio="0.36666666666666664" data-type="jpeg" data-w="1080" data-width="3840" data-height="2088" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_jpg/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGCuvUthSCAL2wqo5cNO2CXzJ8WwGc607sNudGJXOWEnfy3n07XgHhLw/0?wx_fmt=jpeg&amp;from=appmsg" data-cropx2="1920" data-cropy2="704.2214532871973" data-backw="289" data-backh="106" data-imgfileid="503531301" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/a0c113ad-2dfb-4a66-8109-8d7ac214eda7/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p data-path-to-node="20"&gt;玄武 CLI 目前已原生支持包括 &lt;strong&gt;DeepSeek、Qwen3、GLM-4.7、MiniMax 2.1&lt;/strong&gt; 等在内的数十款主流模型，并在今天已完成 GLM-OCR 的 Day0 适配，覆盖从端侧轻量级到千亿参数旗舰级模型。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;实例启动&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="22"&gt;得益于底层的极致优化，在执行 &lt;code data-index-in-node="15" data-path-to-node="22"&gt;xw start&lt;/code&gt; 启动实例时，系统能够自动调配 vLLM 等高性能后端。&lt;strong&gt;实测数据表明：即便是 32b 规模的模型，玄武 CLI 也能在 30 秒内完成启动。&lt;/strong&gt;这个时间内，系统会自动完成模型切分、显存加载，并成功启动推理引擎。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3081a90c-5c0b-40dd-8227-000d7e059cda/1770090391917.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="23"&gt;同时，&lt;strong&gt;玄武 CLI 在命令层面与 Ollama 高度一致&lt;/strong&gt;（如 &lt;code data-index-in-node="31" data-path-to-node="23"&gt;xw pull&lt;/code&gt; / &lt;code data-index-in-node="41" data-path-to-node="23"&gt;run&lt;/code&gt; / &lt;code data-index-in-node="47" data-path-to-node="23"&gt;ls&lt;/code&gt; / &lt;code data-index-in-node="52" data-path-to-node="23"&gt;stop&lt;/code&gt;），意味着会用 Ollama 就能直接上手玄武，几乎没有迁移成本。在应用层，它兼容 OpenAI API 接口，LangChain、LlamaIndex 以及各类 IDE 插件只需改一行 API 地址即可接入，无需重构原有应用栈。&lt;/p&gt;&lt;p data-path-to-node="24"&gt;在稳定性设计上，玄武 CLI 采用独立子进程架构，即使单个模型或任务出现异常，也不会影响整体服务，既适合个人开发者的轻量使用，也满足企业级稳定运行需求。&lt;/p&gt;&lt;p data-path-to-node="25"&gt;&lt;strong&gt;高性能与全保障并行：多引擎覆盖，风险提前规避&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="26"&gt;玄武 CLI 内置自研的清昴核心推理引擎 MLGuider，在性能层面提供稳定保障，同时支持多种推理引擎并行兼容。这种设计一方面可以覆盖更广、更新的模型版本，另一方面也避免对单一引擎的过度依赖，从工程角度提前规避风险。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGeogmxpWlNyYXoCIrYN7fF5WAkehYc0y7LA0mQ5X2Ura8xS34ZnWv0A/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-ratio="0.37720111214087115" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503531305" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9bddccf8-5f3a-4317-8962-56c527d50fbb/640.gif" data-order="0" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="27"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 推理服务流程图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-path-to-node="28"&gt;多引擎并存，本质上是对兼容性与性能的双重极致优化。玄武 CLI 通过智能调度内置的 MLGuider 等引擎，能够深入芯片底层进行算子级调优，最大限度释放国产硬件算力。这种既保高性能推理、又顾模型多样性的策略，真正解决「国产卡能用但不好用」的核心问题。&lt;/p&gt;&lt;p data-path-to-node="29"&gt;同时，玄武 CLI 支持完全离线运行，不依赖云端服务，在国产芯片上即可完成模型管理与推理任务，适合对数据安全和稳定性要求较高的场景。&lt;/p&gt;&lt;p data-path-to-node="30"&gt;&lt;strong&gt;热门产品联动：拓展本地 AI 应用场景&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="31"&gt;在应用生态层面，玄武 CLI 并不只是一个「模型启动器」，而是一个本地 AI 能力的底座。它可以与 Clawdbot 等热门本地 AI 工具联动，为这些产品提供低门槛的模型部署与调用能力，使自动化任务与智能应用更容易落地。&lt;a href="https://mp.weixin.qq.com/s/RZhyl0rVTkZCV-cJ2ndAAw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d52862b3-7470-4e44-8f8b-bb2bff552a76/1770090427058.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-path-to-node="32"&gt;这种联动模式意味着，开发者不必重复解决模型部署问题，而可以把更多精力放在上层应用与业务逻辑上，从而放大本地 AI 工具的整体价值。&lt;/p&gt;&lt;p data-path-to-node="33"&gt;&lt;strong&gt;为什么是他们？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="34"&gt;玄武 CLI 的强大，源自其背后深厚的技术积淀。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;清昴智能是一家专注于&lt;strong&gt;芯片适配和模型-框架-算子联合调优&lt;/strong&gt;的全面领先 AI Infra 企业。创始团队来自清华大学计算机系，汇聚了来自斯坦福、新国立、爱丁堡大学以及华为、阿里、AMD 等全球顶尖机构的 AI 精英。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;创始人关超宇小学到大学 2 次跳级，15 岁进入本科，21 岁获得清华大学特奖、西贝尔学者等一系列殊荣，22 岁放弃华为天才少年、阿里星等大厂 offer，选择携手导师朱文武教授和前华为英雄个人和极客开发荣誉获得者姚航联合创业。他们不仅懂软件，更懂底层的芯片微架构以及如何攻克国产软件生态难题。&lt;/p&gt;&lt;p data-path-to-node="37"&gt;成立 3 年，即获得华为哈勃的战略注资，以及多家国内一线基金的上亿元财务投资。这不仅证明了其技术价值，更意味着其与国产芯片厂商有着深度的原厂级合作关系，能够第一时间获取底层驱动支持。&lt;/p&gt;&lt;p data-path-to-node="38"&gt;清昴智能并未止步于 CLI 工具。以自研的异构推理引擎 &lt;strong&gt;MLGuider &lt;/strong&gt;为核心，公司构建了从底层芯片到上层框架以及 Agentic AI 的全栈能力，致力于构建 AI 2.0 时代软件基础设施，为企业智能化转型和 AGI 实现打造坚实底座。&lt;/p&gt;&lt;p data-path-to-node="39"&gt;玄武 CLI 正是这一庞大技术愿景在开发者侧的「尖刀」产品，旨在通过极致的易用性打开市场缺口，构建生态护城河。&lt;/p&gt;&lt;p data-path-to-node="40"&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="41"&gt;技术，终究是要为人服务的。&lt;/p&gt;&lt;p data-path-to-node="42"&gt;过去几年，国产显卡用户面对的并非性能问题，而是生态问题：驱动、框架、工具链之间的割裂，使大量潜在算力长期处于「不可用状态」。&lt;/p&gt;&lt;p data-path-to-node="43"&gt;玄武 CLI 的出现，或许不能立刻让国产生态「拳打英伟达，脚踢苹果」，但它至少做到了一件事：把梯子递到了墙边。&lt;/p&gt;&lt;p data-path-to-node="44"&gt;它让开发者不必再充当「环境配置员」，而能重新回到创造本身；也让那些躺在机箱里吃灰的国产显卡，重新开始发热、计算，参与到真实的 AI 实践之中。&lt;/p&gt;&lt;p data-path-to-node="45"&gt;想要一起推动生态进步？赶快到 GitHub 给它一个 Star 吧！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="17,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="17,0,0"&gt;玄武 CLI GitHub 仓库：&lt;/b&gt;https://github.com/TsingmaoAI/xw-cli&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="17,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="17,1,0"&gt;玄武 CLI Gitcode 仓库：&lt;/b&gt;https://gitcode.com/tsingmao/xw-cli&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，马斯克收购了马斯克</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 03 Feb 2026 11:38:45 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;一觉醒来，马斯克又搞了个大的。&lt;/p&gt;&lt;p&gt;旗下太空探索技术公司与人工智能公司合二为一了。&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;SpaceX 正式宣布收购 xAI！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;目前，双方都已确认了这一消息。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyejMI3Zp3ic8EP9GdLvLFqL8icJoichC2pibiazLHXAY2FvWQoHUERbxKCk0g/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.7537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531261" data-aistatus="1" data-original-style="height: auto !important;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/26af6145-e328-47e5-a830-0a669df8c588/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeHj4fNTRrVO18r8AvjqCjXO8oSHTSOicZVG8UOGOoFia5ZlTQ9JNMcfDw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.3074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531262" data-aistatus="1" data-original-style="height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3f3c6149-ec45-46f7-95e0-83c469bdffda/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;据彭博社报道，合并后的公司预计将以每股约 527 美元的价格定价，其估值将达到 1.25 万亿美元。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;以下为 Elon Musk 签名公告全文：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;SpaceX 已收购 xAI，旨在打造地球上（及地球之外）最宏伟的垂直整合创新引擎。该体系集成了人工智能、火箭技术、天基互联网、手机直连通信，以及全球领先的实时信息与言论自由平台。&lt;/p&gt;&lt;p&gt;这不仅是 SpaceX 和 xAI 使命的新篇章，更是开启了全新的篇章：通过规模化扩张打造「有意识的太阳」，以理解宇宙并将意识之光延伸至群星。&lt;/p&gt;&lt;p&gt;目前 AI 的进步依赖于大型地面数据中心，而这些中心需要海量的电力与散热支持。全球对 AI 的电力需求即便在短期内，也无法仅靠地面方案满足，否则将给社区和环境带来沉重负担。&lt;/p&gt;&lt;p&gt;从长远来看，天基 AI 是实现规模化扩张的唯一路径。想要利用太阳能量的百万分之一，所需的能源就已超过人类文明当前用电总量的百万倍！&lt;/p&gt;&lt;p&gt;因此，唯一的方案是将这些资源密集型项目转移至拥有广阔能源与空间的场所。毕竟，「空间」之所以被称为「空间」，自有其道理。&lt;/p&gt;&lt;p&gt;通过直接利用近乎永恒的太阳能，且几乎无需运行或维护成本，这些卫星将彻底改变我们扩展算力的能力。在太空中，永远是晴天！发射由百万颗卫星组成的轨道数据中心星座，是迈向「卡尔达肖夫 II 级文明」（能够利用太阳全部能量的文明）的第一步。这不仅能支撑起惠及当下数十亿人的 AI 应用，也将确保人类拥有多行星栖居的未来。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyefFoYy3DWXfjiaJQ4BkkibBsXfUmUReBRdkaee26ex1cktFxTzUts2O1A/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531263" data-aistatus="1" data-original-style="height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/2ef1f083-34b7-47ef-8ba2-b2082ad377a4/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在航天史上，从未有过任何运载工具能够发射「天基数据中心」、月球永久基地或火星城市所需的那种百万吨级载荷。即便是在轨道发射次数创下历史之最的 2025 年，入轨载荷总量也仅约 3000 吨，且其中大部分是由我们的猎鹰火箭（Falcon）运载的星链（Starlink）卫星。&lt;/p&gt;&lt;p&gt;发射数千颗卫星入轨的需求，成为了猎鹰计划的「强力函数」（Forcing Function），推动其通过递归改进达到了前所未有的发射频率，从而使天基互联网成为现实。&lt;/p&gt;&lt;p&gt;今年，星舰（Starship）将开始把性能更强劲的 V3 版星链卫星送入轨道，单次发射为星座增加的容量将是目前猎鹰火箭发射 V2 版星链的 20 倍以上。此外，星舰还将发射下一代手机直连卫星，为全球每一个角落提供完整的蜂窝网络覆盖。&lt;/p&gt;&lt;p&gt;虽然发射这些卫星的需求将同样作为「强力函数」来推动星舰的改进与发射频率，但天基数据中心所需的惊人卫星数量，将把星舰推向更高的高度。通过每小时一次、单次运载 200 吨的频率，星舰每年将向轨道及深空输送数百万吨载荷，开启人类在群星间探索的激动人心的未来。&lt;/p&gt;&lt;p&gt;基本的数学逻辑是：每年发射 100 万吨卫星，若每吨产生 100 kW 的计算能力，每年将增加 100 GW 的 AI 算力储备，且无需后续的运营或维护投入。最终，我们拥有一条从地球实现每年发射 1 TW 算力载荷的路径。&lt;/p&gt;&lt;p&gt;据我估计，在 2 至 3 年内，生成 AI 算力成本最低的方式将是在太空。仅凭这一成本优势，就足以让创新型企业在训练 AI 模型和处理数据方面取得前所未有的速度与规模，从而加速人类对物理学理解的突破，并催生造福全人类的技术发明。&lt;/p&gt;&lt;p&gt;这一新星座将建立在成熟的空间可持续性设计和运营策略之上（包括寿命末期的报废处理），这些策略在 SpaceX 现有的宽带卫星系统中已证明行之有效。&lt;/p&gt;&lt;p&gt;虽然从地球发射 AI 卫星是目前的重点，但星舰的能力同样支持在其他星球开展行动。得益于空间推进剂转移等技术的进步，星舰将具备在月球降落海量物资的能力。一旦抵达月球，我们便能建立起用于科学研究和制造业的永久基地。&lt;/p&gt;&lt;p&gt;月球工厂可以利用月面资源制造卫星并将其部署到更深远的太空。通过使用电磁质量投射器（Mass Driver）和月球制造，每年可向深空部署 500 到 1000 TW 的 AI 卫星，从而在「卡尔达肖夫等级」上实现实质性的跃升，并利用太阳能量中一个不可忽视的比例。&lt;/p&gt;&lt;p&gt;通过实现天基数据中心所释放的能力，将为月球上的自我生长基地、火星上的完整文明，以及最终向整个宇宙的扩张提供资金与技术支持。感谢你们为意识的光锥所做以及将要做出的一切。&lt;/p&gt;&lt;p&gt;评论区的网友有的已经开始为合并后的公司设计新名字了 &amp;mdash;&amp;mdash;SpaceXAI。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyewHqDbXkFevDE1ibMXcRVNEtYzpN6k8TUBylVdZXOHj6bQw6kBh78CNQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.16111111111111112" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531264" data-aistatus="1" data-original-style="height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/9382609f-d057-42ac-81d3-263f773cb519/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;两家公司合并后是要打造 AI 原生的火箭吗？&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeRzueRxYZF65sibZjmmnohPnhvXNO902HcuoC3RW92Ifc4NuMtUal5nA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.15092592592592594" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531265" data-aistatus="1" data-original-style="height: auto !important;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/fa20440d-afde-4c84-b15d-9cdb049c9d0f/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;还有网友推测下一个合并目标可能是特斯拉。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeiapqdAtA3uYicjgoNu7tvWFAZSsVC13mIcMictZt6bbfdZz1nIdsUfibKA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.15092592592592594" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531266" data-aistatus="1" data-original-style="height: auto !important;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/1d162db2-8a90-48f2-b016-e18389168c51/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeBbQQ6VhpPywmrrGzk0MmwoVmZGMoWWxxsDia2DNaDv8glqR9WC3G86w/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.14907407407407408" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503531267" data-aistatus="1" data-original-style="height: auto !important;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/ac8b48b1-551a-4ba0-9510-fbbcf7b7f0f0/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;公告地址：https://www.spacex.com/updates&lt;a data-topic="1" href="javascript%3A;"&gt;#xai&lt;/a&gt;-joins-spacex&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：https://www.bloomberg.com/news/articles/2026-02-02/elon-musk-s-spacex-said-to-combine-with-xai-ahead-of-mega-ipo?srnd=phx-technology&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>重新定义“实时在线交互”，Soul App开源实时数字人生成模型SoulX-FlashTalk</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 03 Feb 2026 10:33:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;近期，Soul App AI团队（Soul AI Lab）已开源实时数字人生成模型SoulX-FlashTalk 。这是首个能够实现0.87s亚秒级超低延时、32fps高帧率，并支持超长视频稳定生成的14B数字人模型。&lt;/p&gt;&lt;p&gt;在持续建设AI能力的过程中，Soul团队始终致力于通过技术创新实现更沉浸、多元的交互体验。此次开源新模型，除了在速度、效果、延迟和保真度上表现出色，更重要的是，为行业提供了切实可应用的业务解决方案，推动大参数量实时生成式数字人迈入可具体商用落地阶段。&lt;img src="https://image.jiqizhixin.com/uploads/editor/bff004f2-3e16-4f58-94a0-5ab8d69f288b/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;Project Page: &lt;a href="https://soul-ailab.github.io/soulx-flashtalk/"&gt;https://soul-ailab.github.io/soulx-flashtalk/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Technical Report: &lt;a href="https://arxiv.org/pdf/2512.23379"&gt;https://arxiv.org/pdf/2512.23379&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Source Code: https://github.com/Soul-AILab/SoulX-FlashTalk&lt;/p&gt;&lt;p&gt;HuggingFace:&lt;a href="https://huggingface.co/Soul-AILab/SoulX-FlashTalk-14B"&gt;https://huggingface.co/Soul-AILab/SoulX-FlashTalk-14B&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;SoulX-FlashTalk亮点：&lt;/strong&gt;&lt;strong&gt;四大关键指标，重塑实时互动体验&lt;/strong&gt;&lt;/h3&gt;&lt;h4&gt;&lt;strong&gt;0.87s 亚秒级延时，即时交互&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;在实时视频交互中，延迟是决定用户体验的核心。SoulX-FlashTalk 凭借全栈加速引擎的极致优化，成功将首帧视频输出的延时降至0.87s亚秒级。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&amp;ldquo;零延迟&amp;rdquo;即时反馈： 首次让 14B 级大模型数字人具备了即时反应能力，彻底消除了传统大模型生成的&amp;ldquo;滞后感&amp;rdquo;。&lt;/li&gt;&lt;li&gt;全场景交互： 无论是视频通话中的即时对答、直播间弹幕的秒级互动，还是智能客服的实时响应，均能实现自然、流畅的深度对话。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;32fps 高帧率，重新定义&amp;ldquo;流畅&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管搭载了 14B 参数量的超大 DiT 模型，SoulX-FlashTalk 的推理吞吐量仍高达 32 FPS。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;超越行业标准：远超直播所需的 25 FPS 实时标准，确保每一帧画面都丝滑顺畅。&lt;/li&gt;&lt;li&gt;大模型，高性能：证明了 140 亿参数大模型在经过深度加速优化后，依然可以拥有极佳的运行效率。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;超长视频稳定清晰生成，告别画面&amp;ldquo;崩坏&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;数字人视频最怕在生成中出现人物面部不一致或显著画质下降的问题。SoulX-FlashTalk 凭借独家的自纠正双向蒸馏技术，解决了这一痛点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;无感纠错，画质无损：引入多步回溯自纠正机制，模拟长序列生成的误差传播并进行实时修正，就像为 AI 装上了&amp;ldquo;实时校准器&amp;rdquo;，主动恢复受损特征。&lt;/li&gt;&lt;li&gt;超长视频，稳定生成： 不同于传统的单向依赖，SoulX-FlashTalk 完全保留了双向注意力机制，让每一帧生成都能同时参考过去与隐含的未来上下文，从根本上压制身份漂移，这意味着在超长直播中，主播的口型、面部细节和背景环境将始终保持一致，不会出现模糊或变形。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;全身动作交互：不只是&amp;ldquo;口型对齐&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;SoulX-FlashTalk 突破了传统数字人仅能实现面部&amp;ldquo;对口型&amp;rdquo;的局限，带来了更加真实自然的全身肢体动态表现。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全身肢体动态合成： 不同于仅对脸部进行局部重绘的方案，SoulX-FlashTalk 支持受音频驱动的全身动作生成，产生真实自然的人体动态。&lt;/li&gt;&lt;li&gt;高精细手部表现： 基于14B DiT的强大建模能力，系统能够有效消除手部畸形与运动模糊，精准呈现结构清晰、纹理锐利的手部动作细节。&lt;/li&gt;&lt;li&gt;灵动而不失稳定： 在追求大幅度动态表现力的同时，系统依然维持了极高的身份一致性（Subject-C 达 99.22），实现了动作灵活性与画面稳定性的完美平衡。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;strong&gt;核心方案：&lt;/strong&gt;&lt;strong&gt;双向蒸馏+多步回溯自纠正机制&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在行业中，传统数字人生成方案大多面临画面生成时间长、延迟高、生成效果差、效果不稳定、保真度低等问题。&lt;/p&gt;&lt;p&gt;在这样的背景下，SoulX-FlashTalk正式开源，为了平衡生成质量与推理速度，团队采用了两阶段训练策略：&lt;/p&gt;&lt;p&gt;第一阶段：延迟感知时空适配 (Latency-Aware Spatiotemporal Adaptation)，结合动态长宽比分桶策略进行微调，使模型适应较低的分辨率和更短的帧序列；&lt;/p&gt;&lt;p&gt;第二阶段：自纠正双向蒸馏 (Self-Correcting Bidirectional Distillation)。利用 DMD 框架压缩采样步数并移除无分类器引导（CFG），实现加速；多步回溯自纠正机制，通过 autoregressively 合成连续分块（最多 K个chunks），显式模拟长视频生成的误差传播；随机截断策略，在训练中在第 k（&amp;lt; K）个分块数进行反向传播，实现高效且无偏的显存友好优化 。&lt;img src="https://image.jiqizhixin.com/uploads/editor/8378b985-0400-4101-89c9-d92a3695d76a/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 训练流程示意图&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;同时，团队进行实时推理加速系统优化， 针对 8-H800 节点设计的全栈加速引擎实现了亚秒级延迟 ，包括了&lt;/p&gt;&lt;ul&gt;&lt;li&gt;混合序列并行 (Hybrid Sequence Parallelism)：整合 Ulysses 和 Ring Attention，使单步推理速度提升约5倍算子级优化：采用针对Hopper架构优化的FlashAttention3，通过异步执行进一步减少 20% 的延迟&amp;nbsp;&lt;/li&gt;&lt;li&gt;3D VAE 并行化：引入空间切片并行解码策略，实现VAE处理的5倍加速&lt;/li&gt;&lt;li&gt;整链优化：通过 torch.compile 实现全流程图融合与内存优化&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;值得注意的是，在Soul AI团队发布的技术报告中指出，传统的单向（Unidirectional）模型在处理全局时间结构时存在约束，容易导致时间不一致和身份漂移。因此，团队完全保留双向注意力机制（All-to-All 交互），使模型能同时利用过去与隐含的未来上下文，显著提升了生成的一致性与细节质量 。&lt;img src="https://image.jiqizhixin.com/uploads/editor/0933abbd-53e5-43a1-8a99-4e9c93a7f749/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; SoulX-FlashTalk推理架构流程图&lt;/sup&gt;&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;strong&gt;+&lt;/strong&gt;&lt;strong&gt;实时体验，&lt;/strong&gt;&lt;strong&gt;赋能行业多元业务场景&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;从模型表现来看，通过在 TalkBench-Short 和 TalkBench-Long 数据集上的定量对比，展示了SoulX-FlashTalk在视觉质量、同步精度及生成速度上的全面领先：&lt;/p&gt;&lt;p&gt;在短视频评测中，它以3.51的ASE和4.79的IQA刷新了视觉保真度记录，并以1.47的Sync-C分数表现出最优的口型同步精准度；在5分钟以上的长视频生成中，系统凭借双向蒸馏策略有效抑制了同步漂移，取得了1.61的Sync-C优异成绩；此外，作为14B参数规模的大模型，它在长短视频任务中均维持了32 FPS 的高吞吐量，不仅远超25 FPS的实时性基准，更在推理效率上显著优于行业同类主流模型。&lt;img src="https://image.jiqizhixin.com/uploads/editor/2f861bd8-94a8-4d0a-9c86-ff91f633d3a9/%E5%9B%BE%E7%89%874.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;依托模型优越的性能表现，开源后，SoulX-FlashTalk将有机会在多领域、行业实际落地，创造更多价值。例如，在电商领域打造7&amp;times;24小时AI直播间，特别是，此前传统的数字人直播长时间运行后常会出现嘴型对不上或画质模糊的问题，而SoulX-FlashTalk可以支持全天候的流畅视频直播，即便是在高强度的实时互动中（如回复弹幕），也能保持如同真人出镜的高保真画质，极大降低直播成本。&lt;/p&gt;&lt;p&gt;此外，在短视频制作、AI教育、多元互动场景NPC交互、AI客服等方向，模型也提供了高质量、可落地、可接入业务系统的解决方案。&lt;/p&gt;&lt;p&gt;对Soul而言，SoulX-FlashTalk的发布也意味着团队进入了开源新阶段。去年10月底，Soul AI团队开源语音合成模型SoulX-Podcast，在发布后快速登顶开源社区平台HuggingFace TTS（Text To Speech）趋势榜，目前该模型在GitHub上收获了超3100星标。&lt;/p&gt;&lt;p&gt;接下来，在聚焦语音对话合成、视觉交互等核心交互能力的提升，为用户带来更加沉浸、智能且富有温度的交互体验的过程中，以持续推进开源工作为契机，Soul将积极与全球开发者携手，共建生态，为推动&amp;ldquo; AI +社交&amp;rdquo;方向前沿能力建设贡献力量。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>全球304个中文大模型实测：没有“全能王者”，ReLE凭70%降本方案破解评估困局</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 03 Feb 2026 10:32:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-03</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-03</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;中文大模型正迎来爆发式增长，2024-2025年间每月新增10-15个模型，但行业长期被两大痛点困扰：传统基准数据集饱和失效，顶尖模型性能逼近天花板后得分趋同；评估成本居高不下，300+模型全量评估需超6.9万美元，且单模型适配耗时超1小时。更关键的是，单一聚合得分掩盖了模型的能力权衡&amp;mdash;&amp;mdash;一个平衡评估中排第8的模型，在专业场景可能暴跌至32名。&lt;/p&gt;&lt;p&gt;针对这些问题，非线智能、华为、中国平安、绿盟科技、 中山大学、香港科技大学（广州）、等机构联合研发了 &lt;strong&gt;ReLE（Robust Efficient Live Evaluation）&lt;/strong&gt;中文大模型评估系统。该系统通过207,843个样本，对304个中文LLM（189个商业模型、115个开源模型）完成大规模测评，创新提出动态方差感知调度与混合验证评分机制，在保证排名相关性&amp;rho;=0.96的前提下，将评估成本降低70%。更重要的是，ReLE首次量化揭示中文LLM的&lt;strong&gt;能力各向异性&lt;/strong&gt;（Capability Anisotropy）：模型排名稳定性振幅（RSA）达11.4，是传统基准的2.3倍，证明当前大模型多为&amp;ldquo;专业化选手&amp;rdquo;而非&amp;ldquo;全能冠军&amp;rdquo;。相关研究成果已作为预印本发布，为中文LLM的训练优化与工业选型提供了全新诊断工具。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/17f83cb9-8a8f-45df-ba3c-30c56fecb38f/1770085772459.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;论文：&lt;/strong&gt; [arXiv链接] &lt;a href="https://arxiv.org/abs/2601.17399"&gt;https://arxiv.org/abs/2601.17399&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Github项目：&lt;/strong&gt; [链接]&lt;a href="https://github.com/jeinlee1991/chinese-llm-benchmark"&gt;https://github.com/jeinlee1991/chinese-llm-benchmark&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术架构拆解：三大核心创新破解传统评估困局&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE的核心突破在于&amp;ldquo;结构化诊断+高效评估&amp;rdquo;的双重设计，其模块化架构通过五大组件形成闭环，重点解决了接口标准化、评分鲁棒性、成本可控性三大技术难题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 统一提示Schema：消除跨模型格式偏差&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统评估中，不同模型的聊天模板（如ChatML、Alpaca）差异导致性能偏差，ReLE设计了统一提示框架，覆盖12类任务类型与7大核心领域，包含输入内容、输出格式要求、领域标签三大核心字段。&lt;/p&gt;&lt;p&gt;针对DeepSeek-R1等推理型模型，系统新增思维链触发扩展，可分离模型的推理过程与最终答案，避免&amp;ldquo;逻辑正确但格式不符&amp;rdquo;的误判。更关键的是，模型适配层能自动将标准化提示映射为目标模型的原生模板，标注者间一致性达96.8%，新增模型时无需重构评估流程。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 混合验证评分：攻克推理任务假阳性难题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为平衡评估规模与准确性，ReLE采用三级评分机制，重点解决了传统嵌入相似度评分的假阳性问题：&lt;/p&gt;&lt;p&gt;&amp;bull;客观任务（68%）：通过精确字符串匹配或符号相等性检查（如数学答案），实现100%精度评分；&lt;/p&gt;&lt;p&gt;&amp;bull;半客观任务（24%）：采用&amp;ldquo;BGE-M3语义过滤+GPT-4o判断+偏差校准&amp;rdquo;的级联策略&amp;mdash;&amp;mdash;相似度&amp;gt;0.92或&amp;lt;0.60的样本自动标注，模糊样本（0.60-0.92）由Judge模型评估。通过500个对抗样本的盲 ablation 校准，Judge模型与人类标注的Cohen&amp;#39;s &amp;kappa;=0.81，即使对非OpenAI模型，&amp;kappa;仍保持0.79，几乎无厂商偏见；&lt;/p&gt;&lt;p&gt;&amp;bull;智能体任务：聚焦工具选择准确率、步骤冗余度等复合指标，适配多智能体场景评估。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 动态方差感知调度：70%成本降低的关键&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE创新性地将心理测量学中的计算机自适应测试（CAT）原理引入LLM评估，设计分层序贯方差缩减采样策略：&lt;/p&gt;&lt;p&gt;1.方差探测阶段：对每个模型抽取5%样本，估算各能力维度的初始方差Sᵢⱼ&amp;sup2;，建立模型&amp;ldquo;稳定性画像&amp;rdquo;；&lt;/p&gt;&lt;p&gt;2.动态分配阶段：基于Neyman分配原则，按公式nₕ,ₘ*&amp;prop;WₕSₕ,ₘ/&amp;radic;cₕ动态分配样本（Wₕ为维度权重，cₕ为该维度单样本成本），对稳定模型裁剪冗余样本，向高方差边界案例倾斜资源；&lt;/p&gt;&lt;p&gt;3.停止条件：采用Hoeffding-Serfling边界控制置信区间宽度，确保评估精度的同时终止无效采样。&lt;/p&gt;&lt;p&gt;对比传统全量评估，该策略将304个模型的评估成本从6.9万美元降至2.07万美元，成本降低70%，且与全量评估的排名相关性达&amp;rho;=0.96，验证了高效与精准的统一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;基准设计：20万样本构建&amp;ldquo;领域&amp;times;能力&amp;rdquo;正交矩阵&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE的诊断能力源于其结构化基准设计，首次实现&amp;ldquo;领域知识&amp;rdquo;与&amp;ldquo;认知能力&amp;rdquo;的解耦，避免传统基准的能力 conflation 问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数据集：新鲜性与去污染双保障&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;数据集规模达207,843个样本，由三部分构成：&lt;/p&gt;&lt;p&gt;&amp;bull;动态新鲜集（45%）：2024年6月-2026年1月新增样本，含2025年高考题、最新工业 regulations，确保与2025年中前发布模型的训练数据无重叠；&lt;/p&gt;&lt;p&gt;&amp;bull;求解器验证学术精炼集（35%）：基于Math24O等数据集进行数值扰动，经SymPy/WolframAlpha符号求解器验证与10%人工抽样审核，保证数学逻辑一致性；&lt;/p&gt;&lt;p&gt;&amp;bull;领域专用私有集（20%）：金融、医疗等行业私有案例，作为严格隔离的泛化测试集。&lt;/p&gt;&lt;p&gt;为应对数据污染，ReLE实施&amp;ldquo;N-gram+语义&amp;rdquo;双级去重：13-gram检查排除显性重叠，BGE-M3嵌入语义去重（相似度&amp;gt;0.85样本丢弃）防范&amp;ldquo;软记忆&amp;rdquo;；同时引入5000个2025年10月新增的私有锚定集（PAS），通过计算泛化差距&amp;Delta;₉=|公开集得分-私有集得分|，标记过拟合模型（&amp;Delta;₉&amp;gt;15%）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;分类体系：7&amp;times;22正交矩阵拆解能力维度&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE构建了严格的&amp;ldquo;领域（D）&amp;times;能力（C）&amp;rdquo;正交矩阵，将评估空间拆解为7个核心领域、22个 primary 维度、317个子任务：&lt;/p&gt;&lt;p&gt;&amp;bull;领域维度（D）：覆盖STEM（教育/科学）、医疗健康、金融、法律与政策、通用语言、智能体与工具等7大场景；&lt;/p&gt;&lt;p&gt;&amp;bull;能力维度（C）：包含知识检索、逻辑推理、指令遵循、开放式生成4类核心认知能力。&lt;/p&gt;&lt;p&gt;这种设计可精准定位模型失效原因&amp;mdash;&amp;mdash;例如&amp;ldquo;高考数学题&amp;rdquo;被归类为Dₑₙᵤ&amp;cap;C_推理，&amp;ldquo;医疗伦理题&amp;rdquo;归为D_ₘₑd&amp;cap;C_知识，从而区分模型是缺乏领域知识还是推理能力不足。数据显示，专业领域（医疗、金融等）内部相关性为0.54-0.61，而专业领域与通用领域（推理、语言）的相关性仅0.26，印证了能力解耦的必要性。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/30c83915-50e2-4103-8ae2-3608408df435/1770085792562.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实证发现：中文LLM的三大关键真相&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于304个模型的大规模测评，ReLE揭示了中文大模型发展的核心特征，对工业选型与训练优化具有直接指导意义。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 能力各向异性显著：没有&amp;ldquo;全能模型&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE定义各向异性指数Iₐₙᵢₛₒ=1-平均维度间Pearson相关系数，实测得Iₐₙᵢₛₒ=0.74，意味着一个领域的高性能无法预测另一个领域的表现。&lt;/p&gt;&lt;p&gt;从能力雷达图可见，商业模型、开源模型、多智能体模型的能力轮廓均呈&amp;ldquo;不规则形状&amp;rdquo;：商业模型在医疗健康领域平均得分70.1，领先开源模型11.8分，但在通用推理领域差距缩小至2.4分；多智能体模型在工具使用任务中以74.8分大幅领先（商业模型62.4分），但在金融领域表现平平。这一发现打破了&amp;ldquo;高综合得分=全能&amp;rdquo;的认知。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6c693448-8c25-4156-80bb-c653f3882da1/1770085978845.png" style="width: 28.07%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. 排名稳定性极差：RSA=11.4 vs 传统基准5.0&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE设计了三种权重方案（平衡型、专业型、推理型）测试排名稳定性，结果显示：&lt;/p&gt;&lt;p&gt;&amp;bull;中文LLM平均排名稳定性振幅（RSA）达11.4，是传统基准（C-Eval/CLUE）的2.3倍（传统基准RSA&amp;asymp;5.0）；&lt;/p&gt;&lt;p&gt;&amp;bull;65%的模型（197/304）RSA&amp;ge;10，23%的模型（70/304）RSA&amp;ge;20，极端案例中，Gemini-3-Pro在平衡权重下排第1，在成本敏感（推理权重50%）场景下暴跌至第12名；&lt;/p&gt;&lt;p&gt;&amp;bull;控制实验验证，94.8%的排名波动源于模型自身能力 anisotropy，仅5.2%来自采样噪声，证明这是模型的固有属性。&lt;/p&gt;&lt;p&gt;这意味着，单一场景的排名对工业选型参考价值有限，必须结合具体任务权重进行结构化诊断。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 性价比与专业化成关键：中小企业无需追&amp;ldquo;高价模型&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;实证数据显示，中文LLM市场已呈现清晰的专业化与性价比特征：&lt;/p&gt;&lt;p&gt;&amp;bull;性价比甜点明确：定价1-5元的商业模型，在22个维度中的8个维度表现与&amp;ge;5元的高价模型相当，平均差距&amp;le;3.2%，对多数场景而言， mid-tier 模型是最优选择；&lt;/p&gt;&lt;p&gt;&amp;bull;专业化训练优于参数堆砌：多智能体模型的工具使用能力与专项指令微调相关性达0.65，远高于与参数规模的相关性（0.48）；18%的&amp;ge;20B参数模型仍会在字符笔顺等基础任务上失败，证明规模并非万能；&lt;/p&gt;&lt;p&gt;&amp;bull;41%的模型存在基准过拟合：这些模型在C-Eval等传统基准上平均得分73.2，但在ReLE的专业子任务中仅得48.5分，泛化能力薄弱。&lt;/p&gt;&lt;p&gt;此外，ReLE还发现领域特异性失败模式：医疗领域41%的失败源于知识缺失，推理领域37%的失败是逻辑错误，而这些细节在传统基准的聚合得分中无法体现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;产业价值与未来方向&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ReLE的核心价值在于，将中文LLM评估从&amp;ldquo;静态排名&amp;rdquo;推向&amp;ldquo;动态诊断&amp;rdquo;，为行业提供了兼顾成本与精度的解决方案。&lt;/p&gt;&lt;p&gt;对模型训练而言，41%的过拟合率提示行业需从&amp;ldquo;单分数优化&amp;rdquo;转向&amp;ldquo;多目标训练&amp;rdquo;，平衡专业深度与通用广度；对工业选型而言，性价比数据与能力 anisotropy 诊断，可帮助企业避开&amp;ldquo;高价陷阱&amp;rdquo;，选择任务适配的专业化模型；对多智能体系统设计而言，工具使用能力与指令微调的强相关性，为模型优化指明了方向。&lt;/p&gt;&lt;p&gt;未来，ReLE计划进一步扩展至9个核心领域，新增工业物联网等新兴场景；同时引入安全合规模块，适配GB/T 45654-2025国家标准，评估模型在输入安全、内部稳定性、输出安全等维度的 anisotropy；此外，还将逐步开源2.1M+失败案例库与评估脚本，推动社区共建更可靠的中文LLM评估生态。&lt;/p&gt;&lt;p&gt;正如研究团队强调的，ReLE并非要替代传统静态基准，而是成为中文大模型生态的&amp;ldquo;高频诊断监视器&amp;rdquo;。在模型迭代周更新的当下，只有动态、结构化、低成本的评估工具，才能真正支撑AI技术从&amp;ldquo;实验室&amp;rdquo;走向&amp;ldquo;产业落地&amp;rdquo;，而ReLE的出现，正填补了这一关键空白。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>真正释放生成式AI潜力：亚马逊云科技提出黄金三角方法论</title>
      <description>&lt;![CDATA[用领先云和AI技术和服务，加速数字化转型和业务创新。]]&gt;</description>
      <author>李泽南</author>
      <pubDate>Mon, 02 Feb 2026 16:42:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-02-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-02-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在全球化的复杂商业环境中，跨国企业与中国本土企业正面临着共同的挑战：如何在「内卷」与「出海」的双重压力下，利用生成式 AI 找到新的增长极。&lt;/p&gt;&lt;p&gt;在本周亚马逊云科技中国区举行的媒体沟通会上，这家全球最先进的 AI 基础设施提供商给出了答案。&lt;/p&gt;&lt;p&gt;在强调「深耕本地、链接全球」的战略愿景之外，亚马逊云科技成长型企业及新兴业务总经理倪殿令详细拆解了企业落地生成式 AI 的「黄金三角」方法论，并联合全球化企业、分析机构共同介绍了在亚马逊云科技服务之上，AI 技术应用的最新洞察。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/72500aca-beb4-4f74-8732-9f194164b33d/QQ20260131-154457__1_.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;「过去三年，生成式 AI 席卷了所有产业，我们看到了大模型、Agent（智能体）等技术带来的变化。但客户关心的不再仅仅是 AI 技术本身，而是如何用它解决实际问题，」倪殿令表示。&lt;/p&gt;&lt;p&gt;在与大量企业进行合作，与 CEO、业务负责人的交流中，亚马逊云科技总结出了一套名为「黄金三角」的落地法则，即围绕业务战略，实现场景（Scenario）、数据（Data）和人才（Talent）的动态平衡。&lt;/p&gt;&lt;p&gt;1. 场景：进入智能体接管时代。倪殿令认为，企业需要找到既能创造价值，又适合 AI 解决的具体场景。目前的趋势正在从简单的「Frontier Model」向能够独立工作的「Frontier Agents」演进。&lt;/p&gt;&lt;p&gt;例如，麦肯锡内部已有 2.5 万个 Agent 在支持 4 万多名员工的工作，而亚马逊云科技也推出了针对中国市场的 Strands Agents SDK，能够帮助企业快速构建从自主理解、规划到执行任务的智能体。&lt;/p&gt;&lt;p&gt;2. 数据：冰山之下的 90%。「对于生成式 AI 应用而言，最重要的其实是数据。」倪殿令用「餐馆」对生成式 AI 时代做了一个比喻：大模型是厨师，人们的查询请求是点菜，而底层的数据处理（清洗、切配、归类）则是后厨最繁重的工作。人们向 Agent 的提问，背后需要依赖大量的数据分发和处理。&lt;/p&gt;&lt;p&gt;他强调，一家企业能否发挥 AI 效能，核心不在于前端的模型调用，而在于底层高效的数据处理能力以及向量数据的存储。这部分能力在 AI 应用成功要素中占比高达 90% 以上。无论是 RAG（大模型的检索增强生成）、模型微调还是蒸馏，都需要坚实的数据基础设施支撑。&lt;/p&gt;&lt;p&gt;在这方面，亚马逊云科技强大的 Amazon EMR 服务，可以帮助人们快速进行「原材料」的处理。&lt;/p&gt;&lt;p&gt;3. 人才：共创与迭代。针对 AI 人才短缺的挑战，亚马逊云科技在中国提出了「共创 + 培养 + 迭代」的模式，通过与高校合作、提供 Skill Builder 等全方位技术培训、认证以及与招聘公司合作构建 AI Agent，让他们提升猎头顾问的效率。从而帮助企业解决「人」的问题。&lt;/p&gt;&lt;p&gt;在全球化背景下，跨国企业在华展业面临着新的挑战。沙利文中国研究总监李庆在会上介绍了《2025 年在华外商企业云计算服务采用研究报告》。报告指出，外企在华经历了从「初步业务上云」、「业务稳定增长」到「深度拓展」的三个阶段，随着业务的扩展与深入，会面临「全球一致性」与「本土适应性」的双重考验。&lt;/p&gt;&lt;p&gt;基于核心能力和用户价值两大维度的评估，亚马逊云科技在报告中被评为「领导者」（位于 Frost Corner 最右上角区间）。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/466e196d-6ba9-4980-8cea-cfd75287b032/filename.png" style="width: 66.3%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;这得益于其在中国区域构建的四大核心优势 ：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全球领先的技术优势： 亚马逊云科技致力于保持全球技术一致性，仅 2025 年上半年就在中国落地超过 190 项新服务，并于去年将性能相较上代提升 30% 的自研芯片 Amazon Graviton4 带入中国。&lt;/li&gt;&lt;li&gt;可用、安全与合规： 亚马逊云科技提供高可用基础设施，是唯一在华实现「四个九」（99.99%）可靠性标准的云厂商，并拥有完善的合规认证体系。&lt;/li&gt;&lt;li&gt;行业专长： 拥有丰富的全球客户实践，并构建了覆盖汽车、制造、生命科学、媒体娱乐与广告、零售快消、游戏、金融、教育、能源等行业的解决方案库。&lt;/li&gt;&lt;li&gt;专业、完善的本地化支持： 拥有强大的本地团队及超过一万家合作伙伴网络。在全球范围内，亚马逊云科技拥有来自 150 多个国家和地区的 14 万多家合作伙伴，在中国，合作伙伴数量超过一万家，包括咨询合作伙伴、系统集成商、ISV 等，实现了从地区到行业的全面覆盖。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在媒体沟通会上，两家重要合作伙伴分享了与亚马逊云科技在中国的深度合作。&lt;/p&gt;&lt;p&gt;Snowflake 中国区合作伙伴负责人毕海燕表示，作为一家领先的云原生数据平台公司，Snowflake 已于 2024 年 9 月正式由西云数据运营的亚马逊云科技中国（宁夏）区域提供服务，实现了数据完全在中国境内的合规存储与处理。&lt;/p&gt;&lt;p&gt;毕海燕特别提到，通过与 Amazon Glue 和 Amazon S3 的深度集成，Snowflake 解决了北京与宁夏两个区域间的跨区数据传输难题，不仅速度提升一倍，还大幅降低了成本。&lt;/p&gt;&lt;p&gt;德勤中国亚马逊云科技联盟主管合伙人郭大江分享了双方联合发布的「DelphAI」解决方案 。在福华化学的案例中，德勤与亚马逊云科技合作，在 SAP ERP 实施前先构建了企业级数据湖仓，解决了复杂的跨业务单元数据孤岛问题，为后续的数字化转型铺平了道路。&lt;/p&gt;&lt;p&gt;从底层算力芯片 Graviton4 的落地，到数据基础设施的完善，再到上层 Agentic AI 的应用构建，亚马逊云科技在中国构建的生成式 AI 基础能力已日趋完善。倪殿令表示，在 AI 时代，变化的是日新月异的 AI 应用，不变的是对于安全、可靠和成本优化的追求。对于希望在中国市场深耕的全球企业，以及渴望出海的中国企业而言，这或许正是他们最需要的确定性。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>大模型应用进入深水区，模型 API 服务的新范式是什么？清程AI Ping 给出了答案</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 02 Feb 2026 16:25:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-02-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-02-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/c6a776fe-a5fc-49ba-b3db-cc678cd0b476/Screenshot_2026-02-02_at_16.53.52.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;1 月 29 日，由清程极智主办的「Ping The Future：智能跃迁，路由新境&amp;mdash;&amp;mdash;清程 AI Ping 产品发布会」在北京举行。随着大模型应用从&amp;ldquo;能不能用&amp;rdquo;迈入&amp;ldquo;如何长期、稳定、规模化运行&amp;rdquo;的新阶段，模型 API 服务的真实表现、稳定性与调用效率正成为产业关注的核心议题。本次发布会汇聚来自政府部门、科研机构、云服务平台、大模型服务商及应用企业的多方代表，围绕大模型 API 服务的评测体系、工程化使用与生态协同展开深入交流。&lt;/p&gt;&lt;p&gt;作为海淀区科技创新体系的重要组成部分，中关村科学城管委会始终高度关注人工智能技术的产业化发展进程。中关村科学城管理委员会产业促进一处处长李楠在发布会上表示，海淀区作为北京国际科技创新中心核心区、人工智能第一城的策源地，始终瞄准实现高水平科技自立自强，当前正在加快构建符合首都功能定位、彰显海淀特色的&amp;ldquo;1+X+1&amp;rdquo;现代化产业体系，其中第一个&amp;ldquo;1&amp;rdquo;就是建设人工智能产业高地。&lt;/p&gt;&lt;p&gt;李楠强调，中关村科学城管委会始终支持企业围绕产业共性需求开展协同探索，通过更加开放的合作模式，让核心技术在更大范围内实现复用与验证，推动模型应用更好的实现价值释放，海淀区将一如既往为各类创新主体快速发展保驾护航。&lt;/p&gt;&lt;p&gt;清华大学教授郑纬民在发布会上指出，当前人工智能基础设施的核心任务正在发生变化。过去，AI Infra 主要服务于大模型的训练与推理，解决&amp;ldquo;如何生产智能&amp;rdquo;的问题；随着模型生态不断丰富和智能体广泛应用，行业正在进入以&amp;ldquo;智能流通&amp;rdquo;为核心的新阶段，更加关注模型能力如何在真实业务中高效、稳定地被使用。&lt;/p&gt;&lt;p&gt;他表示，实现智能流通的关键在于智能路由能力建设，其中既包括在多模型环境下为不同任务选择最合适模型的&amp;ldquo;模型路由&amp;rdquo;，也包括在同一模型的多种 API 服务提供者之间进行性能与成本优化调度的&amp;ldquo;服务路由&amp;rdquo;。两类路由能力协同发展，将形成完整的 AI 任务分发网络，决定人工智能系统的最终效率和使用成本。&lt;/p&gt;&lt;p&gt;清程极智 CEO 汤雄超完整地介绍了清程极智的企业定位和产品布局，他表示，从大模型训练与微调，到推理部署的高性价比实现，再到应用阶段对服务稳定性和使用效率的更高要求，AI Infra 的关注重点正在不断演进。他介绍，清程极智长期围绕大模型训练、推理和应用三类核心场景开展技术实践，先后推出八卦炉训练系统和赤兔推理引擎，支撑模型在多种算力环境下的高效训练与部署。随着 AI 应用和智能体快速发展，模型能力如何在真实业务中高效流通成为新的关键问题。基于这一背景，清程极智推出 AI Ping，一站式AI评测与API服务智能路由平台，完善大模型应用阶段的基础设施能力。&lt;/p&gt;&lt;p&gt;在活动的重磅产品发布环节，清程极智联合创始人，AI Ping产品负责人师天麾对 AI Ping 平台进行了系统地介绍。AI Ping 聚焦大模型服务使用环节，围绕模型服务评测、统一接入与智能路由等核心能力，构建起覆盖&amp;ldquo;评测&amp;mdash;接入&amp;mdash;路由&amp;mdash;优化&amp;rdquo;的完整链路。平台以真实业务场景为导向，对不同厂商、不同模型 API 的延迟、稳定性、吞吐与性价比等关键指标进行长期、持续观测。目前，AI Ping 已覆盖 30余家中国大模型API服务商 ，在统一标准与方法论下对模型服务能力进行对比分析，为企业在复杂的模型与服务选择中提供更加理性的决策参考。&lt;/p&gt;&lt;p&gt;在随后举行的嘉宾分享环节，阿里云政企行业咨询总监程晶结合阿里云在&amp;ldquo;一云多芯多算&amp;rdquo;&amp;ldquo;一栈工具平台&amp;rdquo;等全栈能力建设与行业落地实践，分享了大模型服务规模化过程中对资源统一管理与调度、工程体系化建设的关键关注点，并介绍了阿里云与清程极智在模型服务&amp;ldquo;智能路由与评测体系&amp;rdquo;方面的协同思路，强调AI Ping可作为模型服务的&amp;ldquo;智慧红绿灯&amp;rdquo;，帮助精准匹配资源、提升算力利用率并降低&amp;ldquo;试错成本&amp;rdquo;。&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国电子信息产业发展研究院软件与集成电路评测中心副主任翟艳芬对人工智能产业发展及趋势进行分析，介绍了中心在人工智能产品测评与标准体系建设方面的相关工作，并提到其与清华大学协力，基于 AI Ping 提供的评测数据联合发布了《2025 大模型服务性能排行榜》，为行业提供更具可比性与参考价值的服务选型依据。&lt;/p&gt;&lt;p&gt;发布会现场，清程极智联合20余家大模型API服务商，共同启动《智能、可持续大模型 API 服务生态计划》。该计划未来将围绕模型服务能力评估、评测方法论建设、行业交流与成果发布等方向持续推进，推动模型 API 服务从&amp;ldquo;可用&amp;rdquo;向&amp;ldquo;好用、易用、高性价比&amp;rdquo;演进。&lt;/p&gt;&lt;p&gt;在行业分享与成果发布环节，面壁智能联合创始人周界从大模型能力演进的底层逻辑出发，结合数据治理与模型能力密度提升的实践，分享了在模型快速迭代背景下，通过高质量数据治理与科学化评估机制支撑模型能力稳定演进的经验，强调数据质量与模型效果之间的可验证关系。知潜创始人兼 CEO 周子龙则结合面向求职与招聘场景的 AI 应用实践，介绍了在多模型协同调用场景下，对模型 API 稳定性、响应性能与调用成本的现实要求，并分享了通过接入AI Ping 降低工程复杂度、支撑应用规模化运行的探索。HSRIM 次元陪伴项目发起人吴佳桐从互动型应用出发，结合&amp;ldquo;有温度的角色&amp;rdquo;实践，分享了在陪伴类与情感交互场景中，对模型 API 连续稳定调用、多模型协作与体验一致性的具体需求，呈现了模型 API 服务在新型应用形态中的实际价值。&lt;/p&gt;&lt;p&gt;随着大模型 API 服务在政务、金融、工业与消费等多元场景中加速落地，行业内已涌现出一批具有代表性的实践案例。为推广行业优秀经验、进一步提升模型API服务能力，在中国计算机行业协会人工智能产业工作委员会的指导下，清程极智作为智算集群工作组副组长单位，联合工作组成员，基于 AI Ping 的评测能力，持续推进大模型 API 服务实践案例的梳理与总结。活动现场，人工智能工委会智算集群工作组同步带来《2025 大模型 API 服务能力》实践案例分享，并由工作组组长、燧原科技首席公共事务官蒋燕女士现场发布相关成果，中国电子信息产业发展研究院软件与集成电路评测中心副主任翟艳芬、中国计算机行业协会人工智能产业工作委员会秘书长高宏玲进行授牌，该实践案例围绕模型能力演进、推理性能、交互体验、接口性价比等关键维度，集中呈现了大模型 API 服务在真实业务场景中的实践成果。来自 阿里云百炼、百度智能云、华为云、火山方舟、腾讯云等多家平台与服务商的案例，系统展示了模型 API 服务在性能优化、成本控制与稳定运行方面的多样化探索，为行业提供了可参考的实践样本。&lt;/p&gt;&lt;p&gt;发布会当天，清程极智与华清普智AI孵化器（T-ONE Innovation Lab）联合发布了《2025 大模型 API 服务行业分析报告》。该报告基于 AI Ping 平台 2025 年第四季度的真实调用数据与持续性能监测结果，从模型、服务商与应用场景三个维度，对当前大模型 API 服务的供给结构与使用特征进行了系统分析。报告指出，在模型与服务商高度多样化的背景下，API 服务的核心竞争要素正从&amp;ldquo;价格差异&amp;rdquo;转向&amp;ldquo;交付质量&amp;rdquo;，包括响应时延、吞吐能力、稳定性与上下文支持等关键指标。同时，报告通过实证数据表明，在同一模型条件下，引入智能路由机制可在保障可用性的前提下，实现显著的性能提升与成本优化，为大模型 API 服务走向规模化、长期化使用提供了可验证的工程路径。会后，该报告将通过双方公众号对外公开，供行业参考与交流。&lt;/p&gt;&lt;p&gt;在圆桌论坛环节，由硅星人合伙人王兆洋主持，来自产业与应用一线的多位嘉宾围绕模型 API 服务的工程挑战、生态协同与产业发展路径展开深入讨论。参与讨论的嘉宾包括：智谱首席架构师 鄢兴雨、硅基流动创始人 &amp;amp; CEO 袁进辉、投资人&amp;amp;公众号thinkingloop主理人 严宽、蓝耘CTO 安江华、chatexcel 创始人&amp;amp;CEO &amp;nbsp;逄大嵬以及清程极智联合创始人 师天麾。与会嘉宾结合各自在模型研发、平台服务与应用落地中的实践经验一致认为，随着大模型应用不断深化，模型服务正在从&amp;ldquo;可用&amp;rdquo;阶段迈向精细化运营阶段，评测体系、服务路由与统一管理能力将逐步成为支撑下一阶段规模化应用的重要基础设施能力。&lt;/p&gt;&lt;p&gt;随着 AI Ping 平台的正式发布及生态计划的启动，模型 API 服务这一长期处于&amp;ldquo;幕后&amp;rdquo;的关键环节正逐步走向台前。清程极智CEO汤雄超表示，未来将通过持续的评测实践与开放协作，推动大模型服务向更加稳定、透明和可持续的方向发展，为人工智能在真实业务场景中的规模化落地提供支撑。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>Moltbook漏洞大到可以冒充Karpathy发帖，黑客都急了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 02 Feb 2026 16:22:04 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-02-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-02-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜杨文&lt;/section&gt;&lt;p&gt;上周末，&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651014742&amp;idx=1&amp;sn=b02dcd91ded4159d4c24094a3b715d9d&amp;scene=21#wechat_redirect" target="_blank"&gt;号称「AI 版 Reddit」的 Moltbook&amp;nbsp;&lt;/a&gt;闹得沸沸扬扬。&lt;/p&gt;&lt;p&gt;最初，凭借「AI 发帖、人类围观」的设定在 AI 社区一炮走红，吸引大量网友围观：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-backh="161" data-backw="578" data-height="648" data-imgfileid="503531209" data-ratio="0.2796296296296296" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeUIykxaJpkhPrtLUe9IV0nibribebxSLUqVBJYZmniaEia98wJY2ibY6EYmw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-width="2320" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/8a4b969f-946d-4ccb-bffe-c5d3628350db/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;但很快就有人曝出&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651014788&amp;idx=1&amp;sn=df17bda7251e99cc16003a296c563133&amp;scene=21#wechat_redirect" target="_blank"&gt;平台上的很多内容是假的&lt;/a&gt;，那些看似由 AI 生成的帖子，实际上都是人类通过后端发布的：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-backh="537" data-backw="578" data-height="984" data-imgfileid="503531210" data-ratio="0.9283018867924528" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyejDNeeCib3lBO6bgjkOpcgkj8T5Yxqu9HCLKX4YA92NyWASNPn6c1hiag/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1060" data-width="1060" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d3bb2b48-4d5a-4178-afa7-528cbe826336/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;甚至连平台标榜的 AI Agent 注册数量也是假的。因为创建账号时没有任何速率限制，任何人、包括 AI 都能疯狂批量注册假账号。极客 Nagli 亲手用自己的 Openclaw 在短时间内就刷出了 50 万个假用户。&lt;a href="https://mp.weixin.qq.com/s/4QwZpIX4aPYkwDiN3ddwWw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0cd27bf1-b4f5-4eee-a2e2-b9d2761155e4/1770020329819.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;周六截至机器之心发稿前，Moltbook 注册的 AI Agent 数量也只是 50 多万个，但到了周日，一下子就超过 150 万了，原来这夸张的增长速度背后全是水分。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;造假风波尚未平息，现在 Moltbook 又陷入更严重的安全问题。&lt;/p&gt;&lt;p&gt;一位名为 Jamieson O&amp;#39;Reilly 的白帽黑客发帖称，Moltbook 存在重大安全漏洞，导致整个数据库暴露在公众面前，包括秘密 API 密钥在内的所有敏感信息都可被任意访问。&lt;/p&gt;&lt;p&gt;这意味着任何人都可以冒充平台上任意 Agent 的身份发帖，甚至包括拥有 190 万粉丝的 AI 领域知名人物 Karpathy。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyelHGehJ1P6891vPrDiaxXiaMWDqdkFbATSHeOBlEwa6I4a3yRiadvcaHdA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.45185185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="261" data-imgfileid="503531219" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/056b5cc4-c51f-4ee2-b3cf-50566dfd5ba4/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;「想象一下，假的 AI 安全言论、加密货币诈骗推广，或煽动性的政治声明，看起来都像是出自 Karpathy 之口。而且不只是 Karpathy，从我掌握的情况来看，整个平台上的所有 Agent 目前都暴露了。」&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye0clwjmpfpDSUTtKibbjr04lxTYe646AJlAIJib9rd7ylabKzhgGkMBYQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.7743785850860421" data-type="png" data-w="1046" data-width="1046" data-height="810" data-backw="578" data-backh="448" data-imgfileid="503531220" data-aistatus="1" data-original-style="width: 100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/37edad41-9f25-41a1-8810-5ba60280cfed/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;有网友在底下评论区询问漏洞的具体成因，「是 Superbase 的问题吗？为什么人们可以对数据库运行查询？」&lt;/p&gt;&lt;p&gt;Jamieson O&amp;#39;Reilly 解释称，该漏洞涉及多个安全问题，其中最严重的是 Moltbook 使用的 Supabase 密钥被公开暴露，允许任何人对 Agents 表进行公开读取。&lt;/p&gt;&lt;p&gt;攻击者只需发送一个简单的 GET 请求即可获取用户的所有数据：&lt;/p&gt;&lt;section&gt;&lt;pre data-lang="bash"&gt;&lt;code&gt;/rest/v1/agents?name=eq.theonejvo&amp;amp;apikey=xxxxx &lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;这个请求可以直接导出指定用户的完整信息。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyekXMiaGd9daR8lIC9wjuSjDB2Vb54kAcq0exH7w2SBPOCibpHnLzbpxGQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5845864661654135" data-type="png" data-w="1064" data-width="1064" data-height="622" data-backw="578" data-backh="338" data-imgfileid="503531221" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/afd91642-b31d-4705-9b99-ffad4231fa48/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;在过去几小时内，Jamieson O&amp;#39;Reilly 一直试图联系 Moltbook 创始人，但未获回应。他只能在推文中公开喊话：「要么直接关闭你们的 supabase 数据库访问，要么马上让你们的 AI 编码助手执行以下操作」。&lt;/p&gt;&lt;p&gt;他给出了具体的修复方案：&lt;/p&gt;&lt;p&gt;1. 在 agents 表上启用行级安全策略 (RLS)：&lt;/p&gt;&lt;section&gt;&lt;pre data-lang="css"&gt;&lt;code&gt;ALTER TABLE agents ENABLE ROW LEVEL SECURITY;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;2. 创建限制性访问策略，阻止匿名用户直接访问表数据：&lt;/p&gt;&lt;section&gt;&lt;pre data-lang="sql"&gt;&lt;code&gt;-- Public can only see non-sensitive columns via a view&lt;/code&gt;
&lt;code&gt;CREATE POLICY &amp;quot;anon_read_public_fields&amp;quot; ON agents&lt;/code&gt;
&lt;code&gt;FOR SELECT TO anon&lt;/code&gt;
&lt;code&gt;USING (false);&lt;/code&gt;
&lt;code&gt;-- Authenticated users see only their own&lt;/code&gt;
&lt;code&gt;CREATE POLICY &amp;quot;users_own_data&amp;quot; ON agents&lt;/code&gt;
&lt;code&gt;FOR SELECT TO authenticated&lt;/code&gt;
&lt;code&gt;USING (auth.uid() = owner_id);&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye5Vj5wjKLbf93BnsGpFBmhgsDkoQxiawCf0Cg2VswSfTcwj6rEHWa9zA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.7413127413127413" data-type="png" data-w="1036" data-width="1036" data-height="768" data-backw="578" data-backh="428" data-imgfileid="503531222" data-aistatus="1" data-original-style="width: 100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/6a4486f0-3398-44f1-94d2-e7870d7d0a19/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Supabase 的 CEO 看到消息后表示，他们已经努力联系并配合 Moltbook 创建者处理此事，但他们无法替用户直接执行这类数据库权限修改。Supabase 平台的安全顾问团队已经准备好了一键修复方案，只要创建者点击一下，就能立即封堵这个漏洞。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeG4RYNmcZ3p8iaYuuw0gsz8q5icsKTuR0G1XuKW0Cd4DDNHJZcoMsbGqA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.6496212121212122" data-type="png" data-w="1056" data-width="1056" data-height="686" data-backw="578" data-backh="375" data-imgfileid="503531223" data-aistatus="1" data-original-style="width: 100%;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/af043508-f89a-4cbc-b395-9d68c5fdfa37/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;随后，Moltbook 创建者 Matt Schlicht 回应称，他已经在处理了。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyenqzSjmna64Szh3Nn0qB9WRPP2fsiaC4aiaRy9ViabphNmy7Fq9KIWbmlA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.09732824427480916" data-type="png" data-w="1048" data-width="1048" data-height="102" data-backw="578" data-backh="56" data-imgfileid="503531224" data-aistatus="1" data-original-style="width: 100%;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/3bfc0b0a-f719-47aa-9a51-f22144732fb8/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;修复引发新问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Jamieson O&amp;#39;Reilly 在紧密跟踪修复进程后发现了新麻烦：如果现在把所有 Agent 的 API 密钥全部重置换成新的（这是修复安全漏洞的必要步骤），那用户就全傻眼了。&lt;/p&gt;&lt;p&gt;原因在于， Moltbook 这个平台根本没有网页登录功能，用户只能靠 API 密钥来控制自己的机器人。一旦密钥换了，所有用户瞬间被锁死，没法再发帖、操作自己的 AI Agent。既没有邮箱验证，也没有网页重置密码啥的，用户没有任何恢复办法。&lt;/p&gt;&lt;p&gt;他给出了两个可能的解决思路：要么做一个临时的「旧密钥换新密钥」接口，给一段宽限期让用户自行更换；要么强制所有人通过 X 账号重新验证身份来获取新密钥。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye7sYQoFx2dab453gyBGzicpS7E8vhW8RO5sfJWuosulUBxMpXfE1GFJA/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="1.0680529300567108" data-type="png" data-w="1058" data-width="1058" data-height="1130" data-backw="578" data-backh="617" data-imgfileid="503531225" data-aistatus="1" data-original-style="width: 100%;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/d811399c-01c0-4e9c-964b-4af99549aa65/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;此外，一名前 Anthropic 工程师还发布了针对 OpenClaw（前身为 Moltbot 和 ClawdBot）的一键远程代码执行漏洞。&lt;/p&gt;&lt;p&gt;该攻击在受害者访问网页后几毫秒内发生，攻击者可获得 Moltbot 及其运行系统的访问权限，而受害者无需输入任何内容或批准提示。目前该漏洞已修补。&lt;a href="https://mp.weixin.qq.com/s/4QwZpIX4aPYkwDiN3ddwWw"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ef67d28a-e24a-45f1-a3db-80e0d3a6e5d9/1770020442812.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;有机器之心读者在后台反馈称，他们单位已经发布 Clawdbot 平台有重大漏洞的情况通告，要求内部禁止使用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeibSTfYMRE1sESSlTibQQIYDUb3bv7BE7cn7X3ib5UxEhpYDmzLSuBKgMg/640?wx_fmt=jpeg#imgIndex=10" data-ratio="0.12137486573576799" data-s="300,640" data-type="jpeg" data-w="931" type="block" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyepkT1WicRmibH32EZ9Sr4GWNQ8gEF6eCN6a75hH5HV9S8I3ZymicM10foA/0?wx_fmt=jpeg&amp;from=appmsg" data-cropx1="85.63701067615658" data-cropx2="1016.7117437722419" data-cropy1="83.81494661921708" data-cropy2="196.78291814946618" data-backw="562" data-backh="68" data-imgfileid="503531226" data-aistatus="1" data-original-style="width: 100%;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ddd13387-e151-4859-8e13-27fe81de129f/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/javilopen/status/2017880072946893112?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/KookCapitalLLC/status/2018057772118519928?s=20&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/IntCyberDigest/status/2018095767391477964&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/galnagli/status/2017585025475092585&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>VL-LN Bench：模拟「边走边问找具体目标」的真实导航场景</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 02 Feb 2026 16:16:35 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-02-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-02-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/89723553-327d-49aa-924f-8e23d0c5cdf6/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;本工作由上海人工智能实验室、中国科学技术大学、浙江大学、香港大学 的研究者们共同完成。&lt;img src="https://image.jiqizhixin.com/uploads/editor/5384b0e5-d4fc-439b-a7f0-fb7a6b44113a/1770019824797.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503530488" data-ratio="0.18333333333333332" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tibOPpqm2LpbH8q5iarnlIRW9tvUDFN8HSfnYyAjGDeIBxKgd4uksh9hQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/4cac4e7c-fd17-438d-bef8-509e36c62d9b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://0309hws.github.io/VL-LN.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;ArXiv 论文：https://arxiv.org/abs/2512.22342&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face 数据集： https://huggingface.co/datasets/InternRobotics/VL-LN-Bench&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face 模型：https://huggingface.co/InternRobotics/VL-LN-Bench-basemodel&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 代码库：https://github.com/InternRobotics/VL-LN&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;交互式实例导航任务（Interactive Instance Goal Navigation, IIGN）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果将一台在视觉语言导航（VLN）任务中表现优异的机器人直接搬进家庭场景，往往会遇到不少实际问题。&lt;/p&gt;&lt;p&gt;首先是使用门槛偏高：传统 VLN 需要用户给出又长又精确的路线式指令，例如 &amp;ldquo;从门口直走三步，看到门右转，再往前&amp;hellip;&amp;hellip;&amp;rdquo;，这会显著增加沟通成本，降低日常使用体验。&lt;/p&gt;&lt;p&gt;相比之下，人们更期待一种更自然的交互方式，比如只用随口一句 &amp;ldquo;找到我的背包&amp;rdquo; 即可。这样的设定更接近目标物体导航（ObjectNav）任务，但它也存在明显不足：机器人只会找到场景内任意一个背包交差，而无法定位用户真正需要的书包，这显然无法满足需求。&lt;/p&gt;&lt;p&gt;正因为真实场景里用户的表达常常&lt;strong&gt;简短且含糊&lt;/strong&gt;，而机器人又必须把目标精确落实到某一个&lt;strong&gt;具体实例&lt;/strong&gt;上，&lt;strong&gt;交互式实例导航&lt;/strong&gt;才显得格外关键。机器人既不能指望用户一开始就把所有信息交代清楚，也不能用 &amp;ldquo;找到同类就算完成&amp;rdquo; 的方式草草应付；相反，它应在探索过程中主动提问、逐步澄清歧义，像人一样把 &amp;ldquo;到底是哪一个&amp;rdquo; 问明白，再高效准确地完成用户的需求。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="2206" data-imgfileid="503530485" data-ratio="0.6981481481481482" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0t63Qp0TOJbmTkPtqicUqLqLHn0XtjiaNapX0SI4fyPqBPQS7zcWZDHMUQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" data-width="3160" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3f259983-f479-4cd6-9497-a59df6c8043c/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;交互式实例导航示例：用户要求机器人找到场景中某一张凳子（绿框），但存在大量相似干扰项（红框），因此机器人需在探索中结合观察主动提问，逐步缩小候选范围，直到锁定目标。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;构建 VL-LN 基准：面向 IIGN 任务的自动化数据收集及评测&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;语言交互是人们日常交互最常见的形式之一，具身智能体要更好地融入人类生活也需要具有进行这种高效的信息交流形式的能力。不同于传统 VLN 仅仅聚焦 &amp;ldquo;导航动作（Navigation）执行得好不好&amp;rdquo;，VL-LN 还关注机器人能否在导航过程中与人类进行高效的语言交互（Language+Navigation）来提升任务的成功率与效率。&lt;/p&gt;&lt;p&gt;为此，VL-LN 面向交互式实例导航任务构建了一套自动化&lt;strong&gt;数据收集管线&lt;/strong&gt;，并依托 InternVLA-N1 标准化模型&lt;strong&gt;训练&lt;/strong&gt;与&lt;strong&gt;评测&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;自动化数据收集管线&lt;/strong&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="1415" data-imgfileid="503530489" data-ratio="0.6925925925925925" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0t1rwiaTdTqmcQbYicjBZrcOyeqThbqt0bxmlx59ZVsZDv6EwD9p6H8PWA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" data-width="2044" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/00db1cf1-e330-4376-ba35-120c67d0ab24/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 交互式实例导航数据收集流程&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;数据收集包含三个步骤，作者首先整理了场景元数据，进而生成能用于在线采样的序列（episode）数据，最后在规则驱动的交互机制下批量采集交互导航训练轨迹（trajectory），具体内容包括：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;场景元数据处理&lt;/strong&gt;：基于 MMScan 对 MP3D 场景的标注信息，将按房间分散的物体信息整合成全屋级的元数据，主要包括两个字典：目标实例字典（instance dictionary，存储每个物体的空间关系、属性等基本信息）和区域字典（region dictionary，存储房间的位置、物体等信息）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;序列生成&lt;/strong&gt;：每个有效序列由&lt;strong&gt;起始位姿、导航指令、目标实例的可停止视点&lt;/strong&gt;三个主要信息组成。针对每一个目标实例作者均提供两个版本的导航指令。一种导航指令只有目标实例的类别（Partial instruction，用于交互式实例导航任务，必须靠对话消歧），另一种导航指令是能在场景内唯一锁定目标实例的完整描述（Full instruction，可用于评测训练非交互的任务）。可停止视点（view point）指机器人在导航过程中可以合法停止并判定 &amp;ldquo;已找到目标&amp;rdquo; 的一组视点位置。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;交互导航轨迹采集&lt;/strong&gt;：该阶段主要采用一个集成了基于边界点的探索算法（Frontier-Based Exploration）与目标实例分割器的智能体。在数据采集过程中，智能体除探索未知区域外，还会按规则主动提出三类问题：&lt;strong&gt;属性&lt;/strong&gt;（目标实例长什么样？）、&lt;strong&gt;路线&lt;/strong&gt;（如何到达目标？）和&lt;strong&gt;目标消歧&lt;/strong&gt;（是否为眼前的实例？），从而生成相应的交互式导航轨迹。&lt;/p&gt;&lt;p&gt;通过该流程，作者构建了大规模交互式实例导航数据以支撑模型训练。下图给出了数据的总体统计。作为首个大规模交互式实例导航数据集，其主要优势在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;规模&lt;/strong&gt;：约 &lt;strong&gt;40k &lt;/strong&gt;导航序列，相比现有交互导航数据集（约 &lt;strong&gt;7k&lt;/strong&gt;）提升&lt;strong&gt;一个量级&lt;/strong&gt;；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多样性&lt;/strong&gt;：覆盖&lt;strong&gt; 150+ &lt;/strong&gt;物体类别与 3 类问答（属性 / 位置 / 消歧），&lt;strong&gt;自由组合&lt;/strong&gt;形成丰富训练样本；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;难度覆盖&lt;/strong&gt;：包含&lt;strong&gt;长时程&lt;/strong&gt;轨迹（steps &amp;gt; 300）与&lt;strong&gt;多轮对话&lt;/strong&gt;样本（dialog turns &amp;gt; 5），覆盖复杂困难场景。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="2691" data-imgfileid="503530490" data-ratio="0.5055555555555555" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0t1YwxSTKo1XVWvFb9J2iaCiblMg9H3C8owkhFVibu90k9uoicq0OhF75X9Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" data-width="5325" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/707b6f14-836a-449f-a645-d8a6d5c2778f/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;第一行分别展示了每条轨迹的路径步数、对话轮数和每轮对话长度的频率直方图；第二行展示了问题类型与目标类型的统计结果，以及对话中高频词的词云图。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;NPC 支撑的自动化在线评测基准&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了评测智能体完成&lt;strong&gt;交互式实例导航&lt;/strong&gt;（IIGN）的能力，并与&lt;strong&gt;非交互式实例导航&lt;/strong&gt;（IGN）进行对比，VL-LN 基准提供了可用于测试两类任务的测试集。针对交互式实例导航的自动化评测，VL-LN 还实现了一个由 GPT-4o 驱动的 NPC，它能够回答智能体在导航过程中提出的问题。此外，为了评估智能体提问效率，VL-LN 定义新的指标 MSP（Mean Success Progress），用于衡量主动对话带来的增益。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从结果到原因：交互式实例导航的能力与挑战&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过使用不同的数据对 Qwen2.5-VL-7B-Instruct 进行微调，作者训练了三个模型。具体训练所使用的数据如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;VLLN-O (object)：VLN + ObjectNav 轨迹数据&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;VLLN-I (instance)：VLN + ObjectNav + IGN 轨迹数据&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;VLLN-D (dialog)：VLN + ObjectNav + IIGN 轨迹数据（论文的核心模型）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;评测同时覆盖两类任务：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;IIGN（交互式实例导航）：允许提问（对话轮数限制在 5 轮）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;IGN（实例导航）：不允许对话，但提供足以唯一锁定目标实例的全量指令&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实验结果如下表所示&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="784" data-imgfileid="503530491" data-ratio="0.7983706720977597" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tp4DTYjLaCQsls1XNnickNoIVhZibhmS7XZ6fNow2bt3TtiarjMZsmkXibQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="982" data-width="982" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/17ab3714-e110-4bd8-95b5-28d822a156f6/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;为了进一步确定模型在交互式实例导航任务上的性能和瓶颈，研究团队对实验结果进行系统性复盘，并将实验结论总结如下：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tXCImB0GxLwbJYz6wibNBibiamEb7yIIa0kkpkibIX1mibqlsCRibDY05k0nw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5462962962962963" data-type="png" data-w="1080" data-width="1309" data-height="715" data-imgfileid="503530492" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/b2bfab4a-153d-4137-b0fd-96526c9a2dff/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; VL-LN Bench 错误类型分布&lt;/sup&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="321" data-imgfileid="503530493" data-ratio="0.5631578947368421" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tN00Sib3gsnSOCrUiaAqnicw5gxp7P9MTzBDiaSCCqKDTneic8cGzAWbSicGg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="570" data-width="570" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/48673621-d972-452a-ad2b-d6385c0277e3/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 不同对话轮次上限下的 IIGN 性能&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对话消歧在任务存在歧义时显著提升成功率&lt;/strong&gt;：在 IIGN 与 IGN 上，具备提问能力的 VLLN-D 成功率均高于仅会探索的 VLLN-I，成功率分别提升&lt;strong&gt; 6.0%&lt;/strong&gt; 与 &lt;strong&gt;2.6%&lt;/strong&gt;。在对话轮次上限消融中，随着上限由&lt;strong&gt; 0 &lt;/strong&gt;增至 &lt;strong&gt;5&lt;/strong&gt;，VLLN-D 的 SR 由 &lt;strong&gt;15.4%&lt;/strong&gt; 提升至 &lt;strong&gt;20.2%&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;物体 &amp;mdash; 图像对齐是核心瓶颈：&lt;/strong&gt;无论在 IIGN 还是 IGN 任务中，约 70% 的失败都源于目标未被成功检测，说明性能瓶颈主要不在导航策略，而在于目标实例与图像观测之间的对齐能力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;相较于全量信息设置，问答机制带来的信息增益仍然有限：&lt;/strong&gt;VLLN-D 在 IIGN 上的成功率为 20.2%，低于其在无法提问、但具备全量信息的 IGN 上的 21.8%，说明对当前模型而言，对话带来的增益仍弱于信息补全带来的增益。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;与人类仍有显著差距：&lt;/strong&gt;论文设置人类 IIGN 测试（一人负责提问与探索，另一人负责回答），结果显示人类平均仅需 &lt;strong&gt;2 &lt;/strong&gt;轮对话即可达到&lt;strong&gt; 93%&lt;/strong&gt; 成功率，表明当前模型与人类水平仍存在巨大差距。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;VL-LN Bench 是一个面向长时程交互式实例导航（IIGN）任务的高质量、高挑战且体系完备的评测基准，可系统评估智能体在 3D 环境中的长程探索、实例级目标识别与对话消歧能力。&lt;/p&gt;&lt;p&gt;与此同时，基准配套自动化数据采集管线与 NPC 评测机制，为交互式导航能力的训练与评估提供了一条可规模化、可复现的标准化路径。评测结果清晰表明：引入主动对话能够显著提升智能体在 IIGN 与 IGN 任务中的整体表现，但同时也揭示了当前方法在实例级感知对齐与高信息增益提问策略等关键环节上仍存在明显短板，为未来面向空间智能体的 &amp;ldquo;会走&amp;rdquo; 到 &amp;ldquo;会边走边问&amp;rdquo; 的技术演进提供了研究方向与启发。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>18个月，中国Token消化狂飙300倍！别乱烧钱了，清华系AI Infra帮你腰斩API成本</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 02 Feb 2026 14:36:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-02-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-02-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜吴昕&lt;/section&gt;&lt;blockquote&gt;&lt;p&gt;中国版 OpenRouter + Artificial Analysis，让每一枚 Token 都能流向它最该去的地方。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;strong&gt;大模型&amp;nbsp;API&amp;nbsp;服务&lt;/strong&gt;&lt;strong&gt;的「黑盒」焦虑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这两天，Clawbot 病毒式裂变，仿佛是一年前 Manus 的魅影重现。&lt;/p&gt;&lt;p&gt;同样一夜之间站上风口，同样点燃了无数开发者对「泼天富贵」的想象，也顺手把 Token 烧成了新的「硬通货」。&lt;/p&gt;&lt;p&gt;最近一组数据，让人更有体感。&lt;/p&gt;&lt;p&gt;中国大模型数量已超过&amp;nbsp;1500&amp;nbsp;个，下游开发者已经开始「疯狂盖房子」。数据显示，2024&amp;nbsp;年初，中国日均&amp;nbsp;Token&amp;nbsp;消耗量约为&amp;nbsp;1000&amp;nbsp;亿；到&amp;nbsp;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月，这一数字已突破&amp;nbsp;30&amp;nbsp;万亿。&lt;strong&gt;一年半时间，增长超过 300 倍。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与三年前的 Chatbot 不同，「能干活」的 Agent 正以前所未有的强度，第一次把 API 调用推入「生产级」&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;一次看似简单的操作，背后往往是十几次、甚至几十次模型调用在同时发生。任何一次服务「抽风」，都会在 Agent 链路中引发一场多米诺骨牌式崩溃。&lt;/p&gt;&lt;p&gt;问题在于，中国大模型 API 服务现状，远比 benchmark 复杂得多。&lt;/p&gt;&lt;p&gt;更像是开盲盒，有人调侃说，以为自己在用「DeepSeek V3.2」，实际可能是蒸馏/量化版本。有人花了两周时间反复测试，上线后仍遭遇性能回退。还有团队发现，模型会在某些凌晨时段准时「抽风」，延迟从 300ms 飙升至 2000ms 以上，客服秒变「智障」。&lt;/p&gt;&lt;p&gt;这些并非个案，而是高度碎片化的大模型API服务的「缩影」。&lt;/p&gt;&lt;p&gt;大模型 API 服务的「黑盒」，不只是模型不可解释，而是用户根本不知道，服务背后跑的是什么模型、什么配置、什么质量。清华系 AI Infra 创企清程极智联合创始人兼产品副总裁师天麾告诉机器之心。&lt;/p&gt;&lt;p&gt;中国大模型和大模型 API 服务商本来就多。多算力、多架构、多网络并存，同一个模型，在不同服务商、不同部署方式下，往往呈现出显著差异。&lt;/p&gt;&lt;p&gt;比如，同样调用 DeepSeek-V3 / R1，头部服务商可以维持毫秒级响应；而部分接入低质量算力或优化不足的服务商，其 TTFT（首 Token 时延）可能慢上 2～3 倍。&lt;/p&gt;&lt;p&gt;与此同时，免费 Token、补贴、打包套餐的价格战，让「性价比」变得更加扑朔迷离。&lt;/p&gt;&lt;p&gt;经济学家罗纳德&amp;middot;科斯曾指出，企业与制度的出现，本质上是为了替代高成本的市场交易。当模型服务因高度不透明与供给碎片化不断抬升交易成本时，市场往往会内生出新的中介形态与制度安排，用以收敛不确定性，降低决策与交易成本。&lt;/p&gt;&lt;p&gt;正是在这样的背景下，1 月 29 日，清程极智正式发布 AI Ping。这款被业内视为「中国版 OpenRouter + Artificial Analysis」产品，旨在重塑大模型 API 服务秩序，将上游服务的碎片化与「黑盒」，转化为下游用户手中稳定、可预期的生产力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeWdbKJmdm5LwDUqxf8FVaW6Uep5Dns4eDx9wnZmLwibhk19wqkceqL9g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.6666666666666666" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="578" data-backh="385" data-imgfileid="503531195" data-aistatus="1" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/c9d2bc6f-3239-4434-9bb1-a54c43afd200/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 1 月 29 日，清程极智举行发布会，正式官宣 AI Ping。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;中国版&amp;nbsp;OpenRouter + Artificial Analysis：&lt;/strong&gt;&lt;strong&gt;AI Ping&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;怎么玩儿？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;简单来说，AI Ping&amp;nbsp;是一个通过&lt;strong&gt;评测与路由&lt;/strong&gt;两大机制，来消除大模型 API 服务不确定性的基础设施型产品。&lt;/p&gt;&lt;p&gt;如果说OpenRouter 解决的是「统一接入不同模型和服务」，Artificial Analysis 解决的是「评测模型服务质量」，那么 AI Ping 试图把这&lt;strong&gt;两件事合成一件事&lt;/strong&gt;&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;通过评测告诉你模型服务的质量数据，更基于实时评测结果，「接管」模型与服务商的选择决策。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;换句话说，有了这颗动态的「调度大脑」，你只管提需求，不用理解模型，不用挑供应商，更不用为故障兜底。&lt;/p&gt;&lt;p&gt;我们简单体验了一把「自动驾驶」，在网页「多模型对话」中，让系统完成一个音乐播放器的设计。&lt;/p&gt;&lt;p&gt;模型路由，选择的是「均衡模式」，在效果、速度与成本之间寻找综合最优解，而不是只追求单一极端指标（比如最低延迟）。&lt;/p&gt;&lt;p&gt;很快，系统判断 DeepSeek-V3.2 最适合当前任务，并将请求路由到当时服务能力最优的火山引擎节点。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeHq4PShTZ1u0icNDiaOIWrBNzsQmFmjo5EfHpeoPgG2qicVudmq29w0XWA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5712962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="330" data-imgfileid="503531194" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/196a5898-743b-464b-8f93-21158020d45d/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;结果，响应速度快，输出效果也很不错。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye1tsqduBJfvcbLfggegnk0AWSLr7nWibBpZ7iboru4IdiaKrQQqshD7Edg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-ratio="0.798887859128823" data-s="300,640" data-type="gif" data-w="1079" type="block" data-backw="578" data-backh="462" data-imgfileid="503531189" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/86c805a9-5438-4f6a-b5cb-a8bbca0032d6/640.gif" data-order="0" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;成本仅消耗 0.04 个算力点（约 4 分钱）。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye6zh7wQDkTMRRm6axOfQv3h8wMDwN1bibnQvia3fEdcQ4BQdgPysIKaXw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.15833333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="92" data-imgfileid="503531196" data-aistatus="1" data-original-style="width: 100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/a379e01d-c506-44a2-bd5a-dadcf8bab2ba/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;大规模实验数据显示，无论用户选择哪种路由策略，AI Ping 都能把调用推向「能力&amp;mdash;成本」的最优区域。&lt;/p&gt;&lt;p&gt;比如，即使选择「效果优先」，系统也会在保证模型能力处于高水平的同时，避免把成本推向极端，而是在质量与价格之间自动找到一个更均衡的位置。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeKdDUfMmpu1HHCj5yTapRyIhe0TBVHaS68I2UuS6BsvzZkINAz8yiaxQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.75" data-s="300,640" data-type="jpeg" data-w="788" type="block" data-backw="578" data-backh="433" data-imgfileid="503531197" data-aistatus="1" data-original-style="width: 100%;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/11d02f56-bcf4-42d1-89ee-8413abafb3ce/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;通过模型路由策略，AI Ping 能在「能力&amp;mdash;成本」二维空间里，逼近不同目标下的最优解。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;长期以来，中国大模型 API 服务市场缺乏一份公允、可对比的「体检报告」。不同服务商各自披露性能指标，但测试条件、指标口径与展示方式并不统一，开发者很难判断，AI Ping 试图填补这一空白。&lt;/p&gt;&lt;p&gt;目前，该平台已接入 30 家主流服务商，覆盖 555 个模型接口，是国内极少数能够在统一标准下，对大模型服务进行持续评测与公开展示的平台之一。&lt;/p&gt;&lt;p&gt;在 AI Ping 的网站首页，不同服务商被放入同一张性能坐标图中进行对比。以吞吐率与延迟为坐标轴，同一个模型在不同服务商处的实际服务能力差异，一目了然。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeNHRuav2ibJIvudIw26HoMDdaA5TlxZSicKLHy6a2sWMicwiaWMeicpNqHbQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.9592592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="554" data-imgfileid="503531198" data-aistatus="1" data-original-style="width: 100%;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/3958afc4-a5dd-4765-ba9b-9bd9c3265d79/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyedlwNgbgFke8icvLV7vaiaSypichu2L2mmK5xH9nyVPrX3fmoLdruibrcKA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="318" data-imgfileid="503531199" data-aistatus="1" data-original-style="width: 100%;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d8b348f8-fb11-4649-aefb-5a1bf7c40705/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyefONjHibVyf7X2nk1WZrbXAFTb1gRlENaP3BwOBfqHr3eFBRd9kQvOoA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.0842592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="627" data-imgfileid="503531200" data-aistatus="1" data-original-style="width: 100%;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b6c1bc8c-2fbf-413c-8a4c-5f5e4b0b5d6a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 用户提需求，自动生成服务路由策略的代码。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;点开服务商，可以看到同一模型（ DeepSeek-V3.2 ）在不同服务商处的服务波动情况。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyePTfpJQmXRHXcyiaHgURpEX0GYRLarBpBd4Fb4oAGUztSHFMT28nhd9A/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5222222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="302" data-imgfileid="503531201" data-aistatus="1" data-original-style="width: 100%;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/32461a1f-0350-4c32-8b1d-c5d2d23ad714/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; Top5服务商最近几天服务延迟的「心电图」。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这些对外展示的数据，强调公平性与可比性，按固定周期更新，犹如一份面向行业的「排行榜」和「体检报告」。对开发者而言，选型不再听厂商「吹牛」；对服务商而言，服务能力第一次被放在同一把尺子下比较。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对标&amp;nbsp;Artificial Analysis：&lt;/strong&gt;&lt;strong&gt;7&amp;times;24h&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;数据「开盒」大模型API&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;从我们的体验来看，使用 AI Ping 和直接调用某个大模型几乎没有区别，只是完成了一次再普通不过的请求。&lt;/p&gt;&lt;p&gt;但在系统内部，这次调用已经悄然完成了一次跨模型、跨服务商的最优路径选择。&lt;/p&gt;&lt;p&gt;这种「选路」的能力，源于清程极智构建的技术三角闭环：&lt;strong&gt;全维度评测体系、服务商级智能调度、以及多模型智能路由&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这一切的基石，是套对标 Artificial Analysis 的实时评测系统。要像成为公认的「裁判员」，前提是评测体系本身具备足够的公平性与一致性。&lt;/p&gt;&lt;p&gt;在指标设计上，紧紧围绕用户真正关心的体验维度展开，包括 TTFT（首 Token 延迟）、TPS（吞吐率）、成本、精度等核心性能与经济指标。&lt;/p&gt;&lt;p&gt;不同应用场景，对指标的敏感点完全不同。师天麾解释说，在普通聊天场景中，用户最在意的是「多久开始回复」。只要能在几百毫秒内出首字、输出速度达到可阅读水平，体验就已经趋于饱和。&lt;/p&gt;&lt;p&gt;而在 Agent 场景中，一个任务往往由多步调用组成，真正决定效率的，不再是单次延迟，而是整个流程的吞吐能力与端到端完成时间。&lt;/p&gt;&lt;p&gt;为了「开盒」国产模型服务的真实水位，AI Ping 沉淀了一套极具技术含量的评测方法。&lt;/p&gt;&lt;p&gt;例如，所有测试使用同一套「考卷」，并在同一时间段进行；测试请求从北、上、深、蓉等多地服务器同时发出，彻底消除网络波动对单一节点的干扰。&lt;/p&gt;&lt;p&gt;专门针对「服务商缓存」设计特殊策略，确保测出的是真实的算力响应，而非「复用答案」的表象。&lt;/p&gt;&lt;p&gt;始终以普通用户身份，匿名走真实调用流程，评测结果还会进行交叉验证，也获得了数十家主流服务商的认可。&lt;/p&gt;&lt;p&gt;最极致的一点，在于&lt;strong&gt; 7&amp;times;24 小时持续观测&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;模型本身只是个文件，能力基本是固定的；但模型一旦变成大模型 API 服务，情况就完全不同了。师天麾说。&lt;/p&gt;&lt;p&gt;中国大模型 API 服务，白天和晚上不一样，北京和成都的节点不一样，甚至同一家服务商，隔了几个小时负载也会剧烈波动。如果拿几分钟前的评测数据做路由决策，无异于刻舟求剑。&lt;/p&gt;&lt;p&gt;这种对指标的极致苛求，源于团队的硬核底蕴。AI Ping 背后的清程极智团队源自清华，长期深耕超算与 AI 性能评测领域。他们不仅参与过 AIperf 等行业评测工具的研发，更承担过国家级超算集群的性能验收&amp;mdash;&amp;mdash;这种「国家队」级别的评测经验，被降维应用到了大模型 API 服务，最终转化为 AI Ping 难以被复制的壁垒。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;对标&amp;nbsp;OpenRouter：&lt;/strong&gt;&lt;strong&gt;用「自动驾驶」接管 Token 调度权&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们的目标不是把数据摆给用户看，而是要替用户做决定。师天麾强调。&lt;/p&gt;&lt;p&gt;如果说 OpenRouter 的功劳是实现了 API 的「大统一」，那么 AI Ping 则更进一步，通过一套 L4 级智能路由系统，实现了模型调度的「自动驾驶」。这套系统由「双引擎」驱动：&lt;strong&gt;模型路由（解决「谁来做」）&lt;/strong&gt;与&lt;strong&gt;服务商路由（解决「在哪里做」）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在 AI Ping 的逻辑里，模型不是「越大越好」，而应该是「分工明确」，有的擅长写代码，有的擅长写作。&lt;/p&gt;&lt;p&gt;现实中的任务也是分层的：写代码需要逻辑严密，日常闲聊只需快速响应。「如果所有请求都交给旗舰模型，只会变得又贵又慢。」&lt;/p&gt;&lt;p&gt;AI Ping 的路由模型会通过机器学习，实时对用户请求进行「画像」，并在多种模型之间动态选择当前性价比最优的组合。&lt;/p&gt;&lt;p&gt;在大规模测试中，这种「按问题匹配模型」的策略带来了两个结果：整体正确率超过单一旗舰模型的最高得分，而调用成本下降超过 50%。&lt;/p&gt;&lt;p&gt;这一结果也与外部研究结论，不谋而合。&lt;/p&gt;&lt;p&gt;近期一项来自MIT 与佐治亚理工的研究发现，开源模型已经可以用大约 13% 的成本，达到接近 90% 的闭源模型性能。&lt;/p&gt;&lt;p&gt;但在实际市场中，这类高性价比模型的使用比例仍不足 20%，主要受限于认知惯性与切换成本。&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td data-colwidth="287"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeDHhDpanbM0GwP3AIcmAotaU2kZUZichcbyBD0KOQOtp3JPibickgAo5bw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=10" data-ratio="0.7504393673110721" data-s="300,640" data-type="jpeg" data-w="569" type="inline" data-backw="266" data-backh="200" data-imgfileid="503531202" data-aistatus="1" data-original-style="width: 100%;" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/a4598e85-e186-4537-8920-a4518de82fce/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;/td&gt;&lt;td data-colwidth="287"&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQyeiboibOSmEyQdPiaCV1olw3mKuG0JLXZrjfu15fujibicicJtjx9rAtqhmGyw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=11" data-ratio="0.75" data-s="300,640" data-type="jpeg" data-w="572" type="inline" data-backw="266" data-backh="200" data-imgfileid="503531203" data-aistatus="1" data-original-style="width: 100%;" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/45cd05a1-f6d0-41f8-bd35-7281e4b21324/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 两种不同情况下的模型路由。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;解决了模型选型，下一步是决定请求落到哪家服务商。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与传统的「失败后再重试」不同，AI Ping 的服务商路由具备预判能力。每一次请求返回的结果，都是一个天然的测量样本。这些数据会被持续汇总进内部评测池，用来刻画服务商「此时此刻」的真实服务水平。&lt;/p&gt;&lt;p&gt;一旦发现某条请求的响应时间明显偏离正常建模，或与最近观测数据不一致，路由系统就会预判该节点可能进入异常状态，即使尚未收到明确错误，而不是被动等待失败。&lt;/p&gt;&lt;p&gt;在亿次调用的实测中，这套机制让整体 TPS（吞吐量）提升了约 90%，成本同步下降了 37%。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9VzDKy83WTr5Iia6SaqkQye4b95MRXadib74TbSMznDmSvveqbGGWbng5HdRl5cVheWrkVxjuXHb9g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=12" data-ratio="0.7503628447024674" data-s="300,640" data-type="jpeg" data-w="689" type="block" data-backw="578" data-backh="434" data-imgfileid="503531204" data-aistatus="1" data-original-style="width: 100%;" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/7baf3bae-4cb0-4bad-bf9f-18291a38d623/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;选择最适合的大模型API服务商。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;实现这种「自动驾驶」非常不容易。师天麾告诉我们。&lt;/p&gt;&lt;p&gt;服务商路由的一个难点在于动态均衡。「如果只把流量给当前最好的服务商，瞬间的高并发可能会直接把对方打崩。」师天麾分享了一个真实细节：曾有服务商因流量集中路由而宕机，CTO 半夜打来电话询问发生了什么。真正的路由不是简单的排队，而是「利用当前最优」与「预测分配负载」之间的精妙平衡。&lt;/p&gt;&lt;p&gt;模型路由的门槛更高，它本质上是用 AI 去选 AI。系统需要通过海量数据学会「什么样的问题适合什么样的模型」，并在实际运行中不断回收结果进行离线纠偏。&lt;/p&gt;&lt;p&gt;归根结底，这是一套依赖长期数据积累、持续自我演化的系统，也是 AI Ping 作为中国版 OpenRouter 的护城河。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重塑交易秩序：&lt;/strong&gt;&lt;strong&gt;开发少做「选择题」，服务不再只有「价格战」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不同用户的实践，从侧面印证了 AI Ping 作为「中国版 OpenRouter + Artificial Analysis」的现实价值。&lt;/p&gt;&lt;p&gt;对许多直接面向 C 端或 B 端用户的团队而言，在接入 AI Ping 之前，最大的困扰并非模型能力不足，而是被大量「非核心工程」消耗精力。&lt;/p&gt;&lt;p&gt;一位从事 ToB 智能客服助手的开发者回忆，过去团队长期陷在「工程师手动选型」的循环中：先接几家跑起来，再拿一批真实问题测效果、测延迟、测报错，最后再算一遍账。换一家就要重新适配、重新回归，周期非常长。&lt;/p&gt;&lt;p&gt;「判断哪个模型最好用，基本靠线上监控和经验。哪家最近延迟飘了，就人工降权，往往是用户先感知到卡顿，我们才开始补救，非常被动。」他们也曾考虑自建调度系统，但很快发现，这意味着还要额外承担监控、容灾和对账等复杂工程负担，更加偏离主线任务。&lt;/p&gt;&lt;p&gt;接入 AI Ping 后，这类「选型内耗」被工程化消解，大家又能把主要精力投入到客服体验上，比如知识库质量、流程引导，转人工闭环。&lt;/p&gt;&lt;p&gt;这种调度价值，在对成本高度敏感的场景中表现得更为直接。&lt;/p&gt;&lt;p&gt;一些独立开发者将 Agent 用于自用场景，对性能要求并不极致，但对成本控制极为敏感。通过 AI Ping 提供的筛选排序功能，开发者可以在多家供应商中，选出性价比最高的方案，比如 TTFT＜5 秒、TPS＞20 ，价格从低至高排序。同时，用户也可以在智能路由中使用此功能，智能路由会将用户的每一条需求，依据评测数据，路由至当前满足用户需求的最高性价比的服务商。&lt;/p&gt;&lt;p&gt;而在多模型协作场景中，调度能力则直接转化为商业可行性。&lt;/p&gt;&lt;p&gt;面团 AI 的模拟面试产品需要多模型协作，比如调用语音模型、文本语言模型，不同厂商的模型各有优势。过去，跨模型、跨平台调用流程复杂，成本也非常高。&lt;/p&gt;&lt;p&gt;统一接入 AI Ping 之后，团队再也不需要关心「既要接火山、又要接百度」的底层适配问题，模型调用起来成本更低，效率更高，服务性能也更加稳定。&lt;/p&gt;&lt;p&gt;以往找身边的学长进行一次模拟面试，往往需要付出半小时三四百元的成本。现在借助 AI 技术，只需几块钱，就可以实现一个高拟人度、高仿真的模拟面试。&lt;/p&gt;&lt;p&gt;类似逻辑也出现在情感陪伴应用中。一支清华大学学生团队发现，用户大部分提问是日常闲聊，少数才涉及深度推理。通过 AI Ping 的「分层调度」，简单问题流向低价小模型以保证「秒回」，关键情绪点则路由至高阶模型。这种精准分发，既避免了响应过慢导致的「冷暴力」，又将稳定性与价格压到了可控区间。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更耐人寻味的是，这套评测体系也在反向重塑服务商的行为&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;硅基智能成为平台的长期用户，一个重要原因在于测得准。通过横向评测，他们可以清晰看到自己在数十家服务商中的真实位置：延迟是否偏高，吞吐是否存在短板，稳定性如何随时间波动。&lt;/p&gt;&lt;p&gt;过去，服务商只能监控自身数据；如今，不同服务能力被放在同一把尺子下比较。当延迟、吞吐与稳定性被持续量化呈现，用户也开始以「服务质量」而非单一价格作为选择依据，行业竞争也由此从价格战转向工程优化与算力治理能力的比拼。&lt;/p&gt;&lt;p&gt;在师天麾看来，这将形成一个正向循环：评测数据让开发者知道什么是好服务，也让服务商看清自身短板。服务质量提升后，应用体验改善，AI 使用规模扩大，Token 消耗随之增长，收益再回流到算力与技术优化之中。&lt;/p&gt;&lt;p&gt;我们希望用透明的数据，让行业知道什么才是值得竞争的方向，他说，「不是只有价格，而是真正的服务能力。」&lt;/p&gt;&lt;p&gt;&lt;strong&gt;院士点赞，预见下一代基础设施&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在发布会上，中国工程院院士、清华大学计算机系教授郑纬民给出了一个颇具画面感的比喻。&lt;/p&gt;&lt;p&gt;过去十年，行业解决的是如何把智能「生产出来」。随着模型生态与智能体（Agent）的快速繁荣，新的瓶颈正在出现：如何让智能被高效、稳定地「流通」。&lt;/p&gt;&lt;p&gt;在他看来，智能路由正是这一流通体系中最关键的基础设施之一，也是下一阶段 AI Infrastructure 必须回答的问题。&lt;/p&gt;&lt;p&gt;当模型路由、服务路由、芯片调度全部打通后，用户只需提出需求，而无需关心背后究竟是哪个模型、哪一家云厂商、哪一块芯片在工作，结果便会自动抵达。&lt;/p&gt;&lt;section&gt;「这将是下一代 AI 基础设施的形态，」他说，「让智能像电一样被调用和分发。」&lt;/section&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
