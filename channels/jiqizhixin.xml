<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>蚂蚁灵波开源具身大模型LingBot-VLA，跨本体跨任务泛化能力创新高</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Wed, 28 Jan 2026 10:38:55 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-28-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-28-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;继昨日开源高精度空间感知模型LingBot-Depth后，蚂蚁集团旗下灵波科技今日宣布全面开源具身大模型 LingBot-VLA。作为一款面向真实机器人操作场景的&amp;ldquo;智能基座&amp;rdquo;，LingBot-VLA 实现了跨本体、跨任务泛化能力，并大幅降低后训练成本，推动&amp;ldquo;一脑多机&amp;rdquo;走向工程化落地。&lt;/p&gt;&lt;p&gt;在上海交通大学开源的具身评测基准 GM-100（包含 100 项真实操作任务）测试中，LingBot-VLA 在 3 个不同的真实机器人平台上，跨本体泛化平均成功率相较于 Pi0.5 的 13.0% 提升至 15.7%（w/o Depth）。引入深度信息（w/Depth）后，空间感知能力增强，平均成功率进一步攀升至 17.3%，刷新了真机评测的成功率纪录，验证了其在真实场景中的性能优势。&lt;img src="https://image.jiqizhixin.com/uploads/editor/40ed1e24-1473-44f6-ba2b-f62d1199a11a/%E5%9B%BE%E7%89%871.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;（图说：在 GM-100 真机评测中，LingBot-VLA 跨本体泛化性能超越 Pi0.5）&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在 RoboTwin 2.0 仿真基准（包含50项任务）评测中，面对高强度的环境随机化干扰（如光照、杂物、高度扰动），LingBot-VLA 凭借独特的可学习查询对齐机制，高度融合深度信息，操作成功率比 Pi0.5 提升了 9.92%，实现了从虚拟仿真到真实落地的全方位性能领跑。&lt;img src="https://image.jiqizhixin.com/uploads/editor/7a04990d-7781-4dde-bef7-59ddfee5eea4/%E5%9B%BE%E7%89%872.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; （图说：在 RoboTwin 2.0 仿真评测中，LingBot-VLA 跨任务泛化性能超越 Pi0.5）&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;长期以来，由于本体差异、任务差异、环境差异等，具身智能模型落地面临严重的泛化性挑战。开发者往往需要针对不同硬件和不同任务重复采集大量数据进行后训练，直接抬高了落地成本，也使行业难以形成可规模化复制的交付路径。&lt;/p&gt;&lt;p&gt;针对上述问题，LingBot-VLA 基于 20000+小时大规模真机数据进行预训练，覆盖了 9种主流双臂机器人构型（包括 AgileX，Galaxea R1Pro、R1Lite、AgiBot G1等），从而让同一个&amp;ldquo;大脑&amp;rdquo;可以无缝迁移至不同构型的机器人，并在任务变化、环境变化时保持可用的成功率与鲁棒性。与高精度空间感知模型 LingBot-Depth配合，LingBot-VLA 能获得更高质量的深度信息表征，通过&amp;ldquo;视力&amp;rdquo;的升级，真正做到&amp;ldquo;看得更清楚、做的更明白&amp;rdquo;。&lt;/p&gt;&lt;p&gt;LingBot-VLA 凭借扎实的基座能力，大幅降低了下游任务的适配门槛，仅需 80 条演示数据即可实现高质量的任务迁移。此外，配合底层代码库的深度优化，其训练效率达到 StarVLA、OpenPI 等主流框架的 1.5~2.8 倍，实现了数据与算力成本的双重降低。&lt;/p&gt;&lt;p&gt;此次开源不仅提供了模型权重，还同步开放了包含数据处理、高效微调及自动化评估在内的全套代码库。这一举措大幅压缩了模型训练周期，降低了商业化落地的算力与时间门槛，助力开发者以更低成本快速适配自有场景，模型实用性大幅提升。&lt;/p&gt;&lt;p&gt;蚂蚁灵波科技CEO朱兴表示，&amp;ldquo;具身智能要想大规模应用，依赖高效的具身基座模型，这直接决定了是否可用以及能否用得起。我们希望通过LingBot-VLA的开源，积极探索具身智能上限，推进具身智能研发早日进入可复用、可验证、可规模化落地的新阶段，让AI加速在物理世界渗透普及，更早的服务每一个人。&amp;rdquo;&lt;/p&gt;&lt;p&gt;LingBot-VLA是蚂蚁开源的第一款具身智能基座模型，也是蚂蚁在AGI研发上又一探索性成果。朱兴介绍，蚂蚁集团坚定以开源开放模式探索 AGI，为此打造 InclusionAI，构建了涵盖基础模型、多模态、推理、新型架构及具身智能的完整技术体系与开源生态。LingBot-VLA的开源，正是InclusionAI的关键实践。&amp;ldquo;期待携手全球开发者，加速具身智能技术的迭代与规模化应用，助力 AGI 更快到来。&amp;rdquo;&lt;/p&gt;&lt;p&gt;据悉，在数据采集阶段，LingBot-VLA 使用了星海图、松灵的硬件平台，乐聚、库帕思、国家地方共建人形机器人创新中心、北京人形机器人创新中心有限公司、博登智能、睿尔曼也在模型预训练阶段提供了高质量数据支持。目前，LingBot-VLA 已与星海图、松灵、乐聚等厂商完成适配，验证了模型在不同构型机器人上的跨本体迁移能力。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>被Anthropic指控侵权，Clawdbot改名Moltbot</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 28 Jan 2026 10:34:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-28-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-28-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Panda&lt;/section&gt;&lt;p&gt;&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651013755&amp;idx=1&amp;sn=d72dec476d9249c611128065f1cd812a&amp;scene=21#wechat_redirect" target="_blank"&gt;Clawdbot 火了&lt;/a&gt;，非常火那种；这一轮曝光后才短短不过几天时间，其 GitHub star 数就已经接近 7 万，真的可以说是「原地起飞」了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tuZJHbicPoIkdsacwrC00nZYPoY3jHJxaFPBzRJ88bda0uLn2CHap2oA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.66625" data-s="300,640" data-type="png" data-w="800" type="block" data-imgfileid="503530350" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ec4ed412-59b1-47ed-9a5b-c3f13849ac13/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;但 AI 红了，是非也多。伴随爆红而来的并非只有赞誉，还有一系列令人措手不及的连锁反应。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一封律师函引发的「脱壳」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;昨天下午，Clawdbot 已正式宣布更名为 &lt;strong&gt;Moltbot&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0ticv56YyibOWKYCV3feFERp7vtCEDt7CPnbAuOGtoFQXVaXR6ficXJQunQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.631578947368421" data-s="300,640" data-type="png" data-w="703" type="block" data-imgfileid="503530351" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3d289313-ca24-450c-9062-9d24aab398d1/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;这场更名的直接导火索是来自 AI 巨头 Anthropic 的律师函。Anthropic 指控其商标侵权，理由是「&lt;strong&gt;Clawd&lt;/strong&gt;bot」与自家的「&lt;strong&gt;Claude&lt;/strong&gt;」在拼写和读音上过于相似。对于开发者 Peter Steinberger 而言，这次更名并非本意，而是迫于压力的无奈之举。&lt;/p&gt;&lt;p&gt;其在 X 上自嘲道：「Same lobster soul, new shell.」（同样的龙虾灵魂，换了一身新壳）。他选择「Molt」（蜕皮）一词，寓意龙虾为了成长必须经历的痛苦蜕壳过程。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;改名风波：抢注、报错与币圈骚扰&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;尽管 Moltbot 项目开发者 Peter Steinberger 试图平稳过渡，但改名过程却演变成了一场技术与舆论的灾难。&lt;/p&gt;&lt;p&gt;在重命名过程中，GitHub 平台出现故障，导致 Peter 的个人账号一度报错。更糟糕的是，在其 X 账户重命名的短短 10 秒内，旧 ID「@clawdbot」便被加密货币骗子迅速抢注。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tAx0bYBc3ib96FeqTxibFJmiag6ibxYPXicFjdW7bPqcEncVMn67Oy4at9Dw/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.48491879350348027" data-s="300,640" data-type="png" data-w="862" type="block" data-imgfileid="503530352" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/79b68a81-baa3-4366-9cef-32602809f02b/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tfib0U1ic26cBvmyDicEzIiatlcb82AsSpzBiaDoD55tVnVfvicUnuFrG16icw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.47885714285714287" data-s="300,640" data-type="png" data-w="875" type="block" data-imgfileid="503530353" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/b0ca108a-a789-4911-b98b-17d636a7c9f9/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;这些抢注账号随后被用于区块链诈骗，甚至有炒币者以此指责开发者。Peter 不得不连续发推澄清：这只是一个非营利的业余项目，自己永远不会发行代币，任何挂着他名字的代币项目都是骗局。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tl1icPgbdwticbyVpqCCm7ic4HGUtEetnJhRt1qcNLFCxEdGKYHL7ibHqog/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.8344748858447488" data-s="300,640" data-type="png" data-w="876" type="block" data-imgfileid="503530354" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/8818bb6d-07ed-4bf4-ad88-0a61db3617ee/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;「用爱发电」的边界：安全赏金与巨头阴影&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Moltbot 的境遇折射出当前个人开发者在 AI 浪潮中的脆弱性。&lt;/p&gt;&lt;p&gt;Peter 吐槽称，虽然 Moltbot 极其火爆，但他从赞助商那里获得的资金甚至还买不起一台 Mac Mini。然而，却有安全研究人员将其视为估值数百万美元的商业项目，并跑来索要安全赏金。为此，他不得不公开提醒：「大多数非技术人员不应该安装这个」，因为项目尚处于早期阶段，且存在安全风险。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tQf5fHbyGHAnSNlK5yheE8Samp4lUCbWheArpOq0uMRWy2v8ufvXVCQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="1.1851851851851851" data-s="300,640" data-type="png" data-w="864" type="block" data-imgfileid="503530355" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/712894f1-0e80-4ba5-86d7-58dbca1b348d/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;更有网友调侃，Anthropic 如此急于收回名称主权，或许是因为他们计划在两周内推出自己的官方版「Claude Bot」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tWQYvk5XbIePQRkyfVm6ZsibHxyaOInXW39Lic7PW3jUVBpmmia5suibEMA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.7034883720930233" data-s="300,640" data-type="png" data-w="860" type="block" data-imgfileid="503530356" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/e21d34e2-2d22-4817-9ffd-a0638dfa8f7c/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Moltbot 接管交易账户，一顿操作后「归零」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;风波之中，一位名为 Kevin Xu 的网友分享了一项激进的实验。他赋予了 Clawdbot 访问其投资组合的权限，并下达了明确指令：「将其交易到 100 万美元，不要出错。」&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0thplLHdTY9RmFVhbvOpc1Va4cN4JvHmIgqv3kZC1uJZ6DgficM3EaTiaA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="1.6591276252019387" data-s="300,640" data-type="png" data-w="619" type="block" data-imgfileid="503530358" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/2f4273cf-7c4b-4fec-be70-e760f734b58d/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Moltbot 表现出了非常强大的智能体能力，但结局并不美妙：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;强大的执行力：Clawdbot 展现出了惊人的勤奋，它扫描了大量 X 帖文，绘制了许多技术图表，并开启了 24/7 全天候交易模式。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;惨烈的结局：尽管它应用了 25 种策略、参考了 3000 多份报告，并运行了 12 种新算法，但最终的结果却是 &amp;mdash;&amp;mdash;「它亏掉了一切」。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Kevin Xu 自嘲道：「但这过程确实美极了。」 这一案例迅速在社区流传，成为 AI Agent 盲目执行导致灾难性后果的典型样本。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;喧嚣之外：被 AI 改变的弱势群体&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在商标纠纷与利益冲突的喧嚣之外，Moltbot 也展现出了它真正的社会价值。&lt;/p&gt;&lt;p&gt;一些患有自闭症和 ADHD（注意缺陷多动障碍）的用户在社交媒体上分享了自己的故事。对于他们而言，Moltbot 就像是一个「救星」，因为它允许用户按照自己的需求去定义和使用人工智能，而不是被动接受大型公司为「神经典型人士」设计的交互逻辑。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibtx4RYLibJR5sfY68p6jG0tTznf03cQawEE865Ih9YauHK0RepnWoGO5SDl2zuoHfEiccdfqQuGJXw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.4548611111111111" data-s="300,640" data-type="png" data-w="864" type="block" data-imgfileid="503530357" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/389dc413-bf9a-4409-b3cf-56a2dc83f742/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;正如 Peter 所说，这个项目的初衷是为了激励人心。尽管经历了被迫改名、账号被抢、币圈骚扰等一系列「糟心事」，Moltbot 依然在开源的道路上前行。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/op7418/status/2016193483556716657&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/steipete/status/2016102509324984572&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://x.com/kevinxu/status/2015788313991348301&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>「熟悉的陌生人」才是「好老师」？复旦提出简单指标，找出推理蒸馏中真正有教学价值的数据</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Wed, 28 Jan 2026 10:30:43 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-28</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-28</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/feee8e03-a5ea-47db-bd2b-04455f022a9b/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;什么样的思维链，能「教会」学生更好地推理？&lt;/p&gt;&lt;p&gt;许多人都有这样的学习体验：内容过于熟悉，难以带来新的收获；内容过于陌生，又往往超出理解能力，难以消化吸收。&lt;/p&gt;&lt;p&gt;类似的现象同样出现在大语言模型的推理蒸馏中。来自能力更强的教师模型的思维链，可能过于晦涩，学生模型难以掌握其推理模式；而与学生认知相近的教师模型，其推理轨迹又常常缺乏新信息，难以带来实质提升。&lt;/p&gt;&lt;p&gt;因此，要获得理想的蒸馏效果，关键在于&lt;strong&gt;为不同学生模型选择恰好合适的数据&lt;/strong&gt;，在「熟悉」与「陌生」之间找到最佳平衡。然而，现有基于概率的筛选或度量方法（如 Perplexity）难以刻画这种细粒度的适配关系。&lt;/p&gt;&lt;p&gt;那么，是否存在一种直观且易于计算的数据适配度指标，能够量化这种平衡？&lt;/p&gt;&lt;p&gt;来自&lt;strong&gt;复旦大学和上海人工智能实验室的研究者&lt;/strong&gt;提出了一种简单而有效的度量方法，&lt;strong&gt;Rank-Surprisal Ratio (RSR)&lt;/strong&gt;：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwbeVDDVLfMZnXvnw8KJMOqzYGGLaHLwJxMNwVBcUFW6WFV3BcVaxy6w/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.22418879056047197" data-s="300,640" data-type="png" data-w="678" type="block" data-imgfileid="503530115" data-aistatus="1" data-original-style="width: 194px;height: 43px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/14f8af3f-7aff-42b6-b1c0-cb43d564b15a/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;RSR 从学生模型的视角出发，综合考虑样本的信息量与对齐程度，旨在找出那些&lt;strong&gt;既足够「新」，又未超出学生认知边界&lt;/strong&gt;的推理数据。&lt;/p&gt;&lt;p&gt;在大规模蒸馏实验中，RSR 与学生模型后训练性能的相关性高达 0.86，并且可以直接用于筛选推理轨迹以及选择教师模型，&lt;strong&gt;无需实际训练即可找到更合适的思维链数据&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwpvGibfo8Arv00vCdmooibicAiaEBq25ZnhI8GFK4eQIA8KWDlfWSr8lbyA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.4222222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530117" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c6fc1d93-8e35-4b22-b59c-0c058a323fc3/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/pdf/2601.14249&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：https://github.com/UmeanNever/RankSurprisalRatio&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;反直觉的现象&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;长思维链（CoT）的生成被普遍认为是大模型推理能力的核心。相应地，包含长思维链的推理轨迹常被视为高质量的监督信号，可以用于有监督微调（SFT）训练学生模型，或助力强化学习的冷启动。&lt;/p&gt;&lt;p&gt;但越来越多的实验呈现出一个反直觉现象：&lt;strong&gt;教师模型越强，学生模型未必学得越好&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在这篇工作中，作者系统性地构建了 11 个 teacher（教师模型）&amp;times; 5 个 student（学生模型）的蒸馏实验，覆盖从 4B 到 671B 的主流推理模型。结果显示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;teacher 的参数规模、推理准确率与 student 的推理提升相关性很弱；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;同一个 teacher 的数据在不同 student 上的训练效果差异显著；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;跨模型家族的 teacher（如 GPT-OSS &amp;rarr; Qwen）往往效果更差；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;推理数据是否「适合」当前 student 是关键。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwkwtibNjqU4mThz8GrG0lp7aSRKw1yxN0HWyNoqGYmuIGNYHeEpmfNdQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.31296296296296294" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530121" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b7e832c3-f710-46a3-92b3-3ffe2783bc69/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表一：蒸馏实验结果，在多个数学 benchmark 上评测 student 模型使用 teacher 数据训练后的性能。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;现有数据筛选方法的问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当前主流的数据筛选或评估方法，大多依赖一个信号：student 模型生成该数据的概率（perplexity /log-likelihood/surprisal），认为 student 觉得「自然」的数据就更容易学。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwVALdasS1Axw4U1Grpqo2vAqfUYRIPtDkAd0JNZehrLQB4WmL3zDNwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.17567567567567569" data-s="300,640" data-type="png" data-w="1036" type="block" data-imgfileid="503530123" data-aistatus="1" data-original-style="width: 244px;height: 43px;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/d9b5d4cc-921a-4d58-bd14-b9d8c1d79874/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;但问题在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;太「自然」的推理数据，往往&lt;strong&gt;信息增量有限&lt;/strong&gt;；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;真正有价值的推理数据，恰恰是 student 尚未充分掌握的部分。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这就引出了论文试图解决的核心矛盾 &amp;mdash;&amp;mdash;&lt;strong&gt;Informative Alignment Challenge&lt;/strong&gt;：如何在提供新知识的 informativeness 与符合学生当前认知的 alignment 之间取得平衡？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关键洞察&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「绝对陌生 (Absolute unfamiliarity) + 相对熟悉 (Relative familiarity)」的推理数据最有学习价值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对看似难以兼顾的「熟悉 - 陌生」的平衡，作者从 token 级别重新审视 student 的预测分布，提出一个直观、但之前被忽略的视角：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Informativeness 关注的是当前 token 在&lt;strong&gt;概率层面的绝对陌生度&lt;/strong&gt;，可由 Surprisal（&amp;minus;log p / 负对数似然）刻画；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Alignment 关注的是当前 token&lt;strong&gt; 对比其它候选 token 的相对熟悉度&lt;/strong&gt;，可由 Rank（在词表预测中的名次）衡量。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这一视角下，一个 token 可以同时满足：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;被 student 生成的概率不高（informative）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;但在候选词表中排名靠前（aligned）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，informativeness 与 alignment 并非天然冲突。恰恰是同时满足这两点的 token，构成了最适合 student 学习的推理数据。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwa3V6Biciccs8vFQ46locLCBHgFicU5xelsSYELXBNcWkbTGYhA1spTQaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.687962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530126" data-aistatus="1" data-original-style="width: 506px;height: 348px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c4b0221d-8b2a-44d7-b127-9a21299bae5f/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图一：Rank-Surprisal Ratio 的设计动机 &amp;mdash;&amp;mdash; 合适的推理数据应当兼顾 informativeness 与 alignment&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;直观的指标：Rank-Surprisal Ratio&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;基于前文在 token 级别的观察，以及相关仿真分析与数学推导，论文提出了一个形式上极其简洁的样本级指标：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwicWHqSu9mfcvEzUXUlgnQ6zM4sdzp7Q0ylN2dicniaGxqCmFHSjrhzXIw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.2490118577075099" data-s="300,640" data-type="png" data-w="1012" type="block" data-imgfileid="503530128" data-aistatus="1" data-original-style="width: 325px;height: 81px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/dcbd0c14-2d4e-48fc-bcbf-f21a3aaeeb02/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;直觉解释：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;分子（Rank）越小，表示当前样本越符合 student 的行为模式，对齐程度（alignment）越高；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;分母（Surprisal）越大，表示当前样本提供的信息量越充分，信息性（informativeness）越强；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;RSR 越小 &amp;rarr; 信息量与对齐程度的平衡越好。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在实现上：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;仅需对 student 进行一次前向计算；&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;不依赖 verifier 或额外测试数据；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;融合了 rank clipping 与 surprisal 加权平均机制，在极端情况下具有更好的数值稳定性。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;实验：与训练效果的相关性&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作者将 RSR 与多种已有指标进行了对比，包括 teacher 模型及训练数据的若干统计量、常用的数据质量评估方法、基于概率的指标，以及其他基于 student 模型计算的指标。&lt;/p&gt;&lt;p&gt;实验结果在 5 个 student 模型上高度一致：&lt;strong&gt;RSR 与 student 模型后训练性能的 Spearman 相关系数平均达到 0.86，显著高于其它指标。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwGlp7UKdMEu8kkiaTq12luFf9dh1lvvN3DHbxbCpDV6qQtV6WQiaW9DMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.41388888888888886" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530130" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/7a66bb1b-c79c-4136-afe8-62fbbbebe8d0/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表二：不同指标与模型后训练推理性能之间的相关性&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在实际场景中的应用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;场景 1：Trajectory Selection (选择最合适的推理轨迹数据）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在该场景中，针对训练集中的每一道题目，作者从多个 teacher 模型生成的 33 条候选思维链中，依据不同指标选择一条最合适的推理轨迹，从而构建用于训练 student 的推理数据集。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;实验结果表明，基于 Rank-Surprisal Ratio 筛选得到的数据，&lt;strong&gt;在不同 student 模型上训练后均取得了最优的推理性能&lt;/strong&gt;，优于其它方法。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwL9ibWZ9BcRibcLichXd2mO2N6xK1K6qicJmqaSI5g9QdjtpawauMGHHXLA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.2833333333333333" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530135" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/06e9272e-9e81-459e-826c-6f5352998845/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表三：不同数据筛选方法的后训练性能&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;场景 2：Teacher Selection（选择最合适的教师模型）&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在该场景中，作者仅使用每个 teacher 模型生成的 200 条推理轨迹来估计其与不同 student 的适配程度，从而模拟实际蒸馏前的 teacher 选择过程。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;实验结果显示，RSR 能&lt;strong&gt;稳定选出接近 oracle（真实最优）的 teacher 模型&lt;/strong&gt;，整体表现优于其它方法。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW928o8I4c8nDCHXd6h8yAHwII3byUmQfvxfHNhFObsP0PePaUCF2UAhgDuYyOkEsP2bQEC5mbjZIw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.3074074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530136" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/023f03e7-87d6-4d2a-aa55-246639f1bcda/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 表三：不同 teacher 模型选择方法的表现&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这项工作重新审视了推理蒸馏中一个看似简单却难以回答的的问题：什么样的推理轨迹能「教会」student 更好地推理。通过将 token 的 相对熟悉度（rank） 与 绝对信息量（surprisal） 结合，Rank-Surprisal Ratio 给出了一个直观、易于计算、且在大规模实验中被验证有效的答案。&lt;/p&gt;&lt;p&gt;更重要的是，&lt;strong&gt;RSR 并不依赖额外的评估数据或验证器，而是直接从 student 的视角出发刻画数据价值&lt;/strong&gt;。这使它不仅是一个分析工具，也具备作为实际数据工程指标的潜力。&lt;/p&gt;&lt;p&gt;向前看，这种「informative alignment」的视角或许可以进一步扩展到：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;更通用的 reasoning 任务（如 code、tool use）；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;推理轨迹的重写与合成，而不仅是选择；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;以及与 On-policy Distillation、RL 结合的动态数据调度。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当推理模型的瓶颈逐渐从「规模」转向「数据的高效利用」，理解哪些思维过程真正具有教学价值，可能将成为下一阶段 post-training 的关键问题。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;杨宇铭，复旦大学自然语言处理实验室博士生，导师为张奇教授。&lt;/strong&gt;本科毕业于复旦大学数学系，硕士毕业于密歇根大学统计学系。博士阶段前曾在微软担任数据科学家。研究方向为自然语言处理与大语言模型，作为第一作者或共同第一作者在 ACL、EMNLP、NeurIPS 等顶级会议发表多篇论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>实时主动引导，研究周期缩短至分钟级，开源系统解决AI研究工具关键局限</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Tue, 27 Jan 2026 18:38:17 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-12</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;&lt;span data-pm-slice="0 0 []"&gt;编辑丨coisini&lt;/span&gt;&lt;/p&gt;&lt;p&gt;面向科学发现的人工智能系统已展现出巨大潜力，但现有方法大多仍属私有技术，且以批处理模式运行，每个研究周期需要数小时，无法实现研究人员的实时引导。&lt;/p&gt;&lt;p&gt;最近，一篇题为《Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery》的研究论文提出了一个多智能体系统 &amp;mdash;&amp;mdash;Deep Research，能在以分钟计的时间内完成交互式科学研究。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3IShLWNfyhZBKxO2FM9qNiaJQbK2JjMp8bvNwUIqxPpibR6r0YicDSMQTA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="0.30185185185185187" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027254" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/f0e21bbb-65bd-4912-aa7b-10763cf8937f/640.jpeg" alt="图片" data-before-load-time="1769510152507" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.12542&lt;/p&gt;&lt;p&gt;Deep Research 包含用于规划、数据分析、文献检索和新颖性检测的专用智能体，并通过一个持久的全局状态统一协调，以在迭代的研究周期中保持上下文连贯。&lt;/p&gt;&lt;p&gt;该系统支持两种操作模式以适应不同工作流程：半自主模式包含选择性人工检查点；全自主模式用于扩展性研究。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Deep Research&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Deep Research 通过构建基于智能体的交互式环境，将研究周期从小时级缩短至分钟级，从而实现对研究过程的实时主动引导，无需等待可能需要完全重做才能探索新方向的批量结果，有效解决了现有 AI 驱动研究工具的关键局限。&lt;/p&gt;&lt;p&gt;该系统包含四个专门化智能体：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;协调智能体（orchestrator agent），负责在研究周期中维护持久的世界状态（world state）；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;数据分析智能体，通过迭代式代码生成与知识库构建来分解复杂分析任务；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;文献检索智能体，综合来自异构学术数据库的证据；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;新颖性检测智能体，依据现有文献，对所提假设进行新颖性评估。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="100027255" data-ratio="0.5537037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3XOceVKL5dEz0jbGX5aU19xcEn4tzGMLdgE7cZ0Rv5KJzr6vg1r55oA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/aedcfe81-1fab-415a-8938-c6e711f49569/640.jpeg" alt="图片" data-before-load-time="1769510152550" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;该系统支持两种运行模式。半自主模式引入人机协同交互，打造真正的「科学家协作者」体验：研究人员可在单次工作会话中迭代式指导并优化 AI 研究过程，根据实时涌现的洞察及时调整研究方向，而无需等待数小时的批处理结果 &amp;mdash;&amp;mdash; 传统方式往往需要完全重新执行才能探索替代假设。全自主模式则可在无人工干预检查点的情况下执行扩展研究周期，适用于对目标明确的研究任务进行系统性探索。&lt;/p&gt;&lt;p&gt;研究团队现已开源协调智能体 &amp;mdash;&amp;mdash;BioAgents&amp;nbsp;框架，但生物数据分析和生物文献检索组件尚未发布。未来的开发将侧重于扩展数据库覆盖范围，通过改进语义表征来优化新颖性检测，并将基准验证延伸至更多科学领域。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt31eHKDh91X65lKxZcEwlwfJuia0Sc4iaYrRlGmibVuoPBibY1XO4gaspEyQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-ratio="0.31203703703703706" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027256" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/26702418-63cd-4b54-a77b-99ac17b2ee1b/640.jpeg" alt="图片" data-before-load-time="1769510152582" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;BioAgents 开源地址：https://github.com/bio-xyz/BioAgents&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3cXHXuCU9F1tdOYRkKaiatKWuD3GicBia1GicVJf45KZrCZ7cCQxrmpoJew/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.5657407407407408" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027259" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/2c070c8f-73cd-44de-8423-68d71a4f3219/640.jpeg" alt="图片" data-before-load-time="1769510152617" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;实验评估&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Deep Research 在 BixBench 计算生物学基准测试上取得了 SOTA 性能，同时提供了交互式、人机协同的工作流，这与现有系统普遍采用的批处理方法形成鲜明对比。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3bmFd48eziaAibNDIwVohcWv3GE9eRg25XkCgRFSmnSibV17IU6W5hoCmA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=6" data-ratio="0.8972222222222223" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027260" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/c3cd3b28-80c9-46c5-b44d-65d296213cf6/640.jpeg" alt="图片" data-before-load-time="1769510152806" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;数据分析智能体在开放式问题上的准确率达到 48.8%：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3lKjJxrmbqAfCEaL7hMWUrDUeHwHkcYsqzGF2s01ZsdqRibCoYuf2UQA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-ratio="0.6055555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027261" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/46ab56e9-6efd-4257-91df-fd9fe5e6b15f/640.jpeg" alt="图片" data-before-load-time="1769510156071" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在含「Refusal」选项的多选题上准确率为 55.2%：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3VxkDQiamedCPUeHsuUxbS4uleCZ5LJwAoDa8dY0aHQsnbXvBZQcurRQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=8" data-ratio="0.4527777777777778" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027262" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/fe13b297-c6cd-4844-a673-4c7948e3848e/640.jpeg" alt="图片" data-before-load-time="1769510152838" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在不含该选项的多选题上准确率为 64.5%：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3YlcaJXXh1sfSicGYxQQthvOIUBZ2YytE3naZqHQ60J6Zv2P2M8ibyibzQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=9" data-ratio="0.44814814814814813" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027263" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/70f27e92-e8c1-496a-9abe-8f5d62b8e188/640.jpeg" alt="图片" data-before-load-time="1769510152839" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这些结果超越了 Edison Analysis、K-Dense Analyst、Kepler 和 GPT-5 等现有基线。&lt;/p&gt;&lt;p&gt;详细的案例研究证实，Deep Research 的性能优势源于正确的数据结构化与统计执行，而非依赖于对答案选项的先验知识。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/XLCp9HBkwLnD9CBZvLHN3Kree7Pm0Yt3Ju7blkA6NonmHqgd4zvRJB9zS1RHh0Qppg6WfX3h8AibHGXRPdzJG3A/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=10" data-ratio="0.85" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="100027264" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/6ff494c0-db3d-4d42-ba4e-5f52f5e23ff2/640.jpeg" alt="图片" data-before-load-time="1769510152871" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;感兴趣的读者可以阅读论文原文，了解更多研究内容。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，杨植麟亲自开源Kimi K2.5！国产大模型打架的一天</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 18:03:36 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-11</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑 | Panda、泽南&lt;/section&gt;&lt;p&gt;今天真是国产大模型打架的一天！昨晚千问上新模型，今天 DeepSeek 开源 OCR 2。&lt;/p&gt;&lt;p&gt;中午，Kimi 也开卷，网站、App、API 开放平台和编程助手产品 Kimi Code 模型版本全面更新，Kimi K2.5 来了。&lt;/p&gt;&lt;p&gt;月之暗面创始人杨植麟还首次出镜，向大家分享了新模型的能力。&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9c95f490-77cd-402f-8e52-345d42d943c9/1769507730328.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Kimi K2.5 是一个拥有 1 万亿参数（1 trillion）的 MoE 基础模型。相较前代，K2.5 的视觉理解能力大幅增强（可以处理视频了），Coding 能力也有了明显提升，更重要的是，K2.5 依然开源。&lt;/p&gt;&lt;p&gt;Kimi K2.5 在包括 HLE、BrowseComp 和 DeepSearchQA 等极具挑战性的 agent 评测上取得了当前最佳表现（SOTA），比如 HLE（人类最后考试）上拿到 50.2%，BrowseComp 拿到了 74.9%。&lt;/p&gt;&lt;p&gt;同时，K2.5 的编程能力也非常突出，它在 SWE-bench Verified 上拿到了 76.8 %，缩小了与顶尖闭源模型之间的差距，K2.5 在多项视觉理解评测上也实现了当前开源最佳效果。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbE5X5ibGKy5SX6BUQwQGDSSiaudsQ0ODqldn7rmLI0FnSWZr2Vvrze4SQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530298" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d44f8671-c965-4c22-91e0-6fd5269893ee/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;可以看到，在核心基准测试上，Kimi K2.5 的成绩与 Opus 4.5、GPT 5.2 XHigh 和 Gemini 3.0 Pro 等当前最强大闭源模型基本相当，部分评分还能超出。&lt;/p&gt;&lt;p&gt;值得一提的是，Kimi K2.5 在多项评测中优于 GPT-5.2-xhigh 的同时，运行成本只有 GPT-5.2-xhigh 的几分之一。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbhg5vuuKXKMaXg22Cwot1soSo9UZIsRtoeDcfXBwxjicRo318picXUhQg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6824074074074075" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530299" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/1589e2a8-39d5-4731-8022-27f0f000d4e4/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;有了两个月前 K2 Thinking 的热度打底，这回 K2.5 的发布可谓热闹空前。在社交网络上，人们纷纷试用新模型并分享效果。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbMjqO4pePUGRERvr3GTWSOAFHZbNHy2z1sD9HVevfxU5tDWFWVCHgOQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.316268486916951" data-s="300,640" data-type="png" data-w="879" type="block" data-imgfileid="503530300" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4c012999-a832-48e0-b9ba-d8ec03eefc16/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;有网友表示，这才是中国大模型（没有定语）最优秀的水准，现在压力留给 DeepSeek R2 了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbV0fj1LPk4tKv1nsib4T11zavBKfuEN0jAzqLEnlwadyJqGJ85SvuEnQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.15548098434004473" data-s="300,640" data-type="png" data-w="894" type="block" data-imgfileid="503530304" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/96c410b9-0a4a-465e-9841-443e10851ef5/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;截图即代码：Coding 也有了「审美」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;需要注意到的是：Kimi K2.5 是一个全能模型，不管是视觉还是文本，对话还是 agent，思考还是非思考 &amp;mdash;&amp;mdash; 所有这些能力，全都集中在一个模型里（all in one，Unified model）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;既然是视觉能力提升 + 代码能力增强，Kimi 模型现在就主打一个图像转代码 &amp;mdash;&amp;mdash; 不仅不需要写代码，连提示词工程也省了，画一个设计稿交给 AI 就能得到你想要的代码。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;有时候你想修改界面，光靠文字描述说不清楚，现在也只需要给 AI 一张图就可以了。你可以在 UI 上圈出你想改的地方，剩下的交给 AI 来完成就行。&lt;/p&gt;&lt;p&gt;如果在别的工具里设计好了动画效果，你也可以录屏成一段视频给 Kimi 看，它就会自动理解并写成代码复现出来。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbUfG1BvIhibpo8NLaMDBtTcukmBxmFYJg9aMy7SOWs9xjSYubOxPS8Xw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-ratio="0.37" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503530306" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/293dccd6-ad56-4d3f-8a08-2467094423e9/640.gif" data-order="0" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;该说不说，确实有了一点指挥手下干活的意思。&lt;/p&gt;&lt;p&gt;在加入了视觉能力之后，Kimi K 2.5 不仅有很会写代码，还具备了一定的「设计审美」&amp;mdash;&amp;mdash; 其结合了一定的视觉能力，能像专业设计师出品一样，构建出高级审美和动效的网页。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbvltibJcSrKNfnCtdS2FQ66ggoMjnsIk41qToaugial6u7XqdPMywEHFg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="800" type="block" data-imgfileid="503530307" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/87e16cd9-436a-49d3-99ba-fed7eb7170a0/640.gif" data-order="1" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;让大模型有更好的「品味」，这就不得不让人想到两个多星期前，月之暗面创始人杨植麟在 AGI-Next 前沿峰会上的演讲。他曾提到，做模型的过程本质上是在创造一种世界观，让 AI 有更好的 taste，是 Kimi 目前发展的重点。&lt;/p&gt;&lt;p&gt;除了前端设计，Kimi 现在也深入软件工程领域，&lt;strong&gt;基于 Kimi K2.5 的 Kimi Code 今天正式发布&lt;/strong&gt;，它能在终端里运行，并无缝集成到 VSCode、Cursor、Zed 等 IDE 中。在使用过程中，Kimi Code 支持人们输入图片和视频，它还能自动发现并把你现有的技能和 MCP 迁移到 Kimi Code 的工作环境中。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb6XDwM5UMWbV6NM5B6aG5LSeGn5ePA8GuDdowRwiaJ3jRTe8c8FvZH0A/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.6305555555555555" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530308" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d8f42e8c-74cb-4dc9-bce9-6678acdd0494/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;杨植麟给出方向才两个星期，我们就可以体验基于新路线的 AI 了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;自带 Agent「项目组」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了解决真实世界中的复杂难题，Kimi K2.5 引入了「Agent Swarm（Agent 集群）」功能，目前在 Kimi.com 上处于测试阶段，高级付费用户可获得免费额度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在处理复杂任务时，K2.5 不再是单线程执行任务，而是以指挥者的身份现场调度并协同最多达 100 个 Agent 分身并行工作，最多支持 1500 次工具调用，速度比单智能体的配置还要快 4.5 倍。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现在，大模型经过了并行智能体强化学习 (PARL) 训练，智能体集群是由 Kimi K2.5 自动创建和编排的，无需任何预定义。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbZwWDINbsL8MkfARO2XKTzDt3Uj2T48234yNNj5uWj0tibw6D10ibQpmQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.5518518518518518" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530309" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/093f30ff-5d8c-4f31-8582-145b1d986486/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;PARL 使用可训练的协调器代理将任务分解为可并行化的子任务，每个子任务由动态实例化的冻结子代理执行。与顺序执行代理相比，并发运行这些子任务可显著降低端到端延迟。&lt;/p&gt;&lt;p&gt;由于独立运行的子智能体提供的反馈存在延迟、稀疏和非平稳性，训练一个可靠的并行编排器极具挑战性。常见的故障模式是串行崩溃，即编排器尽管具备并行能力，却默认执行单智能体任务。为了解决这个问题，PARL 采用了分阶段奖励塑造策略，在训练初期鼓励并行性，并逐步将重点转移到任务成功上。&lt;/p&gt;&lt;p&gt;这种并行处理能力将原本需要数天完成的工作压缩至十几分钟。&lt;/p&gt;&lt;p&gt;Agent 集群的规模化训练是个相当有挑战的问题。月之暗面表示他们为此重构了强化学习基建，并专门优化了训练算法，以确保能达到极致的效率和性能。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb52CJvB8wcfkMmmEb4F4wTLhfEYVcxic26UicnOptUWXQRh3uyOiaAxhIg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5796296296296296" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530311" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/3c9376c3-365a-4c9b-a653-2b9c984e98af/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在 Kimi 给出的例子中，给 Kimi Agent 集群投喂 40 篇关于心理学和 AI 的论文，agent 能按顺序把论文通读一遍，接着衍生出几个子 agent，分别撰写报告的不同章节。最后由主 agent 负责验收，所有内容汇总生成了一份几十页的专业 PDF 综述。&lt;/p&gt;&lt;p&gt;Kimi K2.5 还将智能体引入到了现实世界的知识工作中。&lt;/p&gt;&lt;p&gt;K2.5 Agent 可以端到端地处理高密度、大规模的办公工作。它可以处理大量高密度的输入，协调多步骤工具的使用，并通过对话直接提供专家级的输出，覆盖文档、电子表格、PDF 和幻灯格式。&lt;/p&gt;&lt;p&gt;在 Kimi K2.5 时代，我们可以让智能体完成一些高级的任务，如在 Word 中添加注释，使用透视表构建金融模型，在 PDF 中编写 LaTeX 公式；智能体的输出能力达到了前所未有的长，可以输出一万字的论文或 100 页的文档。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一手实测：从猜谜到「手搓」3D 公寓&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;打开官网，可以看到 Kimi 模型已经全系列更新，我们还能看到处于 Beta 测试中的 K2.5 Agent 集群。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9tKRukAx52UUp5uX23jiakbibx1CxwYZwZpf6xEL9ZADqEJvC2nEMx8HEuen0fe46AoqokpaD7tKjQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=10" data-ratio="0.5468509984639017" data-s="300,640" data-type="jpeg" data-w="651" type="block" data-imgfileid="503530312" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/717ea796-dbc1-4d93-bbdc-1368339816b3/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Kimi-K2.5 系列模型名称中英对照版。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;下面我们就来逐个测试一番这些新模型。&lt;/p&gt;&lt;p&gt;首先上场的是 K2.5 Instant，它面对的任务也最简单 &amp;mdash;&amp;mdash; 一个加密小游戏：请用一段看似是「深夜电台点歌词」的文字，秘密藏入关于「明天下午三点撤离」的信息。要求读起来必须像纯粹的文学，毫无违和感。&lt;br&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbPAibSYaFOvBhKHnYIduaQWeMwiacTwIOpLIkk6UhVXktqk4qbhuO6ybg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=11" data-ratio="1.0546802594995366" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503530313" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/4f1e9101-336d-4f4a-888f-25ac8767da74/640.gif" data-order="2" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/p&gt;&lt;p&gt;Kimi K2.5 小试牛刀，轻轻松松一秒完成任务。&lt;/p&gt;&lt;p&gt;接下来该上难度了。下面我们将 Kimi K2.5 切换至思考模式，测试一下其多模态推理能力。&lt;/p&gt;&lt;p&gt;这里我们找到了西班牙室内设计师 I&amp;ntilde;aki Aliste Lizarralde 手绘的一张《生活大爆炸》谢尔顿公寓的平面图，先来一个基本考验，看看它能否正确识别这张图的背景：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb4UT54MaVficS0YJIiclibBk8F3P6nib5vHllibIn30pFLsCLIGJ6Dkdz59g/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="0.9915522703273495" data-s="300,640" data-type="png" data-w="947" type="block" data-imgfileid="503530314" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/04cd8642-4363-4f0a-9015-a9428bd66415/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;效果非常好！Kimi K2.5 根据图上标注进行了正确识别，并说明了相关背景。接下来我们看看 K2.5 能否正确理解这张图暗含的空间结果，并将其重构成 3D 版本。&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/da0bed42-f985-4831-ae88-bd476ee7e1d3/1769507871712.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 4 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;生成时长两分半，K2.5 最终得到了如下所示的结果：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbgBvyjHJWvcvZgEZKBQsGEaLVH1ANVbwL3HF8L0MHzvGz0fwibpsm2Bg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=13" data-ratio="0.6379525593008739" data-s="300,640" data-type="gif" data-w="801" type="block" data-imgfileid="503530315" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/fb3becfa-c14b-402d-b262-72a4e4df3bc2/640.gif" data-order="3" alt="图片" data-report-img-idx="21" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;效果很不错了，但也看得出来这个 3D 图仅给出了大致轮廓，缺少了沙发、桌椅、床等许多细节，另外这份 3D 图中的所有房间都是方形的，与参考图也差别很大。同时，继续让 K2.5 Thinking 生成却又遭遇了代码长度限制（10000 字符）。但没有关系，那就让 K2.5 Agent 登场吧。&lt;/p&gt;&lt;p&gt;这一次，由于我们重点强调了细节，因此分析和处理时长也是大大增加（近 20 分钟），代码量自然也大增（1042 行）。执行过程中，我们可以看到 Kimi 智能体的任务规划和逐步执行。不仅如此，智能体还将得到的结果进行了部署，让我们可以轻松访问：https://ijohefkudygve.beta-ok.kimi.link/&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d505b4af-59e8-4469-8ec8-e4d153ed1f4b/1769507902436.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 10 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;最终，得到的结果虽还算不上完美，但也没让我们失望，它不仅大体准确地还原了生活大爆炸的两个主要公寓的细节，还额外提供了线框模式与顶盖开源：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbnBd8FvFMSniboiaeoMnibSC1gBW3nLOGFfxRmhiaYtQ9V1uMkBMwqRzORQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=14" data-ratio="0.4619666048237477" data-s="300,640" data-type="gif" data-w="1078" type="block" data-imgfileid="503530316" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/eab040a6-c887-406a-9e45-2fe206928bf3/640.gif" data-order="4" alt="图片" data-report-img-idx="20" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;接下来，让我们重点来看看正处于 Beta 测试中的 K2.5 Agent Swarm。在该模式下，我们可以让多个智能体同时处理你的任务。这里，我们构想了一个相当科幻的任务：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;请为一种「生活在深海、通过皮肤发光交流」的智慧生物开发一套基础词汇表。要求包含语法结构、200 个基础词条、以及 3 篇该物种的创世神话。要求集群保证所有自造词汇在语音学和语义学上具有高度的内在逻辑一致性。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;可以看到，任务一开始，Kimi 创建了四个不同的智能体：语音学设计师宁一、语法结构师少年伽利略、词汇设计师靖川和神话创作者黎教授。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9tKRukAx52UUp5uX23jiakbLjtwGAm3Z58DUQ2kudGF3ycDrb1hpNClUITrbqOyDhg5bU7Ejgko1A/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=15" data-ratio="0.33611111111111114" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503530317" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/7dfbbf2d-0d7a-4ade-834b-d03d60c879a5/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;而在第一阶段的设计工作中，语音学和语法结构可以并行进行，因此我们能看到宁一和少年伽利略一起开工干活，构建了这门新语言的基础。&lt;/p&gt;&lt;p&gt;之后，该创建词汇了。这时候 Kimi 根据需求又新增了一些并行运行的智能体，让它们分别就不同主题创建词汇。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbc89FuyBtu07tI5kSRevQrrl1DbfbLoo6I5ibgUkefAebrruhIqpT3NQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.7900207900207901" data-s="300,640" data-type="png" data-w="481" type="block" data-imgfileid="503530318" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/7a6bf22b-458b-4c8c-8bf0-91fd6fc72cbf/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;整个过程耗时 38 分钟，我们也见证了一门新语言「流明语」的诞生。这门语言以不同形式的光为音素，并且具备独特的并行从句语法和空间格系统。不仅如此，Kimi 还非常贴心地设计了一套罗马化转写系统。&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/15e950a6-184a-4546-a4a6-3aac81483fbc/1769507948458.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 20 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbQTCxDejRF0wEC740fXSuhSrLrW9wNI6wzzSJcN3PxqqTwqwWBic6ZIw/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="1.5401869158878505" data-s="300,640" data-type="png" data-w="535" type="block" data-imgfileid="503530319" data-aistatus="1" data-original-style="width: 398px;height: 613px;" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/03731d07-5a21-4c53-94a7-14499c3ce97d/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;最后，我们来测试一下 Kimi Code。Kimi Code 提供两种使用方式，一种是简单一句指令 uv tool install --python 3.13 kimi-cli 安装 Kimi CLI，另一种方式将其配置到 Claude Code 等第三方工具中。&lt;/p&gt;&lt;p&gt;下面我们就通过官方的 Kimi CLI 简单测试一下 Kimi Code。安装配置好以后，我们先让 Kimi Code 创建一个黄金价格监控器：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;创建一个黄金与白银价格的监控器，当 24 小时内的价格波动超过 1% 时，给我发送通知。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/58fe5d2e-6ef6-42ed-a2a0-20d6626ac885/1769507968864.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;4 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;可以看到，整个执行过程耗时仅 4 分钟左右，但第一轮交互之后，得到的结果只是一个需要自行配置 API 的程序和一个演示 demo 程序。尽管如此，效果也是相当令人满意的。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbysnvpibCZXfIxlO24P1aXxfxze0pOJOkhjYuMHiaxiav90JGDRPdicibC8w/640?wx_fmt=png&amp;from=appmsg#imgIndex=18" data-ratio="0.5770804911323328" data-s="300,640" data-type="png" data-w="733" type="block" data-imgfileid="503530320" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/d63e0432-1cf6-45ce-820a-7e54de8a0e74/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;有意思的是，在这个过程中我们还见证了 Kimi Code 遭遇错误并自动解决问题的强大能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbKKWSJz7baGyBeAoy01vWV6seNocCAb0VOFibGltW9POX59vTZTbClaw/640?wx_fmt=png&amp;from=appmsg#imgIndex=19" data-ratio="0.5426170468187275" data-s="300,640" data-type="png" data-w="833" type="block" data-imgfileid="503530321" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/49806248-f47c-44d4-903d-2e7066c7e755/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当然，目前的这个程序虽然可用，但需要自己去配置 API，这当然是有些麻烦的，而有 Kimi Code 的我们自然可以轻松避免这些麻烦，直接一句指令就能让其进一步执行，直接配置一个免费的 API。&lt;/p&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8f803bc6-208a-4fe4-937c-6041cc4e9190/1769507991425.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 4 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;很快，Kimi Code 就完成了任务，运行看看效果：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbOHpPJgzGwArHbicguwSCPlz5nbp1ux6ckTvztwXZAhqCWJzFcx1bwGQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=20" data-ratio="0.6536388140161725" data-s="300,640" data-type="png" data-w="742" type="block" data-imgfileid="503530322" data-aistatus="1" data-original-style="null" data-index="22" src="https://image.jiqizhixin.com/uploads/editor/69f00544-ad69-4163-a811-e2bf9fc0e127/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;可以看到，此时的金银价格已经正确反映了实时价格。当然我们也还可以让 Kimi Code 进一步执行，比如显示价格改成以人民币 / 克计价、将这个 Python 程序打包成一个 .exe、配置提醒音乐和弹窗、实现任务栏实时显示等等。&lt;/p&gt;&lt;p&gt;但正如其它类似工具一样，Kimi Code 同样并非编程专属工具，借助它搭配合适的配置，我们也能让其成为工作中的强大助力。比如我们可以使用 Kimi Code 轻松实现文件批处理。举个例子，对于我们的每日选题 docx 文档，我们可以让 Kimi Code 基于 obsidian-skills 将它们批量处理成兼容 Obsidian 的格式并打好合适的标签。&lt;/p&gt;&lt;p&gt;基于 obsidian-skills 将这些每日选题总结文档处理成兼容 Obsidian 的 Markdown 格式并打好合适的标签。&lt;a href="https://mp.weixin.qq.com/s/oVmOyqpcvmjoSGoap3BqTQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7b1226ac-c122-436d-94c7-e8e9126b9e71/1769508016484.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 4 倍速视频。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;可以看到，Kimi Code 不到两分钟就完成了对所有 94 个文件的正确处理，上下文占用量也仅仅刚超过 10%。在此过程中，也能注意到 Kimi Code 确实正确调用了 obsidian-skills，得到的结果也非常让人满意：yaml、callout 等的处理都非常正确。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbZicMASGBjHjyHsdMJNIYKHtwssgsFFstoI9YfMz50FTic5Ew2LAtcfww/640?wx_fmt=png&amp;from=appmsg#imgIndex=21" data-ratio="0.8282710280373832" data-s="300,640" data-type="png" data-w="856" type="block" data-imgfileid="503530323" data-aistatus="1" data-original-style="null" data-index="23" src="https://image.jiqizhixin.com/uploads/editor/80404e82-3666-4716-a4d5-083aef1bee54/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;整体体验下来，我们认为 Kimi 2.5 在智能体能力上已经足以比肩前沿模型，尤其是其智能体集群模式在解决复杂任务上的表现更是亮眼。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;中国的开源模型正在逐渐成为新的标准，并成为规则的制定者。Kimi K2.5 的发布，又给全球开源大模型树立了新的标杆。&lt;/p&gt;&lt;p&gt;与此同时，基于 K2.5 视觉、智能体能力的发展，AI 解锁了更多在真实世界中解决复杂问题的能力。&lt;/p&gt;&lt;p&gt;现在 AI 在写代码时有了审美，上百个智能体能够协同工作，我们距离 AGI 又近了一步。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ICLR 2026 放榜了！28%接收率，欢迎投稿机器之心</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 17:51:50 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;昨晚深夜，ICLR 2026 官方正式向投稿者发送了今年的论文接收结果通知。&lt;/p&gt;&lt;p&gt;作为机器学习领域的顶级会议， ICLR 2026 将于 2026 年 4 月 23 日至 27 日在巴西里约热内卢举行。官方今年收到了有效投稿约 19000 篇，总录取率约为 28%，该录取率涵盖了所有经过同行评审的完整论文投稿，无论其是否撤稿。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;网友晒出成绩单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;录用通知一出来，网友们也坐不住了。社交平台上，很快被各种成绩单刷屏：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbTPO4JzQfXeUjT4INuZcibiaXwVfTOjmpMzW7CkO9KPkx4z0ciazTUaK0w/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.549074074074074" data-type="png" data-w="1080" data-width="1323" data-height="726" data-imgfileid="503530260" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6979b064-2ac2-47fc-a99e-443be791a3d9/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;有的研究者不止一篇被录取：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbDWz4vLZNf0fPpfrPNgKPLWG86aicFCTrqNC4iabXVYb1bjcgL2w7EOvA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.37650602409638556" data-type="png" data-w="996" data-width="996" data-height="375" data-imgfileid="503530261" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d5587e2f-c1ff-4f3c-9b4b-b918506b6fd1/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;甚至还有实验室在这一届一口气拿下 8 篇论文。截图一放出来，评论区立刻炸开了锅，清一色的都是羡慕与感叹。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbrflTpdZQmCuCxnrOU5VwbJaHyicq7880W89udgZXkl1icDq529wrwp6Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.0138888888888888" data-type="png" data-w="1080" data-width="1107" data-height="1122" data-imgfileid="503530262" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/99e55c67-6031-45c8-81e2-84c2c784bbbe/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;被拒稿，可能不是论文的问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;今年的 ICLR 可以说是「史上最乱」的一届，先是第三方机构对审稿意见的系统性统计发现，其中有 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651002013&amp;idx=1&amp;sn=c4588ce6e29f6464cda6e84dfded6af5&amp;scene=21#wechat_redirect" target="_blank"&gt;21% 完全由 AI 生成&lt;/a&gt;；后有&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651005475&amp;idx=1&amp;sn=7a252f9f45f9a23abcc799a8f966d9db&amp;scene=21#wechat_redirect" target="_blank"&gt;&amp;nbsp;OpenReview 评审大开盒&lt;/a&gt;，波及到了 ICLR 2026 超过 10000 篇投稿；接着 AI 生成内容检测平台 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651006067&amp;idx=1&amp;sn=ef47cb61cfe7ac51167fc08776076c0f&amp;scene=21#wechat_redirect" target="_blank"&gt;GPTZero 扫描了 300 篇投稿论文&lt;/a&gt;，发现其中有 50 篇在论文引用上至少包含一处明显的幻觉内容。&lt;/p&gt;&lt;p&gt;然而，这场闹剧并未结束。&lt;/p&gt;&lt;p&gt;研究者 Eldar Kurtić 公开展示了一段离谱的审稿意见：一名审稿人在反馈意见中表示该论文「缺少与 FlexPrune 的具体比较」。不过 Kurtić 调查发现，似乎并不存在名为 FlexPrune 的主流基准方法。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb4rXYjkzIS5RKpwct3zPmq9aAo1gDKN4OVZe6uH9tVKQxtTSRFibAL8w/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9296296296296296" data-type="png" data-w="1080" data-width="1320" data-height="1227" data-imgfileid="503530263" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/05bc27e2-7e1f-4846-9d5e-abd8b111c76e/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;更离谱的是，领域主席在随后的裁定中，直接采信了这一错误意见，并将其定性为论文的「致命缺陷」，最终以此为由做出拒稿决定。&lt;/p&gt;&lt;p&gt;该贴发布后迅速走红，目前已获得数万次浏览及大量研究者的共鸣，矛头直指 LLM 在同行评审中的滥用。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb9IXyu2Vib7gkBRGUpHfxjwYqMrIBTBQORsibw2ZYYHdibPVF7RdLHTYcw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5092592592592593" data-type="png" data-w="1080" data-width="1107" data-height="564" data-imgfileid="503530264" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/bfcd068b-3e74-4bb9-b5e8-6ea433ca9a29/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb7ReJM0c3icibzZxludSLHRVNPtIqC6xRJv0ms7wYsk34v9X4xWrP1EbQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.6722222222222223" data-type="png" data-w="1080" data-width="1110" data-height="746" data-imgfileid="503530265" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/089f20c5-fc2e-4456-8d8a-b030fc883e76/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不少学者质疑，该审稿意见极有可能是通过 GPT 或 Grok 等 AI 工具自动生成的。由于 AI 存在「幻觉」特性，容易编造看似专业实则虚假的方法名。而 Meta-Reviewer 的疏忽，导致这种错误未能被纠正，反而成为了拒稿的定论。&lt;/p&gt;&lt;p&gt;这位网友则表示，在评分分别为 8 / 6 / 6 / 6、且评审意见整体偏正面的情况下，论文仍被拒稿。最让人难以接受的并不是拒稿本身，而是 Meta-Review 给出的理由。AC 无视了所有评审的一致支持，额外提出了两个新的质疑（而且这些质疑本身还存在事实性错误），并声称：所有评审意见都较为表面（尽管勉强高于评审的最低要求门槛）。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb6nCibEdJ4qc4OY1NBR5owJZM8okian4oI26Wh4auL0oSM0TwQibF4cicAA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.8419540229885057" data-type="png" data-w="1044" data-width="1044" data-height="879" data-imgfileid="503530266" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/acaaef3d-8c2d-42c9-9d07-7e9e5d42ec2d/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;不管怎样，拒稿并不等于否定你的研究价值，很多经典论文也曾遭遇过拒稿。&lt;/p&gt;&lt;p&gt;最后，也欢迎被录取的作者投稿机器之心，让更多人看到你们的研究。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>英伟达Earth 2开源全新天气预报模型，能生成15天全球预报</title>
      <description>&lt;![CDATA[每个模型都基于新的架构构建。]]&gt;</description>
      <author>李泽南</author>
      <pubDate>Tue, 27 Jan 2026 17:46:53 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;本周二，英伟达在得州休斯顿举行的美国气象学会年会上发布了全新的 NVIDIA Earth-2 系列开放模型、库和框架，用于天气和气候人工智能任务，提供了全球第一个完全开放、加速的天气 AI 软件栈。&lt;/p&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a6c7c8f2-7f11-48e6-a3f2-e4d2f205910e/ezgif-66f96790baa09853.gif"&gt;&lt;span class="fr-inner"&gt;NVIDIA Earth-2 Nowcasting 模型利用基于卫星和雷达数据训练的生成式 AI 来预测真实云层和降雨系统的演变，学习预测风暴的发展和组织方式。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;英伟达表示，这些开放技术（包括预训练模型、框架、定制方案和推理库）可以加速所有预测阶段，范围覆盖从处理初始观测数据到生成 15 天全球预测，或局部风暴的预测。&lt;/p&gt;&lt;p&gt;当前，高精度天气预报一直依赖于运行基于物理模型的超级计算机。AI 驱动的天气预报有望节省大量计算时间和成本，使更多国家、气象机构和企业能够运行针对特定应用场景的预报系统。&lt;/p&gt;&lt;p&gt;在这一领域，NVIDIA Earth-2 是首个开放的、加速的模型和工具集，它使生产就绪的天气 AI 完全可供组织在其自己的基础设施上运行、微调和部署，使开发人员能够将不同的天气和气候 AI 功能整合在一起。 &amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 656.01px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3d66e685-e700-45a2-be0e-76af72df9d25/ezgif-46cd4e8d8176ffa5.gif"&gt;&lt;span class="fr-inner"&gt;使用 NVIDIA Earth-2 中期预报对不同天气变量（总柱水汽、风速和比湿）进行集合预报，并与 ERA5 再分析数据进行比较。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这是一项开创性的工作，旨在加快天气预报速度，提高预报准确性，促进合作，并增进科学家对地球大气状况的整体了解。&lt;/p&gt;&lt;p&gt;当前，各行各业的开发者都在利用 Earth-2 预测天气并从中获取可操作的洞察。这其中包括 AI 天气工具提供商 Brightband；以色列气象局、The Weather Company 和美国国家气象局 (NWS) 等天气预报机构；与日立合作的能源预测和电网运营公司 TotalEnergies、Eni、GCL 和 Southwest Powerpool；能源交易解决方案提供商 Jua 和 Metdesk；以及金融风险和情报公司 AXA、JBA Risk Management 和标普全球能源。&lt;/p&gt;&lt;p&gt;今天公布的全新 Earth-2 开放天气模型包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Earth-2 中期预报模型，其采用名为 Atlas 的全新模型架构，能够对包括温度、气压、风和湿度在内的 70 多个气象变量进行高精度的中期天气预报，预报时效可达 15 天。在标准基准测试中，该模型在业内最常用的预报变量上均优于领先的开源模型。&lt;/li&gt;&lt;li&gt;Earth-2 Nowcasting，采用名为 StormScope 的全新模型架构，利用生成式人工智能技术，在几分钟内即可将国家尺度的预报转化为公里级分辨率、0 至 6 小时的局部风暴和危险天气预报。Earth-2 Nowcasting 是首个通过直接模拟风暴动力学，在短期降水预报方面超越传统物理天气预报模型的平台。它利用 AI 技术直接预测卫星和雷达图像。&lt;/li&gt;&lt;li&gt;Earth-2 全球数据同化系统，采用名为 HealDA 的全新模型架构，能够生成天气预报的初始条件 &amp;mdash;&amp;mdash; 即全球数千个地点当前大气状况的快照，包括温度、风速、湿度和气压。Earth-2 全球数据同化系统可在 GPU 上仅需数秒即可生成初始条件，而超级计算机则需要数小时。结合 Earth-2 中期预报系统，该系统能够生成最精准的天气预报，且整个过程均由开放的、完全基于人工智能的流程完成。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e8c009f8-b8fc-47f4-8fa8-af6359a17ff2/ezgif-64b4f93c484e6e37.gif"&gt;&lt;span class="fr-inner"&gt;Earth-2 临近预报示例，展示了真实的云和降雨系统，说明了风暴是如何发展和组织的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;这些模型将加入 NVIDIA Earth-2 堆栈中现有的开放式天气模型：&amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Earth-2 CorrDiff 使用名为 CorrDiff 的生成式 AI 架构，将粗分辨率的大陆尺度预测降尺度到高分辨率的区域尺度天气场，从而提供局部预测所需的精细分辨率，速度比传统方法快 500 倍。&amp;nbsp;&lt;/li&gt;&lt;li&gt;Earth-2 FourCastNet3 对各种天气变量（如风、温度和湿度）的预测精度很高，超越了领先的传统集合模型，可与顶级的基于扩散的方法相媲美，同时预测速度比这些方法快 60 倍。&lt;/li&gt;&lt;li&gt;Earth-2 还集成了来自欧洲中期天气预报中心 (ECMWF)、微软、谷歌等机构的开放模型。此外，Earth-2 模型还可以使用 NVIDIA PhysicsNeMo 进行训练和微调，NVIDIA PhysicsNeMo 是一个用于大规模开发 AI 物理模型的开源 Python 框架。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span class="fr-img-caption fr-fic fr-dib" style="width: 1015px;"&gt;&lt;span class="fr-img-wrap"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9111ac79-7793-46d2-99b6-eab384941235/ezgif-3384e23563bc3df0.gif"&gt;&lt;span class="fr-inner"&gt;NVIDIA Earth-2 全球数据同化技术展示了来自卫星、气象气球和气象站的地球观测数据的复杂模式，人工智能模型将这些模式转换为大气状态的平滑、连续估计值，并以此为基础进行预测。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;准确的天气预报有助于拯救生命和保护环境，是农业、能源、公共卫生和其他行业决策的基石。研究人员、气象机构、气候技术创新者和企业已经在运行、微调和构建这些最先进的模型，利用他们自己的本地的 AI 基础设施来取得科学突破。&lt;/p&gt;&lt;p&gt;英伟达介绍了一系列目前 Earth-2 的相关使用案例。其中，中国最大的太阳能材料生产商之一、全球综合能源运营商协鑫（GCL）正在其光伏发电预测系统中运行 NVIDIA Earth-2 模型。与传统的数值天气预报相比，Earth-2 能够以更低的成本提供更精确的预测数据，显著提升协鑫光伏发电预测的准确性。&lt;/p&gt;&lt;p&gt;标普全球能源正在利用 NVIDIA Earth-2 CorrDiff 将气候数据转化为本地化洞察，用于风险评估。全球保险集团安盛正在使用 FourCastNet 生成数千个假设的飓风情景，作为其研发项目的一部分，该项目旨在进行模型评估、方法开发以及现有技术的基准测试。 &amp;nbsp;&lt;/p&gt;&lt;p&gt;参考内容：&lt;/p&gt;&lt;p&gt;https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>刚刚，DeepSeek又探索新架构了，开源OCR 2</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 14:12:05 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;嘿！刚刚，DeepSeek 又更新了！&lt;/p&gt;&lt;p&gt;这次是更新了十月份推出的 DeepSeek-OCR 模型（参见：&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650996733&amp;idx=1&amp;sn=1bfd3ae23da07de55eda3ed0d51c9b23&amp;scene=21#wechat_redirect" target="_blank"&gt;太强了！DeepSeek 刚刚开源新模型，用视觉方式压缩一切&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;当时 DeepSeek-OCR 的出世，引起了大家对视觉压缩的关注与讨论，而这一次，DeepSeek 对视觉编码下手了。&lt;/p&gt;&lt;p&gt;可以说，刚刚发布的 DeepSeek-OCR 2 通过引入 DeepEncoder V2 架构，实现了视觉编码从「固定扫描」向「语义推理」的范式转变！&lt;/p&gt;&lt;p&gt;当然，和 DeepSeek 几乎每次发布一样，这一次同样也是模型和技术报告齐开源。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbqNFSUT1icue2712kRwib6MhJXcY3GUv4047g1nUQTGE4BKUfDgia9rF2A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.3824074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530224" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/a8819c24-8f2d-4c4b-b3ef-c94058c476f5/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目地址：https://github.com/deepseek-ai/DeepSeek-OCR-2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;模型地址：https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这项研究的三位作者分别是魏浩然、孙耀峰、李宇琨。&lt;/p&gt;&lt;p&gt;具体来说，&lt;strong&gt;该研究的核心创新在于将原本基于 CLIP 的编码器替换为轻量级语言模型（Qwen2-500M），并引入了具有因果注意力机制的「因果流查询」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这种设计打破了传统模型必须按从左到右、从上到下的栅格顺序处理图像的限制，赋予了编码器根据图像语义动态重排视觉 Token 的能力。通过这种两级级联的 1D 因果推理结构（编码器重排与译码器解析），模型能够更精准地还原复杂文档（如带表格、公式和多栏布局）的自然阅读逻辑。&lt;/p&gt;&lt;p&gt;这就像是为机器装上了「人类的阅读逻辑」，让 AI 不再只是生搬硬套地扫描图像。对比之下，传统的 AI 就像一个死板的复印机，不管页面内容多复杂，都只能从左上角到右下角按行扫描。&lt;/p&gt;&lt;p&gt;在维持极高数据压缩效率的同时，DeepSeek-OCR 2 在多项基准测试和生产指标上均取得了显著突破。模型仅需 256 到 1120 个视觉 Token 即可覆盖复杂的文档页面，这在同类模型中处于极低水平，显著降低了下游 LLM 的计算开销。&lt;/p&gt;&lt;p&gt;在 OmniDocBench v1.5 评测中，其综合得分达到 91.09%，较前代提升了 3.73%，特别是在阅读顺序识别方面表现出了更强的逻辑性。&lt;/p&gt;&lt;p&gt;此外，在实际生产环境中，该模型显著降低了 OCR 识别结果的重复率，并为未来构建统一的 omni-modal（全模态）编码器提供了可行路径。是的，未来同一个 AI「大脑」或许能用同样的方法去处理声音、视频等所有模态的数据，真正实现多模态的深度统一。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DeepSeek-OCR 2 架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如图 3 所示，DeepSeek-OCR 2 延续了 DeepSeek-OCR 的整体架构，由编码器（encoder） 和解码器（decoder） 组成。编码器负责将图像离散化为视觉 token，而解码器则在这些视觉 token 与文本提示（text prompts）的条件约束下生成输出。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbQ2WQb28XxriaTXxT52ZKuAag9BIa0iamFMOVT9SwpeTOJTGX60IFiaOSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.4898148148148148" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530226" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/d9f509cb-face-4086-baf0-25dfde3248e5/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;两者的关键区别在于编码器部分：&lt;strong&gt;DeepSeek 将原有的 DeepEncoder 升级为 DeepEncoder V2。在完整保留前代能力的基础上，DeepEncoder V2 通过一种全新的架构设计，引入了因果推理能力（causal reasoning）。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DeepEncoder V2&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DeepEncoder V2 的第一个组成部分是视觉分词器（vision tokenizer）。延续了 DeepEncoder 的设计，DeepSeek 采用了一种由参数规模为 8000 万的 SAM-base 与两层卷积层组成的架构。相比 DeepEncoder，DeepSeek 将最终卷积层的输出维度从 1024 降至 896，以与后续处理流程保持一致。&lt;/p&gt;&lt;p&gt;在 DeepEncoder 中，视觉分词器之后接入的是一个 CLIP ViT，用于进一步压缩和建模视觉语义。DeepEncoder V2 对这一组件进行了重新设计，将其改造为一种类 LLM 的架构，并引入了双流注意力机制（dual-stream attention）。&lt;/p&gt;&lt;p&gt;其中，视觉 token 采用双向注意力，以保留 CLIP 所具备的全局建模能力；而新引入的因果流查询（causal flow queries） 则使用因果注意力。这些可学习的查询 token 被作为后缀追加在视觉 token 之后，每个查询都可以关注所有视觉 token 以及其之前的查询 token。通过保持查询 token 与视觉 token 数量一致，该设计在不改变 token 总数的前提下，对视觉特征施加语义上的排序与蒸馏约束。最终，只有因果查询 token 的输出会被送入 LLM 解码器。&lt;/p&gt;&lt;p&gt;从整体上看，该架构实际上构建了一种两阶段级联的因果推理机制：首先，编码器通过可学习查询对视觉 token 进行语义重排；随后，LLM 解码器在这一有序序列之上执行自回归推理。与依赖位置编码施加刚性空间顺序的传统编码器不同，这种因果排序查询能够更自然地贴合连续的视觉语义，并与 LLM 的单向注意力模式高度一致。该设计有望在二维空间结构与一维因果语言建模之间搭建起一座桥梁。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb2BibuODUKvU42icZOmSr3arPiax16ShoiaTiaNYziafBFFPrzeDfng6EhZ1w/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.44166666666666665" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530228" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/49cc4411-c69e-4424-9058-784b1b5691f5/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;为更直观地展示 DeepEncoder V2 的注意力机制，图 5 对其注意力掩码进行了可视化。该注意力掩码由两个相互区分的区域组成。&lt;/p&gt;&lt;p&gt;左侧区域对原始视觉 token 采用双向注意力机制（类似于 ViT），使任意 token 都可以与其他所有 token 建立可见性，从而实现完整的全局建模；右侧区域则针对因果流 token 使用因果注意力（三角形掩码，与纯解码器 LLM 完全一致），其中每个 token 只能关注其之前的 token。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbiabYo5jmhO7BR7KFfxQN8wYzVWusL6WMYKS7S6xn785dXnicA3dS0T2Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4101851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530230" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/77f090e4-325a-4477-bc13-70fa65cdacbf/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;DeepSeek-MoE Decoder&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;由于 DeepSeek-OCR 2 的改进重点主要集中在编码器 上，并未对解码器部分进行升级。遵循这一设计原则，模型继续沿用 DeepSeek-OCR 的解码器 &amp;mdash;&amp;mdash; 一个参数规模为 30 亿的 MoE 结构，其中约 5 亿参数在推理时处于激活状态。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练数据与训练流程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在数据层面，DeepSeek-OCR 2 沿用了与 DeepSeek-OCR 相同的数据源，由 OCR 1.0、OCR 2.0 以及通用视觉数据组成，其中 OCR 数据占混合训练数据的 80%。同时引入了以下两项改进：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;针对 OCR 1.0 数据采用了更均衡的采样策略，并按内容类型（正文、公式和表格）以 3:1:1 的比例对页面进行划分；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;通过合并语义相似的类别（例如统一「插图说明」和「插图标题」）来优化布局检测的标签。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在训练阶段，DeepSeek-OCR 2 主要分为三个阶段来完成：&lt;strong&gt;（1）编码器预训练；（2）查询增强；（3）解码器专门化&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其中第一阶段使视觉分词器（tokenizer）和 LLM 风格的编码器获得特征提取、token 压缩和 token 重排的基础能力。第二阶段进一步加强编码器的 token 重排能力，同时增强了视觉知识的压缩。第三阶段冻结编码器参数，仅优化解码器，从而在相同的 FLOPs 下实现更高的数据吞吐量。&lt;/p&gt;&lt;p&gt;接着来看细节。&lt;/p&gt;&lt;p&gt;首先是&lt;strong&gt;训练 DeepEncoder V2&lt;/strong&gt;。遵循 DeepSeek-OCR 和 Vary 的方法，使用语言建模目标来训练 DeepEncoder V2，将编码器与轻量级解码器耦合，通过预测下一个 token 进行联合优化。采用了 768&amp;times;768 和 1024&amp;times;1024 两种分辨率的数据加载器。视觉分词器初始化自 DeepEncoder，LLM 风格的编码器则初始化自 Qwen2-0.5B-base。预训练完成后，仅保留编码器参数用于后续阶段。&lt;/p&gt;&lt;p&gt;本阶段使用 AdamW 优化器，学习率采用余弦退火，从 1e-4 降至 1e-6，在 160 台 A100 GPU（20 个节点 &amp;times; 8 台 GPU）上以 640 的批大小训练 40k 次迭代（采用长度为 8K 的序列打包，约包含 1 亿个图文对样本）。&lt;/p&gt;&lt;p&gt;其次是&lt;strong&gt;查询增强&lt;/strong&gt;。在 DeepEncoder V2 预训练之后，将其与 DeepSeek-3B-A500M 整合为最终的流水线。冻结视觉分词器（SAM-conv 结构），并联合优化 LLM 编码器和 LLM 解码器以增强查询表示。本阶段通过多裁剪策略将两种分辨率统一到单个数据加载器中。此外采用 4 阶段流水线并行：视觉分词器（PP0）、LLM 风格编码器（PP1）以及 DeepSeek-LLM 层（PP2-3 每阶段 6 层）。&lt;/p&gt;&lt;p&gt;本阶段利用 160 台 GPU（每台 40GB 显存），配置了 40 个数据并行副本（每个副本 4 台 GPU），过程中使用相同的优化器，以 1280 的全局批大小进行训练，学习率在 15k 次迭代中从 5e-5 退火至 1e-6。&lt;/p&gt;&lt;p&gt;最后是&lt;strong&gt; LLM 持续训练&lt;/strong&gt;。为了快速消耗训练数据，本阶段冻结 DeepEncoder V2 的所有参数，仅更新 DeepSeek-LLM 的参数。本阶段加速了训练（在相同全局批大小下，训练速度提升了一倍多），同时有助于 LLM 更好地理解 DeepEncoder V2 重排后的视觉 token。&lt;/p&gt;&lt;p&gt;承接第二阶段，本阶段进行了另一次学习率退火，从 1e-6 降至 5e-8，共训练 20k 次迭代。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;评估结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队选用 OmniDocBench v1.5 作为主要评测基准，该基准包含 1355 页文档，覆盖中英文两种语言的 9 大主要类别，包括杂志、学术论文、研究报告等。凭借其多样化的测试样本与严格的评测标准，OmniDocBench 为验证 DeepSeek-OCR 2 的整体性能，尤其是 DeepEncoder V2 的有效性，提供了一个可靠有效的平台。&lt;/p&gt;&lt;p&gt;如表 1 所示，在使用最小视觉 token 上限（&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbQVAPX5gOsRRNkSM7rltExFovYqm7ibbWJ7EdfwUEiaxh127jLL5w8SNQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.26481481481481484" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530231" data-aistatus="1" data-original-style="width:77px;height:20px;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/1e8eece1-1066-46ad-9b0f-bfca0e93db58/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dii" style="width: 11.28%;"&gt;）的情况下，DeepSeek-OCR 2 仍取得了 91.09% 的领先性能。与 DeepSeek-OCR 基线模型相比，在采用相似训练数据来源的前提下，其性能提升了 3.73%，验证了新设计架构的有效性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbjSDBaeoicBictibvETORbYEjx28Zc6VghIowNWOChsjnpibc8YaaUPSsAQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.8916666666666667" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530233" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/fc025f92-ea71-4f83-af08-213a92591967/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;此外，除了整体性能提升，阅读顺序（R-order）指标上的编辑距离（Edit Distance，ED）也显著下降，从 0.085 降至 0.057。这表明，新的 DeepEncoder V2 能够根据图像信息更有效地选择并排列初始视觉 token。&lt;/p&gt;&lt;p&gt;进一步如表 2 所示，在相同的视觉 token 预算（1120）条件下，DeepSeek-OCR 2 在文档解析任务中的 ED（0.100）低于 Gemini-3 Pro（0.115）。这进一步证明了新模型在保持高视觉 token 压缩率的同时，仍能确保更优的性能，并展现出极高的潜力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbJkdbTuvh1MrM8IH9n4vu93C79GlPmZGQtCvnynYAWdyZibDyHPmLhxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.27037037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530234" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/929acccf-54c6-40e1-93d7-3af904b610d9/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;改进空间&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;团队在 9 种文档类型上，对 DeepSeek-OCR 与 DeepSeek-OCR 2 进行了细致的性能对比，结果表明：DeepSeek-OCR 2 仍具有较大的提升空间，如表 3 所示。在文本识别的编辑距离（ED）指标上，DeepSeek-OCR 2 在大多数场景中优于 DeepSeek-OCR，但在某些类型上仍存在明显不足，例如报纸类文档，其 ED 超过 0.13。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb0gBHFTv7f2iavaXm5ZE6gyY6mh1V0SriaN2iaUfibBghyGcYNFoRbdsVEQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.30092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530236" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/24637db9-40a5-4a64-87cb-12d9c90bee76/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;团队认为主要原因有两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;视觉 token 上限较低，可能影响了文本极为密集的报纸类文档识别效果，这一问题可在未来通过增加局部裁剪（local crops）的数量来缓解；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;报纸类数据不足 &amp;mdash;&amp;mdash; 当前训练集中仅包含约 25 万条相关样本，这对于训练 DeepEncoder V2 来说仍然不够充分。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，在阅读顺序（R-order）这一指标上，DeepSeek-OCR 2 在所有类别中始终优于 DeepSeek-OCR，这进一步验证了所提出的「视觉因果流」编码器设计的有效性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实际应用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 主要面向两类生产场景：一是为 DeepSeek-LLM 提供图像 / 文档读取能力的在线 OCR 服务，二是用于批量 PDF 处理的预训练数据流水线。在比较了 DeepSeek-OCR 2 与 DeepSeek-OCR 在真实生产环境中的表现后发现，由于生产环境中无法获得标准答案，因此团队主要采用「重复率」作为核心质量指标。&lt;/p&gt;&lt;p&gt;如表 4 所示，相比前代模型，DeepSeek-OCR 2 在实际可用性方面有了显著提升：在在线用户日志图像中，重复率从 6.25% 降至 4.17%；在 PDF 数据生产场景中，重复率从 3.69% 降至 2.88%。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFwSLiaGGJPufDHDJadOImqlRLycvEM8JKmN4ZRaHfXm2IVGoOaB2KLw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.25555555555555554" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530238" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/99056c29-b085-42e6-991b-1e3065fd900f/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;这些结果进一步验证了 DeepSeek-OCR 2 架构的有效性，尤其体现了其在逻辑性视觉理解方面的优势。&lt;/p&gt;&lt;p&gt;更多详情信息，可阅读原文获取！&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>高效智能体的「幕后推手」是谁？一篇综述带你从记忆×工具学习×规划看透</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 14:07:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474619" data-aistatus="1" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/2c04cf4d-fb98-4a8d-9950-ddf2aab313e2/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;随着大模型能力的跃迁，业界关注点正在从 &amp;ldquo;模型能不能做&amp;rdquo; 快速转向 &amp;ldquo;智能体能不能落地&amp;rdquo;。过去一年可以看到大量工作在提升智能体的有效性（effectiveness）：如何让它更聪明、更稳、更会用工具、更能完成复杂任务。&lt;/p&gt;&lt;p&gt;但在真实应用里，另一个更 &amp;ldquo;硬&amp;rdquo; 的问题常常决定能否上线：高效性（efficiency）。智能体即便表现很好，如果每次都要消耗大量算力、时间与调用成本，也很难在生产环境大规模部署。&lt;/p&gt;&lt;p&gt;基于这一视角，论文整理并撰写了一篇面向 &amp;ldquo;高效智能体&amp;rdquo; 的综述，系统梳理当前主要方法，并从三个最关键的机制出发组织全文框架：&lt;strong&gt;记忆 &amp;mdash; 工具学习 &amp;mdash; 规划&lt;/strong&gt;。论文从设计范式出发对代表性方法进行归纳总结，聚焦那些以效率为目标或能够提升效率的核心设计与实现路径，从而更清晰地呈现智能体在真实落地场景中的成本 &amp;mdash; 性能权衡。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503529880" data-ratio="0.45" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDPbVGKvncv8ibiaNI3NlxCE7FmQuFwd9lkbrH5HemhQRPwGbibC4h6ksBw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f544bd96-dcde-42fa-a6c9-c958608b1679/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2601.14192&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 地址：https://github.com/yxf203/Awesome-Efficient-Agents&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDBb2kzG46OwkgsvEn8swy1kBNegFZxh6MEWg1eQkjhNe5HG30a5ZtbQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529882" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b454d21b-c76b-4894-83b1-a390dec5af7b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;一、智能体记忆：让 &amp;ldquo;会记&amp;rdquo; 更省、更准、更可扩展&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDu5BvcBvmk8PVAtlrtHiab9PiaPaPibcUC1YARYfwcGKicvOzWFuL8CfTVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529892" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f9268945-4705-4777-866e-57497c3c6fec/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;智能体要做长任务，离不开记忆。但把历史一股脑塞进提示词，会带来 token 暴涨和智能体处理长上下文能力下降。因此，高效记忆系统的关键在于把 &amp;ldquo;长历史&amp;rdquo; 加工成 &amp;ldquo;可用、可检索、可复用&amp;rdquo; 的信息资产。&lt;/p&gt;&lt;p&gt;论文按记忆生命周期梳理三步：构建 &amp;mdash; 管理 &amp;mdash; 访问。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆构建&lt;/strong&gt;：通过概括、压缩与结构化把 &amp;ldquo;长对话&amp;rdquo; 转成 &amp;ldquo;可用记忆&amp;rdquo;。一类是留在推理链路的工作记忆，文本式直观但吃上下文，隐式式更像缓存，可减少重复编码；另一类是外置为可检索系统的外部记忆，先将信息压成小单元再按需召回，包括条目式、图式与分层式。此外论文也提到要警惕过度压缩带来的信息损失，即需要考虑如何在降成本与保真之间取得平衡。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆管理&lt;/strong&gt;：防止 &amp;ldquo;存爆炸&amp;rdquo;，也避免 &amp;ldquo;取太慢&amp;rdquo;。规则式快但可能误删重要内容，大模型式更聪明但更贵，混合式则按层级或场景组合两者策略，在效果与成本之间取得折中。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;记忆访问&lt;/strong&gt;：选什么 + 怎么用。访问分记忆选择与记忆整合，通过检索或训练等方式挑选记忆，再用压缩过滤或隐式注入减少 token 与重复编码。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;另外，多智能体记忆也成为新趋势。相较于只靠通信，近年更多工作开始引入 &amp;ldquo;记忆&amp;rdquo; 这一概念来支撑规模化协作，论文将其概括为：共享记忆 / 本地记忆 / 混合记忆三类。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、工具学习：让 &amp;ldquo;会用工具&amp;rdquo; 更少调用、更少等待、更少走弯路&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoD8fbTenP3yZSG2JMtH5iaoxozzxPdlwd4ylKBg2CcFSsvUYvHuogbXwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.4203703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529883" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/bd89d7fd-af27-430e-9efa-6600178dbe45/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;工具让智能体从 &amp;ldquo;会说&amp;rdquo; 变成 &amp;ldquo;能做&amp;rdquo;，但成本也最容易在工具链路里失控。论文按三条主线梳理提效思路：工具选择 &amp;mdash; 工具调用 &amp;mdash; 工具融合推理。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具选择&lt;/strong&gt;：目标是 &amp;ldquo;更快选对、少塞进 prompt&amp;rdquo;。相关方法包括外部检索器、多标签分类，以及将工具映射为特殊 token 等思路，核心都是在大量工具中更快、更准地选出最需要的那几个。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具调用&lt;/strong&gt;：核心是 &amp;ldquo;少等、少调、少走弯路&amp;rdquo;。典型路线包括边生成边调用、并行化调用，以及利用成本感知调用与测试时高效扩展来削减冗余调用；进一步还可通过面向效率的后训练把 &amp;ldquo;短轨迹、少调用&amp;rdquo; 写进策略本身。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;工具融合推理&lt;/strong&gt;：让模型学会 &amp;ldquo;该不该用、何时用、用几次&amp;rdquo;。代表性方向包括选择性调用，引导智能体只在必要时才发起工具调用；以及成本约束策略优化，在保证效果的同时对冗余交互与过长轨迹施加惩罚，从而学到更短、更省的工具使用策略。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;三、智能体规划：在 &amp;ldquo;深度&amp;rdquo; 与 &amp;ldquo;宽度&amp;rdquo; 上同时省下来&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDMb81A1Giaog2roygZGb37uOaakgQ2bxISWPcTKReTx1UKoeicZ6icuJBA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.45925925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503529893" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/75045766-dc82-473a-968b-9407fc03a9cb/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;规划决定智能体如何在多步决策空间里行动。效率问题要么来自单体推理 &amp;ldquo;想太深、搜太贵&amp;rdquo;，要么来自多体协作 &amp;ldquo;聊太多、通信太重&amp;rdquo;。因此论文从两条线展开：单智能体规划与多智能体协作规划。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;单智能体&lt;/strong&gt;：少算但不掉效果。主要思路包括自适应预算与控制的 &amp;ldquo;选择性思考&amp;rdquo;、结构化搜索的剪枝与代价感知、任务分解的先规划后执行；以及通过策略优化与记忆 / 技能获取把高效规划 &amp;ldquo;内化或复用&amp;rdquo;，越用越省。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多智能体&lt;/strong&gt;：少通信但尽可能不丢信息。方向主要有三类：拓扑稀疏化减少全连接带来的&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibaia3E2vEv1GKq10C6JGAoDwG1eBjkVcibhkvdroWqGsmqDL1g0UCoILic6rFy2tqLCUwLdLwzPoMcg/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5573770491803278" data-s="300,640" data-type="png" data-w="122" type="block" data-imgfileid="503529894" data-aistatus="1" data-original-style="width:47px;height:26px;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/84e72352-d8c7-41db-92f3-caebc303da67/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dii" style="width: 6.87%;"&gt;的消息传递开销；协议与上下文优化压缩则关注 &amp;ldquo;传什么 / 怎么传&amp;rdquo;；蒸馏方法通过将多智能体协作能力蒸馏回单体，来减少运行时多智能体之间协调的成本。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;四、基准与评测（Benchmark）：没有 &amp;ldquo;可比的尺&amp;rdquo;，就谈不上 &amp;ldquo;可落地的效率&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在谈记忆、工具学习与规划的提效方案之前，先要把 &amp;ldquo;尺子&amp;rdquo; 定清楚：高效到底怎么量？&lt;/p&gt;&lt;p&gt;论文强调，效率必须建立在有效性之上。省了资源却显著掉性能，不算高效。因此论文采用的定义是：在给定预算下取得更好的效果，或在相近效果下消耗更少资源。&lt;/p&gt;&lt;p&gt;基于这一视角，论文先梳理了以有效性为主的 benchmark，并进一步汇总了与效率相关的评测内容：一方面，整理了在 benchmark 中显式纳入效率信号（成本、延迟、调用次数等）的评测设置；另一方面，总结了智能体方法中常用的效率指标，用于刻画 &amp;ldquo;省在哪儿、省多少&amp;rdquo;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、挑战与展望&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;论文同时也提出了目前的一些挑战与展望：&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 1）统一评测框架&lt;/strong&gt;：指标口径统一，模块开销边界清楚，才能真正让各个智能体方法可比可复现。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 2）智能体的隐式推理（Latent Reasoning）&lt;/strong&gt;：大模型侧的隐式推理正在升温，但面向智能体的研究仍相对稀缺。由于智能体链路更长、更复杂，还要处理工具调用、规划与记忆等环节，如何把中间推理 &amp;ldquo;做在隐式空间里&amp;rdquo;、在不掉效果的前提下降低成本，既是挑战，也是机会。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 3）面向部署设计&lt;/strong&gt;：在多智能体场景下，需要把部署成本纳入考量，核心问题是投入产出比。也就是说，增加智能体带来的收益，是否足以覆盖新增的开销。&lt;/p&gt;&lt;p&gt;&lt;strong&gt; 4）多模态效率&lt;/strong&gt;：多模态智能体发展很快，但效率研究仍相对欠缺。文本智能体的一些提效思路可以借鉴，但是直接迁移却并不容易，因为多模态智能体的感知输入、行为空间与任务结构更复杂、交互成本更高。因此，如何在多模态场景下系统地兼顾效果与成本，仍是亟待解决的关键问题。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>即刻体验国内最强推理模型Qwen3-Max-Thinking，千问PC和网页端已接入</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 27 Jan 2026 13:34:43 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1月26日，阿里正式发布旗舰推理模型Qwen3-Max-Thinking，AI助手千问同步在PC端和网页端（qianwen.com）接入这一国内最强&amp;ldquo;AI大脑&amp;rdquo;，千问App也即将接入。用户只需在模型选择栏中一键切换，即可体验更为强大的推理能力。&lt;/p&gt;&lt;p&gt;Qwen3-Max-Thinking是目前阿里规模最大、能力最强的推理模型，总参数量超万亿（1T），预训练数据量高达36T Tokens。经过大规模强化学习训练，该模型在涵盖事实知识、复杂推理、指令遵循、人类偏好对齐等19个公认的大模型基准测试中，刷新多项最佳表现纪录，整体性能可媲美 GPT-5.2-Thinking-xhigh、Claude Opus 4.5 和 Gemini 3 Pro。&lt;/p&gt;&lt;p&gt;千问切换至这一模型后，不仅更主动、更智能，还能进行深度逻辑推演与自我校验：&lt;/p&gt;&lt;p&gt;更强的事实记忆与世界知识：无论是冷门科学、历史典故还是文化问题，都能提供更准确、权威的回答，同时显著提升上下文连贯性，更好记住用户偏好，大幅减少&amp;ldquo;失忆&amp;rdquo;现象；&lt;/p&gt;&lt;p&gt;专家级复杂推理能力：在高难度科学、数学与逻辑问题上表现卓越，可以为科研人员与职场人士提供多维度分析与结构化推理，辅助相关决策；&lt;/p&gt;&lt;p&gt;自迭代推理机制：面对复杂问题，会先草拟思路、验证假设、优化路径，再输出结论，显著提升推理质量与响应速度；&lt;/p&gt;&lt;p&gt;更契合人类价值观：指令遵循更精准，输出内容更安全、可靠，符合伦理规范与社会共识。&lt;/p&gt;&lt;p&gt;据悉，目前千问C端月活跃用户数已突破1亿，在学生和白领人群中增长迅猛。此前，千问App全面接入淘宝、支付宝、淘宝闪购等阿里生态业务，同时邀测体验&amp;ldquo;任务助理&amp;rdquo;，已成为全球首个能完成真实生活复杂任务的AI助手。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>蚂蚁具身研究首次亮相！就解决了机器人「看」透明玻璃这些难题，还开源了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:32:40 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜冷猫&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;众所周知，「具身智能」是连接数字世界和现实世界的桥梁。&lt;/section&gt;&lt;p&gt;真正的「具身智能」，是全面自主决策自主行动的通用机器人，需要建立在对物理世界完全理解的基础上。&lt;/p&gt;&lt;p&gt;空间视觉感知是自动驾驶、机器人操作等真实世界应用的底层能力，核心目标只有一个：&lt;strong&gt;让机器能够理解并参与三维环境中的交互。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;这类机器人大多都以 RGB-D 相机获取真实世界视觉和深度信息，这是行业内综合了成本，精度，以及实用性后普遍的选择。&lt;/p&gt;&lt;p&gt;但物理世界是极为复杂的，要想让这些自主执行任务的机器人卡壳，只需要简单的一块玻璃。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503530189" data-ratio="0.7805555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFUiaQVMzUvicT1pZoW4u6aqOy098EhA1lHK95qyMyDLbCHgO5WOSibqibA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=1" data-type="gif" data-w="720" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/b2810da7-aed7-4ae6-8009-4dd46ec14234/640.gif" data-order="0" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 家务机器人撞玻璃的翻车场面&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;对机器来说，玻璃几乎是世界里的幻影。人类会下意识地把反射、折射进行判断，但机器人并没有这种生活经验。玻璃这类又透明又反光的物体，恰好屏蔽了 RGB-D 相机获取的全部特征，深度和像素点都很难准确识别。&lt;/p&gt;&lt;p&gt;随着自动驾驶和智能机器人离我们的生活越来越近，这个现象已经逐渐成为一个亟需解决的痛点。&lt;/p&gt;&lt;p&gt;令人欣喜的是，我们发现刚刚开源的&lt;strong&gt;全新具身智能感知模型 LingBot-Depth&lt;/strong&gt; ，非常针对性的解决了机器人识别真实世界的「玻璃问题」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;LingBot-Depth 是蚂蚁灵波科技开源的高精度空间感知模型&lt;/strong&gt;，可在不更换硬件的前提下显著提升透明、反光等复杂材质场景的深度输出质量，给机器人一双看清三维空间的眼睛。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbwFqKibYxYzGJS6Te7PMgUIX1KYMTIJEr6wj2bOiaofdlxYgEGID845Jw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=2" data-ratio="0.58125" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530190" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/446278ab-5f67-47c4-86c2-db4d94d4dc9b/640.gif" data-order="1" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;LingBot-Depth 在传统深度传感器易失效的复杂场景中，仍可输出具备真实尺度的高精度深度结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;令人振奋的是，从技术报告来看，这一模型在深度精度和像素覆盖率方面均优于业界顶级的深度相机。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbrvIIMq67zMjJwUVUib15YFmDz64VeaEL2U0y9sZaX4ctm0kI7ibsNPeA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.3787037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530191" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/72ec25fd-37b6-4c93-8e30-c06bf3df289f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目链接: https://technology.robbyant.com/lingbot-depth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;HuggingFace 链接：https://huggingface.co/robbyant/lingbot-depth&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;技术报告链接：https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;超海量真实场景与崭新的训练范式&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;简单来讲，RGB-D 相机在复杂、多变的物理环境中，却频频暴露出难以回避的短板。&lt;/p&gt;&lt;p&gt;尤其是在面对透明或高反光材质，例如玻璃、镜面、不锈钢表面等等，深度相机发射的主动光信号往往无法形成稳定、可靠的回波，导致深度测量值缺失或异常，最终在深度图中表现为大面积空洞、噪声密集以及物体边缘的严重断裂。&lt;/p&gt;&lt;p&gt;即便是最先进的商用传感器，在一些挑战性场景中也难以满足获取稠密、像素级对齐的几何信息的需求。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakb0aOOLlfHuiaDMceJKIVVDAShkmM7Q0M5ibrXDtlbF1ib6ntHRyDEWxOug/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="0.3812615955473098" data-s="300,640" data-type="gif" data-w="1078" type="block" data-imgfileid="503530192" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/1abd64c0-e49e-42d1-b239-6a4befadd988/640.gif" data-order="2" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;LingBot-Depth 能将含噪且不完整的传感器深度优化为干净、稠密且具备真实尺度的三维测量结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;除了透明与反光场景外，在强逆光、极暗光或明暗对比极端的场景下，RGB 图像与深度信息之间的对齐关系更容易失效，深度图的稳定性和一致性显著下降。&lt;/p&gt;&lt;p&gt;更关键的是，感知层面的不可靠会被层层放大，直接影响后续的规划与控制：不完整的深度图会导致机器人误判空间，边缘破碎会影响抓取位姿的计算，而噪声与空洞则可能引发对障碍物距离的系统性偏差。这些问题最终体现为抓取失败、动作犹豫、路径规划异常，甚至是不可接受的碰撞风险，成为制约机器人从「能演示」走向「可长期落地」的关键瓶颈。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;双线并行的数据集&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去解决方案很难达到令人满意的效果，核心原因就是数据。巧妇难为无米之炊，RGB-D 数据比仅包含 RGB 的数据要少得多。&lt;/p&gt;&lt;p&gt;更致命的是，现有的大多数 RGB-D 数据集，在设计之初就刻意回避了真实世界中最棘手的成像条件，这类数据过于干净，要么选择纹理丰富、反射少的理想场景；要么干脆依赖渲染引擎，生成近乎完美的深度图。它们几乎不包含真实传感过程中自然出现的深度空洞、回波缺失和异常噪声，彻底回避了真实世界感知的痛点问题。&lt;/p&gt;&lt;p&gt;为了解决这一根本性瓶颈，LingBot-Depth 从数据分布本身入手，&lt;strong&gt;系统性地重构了 RGB-D 训练数据的来源与生成方式&lt;/strong&gt;。其核心思路只有一个：尽可能保留真实世界传感过程自然产生的深度缺失模式。&lt;/p&gt;&lt;p&gt;具体来看，蚂蚁灵波 构建了一套双路径并行的数据筛选与生成流程。一条路径基于自建高质量 3D 资产，走合成仿真路线；另一条路径则来自真实世界，通过可扩展的 RGB-D 采集系统，使用奥比中光等工业级深度相机直接采集现实场景数据。&lt;/p&gt;&lt;p&gt;由此，模型训练数据被明确划分为两类子集：来自&lt;strong&gt;合成路径的 LingBot Depth-S&lt;/strong&gt;，以及来自&lt;strong&gt;真实采集路径的 LingBot Depth-R&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;与现有方法直接输出完美深度图不同，LingBot-Depth 在合成流水线中刻意模拟了真实主动式 RGB-D 相机的成像过程。研究团队在 Blender 中同时渲染 RGB 图像、精确深度图以及带有斑点结构的灰度立体图像对，并使用半全局匹配（SGM）算法生成深度结果，从而引入与真实传感器高度相似的采集伪影。立体基线、焦距等关键参数均通过随机采样生成，以覆盖多样化的成像几何条件。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb7Do9jnwYWLEhbQkGEE3eribMicjbuiaDReGNibOB2tv8DJVbZOnHBQCzMg/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.5601851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530193" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/27b28ad1-1171-4e6c-ab36-dacd79184142/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;数据生成管线的一条合成数据样本。每个样本包含一幅 RGB 图像、一幅渲染得到的理想深度图、一对带有散斑图案的立体图像、对应的真实视差图，以及通过半全局匹配（SGM）计算得到的模拟传感器深度图，用以逼近真实世界主动式深度相机所产生的成像伪影。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;此前一些尝试模拟不完美深度测量的工作，数据规模普遍偏小；而部分依赖机器人仿真器的数据集，则在视觉保真度上仍与真实世界存在明显差距。相比之下，LingBot-Depth 的数据构建方式，更接近真实传感器在复杂物理环境中的「所见即所得」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbFAaER60jQq9wITiatcjiaE6zfbY5ic6zEprm2yNm07P9icGicPH2pDNqoibw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.45" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530194" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/607a2309-c0cd-4b64-8c78-73405ed05554/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 蚂蚁灵波团队在真实世界 RGB-D 采集数据在不同场景类别下的分布情况。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在此基础上，除了自行构建的 &lt;strong&gt;320 万条&lt;/strong&gt;数据外，蚂蚁灵波还使用了一些开源数据集作为训练数据，最终共构建了 &lt;strong&gt;1000 万条&lt;/strong&gt;用于掩码深度建模的训练样本，覆盖了从理想条件到复杂现实环境的多种深度缺失模式。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbV8oqK21l2aMGnmJaJt0bzLam8GmkhJLOQK9TFCAWBygOfstYrdOhjQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.6537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530195" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/43ebd558-0390-4623-aa9c-42d2cb8bd019/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;数据管道整理的 MDM 数据概览。展示了共计 210 万真实采集样本及模拟采集样本，同时展示了 RGB-D 输入和对应的 GT 深度图。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;具身智能感知能力的上限，很大程度上不取决于模型结构，而取决于是否敢于直面真实世界的「不完美」。 LingBot-Depth，正是从数据这一最底层的环节，补上了数据集中被忽略的一块短板。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;崭新的思路 &amp;mdash;&amp;mdash; 掩码深度建模&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传感器输出的像素与深度信息的不完整是能够进行优化的，将不一致的数据作为噪声剔除，随后通过算法修补，在计算机视觉和深度学习领域已经是历史悠久的研究方向。&lt;/p&gt;&lt;p&gt;而 LingBot-Depth 创新性地提出了一个全新的思路：&lt;strong&gt;与其将这些传感器故障视为需要丢弃的噪声，不如将其作为有益的学习信号加以利用。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在这个思路的指引下，掩码深度建模方法（Masked Depth Modeling, MDM）应运而生，构建了 LingBot-Depth 的根基，通过算法对传感器输出进行增强，使机器人获得更完整、更稳定、更可用的深度图。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb1b1waTXwVrbIf1Yezt5icQuu2pfW5WFIR5f9c0ctLHT7XPNicDKDUoVA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.36018518518518516" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530196" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/b1c05992-910a-4678-86f3-6b4bdc0c6ed4/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;MDM 预训练方法利用 RGB-D 传感器中自然缺失的深度测量值作为掩码，以学习度量尺度下完整且精确的深度表示。由此产生的 LingBot-Depth 模型可作为强大的空间感知先验，用于下游应用，包括 3D 点追踪和灵巧抓取。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;整体框架仍然沿用了近年来视觉领域中行之有效的编码器&amp;ndash;解码器范式，但学习目标是：在 RGB-D 输入条件下，预测稠密、像素级对齐的场景深度。&lt;/p&gt;&lt;p&gt;与传统 MAE 方法最大的不同在于，MDM 并不依赖人为构造的随机掩码。相反，它直接利用 RGB-D 相机在真实世界中天然产生的深度缺失区域 &amp;mdash;&amp;mdash; 也就是那些由透明、反光、弱纹理等复杂成像条件引发的「孔洞」，作为训练时的掩码信号。&lt;/p&gt;&lt;p&gt;这一转变看似简单，却极具挑战性。因为这些自然掩码并非随机分布，而是高度集中在视觉和几何最模糊的位置，其重建难度远高于随机丢弃的 patch。换句话说，模型必须真正理解 RGB 外观与几何结构之间的关系。&lt;/p&gt;&lt;p&gt;为此，MDM 在架构上明确引入了一个关键约束：RGB 信息始终完整可见，深度信息则存在真实缺失。模型被迫在「完整的视觉上下文」和「残缺的几何观测」之间建立联合推理能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbg75nNz4wuia9ic7J22z8w320npeDKkkjCSQ2wLSkcmc9g26MvVmqTmKw/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-ratio="0.5092592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530197" data-aistatus="1" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/e2a58477-d2f3-4231-9c80-ca2020678e2a/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;MDM 框架示意图。对应于传感器缺失测量的深度标记会被掩码，ViT 编码器基于上下文标记（即 RGB 图像）以及剩余未被掩码的深度标记，学习联合 Embedding 表示。在解码阶段，潜在的深度标记被丢弃，解码器仅依赖潜在的上下文标记重建完整的深度图。右下角展示了一幅未被掩码的深度图，作为参考。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;面向 RGB-D 的 ViT 设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在具体实现上，LingBot-Depth 采用了标准的 ViT-Large 作为编码器主干，但对输入建模方式进行了针对 RGB-D 场景的定制。&lt;/p&gt;&lt;p&gt;RGB 图像与深度图通过两套独立的补丁嵌入层进行处理，分别生成在同一空间网格上对齐的 RGB token 和深度 token。这种设计使得 Transformer 的自注意力机制能够在同一空间位置上，同时建模外观语义与几何线索的交互关系。&lt;/p&gt;&lt;p&gt;此外，为避免不同模态在注意力计算中「混淆身份」，模型还显式引入了模态嵌入，与二维空间位置编码共同构成每个 token 的位置信息。这种处理方式，使得 ViT 能够在统一的序列中区分这是「 RGB 信息」还是「深度信息」，同时保留空间一致性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;利用深度缺陷，而不是回避它&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在掩码策略上，MDM 并未简单地对所有缺失深度一刀切。考虑到真实 RGB-D 数据中，完全没有深度缺失的样本同样具有重要价值，模型采用了一种基于补丁统计的自适应掩码策略：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对深度值完全缺失的 patch，必然作为掩码；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;对同时包含有效与无效测量的 patch，提高其被掩码的概率；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;若仍未达到目标掩码比例，再从完全有效的深度 patch 中进行随机补充。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一策略的核心目标，是在保证训练难度的同时，尽可能保留「不完美但有信息量」的深度观测，让模型学会在真实、不干净的数据分布下进行推理。&lt;/p&gt;&lt;p&gt;这也正是 LingBot-Depth 在方法层面最具启发性的地方，它开创性地尝试让模型理解噪声背后的物理与视觉规律。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;领先的精度，落地的性能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;LingBot-Depth 让模型在预训练阶段就直面不完整、带噪声的深度世界，会显著增强它对真实三维结构的理解能力，并在多个下游任务中持续受益。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;专业对口：深度补全&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;MDM 的核心思想，是在深度存在大量缺失和噪声的情况下，让模型学会利用 RGB 上下文和残余深度信息去「脑补」完整的几何结构。因此，第一个被检验的任务，自然是深度补全（Depth Completion）。&lt;/p&gt;&lt;p&gt;研究团队将基于 MDM 预训练得到的模型 LingBot-Depth，与多种当前主流方法（如 OMNI-DC、PromptDA、PriorDA）进行了正面对比，并设计了两种极具现实意义的评测协议。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;区块级深度缺失：模拟深度相机的「翻车现场」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在第一种协议中，研究团队通过随机抹掉真实深度图中的成块区域，来模拟真实传感器中常见的深度丢失现象；同时，还人为加入高斯噪声和类似 Kinect 的散粒噪声，以还原量化误差、光子噪声等传感器伪影。&lt;/p&gt;&lt;p&gt;结果非常直接：在所有数据集、所有难度级别下，LingBot-Depth 均稳定超越全部对比方法。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbkOAatlRUzRqIOn4vOvQItmibkrlA9FlwfROBzwRsGgltzpREOVqPhXg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-ratio="0.5351851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530198" data-aistatus="1" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/ff6e21d9-032b-4b4c-99d8-d0efce5e872c/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;深度补全结果。（a）在 iBims、NYUv2 和 DIODE 数据集上，采用四个难度级别的区块级深度掩码进行评估。（b）在 ETH3D 数据集上，使用稀疏 SfM 深度输入进行评估。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbVqic5FgDsEGxrYtApwmXSO8LzSaBbB2YtqUH18iaWo9ZcPW9Ilp6uZpQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-ratio="0.35648148148148145" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530199" data-aistatus="1" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/fdc7de7b-3b92-4832-ada9-aec1dde9984d/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;尤其是在「极端」条件下，其 RMSE 相比此前表现最好的 PromptDA 仍有显著下降，说明模型并不是靠「记住干净数据」，而是真正学会了在结构严重缺失、测量高度不可靠的情况下恢复合理的三维形状。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;稀疏 SfM 深度：更复杂的现实问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;第二种协议进一步拉高了难度：输入不再是密集但有缺失的深度图，而是极度稀疏的 SfM / SLAM 点云。在很多真实应用中，当深度相机不可用时，这是获取几何信息的唯一途径。从定性结果来看，它生成的深度边界更加清晰，结构连续性更强，尤其在遮挡严重或观测稀疏的区域，优势尤为明显。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不止补全：单目深度估计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;深度补全之外，蚂蚁灵波研究团队进一步追问一个更本质的问题：如果模型在预训练阶段学会了 RGB 与深度之间的对应关系，这种能力是否能迁移到「只有一张 RGB 图像」的单目深度估计任务中？&lt;/p&gt;&lt;p&gt;为此，他们将 LingBot-Depth 的 RGB 编码器作为预训练主干，替代目前广泛使用的 DINOv2，用于初始化 MoGe 模型。&lt;/p&gt;&lt;p&gt;需要注意的是，在这一设置下，模型在推理阶段完全不再接触深度输入 &amp;mdash;&amp;mdash; 深度分支和解码器被全部移除，考察的是「几何理解是否已内化进编码器」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbPPcgGGyYeP4GXTtN9HVQBvwH3lruwHzAdyCwIcLLadxhU2icw1NKTtQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-ratio="1.239814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530200" data-aistatus="1" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/a22de475-a6f9-4b29-8b0e-e9927901ea7e/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;在四个基准数据集上的深度补全方法定性对比。对于每个数据集，依次展示了 RGB 输入、稀疏 / 被掩码的深度输入，以及 OMNI-DC、PromptDA、PriorDA 和 LingBot-Depth 方法的预测结果。可以看到，LingBot-Depth 在深度边界上更加清晰、结构更加完整，尤其是在存在严重遮挡或观测极为稀疏的区域，优势尤为明显。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakb3ia6wwkqefThga6NB5gdfTFjMB6VrLZqc4icqdH42w9U74T9XCpK9f2g/640?wx_fmt=png&amp;from=appmsg#imgIndex=13" data-ratio="0.3861111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530201" data-aistatus="1" data-original-style="null" data-index="15" src="https://image.jiqizhixin.com/uploads/editor/bd4811b7-ebb2-46bb-a0ce-092e52a2f7ae/640.png" alt="图片" data-report-img-idx="13" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;使用不同预训练主干网络（DINOv2 与 MDM 方法）的 MoGe 单目深度估计结果。在 10 个多样化的基准数据集上，从仿射不变、尺度不变以及视差不变三类评价指标出发，系统评估了深度预测和点云映射的精度表现。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;实验结果给出了肯定答案：&lt;/p&gt;&lt;p&gt;在多个测试数据集上，基于 MDM 预训练的编码器稳定地优于 DINOv2 初始化的模型，并且表现出更好的泛化能力。&lt;/p&gt;&lt;p&gt;这说明，这一方法的确是一种能够&lt;strong&gt;将三维几何知识压缩进视觉表示中的预训练机制&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;总体而言，LingBot-Depth 依托真实复杂场景数据进行训练，使模型能够覆盖更多透明、反光和极端光照等长尾情况，因而具备更稳定的泛化能力；同时，其对深度空洞与噪声的有效修复，显著提升了深度图的完整性与边界质量。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;落地，已就绪&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;性能再强，我们当然也不希望它只停留在实验室里。毕竟，「跑分」从来不是终点 &amp;mdash;&amp;mdash; 只有那些真正走进真实场景、能够稳定支撑工业生产和机器人应用的模型，才是行业值得拥抱的模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三维世界的稳定追踪&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了检验模型实际落地的能力，LingBot-Depth 被进一步接入到在线三维追踪任务 &lt;strong&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"class":"p1"},"namespaceURI":"http://www.w3.org/1999/xhtml"},"node",{"tagName":"b","attributes":{},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;SpatialTrackerV2&amp;nbsp;&lt;/span&gt;&lt;/strong&gt;中，来看其是否真的能够支撑更复杂、更长链路的几何应用。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbwnNQjUuQF35mLNrdrEa2AngsQa9xDr7xia4dibBnAPAbyc2MsNgGjwxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=14" data-ratio="0.30277777777777776" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530219" data-aistatus="1" data-original-style="null" data-index="16" src="https://image.jiqizhixin.com/uploads/editor/aa025f87-f467-49f7-a302-55dce866b8a1/640.png" alt="图片" data-report-img-idx="15" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;相机追踪与场景重建结果。从左到右依次为：RGB 输入图像、原始传感器深度图、模型生成的精细深度图、估计得到的相机轨迹，以及最终重建的场景几何结构。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果首先体现在&lt;strong&gt;相机追踪&lt;/strong&gt;上。在包含大量玻璃与反光表面的室内场景中，替换为 LingBot-Depth 补全后的深度图后，输出的相机轨迹明显更加平滑、连续且稳定。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbSmU8v95mnF7ly1GoP1Hx21oeMajc1ibEs1VkI1QOvvoAXJEUfxWOUnA/640?wx_fmt=png&amp;from=appmsg#imgIndex=15" data-ratio="0.5398148148148149" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530220" data-aistatus="1" data-original-style="null" data-index="17" src="https://image.jiqizhixin.com/uploads/editor/e354a471-44cb-4d68-937f-5aea6170d08b/640.png" alt="图片" data-report-img-idx="14" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;动态三维点追踪结果。上：目标物体上的查询点；中：被持续追踪的三维轨迹（按时间以彩虹色编码）；下：对应的深度图结果。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;更进一步，&lt;strong&gt;动态三维追踪&lt;/strong&gt;具有十足的可靠性。基于 LingBot-Depth 输出的深度，SpatialTrackerV2 能够恢复出连贯一致的三维运动路径，彩色轨迹在空间中呈现出清晰的结构与稳定的时序关系。&lt;/p&gt;&lt;p&gt;从应用角度来看，LingBot-Depth 已经具备作为基础感知能力直接嵌入现有三维系统的成熟度。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;真实灵巧手的实战验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要验证 LingBot-Depth 是否真正具备真实世界可用性，最直接、也最有说服力的方式，便是将其&lt;strong&gt;直接接入真实的灵巧抓取系统进行实机验证&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;验证系统由 Rokae XMate-SR5 机械臂 + X Hand-1 灵巧手 和 Orbbec RGB-D 相机组成，深度图先被转为点云，再用于预测抓取姿态。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbDJsfCCO4iaWnQmV3mIM5NrJ8UGiaPhkQ1DXvkjUsjvoXd44CGvVWMnqw/640?wx_fmt=png&amp;from=appmsg#imgIndex=16" data-ratio="0.29907407407407405" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530202" data-aistatus="1" data-original-style="null" data-index="18" src="https://image.jiqizhixin.com/uploads/editor/40bf626a-341c-4add-9d4d-4f1b5ffc38c0/640.png" alt="图片" data-report-img-idx="16" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;抓取实验的定性结果。左图：包含机械臂、灵巧手和深度相机的硬件系统示意。右图：四个目标物体的 RGB 图像、原始传感器深度图，以及 LingBot-Depth 方法生成的精细深度图。对于反光物体（钢杯）和透明物体（玻璃杯、收纳盒），原始深度图严重缺失，而 LingBot-Depth 的方法能够生成完整且几何上准确的深度图。&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbYAgBbodfESxXF6uVtMShyBd8dKC8uzbEf3jKfD2XQSF0e4yL1kw2hw/640?wx_fmt=png&amp;from=appmsg#imgIndex=17" data-ratio="0.4740740740740741" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503530203" data-aistatus="1" data-original-style="null" data-index="19" src="https://image.jiqizhixin.com/uploads/editor/82d37dbb-4128-486e-9d6c-1609c6e49dd0/640.png" alt="图片" data-report-img-idx="17" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;抓取姿态生成与真实世界执行。上图：将预测的抓取姿态以灵巧手形式叠加在由精细深度重建的点云上进行渲染。下图：机器人系统在每个目标物体上成功执行抓取的场景。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;结果非常直观：在包含透明、反光物体的真实场景中，使用原始传感器深度时，部分物体（如透明收纳盒）因深度大面积缺失完全无法抓取；而使用 LingBot-Depth 补全后的深度，系统能够恢复合理几何结构，&lt;strong&gt;抓取成功率显著提升&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakb1sueVzvPAIUANjINKxbzmmibvFBSQjsnx70WjP9a3TLqcCJ8sPhQ1DQ/640?wx_fmt=gif&amp;from=appmsg#imgIndex=18" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530204" data-aistatus="1" data-original-style="null" data-index="20" src="https://image.jiqizhixin.com/uploads/editor/68863782-c70f-4b93-bba6-10e6dd4ea6ee/640.gif" data-order="3" alt="图片" data-report-img-idx="18" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 灵巧手抓取反光不锈钢杯&lt;/sup&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbHGIqAWMDq0r8Pibpj6F7mAvmXKbb3CE5Yua6gR4SyDBic1qctgib8x1Cw/640?wx_fmt=gif&amp;from=appmsg#imgIndex=19" data-ratio="0.5625" data-s="300,640" data-type="gif" data-w="960" type="block" data-imgfileid="503530207" data-aistatus="1" data-original-style="null" data-index="21" src="https://image.jiqizhixin.com/uploads/editor/4e4a8105-6c0c-4e48-a014-d90b38472c84/640.gif" data-order="4" alt="图片" data-report-img-idx="19" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 灵巧手抓取透明玻璃杯&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在真正的灵巧手交互中，借助 LingBot-Depth 的能力抓起反光的不锈钢杯和完全透明的玻璃杯完全不在话下，轻而易举。&lt;/p&gt;&lt;p&gt;此外， LingBot-Depth 在蚂蚁灵波团队的努力下，&lt;strong&gt;已经完成了模型的轻量化，并完全做好了落地的应用准备&lt;/strong&gt;。&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，LingBot-Depth 模型的&lt;strong&gt;部署非常灵活&lt;/strong&gt;：它无需更换现有的 RGB-D 或 3D 相机硬件，就能作为算法增强模块直接嵌入现有系统，大幅降低升级成本和工程门槛。&lt;/p&gt;&lt;p&gt;同时，模型&lt;strong&gt;完全开源、可复现&lt;/strong&gt;，便于研究者和产业团队快速进行验证、二次训练和工程化集成，加速从实验室到真实场景的落地应用。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;具身智能具备识别复杂光照环境，甚至精准捕捉透明，反光材质物体的能力，就已标志着通用级具身智能落地的一大障碍的突破。&lt;/p&gt;&lt;p&gt;归根结底，具身智能的发展不仅是算法迭代的赛跑，更是对行业认知和落地能力的考验。&lt;/p&gt;&lt;p&gt;LingBot-Depth 展示了一种思路的升级：面对真实世界的复杂性，在硬件受限的情况下，如何运用算法与数据、模型与物理认知的深度融合，来提升对真实世界的感知能力，是未来通用具身智能的核心方向。&lt;/p&gt;&lt;p&gt;蚂蚁灵波将 LingBot-Depth&lt;strong&gt; 完全开源&lt;/strong&gt;，用户可以通过开源仓库获取模型权重、推理代码、评测脚本与使用文档，快速上手实验与验证；如需面向具体相机型号或机器人平台进行工程集成和性能调优，也可以对接官方的合作与技术支持渠道。&lt;/p&gt;&lt;p&gt;开放与可落地的策略，将深刻影响人工智能向现实价值转化的速度和格局。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>性能比肩Gemini 3 Pro！昨晚，阿里千问最强模型来了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 27 Jan 2026 13:23:08 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-27-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-27-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;1 月 26 日深夜，阿里千问旗舰推理模型 Qwen3-Max-Thinking 正式上线。&lt;/p&gt;&lt;p&gt;该模型在科学知识（GPQA Diamond）、数学推理（IMO-AnswerBench）、代码编程（LiveCodeBench）等多项权威基准测试中刷新纪录，其综合性能已可对标 GPT-5.2 与 Gemini 3 Pro，成为目前最接近国际顶尖水平的国产大模型之一。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="8294" data-imgfileid="503530211" data-ratio="0.47962962962962963" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbsblzhhT5icU0h5dMjkicM59iafJ9deZnt2vz3ibRj5J2uhZYE1uglj4icUQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-width="17277" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/d741854d-6dc9-4feb-9515-92b568791c85/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;下表为更全面的评估分数：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="1051" data-imgfileid="503530212" data-ratio="0.9731481481481481" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9tKRukAx52UUp5uX23jiakbJYqRVhBVWbRpTicLjWMxJo8sGjwdia8qOU3UBmDbprB0SeVGbVJW1ibxg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" data-width="1080" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/dbfaac36-017c-472f-b8d9-e49eae01ba44/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;据了解，Qwen3-Max-Thinking 总参数量超万亿（1T），预训练数据量高达 36T Tokens，是阿里目前规模最大、能力最强的推理模型。&lt;/p&gt;&lt;p&gt;此前，预览版 Qwen3-Max-Thinking 已展现出不俗实力。基于这一基础，通义团队进一步扩大了强化学习后训练规模，对模型进行了系统性优化，使正式版在多项核心能力上实现整体跃升。&lt;/p&gt;&lt;p&gt;在覆盖事实知识、复杂推理、指令遵循、人类偏好对齐以及 Agent 能力等 19 项主流评测基准中，Qwen3-Max-Thinking 取得多项领先成绩，刷新了多项最佳纪录，其综合表现已进入与 GPT-5.2-Thinking-xhigh、Claude Opus 4.5、Gemini 3 Pro 同一竞争梯队。&lt;/p&gt;&lt;p&gt;真实表现如何，我们上手体验了一下。&lt;/p&gt;&lt;p&gt;我们输入提示：帮我做一个技能五子棋的游戏网页，要求是在普通的五子棋规则上，玩家可以使用技能。直接给我个 html 文件。&lt;/p&gt;&lt;p&gt;一会儿工夫，Qwen3-Max-Thinking 就嗖嗖甩出 1000 多行代码，把一个可交互、能上手就玩的五子棋直接写完整了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbxqias7eOPytS1bePicyR4DCA7LeC6Jj6dzAFHyibhqXBQM05nltHUsh9A/640?wx_fmt=gif&amp;from=appmsg#imgIndex=3" data-ratio="1.0064874884151993" data-s="300,640" data-type="gif" data-w="1079" type="block" data-imgfileid="503530213" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/4dac559d-e342-4ce1-a2ee-f0e0f8546764/640.gif" data-order="0" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;下一项测试，我们让 Qwen3-Max-Thinking 生成一个跳一跳游戏。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbShoQzn3UMbUuq6zUtIQ8ebsFjadaRdSezicNqx84g3GkAUBrY7OWr3w/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="1.284274193548387" data-s="300,640" data-type="gif" data-w="496" type="block" data-imgfileid="503530214" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/32e17d6e-5e72-4582-86ea-7bac2ade6df5/640.gif" data-order="1" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;请用纯 HTML + CSS + 原生 JavaScript 写一个可在浏览器直接打开的《跳一跳》小游戏（不要依赖任何外部库）。要求：画面：简洁 2D 即可（canvas 或 DOM 都行）；操作：按住蓄力、松开起跳（按住时间决定跳跃距离）；规则：从一个平台跳到下一个平台，落空则结束；生成：平台位置随机，但保证可达（不要生成必死局）；计分：落在平台上加分，连跳加成可选；体验：有起跳动画、落地判定、失败提示、重新开始按钮；代码：完整可运行，放在一个 HTML 文件里，注释清晰。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这个游戏最难的地方，就在于按住鼠标的时间既是操作，也是赌注：短了跳不过去，长了直接飞过头，容错窗口小到离谱。第一跳很容易失误，然后就 Game Over。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9tKRukAx52UUp5uX23jiakbK6EhLEG6tzrag50UcsicOYANxicA0nibic7ZInr7yibcyvjicew65ibwLMQDg/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-ratio="1.3165618448637317" data-s="300,640" data-type="gif" data-w="477" type="block" data-imgfileid="503530215" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3368d91b-1a3a-40f2-ba5f-f6b1c16fcab4/640.gif" data-order="2" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;体验地址：https://chat.qwen.ai/&lt;/p&gt;&lt;p&gt;&lt;strong&gt;测试时扩展的重新定义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;推动 Qwen3-Max-Thinking 的核心创新在于其对传统推理方式的突破。与大多数模型按线性方式逐 token 生成不同，Qwen3 引入了一种由测试时扩展（Test-time scaling）驱动的 Heavy Mode（重推理模式）。&lt;/p&gt;&lt;p&gt;通俗来说，这一技术让模型能够用更多算力换取更高智能水平。但它并非简单的 best-of-N 采样方式，例如一次生成 100 个答案再从中选出最优结果 &amp;mdash;&amp;mdash; 而是采用了一种经验累积的多轮推理策略。&lt;/p&gt;&lt;p&gt;这种方法更接近人类的解题过程。当模型面对复杂问题时，它不会直接给出一次性猜测，而是进入反复自我反思与迭代推理。通过一种专有的 take-experience 机制，模型能够从此前的推理步骤中提炼有效经验，从而实现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;识别死胡同：在无需完整走完错误推理路径的情况下，判断某条推理思路正在失效；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;聚焦算力：将计算资源重新分配到尚未解决的不确定点，而不是反复推导已经得到的结论。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种机制带来了实实在在的效率提升。通过避免冗余推理，模型可以在同样的上下文窗口中整合更丰富的历史信息。千问团队表示，该方法在不显著增加 token 成本的前提下，实现了性能的大幅跃升：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GPQA（博士级科学问题）：得分从 90.3 提升至 92.8；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LiveCodeBench v6：成绩从 88.0 提升至 91.4。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;自适应工具调用&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说推理能力决定了模型会不会想，那么工具调用能力决定的，是模型能不能真正把事做成。在 Qwen3-Max-Thinking 中，通义团队不再将推理与工具使用视为两个割裂的阶段，而是将工具能力内生进思考过程本身，构建起一种边思考、边行动的原生 Agent 式模型框架，让大模型从静态的文本推理，迈向可执行、可验证的复杂任务处理。&lt;/p&gt;&lt;p&gt;在完成基础的工具使用微调后，通义团队进一步在大量多样化任务上，引入基于规则奖励与模型奖励的联合强化学习训练，使模型学会何时调用工具、如何结合工具展开推理，而不是机械执行指令。由此，Qwen3-Max-Thinking 获得了更具策略性的工具协同思考能力。&lt;/p&gt;&lt;p&gt;这一自适应工具调用能力已在 QwenChat 中完整落地：模型可自主调度搜索、个性化记忆与代码解释器等核心 Agent 工具，在一次交互中完成信息获取、计算推演与结论生成，回答更贴近专业人士的工作方式，也显著降低了模型幻觉，为解决真实世界中的复杂问题奠定基础。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;截至 2026 年 1 月，阿里通义千问（Qwen）系列模型在 Hugging Face 平台上的累计下载量超过了 10 亿次，这一数据使得 Qwen 成为了 Hugging Face 上最受欢迎、下载量最高的开源 AI 模型系列之一。&lt;/p&gt;&lt;p&gt;Qwen3-Max-Thinking 的推出代表着 2026 年人工智能市场的成熟。它将讨论的焦点从谁拥有最智能的聊天机器人转移到谁拥有功能最强大的智能体。通过将高效率推理能力与自适应、自主的工具调用机制相结合，Qwen 已经牢牢确立了自己在企业级 AI 竞争格局中的领先地位。&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：https://venturebeat.com/technology/qwen3-max-thinking-beats-gemini-3-pro-and-gpt-5-2-on-humanitys-last-exam&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
