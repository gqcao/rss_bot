<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-b906e030b985e6914990f04d1a50f7b99e514db6f9a9f5c6799dd321730a7549.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>Vidu Q3 全球屠榜， 击败马斯克xAI Grok、 OpenAI Sora、Google Veo 3，登顶 Artificial Analysis 榜首</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 15:06:20 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2 月 10 日，国际权威 AI 基准测试机构 &lt;strong&gt;Artificial Analysis&lt;/strong&gt;发布最新评测榜单。在该榜单中，&lt;strong&gt;Vidu Q3&lt;/strong&gt;位列 &lt;strong&gt;全球第一&lt;/strong&gt;，综合表现超越 &lt;strong&gt;xAI Grok&lt;/strong&gt;、&lt;strong&gt;Runway Gen-4.5&lt;/strong&gt;、&lt;strong&gt;Google Veo 3.1&lt;/strong&gt;以及 &lt;strong&gt;OpenAI Sora 2&lt;/strong&gt;等一众国际主流模型，展现出全球领先的技术与生产级能力。&lt;img src="https://image.jiqizhixin.com/uploads/editor/70e673e9-5f3c-44a2-8875-66c12c7b8b59/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;作为全球首个「为剧而生」的视频大模型，Vidu Q3 在评测中展现出面向真实内容生产场景的突出优势：支持 16 秒声画同出、1080P 高清画质，拥有稳定的多镜头叙事与精准切镜能力，并具备多国语言文字渲染及多语言输出功能，可满足漫剧、短剧、影视剧等专业内容创作需求。&lt;/p&gt;&lt;p&gt;值得注意的是，近期 Seedance 2.0 同样在市场侧引发高度关注:-边是 Vidu Q3 登顶权威榜单、屠榜评测指标,另一边是 Seedance 2.0在效果呈现与创作体验上的持续出圈。两条路径各有侧重、各擅胜场，但在&amp;ldquo;效果质量&amp;rdquo;这一核心维度上，两家均已达到国际一流水平，并在多个关键指标上实现对海外模型的整体超越。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>用千万级 MPRA 数据训练深度模型，首次系统性解码人类启动子的调控语法</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Tue, 10 Feb 2026 14:19:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-6</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-pm-slice="0 0 []"&gt;编辑丨&amp;amp;&lt;/p&gt;&lt;p&gt;人类基因表达调控研究中，启动子（promoter）一直处在一个微妙的位置：它们在功能上至关重要，但在方法上却长期被「简化处理」。传统模型往往依赖表观组信号（如染色质开放性、组蛋白修饰）来推断启动子活性，本质上捕捉的是&lt;strong&gt;相关性&lt;/strong&gt;而非&lt;strong&gt;序列层面的因果规则&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这导致一个根本问题始终悬而未决：&lt;strong&gt;如果只给 DNA 序列本身，研究者是否真的能预测启动子的转录驱动能力？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;来自荷兰 Oncode 研究所等的研究者们提出了一种名为 &lt;strong&gt;PARM（Promoter Activity Regression Model）&lt;/strong&gt; 的深度学习框架。该模型在实验和计算上都很轻量，因此可以生成针对细胞类型和条件的模型，仅凭DNA序列就能可靠预测基因组中的自主启动子活性。&lt;/p&gt;&lt;p&gt;相关研究内容以「Regulatory grammar in human promoters uncovered by&amp;nbsp;MPRA-based deep learning」为题，于 2026 年 2 月 4 日刊登在《Nature》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5hq6GFHibJv6Gs9jGmwZmArwbcA3oURHGl125qibZm0kwfJJ4YRJ6liarATQm8HCmIVC6AHNicXXWVcPj41g0QKkFh8OjVeRnzlm5Rib7VKwgvsU/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.41509433962264153" data-type="png" data-w="954" data-width="954" data-height="396" data-backw="546" data-backh="227" data-imgfileid="100027397" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/159cda62-8575-4dfb-88b6-3c8534a6b0d4/640.png" alt="图片" data-before-load-time="1770704316522" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：&lt;em&gt;https://www.nature.com/articles/s41586-025-10093-z&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;MPRA &amp;times; 深度学习&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;PARM 的技术路线并不复杂，但非常「克制」。研究的基础数据来自大规模 &lt;strong&gt;MPRA（大规模平行报告基因实验）&lt;/strong&gt;：研究团队系统性地合成并测试了&lt;strong&gt;超过 1000 万条启动子及其变体序列&lt;/strong&gt;，在统一实验条件下测量其转录活性。&lt;/p&gt;&lt;p&gt;这些数据的关键价值在于两点：第一，MPRA 将序列与表达输出直接绑定，天然具有&lt;strong&gt;因果属性&lt;/strong&gt;；第二，规模足够大，允许模型学习到超越单一 motif 的高阶调控模式。&lt;/p&gt;&lt;p&gt;在模型设计上，PARM 采用的是结构相对简洁的一维卷积神经网络（CNN），参数量控制在 &lt;strong&gt;约 72 万&lt;/strong&gt;，远低于当前主流的基因调控大模型。研究团队明确强调，他们的目标并非「参数规模竞赛」，而是验证：&lt;strong&gt;高质量因果数据是否足以支撑高精度启动子建模&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f9f1fdd9-ee21-4ca3-b439-6d3f4fbf8ca0/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20260210143923_28_629.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;图1:PARM原理及验证。&lt;/p&gt;&lt;p&gt;通过该平台，数据生成和计算建模的成本都有所降低，这一进展使研究团队能够构建十种不同细胞类型中所有人类启动子的序列到活性模型，并在细胞暴露于多种刺激后进行。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能与验证&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在多个独立测试集上，PARM 对启动子活性的预测与实验测量结果之间的相关系数最高可达 &lt;strong&gt;R &amp;asymp; 0.9&lt;/strong&gt;。这一性能并非是训练集的功劳，而是建立在未见过的天然人类启动子序列、合成启动子、含有系统性突变的序列库之上，该模型在多种场景验证下，均保持了相当不错的稳定性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5hq6GFHibJv72VpDAGLwPfeOahGmpo3KHxOHznrtdHQibH6ica6XTMXe3jLxKtvZK5ia9QJMofW4bKLGDL3wDYjdYicC9tet7SAvzaUpaWqZqaia4/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.9532846715328467" data-type="png" data-w="685" data-width="685" data-height="653" data-backw="546" data-backh="520" data-imgfileid="100027399" data-aistatus="1" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b9fab212-6b1a-4eeb-8014-29ee8ae6eaba/640.png" alt="图片" data-before-load-time="1770704317056" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 2：PARM 的单核苷酸功能预测。&lt;/p&gt;&lt;p&gt;团队主要采用的是基于捕获的策略，创建了对人类基因组文库中启动子重叠片段高度富集（90%）的新 MPRA 文库。这一思考建立在团队仅使用启动子重叠片段的数据来训练 PARM 的推论之上。&lt;/p&gt;&lt;p&gt;一个文库包含 400 万个足够代表的独特片段，约比全基因组文库少 600 倍。后续的所有实验均采用该文库作为实验来源。当应用于 K562 和 HepG2 细胞时，PARM 的启动子活性和整体预测力均与全基因组 MPRA 数据相当。&lt;/p&gt;&lt;p&gt;当研究者将 PARM 与依赖 ATAC-seq、ChIP-seq 等表观组输入的模型进行比较时发现：&lt;strong&gt;在启动子层面，纯序列模型已经可以达到相当、甚至更稳定的预测能力&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5hq6GFHibJv6nHKq5iarf0pFg36uqn3QvIh2Nxh8TpckAsTLpcSKt0N5PSzTqOibZp1kr6ic63o6icw4anvL30piagMcsyQ35q6yEy3KthKJj3oBk/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.9386861313868613" data-type="png" data-w="685" data-width="685" data-height="643" data-backw="546" data-backh="513" data-imgfileid="100027398" data-aistatus="1" data-original-style="width:100%;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7801a472-03a1-4511-9aaf-51d8cfd226fc/640.png" alt="图片" data-before-load-time="1770704317088" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图 3：RS的明显优先定位。&lt;/p&gt;&lt;p&gt;此外，模型还揭示了激活型与抑制型调控元件在空间分布上的系统性差异，这些规律并未完整记录在现有注释数据库中。在此基础上，研究团队进一步测试了 PARM 的生成能力。他们利用模型进行序列优化，生成了一批&lt;strong&gt;并不存在于人类基因组中的合成启动子&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;去表观组依赖&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;PARM 补充了其他深度学习方法，可用于建模增强子元素的语法或设计人工启动子。它证明了在小型功能基因组数据集上训练的轻量级模型，是大规模建模工作的可行且强大的替代方案。&lt;/p&gt;&lt;p&gt;未来的优化中，进一步优化聚焦的MPRA库和深度学习方法论，可能会提升性能。团队强调，PARM应被视为一种还原主义模型，能够洞察启动子的基本特性。这为后续研究留下了一个明确方向&amp;mdash;&amp;mdash;不是一味扩大模型规模，而是思考：哪些生物学层级，真的需要多模态；哪些层级，序列本身已经包含足够信息？&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>穹彻智能完成A轮融资｜构建软硬件一体化产品矩阵、推动机器人“通用大脑”落地</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 12:08:31 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-5</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;近日，&lt;strong&gt;穹彻智能宣布完成 A 轮融资，&lt;/strong&gt;&lt;strong&gt;本轮&lt;/strong&gt;&lt;strong&gt;融资金额达数亿元人民币&lt;/strong&gt;，由C资本领投，Sea Limited，普华资本等多家海外产业方和国内头部财投跟投，老股东Prosperity7 Ventures超额追投。此次融资将进一步加速穹彻智能具身大脑的研发迭代、多场景的商业化落地，联动更多产业方加速国际化进程，夯实公司在具身智能领域的领先地位。&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UtsQzWpeegNzWzhgYLaQtnFK7vgGwHvFtI6E7Bm1CP0VXyabm4ufNEpA53DNYNWW4trQuBe43ZCfVwDsKDhtiaNS9d07xjM18kT7mYYlrURM/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=0" data-ratio="0.562962962962963" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="578" data-backh="325" data-imgfileid="100001935" data-aistatus="1" data-original-style="width:100%;" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/87317992-0383-4a70-964b-429dda149966/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;成立两年多以来，穹彻智能在具身智能数据定义与工具链开发取得显著突破。针对具身智能数据采集难、规模化痛点，首次提出&lt;strong&gt;&amp;ldquo;伴随式数据采集&amp;rdquo;&lt;/strong&gt;理念，并自研外骨骼CoMiner、口袋机采RoboPocket等数采设备，以具身数字基因构建物理世界数据库，打造了涵盖数据采集、数据管理、硬件支撑的全链路数采工具平台，并与优必选等&lt;strong&gt;头部人形机器人企业、地方大型数采中心&lt;/strong&gt;及&lt;strong&gt;跨国数采基地&lt;/strong&gt;达成战略合作。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UtsQzWpeegPgDmia3FicEuYia6YwxS1ic06a6xHetAhbgPMo5cLBfTFoILjeaibVibAArlXBiaUG3oQFyAiacHicMibZNyXucaiaM2WNOtN2MU8jkAdg5A/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="0.6657407407407407" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="578" data-backh="385" data-imgfileid="100001929" data-aistatus="1" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/147eba34-c9ef-4906-90df-a7ead85b4189/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;商业化方面，穹彻智能已推出搭载自研大脑的&lt;strong&gt;软硬件一体解决方案&lt;/strong&gt;，并携手酒店服务、零售药房等领域产业巨头，推动在&lt;strong&gt;真实场景中&lt;/strong&gt;的批量部署。其中，智能药房场景已实现&lt;strong&gt;全流程落地&lt;/strong&gt;，机器人可完成接收订单、取货、打包全流程长任务作业，充分验证了具身智能技术在真实商业环境中的可用性与可靠性。同时，公司深耕酒店洗衣、家庭整理、物流分拣等多元应用场景，彰显出技术的高度泛化，并持续打磨快速部署能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/J1ib4iazxrjSkTQSznlicQwdmUicgvcv4rYdJ3Hofkm9rAwUA4cib9df2ZMeu0rIzs2jYAibaelOy0Npht5nGnyRCMzQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.75" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="433" data-imgfileid="100001918" data-aistatus="1" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/735c8a3c-78e1-4522-930f-b62be3045e1b/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;投资方观点&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;本轮领投方 &lt;strong&gt;C资本&amp;nbsp;&lt;/strong&gt;表示，&amp;ldquo;2026年具身智能的scaling law将加速演绎，正迎来从GPT到GPT-3的关键临界点，而穹彻智能是国内端到端具身通用大模型的引领者，不仅在模型的泛化性与复杂操作能力上明显领先，而且具备前沿的模型算法迭代思路以及高效运转的数据引擎，持续推动行业对数据的理解与方法论升级。穹彻智能仅用2年时间在食品制造，商业零售，家庭服务等多个场景快速实现了从技术研发到产品落地的完整闭环。我们坚定看好公司在下一代世界模型研发上的创新突破潜力，更期待与团队深度携手，依托 C资本在遍布亚洲、欧洲、澳洲等地区的全球化资源网络，助力穹彻智能完成海外场景的深度落地与深耕。&amp;rdquo;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;本轮投资方 &lt;strong&gt;普华资本&amp;nbsp;&lt;/strong&gt;表示，&amp;ldquo;穹彻智能凭借原创具身智能基座模型和顶尖学术团队，与非夕生态深度协同，已在具身大脑领域建立领先优势，契合行业对高泛化、高可靠系统的核心需求。前瞻的技术路线与产业协同，为 Noematrix Brain 的规模化落地奠定高确定性基础，将助力公司快速迈向多场景商业运营，构建坚实且可持续的技术壁垒。我们相信，凭借团队卓越的执行力与对行业的深刻洞察，穹彻智能将以底层创新赋能全行业，推动智能机器人加速普及。&amp;rdquo; 普华资本作为中国本土领先创投机构，长期关注科技创新与新兴产业，未来在具身智能赛道持续布局并深度赋能企业发展。&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;本轮投资方 &lt;strong&gt;Prosperity7 Ventures&lt;/strong&gt; 执行董事总经理 Raed Twaily 表示：&amp;ldquo;具身智能将成为全球未来机器人和智能系统的关键基础。穹彻智能构建了一个以真实世界数据、可扩展模型以及快速场景部署为核心的差异化平台。我们将持续支持穹彻智能及其国际化扩张，其技术在全球范围内应对广泛的工业与服务场景方面拥有巨大潜力。&amp;rdquo;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;穹彻智能感谢新老投资方对公司的支持与信任，本轮融资将重点用于&lt;strong&gt;核心技术迭代及产品研发、规模化落地及全球化布局&lt;/strong&gt;。公司将充分结合新老投资方的产业及地域优势，持续夯实软硬件一体的核心能力，坚守技术创新初心，打造真正可落地的智能机器人&amp;ldquo;通用大脑&amp;rdquo;。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>想让机器人春晚包饺子？阿里达摩院：别急，先把「大脑」优化一下</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 10 Feb 2026 12:06:17 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-4</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜张倩&lt;/section&gt;&lt;p&gt;2026 年，那么多机器人上春晚，能给大家表演个包饺子吗？相信这是很多人会好奇的一个问题。&lt;/p&gt;&lt;p&gt;但根据最近的彩排报道，这个可能性不大，机器人更有可能被设计为托着托盘呈上饺子。&lt;/p&gt;&lt;p&gt;其实业内人士都知道，如果不靠编程或摇操，让机器人包饺子这事儿远比移动、导航要复杂，还涉及到「饺子皮」这种堪称机器人图灵测试的柔性物体，没有一个足够聪明的「大脑」肯定是做不到的。这也是为什么，在过去的一年，&lt;strong&gt;越来越多的研究力量和资金开始涌向「大脑」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;阿里达摩院最近的一项工作 &amp;mdash;&amp;mdash;&lt;strong&gt;RynnBrain &lt;/strong&gt;也瞄准了这一方向。不过和一些表演叠衣服、做早餐的研究不同，他们关注的问题还要更底层一些：如果机器人在做家务的时候被打断，临时去门口接收快递，它还能不能回来接着刷碗？如果机器人被要求完成一件需要借助很多工具的任务，它制定的计划会不会包含手头压根没有的工具？&lt;/p&gt;&lt;p&gt;在关于机器人的各种宏大叙事里，这些问题可能没有那么起眼，甚至连相关的 benchmark 都是缺失的，但却是机器人走出实验室必须迈过的门槛。在 RynnBrain 的构建中，达摩院具身智能团队选择从底层出发，将&lt;strong&gt;时空记忆&lt;/strong&gt;和&lt;strong&gt;物理空间推理&lt;/strong&gt;直接训进模型里，并且达到了不错的效果，&lt;strong&gt;在 16 项具身的 Benchmark 上实现了 SOTA&lt;/strong&gt;。&lt;a href="https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2f1969d3-ec7f-4f6c-a4c0-e0fbeaeb9afa/1770696160428.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;面对「三个面包、两个盘子」的约束条件，模型能够进行空间与长程规划，推导出合理的分配方案，体现了其在受限物理条件下的规划与推理能力。&lt;a href="https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/3cd3d81c-1005-4132-829c-d042aa9ed24d/1770696173283.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;在对杂乱桌面进行分拣规划的过程中，机器人能够在任务被打断后，准确记住已完成的步骤并继续执行，展示了多任务交错下的记忆与规划能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;达摩院还一口气开源了 RynnBrain 全系列共计 7 个模型，其中包括&lt;strong&gt;&amp;nbsp;RynnBrain-30B-A3B&lt;/strong&gt;。这是&lt;strong&gt;业界首个 MoE 具身基础模型&lt;/strong&gt;，其只需要 3B 的推理激活参数就全面超越了当前规模最大的具身基础模型 Palican-VL-72B。使用这个模型，可以让机器人在保持最强大感知和规划能力的基础上拥有更加快速的动作响应和更加丝滑的行为模式。目前，包括模型权重、评测基准及完整训推代码在内的全套资源均已向社区开放。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFstV9pRzEibFoIjLxMU2N44UbmjFiad5vKutG9KiczC059FoqLTf4HCU9dHfnQToe1Uw54a9OsgLpLePxYBX3JZric5QfhnG0BNJo/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.4638888888888889" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532565" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/0d4617cd-44c4-4c6d-b8fa-b20f926fc9d5/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Github链接: https://github.com/alibaba-damo-academy/RynnBrain&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;HuggingFace&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;text-align: justify;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;链接&lt;/span&gt;: https://huggingface.co/collections/Alibaba-DAMO-Academy/rynnbrain&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://alibaba-damo-academy.github.io/RynnBrain.github.io/&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;把大模型塞进机器人？这事儿真没那么简单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;关于具身大脑，业界流传着一个有趣的调侃 &amp;mdash;&amp;mdash;「把 DeepSeek 等大模型放进宇树不就行了」。当然，做过的人都知道这完全行不通。&lt;/p&gt;&lt;p&gt;本质上，&lt;strong&gt;在 2D 世界数据上训练出的模型，在走进物理世界的时候面临的是一个完全不同的环境&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;还是以机器人包饺子任务为例，在原来的 2D 世界里，顶尖的 VLM（视觉语言模型）早就能够看懂包饺子的完整流程，模型的任务本质上是对静态画面做出理解，不需要与环境交互。而在真实的年夜饭厨房 &amp;mdash;&amp;mdash; 那个物品散落、空间逼仄的高熵战场 &amp;mdash;&amp;mdash; 一个仅凭 VLM 语言、视觉经验行事的机器人往往会显得手足无措：比如机器人刚将饺子皮擀好、放上馅料、正准备捏合，但不小心碰倒了旁边的调料瓶，想要拿抹布擦拭但眼前并没有抹布，也想不起来放在哪儿，于是就卡住了。再比如，它「看到」桌上有馅料，便自信地规划出「用挖馅勺取馅」的动作，却对「挖馅勺没有被拿上桌」这一关键缺失视而不见，最终导致任务失败。&lt;/p&gt;&lt;p&gt;这些场景尖锐地揭示出当前通用大模型的局限：它们虽「见多识广」，但在物理世界里往往是「&lt;strong&gt;纸上谈兵&lt;/strong&gt;」，缺乏连续的三维空间感，不懂真实的物理交互逻辑，更难以避免因脱离物理约束而产生的幻觉式规划。&lt;/p&gt;&lt;p&gt;这正是达摩院推出 RynnBrain 所要解决的核心问题。他们的思路是通过系统性地引入时空记忆、物理空间推理等能力，将这个原本「飘在云端」的认知大脑，强行拽回物理世界。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从 RynnEC 到 RynnBrain &amp;nbsp;让大模型长出「物理直觉」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;让大模型扎根物理世界不是一蹴而就的。在 RynnBrain 之前，达摩院还做过一项奠基性的研究 &amp;mdash;&amp;mdash;RynnEC。&lt;/p&gt;&lt;p&gt;简单来说，&lt;strong&gt;RynnEC 就像给大模型赋予一双「眼睛」&lt;/strong&gt;。它可以精确回答关于物体（属性、数量、功能、分割等）或空间（以自我为中心的世界感知 + 以世界为中心的尺度感知）的问题。比如在执行「将平板电脑放到书架上」这个任务时，它会先思考「电脑有多宽，能不能放书架上不掉下来？」；而在伸手拿酱油之前，它会先算一下自己和酱油瓶之间的距离，判断原地不动能不能够得着。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFmicZOOeq5JRO5QouclsTvG5FVymYSWm18HV5mg8icSF5icE6dOicPpA8gpO4yjqKViaQaHzdsVhGAM3iavEq0DmHr2DnfVqFRVDf4M/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.6851851851851852" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532566" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/dba2c584-2223-4d8d-a4f8-91494ec66474/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图源：RynnEC 论文。链接：https://arxiv.org/pdf/2508.14160&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;这双「眼睛」所带来的细粒度认知输入，是连接高层规划与底层控制的关键桥梁。而 RynnBrain 不仅完整地继承了这些能力，还扩展出了多样化的时空记忆以及物理空间推理能力。&lt;/p&gt;&lt;p&gt;先说&lt;strong&gt;时空记忆&lt;/strong&gt;。这一能力的引入，直指当前具身大模型的「视野」痛点。现有的大脑模型往往只能解决当前视野（图片）内的定位任务，一旦需要寻找的目标物体或关键点处于视野之外（比如前面提到的「抹布」），模型便无能为力。尽管业界存在一种通用的「暴力解法」，也就是把所有的历史图片重新过一遍模型来寻找目标，但在达摩院看来，这种方式割裂了时空，忽略了具身场景本质上是一个连续的、整体的三维世界。&lt;/p&gt;&lt;p&gt;因此，RynnBrain 选择了一条更符合认知的路径：它利用历史记忆来帮助模型构建起一个更加完整的三维认知。这意味着，机器人的决策与理解不再受限于眼前的瞬间场景，而是能够真正基于一个完备的三维世界模型进行全局考量。&lt;a href="https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/444aa4c6-e32b-4374-b858-8813f8f28039/1770696234263.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;在复杂的视频变化与干扰下，模型能够持续追踪并识别被使用过的矿泉水，展示了对动态场景中物体的长期记忆与理解能力。&lt;a href="https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f77d0a64-8987-4c6e-8f08-84c99c192f7e/1770696250558.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;机器人在主要物体被移动后，仍能保持对其空间位置的记忆，并将物体准确放回原处，体现了稳定的物体记忆与空间记忆能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;那么，这种「类人」的全局时空回溯是如何实现的？其背后的核心在于涵盖空间、位置、事件、轨迹等多维度信息的「统一表征」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqGXaZG2WkB0W4cy0FJmCGUGn7EP1eUC0aXHESZ7RRp6uVCQrDpNeyUCJ5IJceefemEec0PiauVIVFmbmQhRM7xLbb6lqkXxMs3Y/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.0342592592592592" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532567" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/f864e797-cd86-4653-974d-58ea12b7efea/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; RynnBrain 拥有认知、定位、推理、规划等多种能力&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在复杂的具身交互中，机器人所面对的信息是高度异构的。传统的模型往往难以兼容这些异构的信息，而 RynnBrain 的突破在于，它构建了一套统一的框架，将这些信息全部统一映射到模型的输出空间里。这意味着，模型在「脑海」里处理的不再是割裂的视觉切片，而是将时间维度、空间坐标与语义理解融为一体，从而在底层逻辑上实现了对物理世界的精准「拿捏」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqH0WUDpkPkZnmZOyMicz7dp8fTuu7v2Cqwe898aRbICy2iabs6LXslYRuNwDbtbcXGwKQWSoSicEt4rxIJSewhV0FgN2IAkqFhvn8/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.5416666666666666" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532568" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5dadc89c-1c3e-4c4d-ac7f-8a5d09a7d9c1/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; RynnBrain 模型架构图。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;接下来看&lt;strong&gt;物理空间推理能力&lt;/strong&gt;。在传统的 VLM 中，推理主要发生在语言层面，并未被强制与具体的空间位置或物理状态绑定。模型可能会生成看似完美的计划，比如前面提到的「用挖馅勺取馅」，但实际上，它眼前并没有挖馅勺，也不知道这个工具在哪里。这种「语义与空间解耦」的模式，是导致机器人产生「物理幻觉」的根源。于是，指令发出去了，任务却完不成。&lt;/p&gt;&lt;p&gt;为了消除这种割裂，RynnBrain 采用了一种「文本与空间定位交错」的推理策略。简单来说，就是要求模型「边说边指」。在 RynnBrain 生成推理文本的过程中，每当涉及到具体的物理对象或位置时，它必须同步预测出对应的空间坐标或区域掩码。这种强制性的约束，迫使模型在生成「拿起挖馅勺」这句话的同时，必须在像素级或三维坐标系中精准地标出那个挖馅勺。&lt;/p&gt;&lt;p&gt;通过这种机制，RynnBrain 将抽象的语言逻辑与具象的物理环境强力锁定。这种扎根于物理世界的推理方式，极大地降低了任务执行中的不确定性，让每一个决策 Token 都有据可依。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从 SOTA 刷榜到下游实战 &amp;nbsp;一个「六边形基座」的诞生&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;说了这么多，RynnBrain 到底表现如何？其实，如果只是拿现有的 Benchmark 去测，RynnBrain 的部分能力是很难测出来的，如时空定位、操作点识别等。目前的开源评测基准，普遍缺少对这些细粒度信息理解能力与时空定位能力的评估。&lt;/p&gt;&lt;p&gt;为了填补这一空白，达摩院推出了一个名叫 RynnBrain Bench 的新基准。这个基准涵盖物体认知、空间认知、物体定位、具身点预测四大维度，共计 20 项具身相关任务。它和现有的其他 benchmark 一起，对模型能力提出了综合考验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHy7KJ2X7nDkzYRhsvuY4SUXiariboq3b0ibYU4NfFkCzIvibcjxA7b9Tp1DNicAES2KKia9ribtib3NLFkE7KPOE4NXJOm9T3f03SLwiaA/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.549074074074074" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532569" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/68ebf5b9-fca7-4da1-bb1d-2aad87bcee1c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在这套严苛的「试卷」面前，RynnBrain 首先展现出了全面且扎实的基础模型能力。&lt;strong&gt;其 8B 版本不仅在具身认知与定位任务上全面领先于 Gemini Robotics ER 1.5、Mimo-Embodied、RoboBrain 2.0、Pelican-VL、Cosmos-reason 2 等业内最先进的模型，在许多细分能力上甚至实现了 30% 以上的性能飞跃。&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHO13J7BH2wLib17LJhpGAMx6vV802vpy4enpvQs21QicxQ3djfLqFua0usqJgdeeOHZWEsRiaHB5IIlsE9vUPenkUfSMOhbqcWhE/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.775" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532574" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/ed8dd047-0d8b-47e9-a34e-bbb1aeb71352/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; RynnBrain 在 16 项具身评测上实现 SOTA&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;更难得的是，&lt;strong&gt;RynnBrain 在泛化性方面并没有明显的损失&lt;/strong&gt;。我们知道，许多专门为机器人任务训练的「具身大脑」模型，容易过拟合到特定任务上，导致其丧失作为通用大模型原有的强大能力（比如文档理解、文本推理等）。而 RynnBrain 在取得具身任务 SOTA 的同时，继承了基座模型（Qwen3-VL）通用视觉能力。&lt;a href="https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f45f77d5-aa76-429e-a3af-a42a0ca8a0ac/1770696296070.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em data-end="1558" data-start="1502"&gt;&lt;sup&gt;模型能够理解用户的饮食需求，结合常识判断与中文 OCR 识别，从多个带文字标签的物品中筛选出符合条件的选项。&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;此外，其开源的 MOE 版本（RynnBrain-30B-A3B）让机器人在保持最强感知与规划能力的同时拥有更快的响应速度。它仅需 3B 的推理激活参数，就在各项指标上击败了当前规模最大的具身基础模型 Pelican-VL-72B，真正实现了以小博大。&lt;/p&gt;&lt;p&gt;作为一个旨在赋能下游任务的基座，&lt;strong&gt;RynnBrain 还在后训练阶段爆发出了巨大的潜力&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;实验数据表明，其预训练成果对下游任务有显著的加持作用：在导航任务中，仅作为基座进行微调（RynnBrain-Nav），就能比使用 Qwen3-VL 的模型能力提升 5%，且在不修改架构的情况下，导航成功率比当前的 SOTA 模型 StreamVLN 高出 2%-3%。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGZtmBkLLPWOZHFOHpXJuXbwa5M5vavRzrldVeWLVP0azaBnIiaibeAUEibojhiaMzFCE5Yogj0ickdv6z7HTq6OJdV4goSdhZLovZU/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.4495192307692308" data-s="300,640" data-type="png" data-w="832" type="block" data-imgfileid="503532572" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/1db20bea-b3dd-4a1a-ae3e-70748b504c53/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;而在操作规划方面，RynnBrain 展现了惊人的数据效率，仅需几百条样本微调，其 RynnBrain-Plan 模型就能具备强大的长周期规划能力，无论是在域内还是域外任务上均全面超越了 Gemini 3 Pro。这种「一点就通」的特质，充分验证了其独创的「文本与定位交错」推理方式比传统模式更适应复杂多变的物理世界，其强泛化能力的保留使其能够更快地迁移到所需场景。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFyb1AHUUQDISUHkVYvLBLSl7icz19EO9oSQicGD5oa04ddXuKSZ6CLLicZLaypSV2ncUNWqEYmHUFzWohDX9RzI0lzReZ6HoLzgE/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-ratio="0.2490740740740741" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532573" data-aistatus="1" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/6e364e36-7c87-446f-8ba8-d4d913ef9738/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;至此，RynnBrain 不仅具备了系统性的认知架构，更补全了从「理解」到「行动」的关键环节，成为首个支持移动操作的具身基础模型。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;与其押注路线 &amp;nbsp;不如先给行业「打地基」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;关于机器人「大脑」该怎么做，业内其实一直没有标准答案。达摩院的研究人员在交流中提到，当前的探索大致分成两种思路：一种从动作出发，直接学习如何操作真实世界，这条路发展出了 VLA 模型，但问题也很现实 &amp;mdash;&amp;mdash; 高质量数据难找，泛化始终受限；另一种则试图利用大模型本就拥有的泛化能力，希望先让模型看懂世界，再谈行动，但如何把这种理解准确对齐到真实、连续的物理空间，依然是绕不开的难点。&lt;/p&gt;&lt;p&gt;在这种背景下，达摩院没有急着选边站，而是选择先把基础能力补齐。RynnEC 负责打好对物理世界的感知与理解，RynnBrain 则在此之上继续往时空记忆、空间推理和长程规划推进。这些基础打好之后，RynnBrain 既可以作为下游模型的「大脑」参与真实操作，也有机会通过后训练直接演进为操作基座。这些能力被开源出来，也是希望社区能在同一套底座上继续探索，而不是各自重复造轮子。&lt;/p&gt;&lt;p&gt;与此同时，达摩院也在并行推进以视觉为主导的 VLA 路线（如 RynnVLA），并通过 RCP 等系统级技术，把模型、数据和真实机器人连成一条完整链路，从「看见」到「决定」再到「动手」。&lt;/p&gt;&lt;p&gt;谈及更远的未来，达摩院透露，他们在思考一种更平台化的方案，试图在碎片化的硬件和算法生态之上，搭起一套更统一的具身智能基础设施。毕竟，要解决具身智能这道世纪难题，需要的不是某一家机构的孤军奋战，而是整个开源社区的共同进化。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>2026开年关键词：Self-Distillation，大模型真正走向「持续学习」</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 10 Feb 2026 11:59:48 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-3</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;2026 年刚拉开序幕，大模型（LLM）领域的研究者们似乎达成了一种默契。&lt;/p&gt;&lt;p&gt;当你翻开最近 arXiv 上最受关注的几篇论文，会发现一个高频出现的词汇：&lt;strong&gt;Self-Distillation&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;近年来，基础模型取得了显著的成功，为语言、视觉、机器人等领域的 AI 应用提供了强大的支持。&lt;/p&gt;&lt;p&gt;但在真正落地、长期使用的过程中，研究者逐渐发现：如何让模型在不断吸收新知识的同时，不丢失已有的核心能力 &amp;mdash;&amp;mdash; 即「持续学习」，正成为制约大模型进化的关键瓶颈。&lt;/p&gt;&lt;p&gt;传统的强教师依赖范式因成本与数据依赖，难以适配高频的持续进化。Self-Distillation（自蒸馏） 随之成为破局点 &amp;mdash;&amp;mdash; &lt;strong&gt;通过合理的上下文引导或反馈机制&lt;/strong&gt;，模型完全可以构建出一个比当前权重更聪明的临时自我，让模型在没有外部强教师的情况下实现内生增长。&lt;/p&gt;&lt;p&gt;基于这一深刻洞察，由 MIT、ETH Zurich、Meta 及斯坦福等顶尖机构组成的紧密学术圈，在 2026 年 1 月密集发布了三项研究成果。&lt;/p&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml67nd6z1jdn" data-pm-slice="0 0 []"&gt;1.Self-Distillation Enables Continual Learning&lt;/span&gt;&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="236" data-imgfileid="503531344" data-ratio="0.1712962962962963" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGXbDJ5kTyCGuOHpEQYibbbL310IY2PibHMZY4DLOic6bFOic4bIlNXe4H9w/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" data-width="1380" data-original-style="background-color: transparent;width: 578px;height: 99px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/5e99e9c0-802f-408c-9cb5-968c26cd0ee0/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml67vofiy6w" data-pm-slice="0 0 []"&gt;论文标题：Self-Distillation Enables Continual Learning&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://www.alphaxiv.org/abs/2601.19897&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://github.com/idanshen/Self-Distillation&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在持续学习领域，传统的监督微调（SFT）常因 &lt;strong&gt;「灾难性遗忘」 &lt;/strong&gt;备受诟病，它的副作用非常明显：当你教模型学会一套新的知识，它原有的代码能力或常识推理往往会发生断崖式下跌。&lt;/p&gt;&lt;p&gt;研究团队提出了一种自蒸馏微调（SDFT）方法，该方法能够直接从演示中实现基于策略的学习。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="334" data-imgfileid="503531345" data-ratio="0.47714285714285715" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGE135hiavAnaJgGlBc7v57MN6n23Ag57XsUXWReuYhBnfl1T7icva0tibw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="700" data-width="700" data-original-style="background-color: transparent;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/eb244268-b691-457a-9a17-5962c0c71e05/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml684f1qheo" data-pm-slice="0 0 []"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; SDFT 机制概览&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心机制&lt;/strong&gt;： 该方法假设预训练模型已具备强大的 ICL 潜力。在学习新知识时，首先构造包含少量专家演示（Few-shot）的上下文，诱导模型生成高质量的教师分布；随后要求模型在不带演示的情况下，通过自蒸馏去拟合这一分布。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术突破&lt;/strong&gt;： 该方法将持续学习转化为一个 &lt;strong&gt;策略内对齐问题&lt;/strong&gt;。由于训练信号源于模型自身的 ICL 状态，它能最大限度地保持模型原始的概率流分布，避免参数在微调过程中产生剧烈漂移，从而解决了监督微调（SFT）中常见的灾难性遗忘。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGeibBHz2xBGhyiaKEIGpoV1ibpeEcR9OSvk3iarB5jkXaPRcf2abGPGcAVA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.33611111111111114" data-type="png" data-w="1080" data-width="1173" data-height="394" data-imgfileid="503531346" data-aistatus="1" data-original-style="background-color: transparent;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/5ed0fcb9-fa79-46dd-8709-466e83912512/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在技能学习和知识获取任务中，SDFT 的表现一致优于 SFT：它不仅实现了更高的新任务准确率，还显著减少了灾难性遗忘。在顺序学习实验中，SDFT 使单一模型能够随时间累积多种技能而不会出现性能退化，证明了同策略蒸馏是从演示中实现持续学习的一种实用路径。&lt;/p&gt;&lt;p&gt;2.Reinforcement Learning via Self-Distillation&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="390" data-imgfileid="503531347" data-ratio="0.3037037037037037" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGickwQEvFopfbkVAXaJDjqWTxd9Zf02GPRXGGc1amqz13K0H3T0uu4rw/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" data-width="1284" data-original-style="background-color: transparent;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7273add9-30ab-43c2-9eb0-5d77da40d329/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml67vofiy6w" data-pm-slice="0 0 []"&gt;论文标题：Reinforcement Learning via Self-Distillation&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://arxiv.org/pdf/2601.20802&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://github.com/lasgroup/SDPO&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前的强化学习（如 GRPO）通常只能拿到一个二值反馈，这在长程推理中会导致严重的「信用分配」问题。此外，在 GRPO 等算法中，如果模型在某组尝试中全军覆没（奖励均为 0），学习信号就会消失，导致模型进化停滞。&lt;/p&gt;&lt;p&gt;研究团队认为，问题的关键并不在于强化学习本身，而在于&lt;strong&gt;常见的二值反馈信息密度极低&lt;/strong&gt;，无法为长逻辑链条提供精细的指导。&lt;/p&gt;&lt;p&gt;针对这一困境，研究团队提出了 SDPO（自蒸馏策略优化） 框架，&lt;strong&gt;旨在将环境中的 「富反馈」 转化为高效的学习信号&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="446" data-imgfileid="503531348" data-ratio="0.33611111111111114" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGmEsDbKuK4bQtTicv6Puuc9pRcK55c0N5jfmWr5LMEZjkS8psLLObtIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" data-width="1328" data-original-style="background-color: transparent;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f6664e8a-a2f9-4b07-b573-4a2c113a352e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; RLVR 与 RLRF 强化学习范式对比&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心机制&lt;/strong&gt;： SDPO 引入了 富反馈（Rich Feedback） 环境。当模型生成错误答案时，环境会返回具体的报错信息（如逻辑判读）。模型将这些报错信息重新注入上下文，作为一个 「自省教师」 来重新审视并校准之前的错误尝试。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术突破&lt;/strong&gt;： 该方法通过自蒸馏机制，将原本模糊的标量奖励转化为 &lt;strong&gt;Token 级的密集监督信号&lt;/strong&gt;。通过对比 「反馈后分布」 与 「初始分布」 的差异，SDPO 能精准定位导致失败的关键 Token，指引模型降低错误路径的概率，并提高修正后逻辑的置信度。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGsScVMkuUZQrxlEhgic6kvHyKZjpndTkNzKa3wr2YofTNDicASdp0dyqQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.47129629629629627" data-type="png" data-w="1080" data-width="1368" data-height="645" data-imgfileid="503531349" data-aistatus="1" data-original-style="background-color: transparent;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/8c13e91a-ace4-4706-a0be-c745bd1e31d5/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在极难任务（左图）中，SDPO（绿线）展现了极高的采样效率，仅需约 &lt;strong&gt;1/3 的尝试次数（3&amp;times; speedup）&lt;/strong&gt;即可达到其他算法的解发现率。而在整体训练维度上，它能以更少的样本量快速收敛，在 k=1000 时已能解决 &lt;strong&gt;70%&lt;/strong&gt; 的困难任务，显著突破了传统算法的性能瓶颈。&lt;/p&gt;&lt;p&gt;在 LiveCodeBench 等竞赛级编程测试中，SDPO 展现了惊人的学习效率：它仅需传统 GRPO 算法 &lt;strong&gt;1/4 的生成样本量&lt;/strong&gt; 即可达到同等精度。它证明了即便没有外部强教师，模型也能通过利用环境反馈进行深度自省，从而打破标量奖励带来的进化僵局。&lt;/p&gt;&lt;p&gt;3.Self-Distilled Reasoner:&amp;nbsp;&lt;/p&gt;&lt;p&gt;On-Policy Self-Distillation for Large Language Models&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGrb3X9icbMkzrmn8q15XnvQfZU8TvhYcYicQIGrzO9wcBoMRIvibC2xqIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.21296296296296297" data-type="png" data-w="1080" data-width="1312" data-height="280" data-imgfileid="503531350" data-aistatus="1" data-original-style="background-color: transparent;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/6e3b0a67-1972-4a2e-b22b-c44e4fe12658/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span data-mpa-action-id="ml67vofiy6w" data-pm-slice="0 0 []"&gt;论文标题：Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;https://arxiv.org/pdf/2601.18734&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在复杂推理任务中，大模型往往面临&lt;strong&gt;搜索空间过大、奖励信号稀疏&lt;/strong&gt;的问题。尽管强化学习能提升模型上限，但在没有外部 「强教师」 辅助的在线学习场景中，模型很难在短时间内找到通往正确答案的深层逻辑路径。&lt;/p&gt;&lt;p&gt;研究团队提出了 OPSD（策略内自蒸馏） 框架，通过在同一模型内部构建 「信息不对称」 来引导自我进化。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="281" data-imgfileid="503531351" data-ratio="0.2744140625" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWGngay6icNrMntiaXX1HmyzTUsDxaLibEnDFDM76YWzibWbhU3oGEJuEjia6A/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1024" data-width="1024" data-original-style="background-color: transparent;" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/ce02f462-ab44-4d33-8dd5-ad32a8d865a6/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; OPSD 框架概览&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;核心机制&lt;/strong&gt;： 该框架将模型配置为两种状态。教师策略在输入中包含 「特权信息」（如标准答案或经过验证的推理轨迹），能够产生高质量的 Token 概率分布；而学生策略则在不接触特权信息的情况下仅凭题目进行作答。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;技术突破&lt;/strong&gt;： OPSD 采用 策略内（On-Policy）采样，核心训练目标是最小化学生分布与教师分布之间的 KL 散度。这种设计强制模型在不借助外部参考的情况下，通过内生分布的对齐，学会如何从题目直接推导出具有逻辑深度的推理链路。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="395" data-imgfileid="503531352" data-ratio="0.23796296296296296" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibnuawEGIclTM0wnuLx1ZWG7P87rndRYXiaYzpUrOp4nDx8x24EW4kiarB6ZZwAhibNglAA7VVc0Jkqg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" data-width="1660" data-original-style="background-color: transparent;" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/12f35e6e-b884-4240-9d72-c3b5c8d01d16/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;在 MATH 和 GSM8K 等高难度推理基准测试中，OPSD 展现了极高的学习效率：它在 Token 利用率上比传统的 GRPO 算法高出 &lt;strong&gt;4-8 倍&lt;/strong&gt;。实验证明，SFT 虽然能提供初始方向，但 OPSD 能够更进一步地挖掘模型内在的&amp;ldquo;推理潜力&amp;rdquo;，证明了通过特权信息诱导出的自我博弈，是实现推理能力飞跃的一条捷径。&lt;/p&gt;&lt;p&gt;这三篇论文核心逻辑高度一致：利用模型已有的内生能力，通过不同的上下文构造出 「信息差」，从而实现自驱动的闭环升级，Self-Distillation 正在成为大模型后训练阶段（Post-training）的标准配置。&lt;/p&gt;&lt;p&gt;&lt;span data-pm-slice="0 0 []"&gt;2026 年，也许我们不再需要教模型怎么变强，只需要给它一个「持续学习」的机会。&lt;/span&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>破解机器人「慢半拍」难题：南洋理工解决VLA致命短板，动态世界断层领先</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 10 Feb 2026 11:55:19 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10-2</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/5e61254f-51f8-4569-bf66-2fb9560b1c94/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;blockquote&gt;&lt;p&gt;当物体在滚动、滑动、被撞飞，机器人还在执行几百毫秒前的动作预测。&lt;/p&gt;&lt;p&gt;对动态世界而言，这种延迟，往往意味着失败。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在过去几年中，Vision-Language-Action（VLA）模型迅速成为机器人领域的焦点：机器人可以 &amp;ldquo;看懂&amp;rdquo; 画面、&amp;ldquo;理解&amp;rdquo; 语言指令，并直接输出连续动作，在静态抓取、摆放、桌面操作等任务中取得了显著进展。&lt;/p&gt;&lt;p&gt;但一个长期被忽视的问题是 &amp;mdash;&amp;mdash;&lt;strong&gt;真实世界几乎从来不是静态的&lt;/strong&gt;。当物体开始移动、加速、碰撞、改变轨迹，当前主流 VLA 模型往往会出现反应迟缓、动作失配、甚至完全失败的情况。&lt;/p&gt;&lt;p&gt;问题不在于模型不聪明，而在于：它们跟不上时间。&lt;/p&gt;&lt;p&gt;近日，来自 NTU S-Lab 的研究团队提出 DynamicVLA，首次系统性地从模型架构、推理机制和数据体系三个层面，重新审视并解决&lt;strong&gt;动态物体操控（Dynamic Object Manipulation）&lt;/strong&gt;这一长期空缺的问题。&lt;a href="https://mp.weixin.qq.com/s/_kTNDZdc1giKaSvpammqiA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a4379c48-fbe3-4ed9-8628-e57bd823d0f5/1770695525645.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;想深入了解 DynamicVLA 的技术细节？我们已经为你准备好了完整的论文、项目主页和代码仓库！&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531694" data-ratio="0.19907407407407407" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBkPkUJlfyvMhK72QTHUYF4Jlt7ARExcTS0nibkBbPK3VgNGMzAgWfQWQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/1af45716-dfe5-45b4-a369-b5faa5e92545/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文链接：https://arxiv.org/abs/2601.22153&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目链接：https://haozhexie.com/project/dynamic-vla/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GitHub 链接：https://github.com/hzxie/DynamicVLA&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;为什么 &amp;ldquo;动态操控&amp;rdquo; 对 VLA 来说如此困难？&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531696" data-ratio="0.4287037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBM49MBgKx1FnJVZp0ncRIGWibOb2T9NSzZZf7uMqqXxQgfibGZEL74xSw/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/1d8e165c-5b6c-4209-b070-208ca5fe1aa8/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在静态场景中，VLA 模型通常遵循如下流程：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;感知 &amp;rarr; 推理 &amp;rarr; 生成一段动作 &amp;rarr; 执行完 &amp;rarr; 再次推理&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;当环境基本不发生变化时，这种方式可以正常工作；但一旦物体开始运动，这一流程便迅速失效。&lt;/p&gt;&lt;p&gt;问题并不在于模型能力不足，而在于时间结构本身不适用于动态世界，主要体现在两个方面：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;感知 &amp;mdash; 执行时间错位（Perception&amp;ndash;Execution Gap）：由于推理存在不可避免的延迟，当模型完成决策时，物体状态早已发生变化，动作天然 &amp;ldquo;滞后于现实&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;动作分块等待（Inter-chunk Waiting）：多数 VLA 必须等上一段动作完全执行后才能启动下一次推理，使机器人在动态环境中始终处于被动追赶状态。&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这两个问题叠加，使得即便在静态任务中表现良好的 VLA，也难以应对真实世界中的动态操控。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DynamicVLA 的核心思路：让机器人 &amp;ldquo;边想边做&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531697" data-ratio="0.3037037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBVFL1zn7ITGfsgDiaiaCRO0ySXve5B1IlZm6fibvOwUyD3r86Uib5O4UjCA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/3f59cfaf-d14e-45bf-abcb-1cd1671e9c34/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;DynamicVLA 并没有选择通过增大模型来 &amp;ldquo;预测更远的未来&amp;rdquo;，而是围绕一个更根本的问题重新设计系统：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;在推理延迟无法消除的情况下，如何保证机器人执行的动作仍然与当前世界状态时间对齐？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;为此，DynamicVLA 从&lt;strong&gt;推理机制、执行策略和模型结构&lt;/strong&gt;三个层面提出了对应设计。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Continuous Inference：让推理与执行不再相互等待&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在传统 VLA 中，推理与执行严格串行；&lt;/p&gt;&lt;p&gt;而 Continuous Inference（连续推理）允许模型在上一段动作尚未执行完时，就启动下一轮推理，从而解决的是 Inter-chunk Waiting 带来的反应迟滞问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;推理与执行形成流水线&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;不再存在 &amp;ldquo;动作执行完才能继续思考&amp;rdquo; 的空窗期&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;机器人始终保持一个持续更新的动作预测流&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;2. Latent-aware Action Streaming：修复推理延迟造成的时间错位&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;即使采用连续推理，推理延迟本身仍然存在。这意味着：模型生成动作时所依据的观察，往往已经落后于真实世界。Latent-aware Action Streaming（LAAS）正是针对这一 &lt;strong&gt;Perception&amp;ndash;Execution Gap&lt;/strong&gt; 设计的执行机制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;显式丢弃因推理延迟而 &amp;ldquo;过时&amp;rdquo; 的动作&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;只执行在时间上仍与当前环境状态对齐的预测&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;当新预测到来时，优先采用更新、更接近当前状态的动作&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;3. 为动态而生的轻量化 VLA 架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;上述机制能否成立，还依赖于足够低的推理延迟。因此 DynamicVLA 采用了专为动态操控设计的轻量化架构：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;卷积式视觉编码器，避免多帧输入下 token 爆炸&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;截断语言模型层数，在速度与理解能力之间取得平衡&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;整体模型规模控制在 0.4B 参数量级&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;动态操控数据的核心缺口：从仿真到真实世界&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531698" data-ratio="0.47129629629629627" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqE3j1kLy6PdFA0ib5191HGiayjz9CnjUPKcmYpvqyucuTACtjiaFvibzicVpwtoLFh7VaazicMfW9CkB2m3Qicd8s5lZv5jl99xXzaHGE/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/7c8aed01-82f0-45f9-8180-fe7360b9fe4b/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;当前，无论是仿真还是真实机器人，&lt;strong&gt;主流 VLA 数据集几乎都聚焦于静态操作&lt;/strong&gt;，而对动态物体交互的系统性覆盖仍然缺失。这一数据结构性偏差，直接限制了 VLA 在真实动态环境中的泛化能力。&lt;/p&gt;&lt;p&gt;在仿真侧，DynamicVLA 基于 Isaac Sim 构建了大规模动态操控数据：&lt;strong&gt;覆盖 2800+ 场景、206 种物体&lt;/strong&gt;，通过多样化的物体运动与交互模式，生成丰富且可控的动态仿真数据，为模型提供了系统性的动态训练基础。&lt;/p&gt;&lt;p&gt;相比之下，&lt;strong&gt;真实世界的动态数据采集处于 &amp;ldquo;几乎不可行&amp;rdquo; 的状态&lt;/strong&gt;：动态物体运动速度快，人类遥操作反应时间不足，且难以实时获取高质量的 6D 位姿与速度标注，使得规模化、可复现的真实动态操控数据一直缺位。&lt;/p&gt;&lt;p&gt;DynamicVLA 的做法并不是强行遥操作，而是把真实世界 &amp;ldquo;做成仿真接口&amp;rdquo;（Real-world Simulator）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;多视角 RGB 感知，实时追踪物体运动&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在线估计物体 6D 位姿 + 速度&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;将真实环境抽象为与仿真一致的状态输入&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;直接复用同一套状态机与控制逻辑&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;首个动态操控基准：DOM Benchmark&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503531699" data-ratio="0.30833333333333335" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8cOew3J5perBzAQ2ibJ36SBichLBZx6u8SIRrabgfianX1mANyfgQQ8LLiatbum049vE8ALxZBm9vs5w/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/b04d2e0d-a698-4b2f-a37d-63c4d7a2815c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在上述自动化数据体系之上，团队进一步构建了 Dynamic Object Manipulation（DOM）Benchmark，这是首个专为动态物体操控设计的系统性评测基准。&lt;/p&gt;&lt;p&gt;与以往侧重 &amp;ldquo;是否完成任务&amp;rdquo; 的静态评测不同，DOM 从动态操控的本质出发，将能力拆解为 3 个核心维度、9 个子维度：&lt;/p&gt;&lt;p&gt;1. 交互能力（Interaction）评估机器人在物体持续运动下的实时控制与决策能力，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Closed-loop Reactivity：对不同运动速度的即时响应能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Dynamic Adaptation：在碰撞、变向等突发事件后的快速调整能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Long-horizon Sequencing：在长时间动态交互中保持策略一致性的能力&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. 感知与理解（Perception）评估模型在动态场景中的多模态理解能力，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Visual Understanding：区分外观相似物体的能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Spatial Reasoning：理解空间关系与相对位置的能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Motion Perception：感知与判断物体运动状态（速度、方向）的能力&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;3. 泛化与鲁棒性（Generalization）评估模型在分布外动态条件下的稳定性，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Visual Generalization：面对未见物体与新场景的适应能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Motion Generalization：应对新速度范围与运动模式的能力&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Disturbance Robustness：在外部扰动下维持稳定控制的能力&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DOM Benchmark 显示，DynamicVLA 在动态交互相关能力上显著领先，但在感知理解与扰动鲁棒性上仍存在明显不足。这一限制并非偶然，而是源于为保证实时性而选择的小模型架构。如何在响应速度与推理能力之间取得更优平衡，将是动态操控 VLA 的重要方向。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果：动态世界中的断层领先&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在仿真与真实机器人实验中，DynamicVLA 在多个维度上显著领先现有方法。&lt;a href="https://mp.weixin.qq.com/s/_kTNDZdc1giKaSvpammqiA"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/9813398e-458f-492d-bd5d-ea1530bcb27d/1770695642275.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;DynamicVLA 的意义：机器人开始真正 &amp;ldquo;活在时间里&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DynamicVLA 传递了一个清晰信号：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;下一代机器人智能的核心，不只是 &amp;ldquo;看懂世界&amp;rdquo;，而是在世界变化的过程中持续做出正确反应。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;从 Continuous Inference，到 Latent-aware Action Streaming，再到 Real-world simulator，DynamicVLA 为动态操控提供了一套&lt;strong&gt;可复现、可扩展、可落地&lt;/strong&gt;的系统范式。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>技术深耕与生态共建｜SGLang 上海 Meetup顺利举行</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 10:16:32 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;在当前人工智能从&amp;ldquo;聊天&amp;rdquo;范式加速向&amp;ldquo;能办事&amp;rdquo;的智能体时代演进的关键节点，LLM 系统优化与技术落地的实践探索，更需要开发者们的深度联结与经验共创。基于此，由 SGLang 社区、机器之心、张江孵化器联合举办的「SGLang 上海 Meetup」于2月6日在浦东&amp;middot;纳贤路 800 号 1 层顺利举行。&lt;img src="https://image.jiqizhixin.com/uploads/editor/f5e00e14-ef66-4241-8396-7583eac65a24/%E5%9B%BE%E7%89%871.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;本场活动特邀 SGLang 核心开发成员张柏舟，Omni-infer 核心开发者郑锦焕，清华大学博士生、Slime核心开发者谢承兴，SGLang 核心开发者、Mooncake 核心开发者蔡尚铭，蚂蚁集团系统工程师、SGLang Contributor 李泽寰五位嘉宾，围绕「LLM 系统优化与落地实践的新可能」这一主题，让贡献者走到台前、优化者分享心法，为与会者呈现了一场兼具技术深度与工程实践价值的技术盛宴，并为 SGLang 开源生态的蓬勃发展持续助力。&lt;img src="https://image.jiqizhixin.com/uploads/editor/bfceb537-3352-4759-9a96-b139dd38fec7/%E5%9B%BE%E7%89%872.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;张柏舟：SGLang 核心开发成员&lt;/p&gt;&lt;p&gt;SGLang 核心开发成员张柏舟在《SGLang Roadmap》分享中，系统回顾了 SGLang 开源推理框架从大规模部署到强化学习集成的演进历程，重点展示了 DeepSeek、GPT-OSS 等主流模型的 Day-0 支持能力。展望 2026 年，他披露了 PD 分离、投机解码、并行策略重构等技术路线，强调 SGLang 将持续深化与产业伙伴协同，打造高性能、高兼容性的开源推理基础设施。&lt;img src="https://image.jiqizhixin.com/uploads/editor/ff01136f-ec7f-48f9-9d5b-2a9c211c3aae/%E5%9B%BE%E7%89%873.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;郑锦焕：Omni-infer 核心开发者&lt;/p&gt;&lt;p&gt;Omni-infer 核心开发者郑锦焕带来《Omni-infer 对 SGLang 的性能优化实践》主题分享，深度剖析Omni-infer的集成架构与性能调优策略，重磅介绍了Omni-Ai V1新版本的核心升级亮点，为开发者提供更高效的AI开发与部署工具。他提出基于最早完成时间的均衡调度算法，有效降低排队时延；通过并行 KV Cache 传输，显著减少传输开销并配合异步调度提升kv cache复用效率，构建全链路可视化方案，结合NPU硬件特征开展针对性优化。最终在 DeepSeek v3.1 实测中，系统 QPM 从 356 提升至 460，充分验证了系列优化的显著成效。此外，郑锦焕也同步公布了Omni-Ai V1的代码仓链接：https://gitee.com/omniai/omniinfer，方便开发者快速获取、部署与二次开发。&lt;img src="https://image.jiqizhixin.com/uploads/editor/853d41de-1f0b-4ed7-9567-7bf534771bbf/%E5%9B%BE%E7%89%874.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;谢承兴：清华大学博士生、Slime 核心开发者&lt;/p&gt;&lt;p&gt;清华大学博士生、slime 核心开发者谢承兴以《slime：面向 RL Scaling 的 LLM 后训练框架》为题，分享了由智谱开源的后训练框架 slime。针对 Agentic RL 时代多轮交互、长上下文等复杂应用场景，他系统介绍了 slime 的 Server-Based Rollout 架构与解耦式 rollout 函数设计，有效降低了用户的使用门槛。同时，框架通过引入 Importance Sampling、True On-Policy 对齐等机制，缓解并降低了训练过程中的不稳定性。目前，slime 已成功支撑 GLM 系列模型的后训练，并也支持 DeepSeek R1、Kimi k2 等大规模 MoE 模型的强化学习训练。&lt;img src="https://image.jiqizhixin.com/uploads/editor/aaa3cf70-39aa-464b-b855-eb7ca1dd7d72/%E5%9B%BE%E7%89%875.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;蔡尚铭：SGLang 核心开发者、Mooncake 核心开发者&lt;/p&gt;&lt;p&gt;SGLang 核心开发者、Mooncake 核心开发者蔡尚铭在《SGLang CPP：面向超长上下文的 Scaling out 黑科技》中，深入解析了 SGLang 针对超长上下文推理场景所设计的高性能 Chunked Pipeline Parallelism（CPP）实现。在原有PP架构的基础上，SGLang通过引入异步P2P通信与动态分块预填充两大核心技术，显著降低了流水线气泡，同时兼容PD分离与HiCache，为万亿参数模型提供了高效的多节点横向扩展方案。实测显示，在 H20 集群上部署 DeepSeek-V3.1模型，新架构在扩展至 PP4 TP8 时，预填充吞吐量相比 TP8 提升至 3.31 倍，TTFT 降低 67.9%，性能显著优于原有实现与TP32扩展方案。&lt;img src="https://image.jiqizhixin.com/uploads/editor/3f313b62-0559-4c68-a925-a0f6d39fee97/%E5%9B%BE%E7%89%876.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;李泽寰：蚂蚁集团系统工程师、SGLang Contributor&lt;/p&gt;&lt;p&gt;蚂蚁集团系统工程师、SGLang Contributor 李泽寰带来《从自回归到扩散，SGLang diffusion LLM 的探索与实践》，分享了扩散语言模型在 SGLang 中的工程实践。他对比了三种解码范式，指出 Block Diffusion 兼具任意长度输出与并行解码优势。通过将 dLLM 嵌入 SGLang 框架，实现 LLaDA2.0-flash 等扩散语言模型的高效推理，大幅降低评测与 RL 后训练耗时，并成功支撑起 dLLM 的生产级服务部署。&lt;/p&gt;&lt;p&gt;本次 Meetup 在热烈的自由交流中圆满落幕。从框架内核到部署优化，从训练范式到硬件适配，五位嘉宾的分享勾勒出 SGLang 生态的技术全景。这些真知灼见不仅为社区演进提供了宝贵参考，更为 LLM 系统优化领域的开发者注入了新的灵感与动力。未来，SGLang 社区将持续推动开源协作与技术创新，期待与更多开发者携手，共同探索大模型时代的无限可能。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>ProjDevBench：AI编程智能体真的能从零构建完整软件项目吗？</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Tue, 10 Feb 2026 10:08:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-05-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-05-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;上海交通大学、上海创智学院与加州大学默塞德分校联合发布ProjDevBench——首个通过OJ细粒度反馈评估AI编程智能体端到端项目开发能力的基准测试，要求智能体仅凭自然语言需求文档，从零开始构建完整、可运行的软件仓库。&lt;/p&gt;&lt;p&gt;结果令人深思：所有智能体总体提交AC率仅27.38%，当任务从"补全现有代码"变为"从零构建"时，性能出现断崖式下跌。&lt;/p&gt;&lt;p&gt;•论文链接：https://arxiv.org/abs/2602.01655&lt;/p&gt;&lt;p&gt;•项目链接：https://github.com/zsworld6/projdevbench&lt;/p&gt;&lt;p&gt;【结果总结】&lt;/p&gt;&lt;p&gt;•六种主流编程智能体（Cursor、GitHub Copilot、Claude Code等）的总体提交AC率仅为27.38%，在从零构建任务中性能大幅下滑。&lt;/p&gt;&lt;p&gt;•OJ提供的细粒度诊断反馈（编译错误（CE）、运行时错误（RE）、超时（TLE）、内存超限（MLE）、答案错误（WA）等）是评估端到端开发能力的关键组成部分，远优于传统的pass/fail二元判定。&lt;/p&gt;&lt;p&gt;•交互轮次与性能呈强负相关（-0.734），智能体在遇到困难时陷入低效试错循环，而非通过反思实现突破。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;一、为什么需要端到端项目开发基准&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;现有基准测试如HumanEval、MBPP聚焦于函数级代码生成，SWE-bench关注issue修复，但真实软件工程需要的远不止这些。当开发者使用Cursor或GitHub Copilot进行"vibe coding"时，他们期望智能体能够：从零设计系统架构、创建和组织多个源文件、配置依赖和构建系统（如CMakeLists.txt）、最终交付一个可编译运行的完整项目。&lt;/p&gt;&lt;p&gt;这种端到端的项目构建能力此前从未被系统性评估过。ProjDevBench填补了这一空白。&lt;/p&gt;&lt;p&gt;与传统基准的本质区别在于：HumanEval等要求智能体补全代码片段，SWE-bench要求修复现有代码库中的bug，而ProjDevBench要求智能体像真正的软件工程师一样，在没有任何初始代码模板的情况下，自主完成从架构设计到多文件编码的全流程。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ba2585c6-404d-4917-aed9-2aa2db76c26e/1770275491867.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;二、双重评估机制：OJ测试 + 代码审查&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与以往仅返回pass/fail的测试不同，ProjDevBench采用双轨制评估：&lt;/p&gt;&lt;p&gt;OJ执行评分（80%）：通过在线判题系统进行严格的黑盒测试，提供细粒度诊断信号——编译错误（CE）、运行时错误（RE）、超时（TLE）、内存超限（MLE）、答案错误（WA）等。这些信号支持智能体进行迭代调试，模拟真实开发中"编写代码-遇到报错-修改代码"的循环。&lt;/p&gt;&lt;p&gt;代码审查评分（20%）：结合规则脚本和LLM模拟的代码审查，检测OJ测试无法捕捉的问题：是否违反显式规则（如使用禁止的库）、是否存在作弊解法、是否利用测试套件漏洞而非遵循实际约束。&lt;/p&gt;&lt;p&gt;这种设计的核心洞察是：仅靠测试用例无法全面评估代码质量。一个能通过所有测试的解法，可能采用了投机取巧的方式，而非真正理解并遵循问题规范。如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e5e59bdb-e566-4ef8-a0ae-df3d71b2f0f1/1770275502360.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;三、任务设计与数据来源&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队从上海交通大学ACM班（https://acm.sjtu.edu.cn/home）的在线判题平台精选20道高难度编程项目，涵盖算法、数据结构、解释器、管理系统、存储组件等8大类别。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f35f090f-7c20-4794-a739-29e770a42f90/1770275509382.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;这些题目经过三阶段筛选：&lt;/p&gt;&lt;p&gt;初始收集：从约2,800道候选题目中筛选 范围过滤：保留需要多文件实现、模块组织、构建配置的项目级任务，排除纯算法单文件题目，剩余约100道 质量过滤：选取规范清晰、测试套件完善、难度非平凡的题目，最终保留20道&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/939f8aad-b500-4c3b-861f-fdbe72eee39c/1770275516876.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;两种任务模式：&lt;/p&gt;&lt;p&gt;•Easy模式（有代码库）：提供部分代码，要求补全项目&lt;/p&gt;&lt;p&gt;•Hard模式（无代码库）：仅提供自然语言规范，要求从零构建&lt;/p&gt;&lt;p&gt;人类参考解法平均包含约10个源文件，智能体平均需要138轮工具调用、消耗4.81M tokens才能完成一道题目，最复杂的任务需要超过两小时。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;四、实验结果解读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队评估了六种主流编程智能体：Cursor、GitHub Copilot、Claude Code、Augment、Codex CLI、Gemini CLI，搭配GPT-5、Claude Sonnet 4.5、Gemini 3 Pro等前沿模型。&lt;/p&gt;&lt;p&gt;整体表现：Codex + GPT-5取得最高综合得分77.85，但所有智能体的总体提交AC率仅为27.38%。&lt;/p&gt;&lt;p&gt;从零构建时性能断崖式下跌：这是最关键的发现。当任务从Easy（有代码库）变为Hard（无代码库）时，大多数配置出现显著性能下降。例如：&lt;/p&gt;&lt;p&gt;•GitHub Copilot + Sonnet-4.5：71.10 → 36.63&lt;/p&gt;&lt;p&gt;•Gemini CLI + Gemini-3-Pro：74.57 → 35.53&lt;/p&gt;&lt;p&gt;•Codex + Sonnet-4.5：66.07 → 31.88&lt;/p&gt;&lt;p&gt;这表明当前智能体擅长在现有代码基础上进行修补，但缺乏从零开始进行宏观架构设计的能力。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/0cc45da4-5fad-455a-8afd-b98119ffc5d4/1770275526362.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;五、失败模式深度分析&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队对所有提交进行了系统性分析，揭示了智能体的核心短板：&lt;/p&gt;&lt;p&gt;提交状态分布：&lt;/p&gt;&lt;p&gt;•Accepted：27.38%&lt;/p&gt;&lt;p&gt;•Wrong Answer：41.86%&lt;/p&gt;&lt;p&gt;•Time Limit Exceeded：13.91%&lt;/p&gt;&lt;p&gt;•Runtime Error：7.01%&lt;/p&gt;&lt;p&gt;•Compile Error：4.52%&lt;/p&gt;&lt;p&gt;•Memory Leak：3.51%&lt;/p&gt;&lt;p&gt;规范理解偏差：智能体经常生成语法正确但遗漏关键业务逻辑的框架代码。在火车票管理系统任务中，所有智能体都实现了用户管理和列车查询，却遗漏了座位管理系统。在扫雷任务中，智能体访问了3,789个安全格子中的3,825个，表明实现不完整而非逻辑错误。&lt;/p&gt;&lt;p&gt;边界情况处理薄弱：大量运行时错误源于空指针解引用、数组越界等问题。在map实现中，红黑树的旋转函数缺乏空指针检查；在Bookstore任务中，所有智能体都未能通过隐藏测试点，暴露了对空字符串、文件I/O异常、嵌套场景的处理不足。&lt;/p&gt;&lt;p&gt;时间复杂度分析缺失：在ICPC管理系统任务中，智能体在每次解冻操作后重新排序所有队伍，得到O(K×N log N)的解法，而正确做法是利用排名变化的局部性实现O(K log N)。智能体倾向于使用熟悉但次优的模式，而非分析问题特性进行针对性优化。&lt;/p&gt;&lt;p&gt;资源管理局限：在BASIC解释器任务中，当std::stoi()抛出异常时，已分配的表达式对象未被释放，导致内存泄漏。智能体处理显式错误路径，却忽略正常操作中可能出现的异常。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;六、交互长度与性能的负相关&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队发现了一个反直觉的现象：智能体的交互轮次越多、消耗的token越多，最终得分往往越低。&lt;/p&gt;&lt;p&gt;•Tokens与得分的相关系数：-0.734&lt;/p&gt;&lt;p&gt;•交互轮次与得分的相关系数：-0.668&lt;/p&gt;&lt;p&gt;•交互轮次与token消耗的相关系数：0.898&lt;/p&gt;&lt;p&gt;这意味着当智能体遇到困难时，它们往往陷入低效的"尝试-报错-再尝试"死循环，无法像人类专家那样通过深度思考找到更优解。增加的token主要来自重复的交互轮次，而非少量但深入的长推理步骤。&lt;/p&gt;&lt;p&gt;静态代码复杂度（文件数量、修改行数）与性能的相关性较弱，表明任务难度主要体现在延长的交互和降低的性能上，而非直接由代码规模决定。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;七、代码审查的独特价值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除执行结果外，代码审查揭示了智能体在软件开发工作流理解上的盲点：&lt;/p&gt;&lt;p&gt;版本控制误解：智能体经常在本地修改代码并创建commit，却未push到远程仓库，导致提交不完整。这表明智能体隐式假设"写代码=完成任务"，忽略了进度必须通过版本控制显式记录和提交的要求。&lt;/p&gt;&lt;p&gt;规范遵从失败：构建系统配置错误、生成错误名称的可执行文件、使用禁止的标准库头文件、遗漏必需文件、修改受保护的模板。这些问题揭示了智能体将规范要求视为次要于功能正确性的倾向。&lt;/p&gt;&lt;p&gt;这些发现表明，智能体尚未将软件开发理解为一个结构化的工作流程，而仅仅是代码生成任务。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;八、总结与意义&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;ProjDevBench首次证实了当前AI编程智能体在处理真实、复杂的端到端软件开发任务时仍处于初级阶段。它们擅长局部代码修补，但在全局架构设计、时间复杂度优化、资源管理及复杂逻辑推理上尚未达到可用标准。&lt;/p&gt;&lt;p&gt;学术贡献：&lt;/p&gt;&lt;p&gt;•提出首个端到端提供细粒度反馈的项目开发基准，要求智能体从零构建完整可运行的软件仓库&lt;/p&gt;&lt;p&gt;•建立结合OJ细粒度反馈与LLM代码审查的双重评估协议&lt;/p&gt;&lt;p&gt;•系统性揭示智能体在规范对齐、边界处理、复杂度优化、资源管理等方面的失败模式&lt;/p&gt;&lt;p&gt;实际意义：&lt;/p&gt;&lt;p&gt;•为评估和改进下一代自主软件开发智能体提供了更贴近真实工程场景的标准&lt;/p&gt;&lt;p&gt;•明确了从"代码补全工具"到"软件工程师"的能力鸿沟&lt;/p&gt;&lt;p&gt;•指出了未来研究方向：如何让智能体在交互中更有效地利用反馈信号，从单纯的"试错"转向真正的"推理"&lt;/p&gt;&lt;p&gt;局限性：目前基准仅包含20道任务，主要集中于C++语言，尚未涵盖其他编程语言或人机交互式开发场景。未来将扩展任务规模、引入更多语言和任务类型。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>这个春节，AI 不聊天了，开始替我买单</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:59:31 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-10</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p data-path-to-node="5" data-pm-slice="0 0 []"&gt;大家发现了吗？这个马年春节，一场甚至比春运还要拥挤的「AI 春节大战」早已硝烟弥漫。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;腾讯元宝打响了第一枪，豪掷 10 亿现金红包，还推出了「元宝派」这种 AI 社交新玩法，试图重现 11 年前的微信红包时刻；百度文心一言也没闲着，拿出了 5 亿红包，还联合了北京台春晚；字节跳动则更是直接把火山引擎搬到了央视春晚的后台&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p data-path-to-node="7"&gt;一时间，微信群里满屏都是抢红包链接。大厂们都在焦虑：在「应用为王」的阶段，谁能在这个春节多吸引用户目光，谁就有可能成为下一个互联网超级入口。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;但就在这铺天盖地的「聊天红包」和「社交裂变」中，阿里旗下的千问 APP 突然杀进场，直接把筹码推到了 30 亿，也打响了一场与众不同的「现象级」AI 生活方式的全民演习。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;2 月 2 日，千问 APP 宣布启动 30 亿「&lt;strong&gt;春节请客计划&lt;/strong&gt;」。这不仅仅是发红包，而是&lt;strong&gt;阿里调动了淘宝、飞猪、大麦、高德等整个生态圈&lt;/strong&gt;，直接请全国人民「吃喝玩乐」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFicerkiaWASFdNicGxiakr7CXprH1GEicsicYK6mTbFuyE16923HEDpDlEQDeu2TSAVqSwmfxJ9zWrfYIY5WTgvQzPZ3r4eZTTNyZdY/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="1.7510204081632652" data-s="300,640" data-type="png" data-w="735" type="block" data-imgfileid="503532526" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ac90ff6f-9eeb-44c9-a608-7e86cca832ca/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="10"&gt;2 月 6 日活动首日，战果显赫。当天活动开始 9 小时，通过千问 APP 完成的 AI &lt;strong&gt;订单量已迅速突破 1000 万单&lt;/strong&gt;。同时后台收到了超过 3000 万次「帮我买」的指令。&lt;/p&gt;&lt;p data-path-to-node="11"&gt;下面这张图想必能让大家感受到这波千问的实力：&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHK82vDOtiakOJuFGenib669GgicyabrCOWNVMic0smhV7qh2icuMgjFmmGzQCkJ9mD60T36L8VczOwjqox9LS2tqpMFwmsxeGKRVEU/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.5814814814814815" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532527" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/56f4b6f1-13b0-476a-a590-7b0535b00277/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="13"&gt;这种爆炸式的热情甚至一度让服务器陷入拥堵，千问官方也发文「求放过」。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEkmI3RgibFW49g2t8uW7xH1EeVLJJquAVDDRsicXCib72vhh4JvprY9tWqNc6nOu6ib3dcTjRPOW8ibasicsCqPeicfTCDNmoLKWiaJ4I/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.8215297450424929" data-s="300,640" data-type="png" data-w="706" type="block" data-imgfileid="503532528" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/7e41c8b1-0b19-4a26-8b30-90424c3972b4/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="15"&gt;这千万级订单的背后，不仅是 30 亿真金白银的投入，也是在测试一种新的消费习惯。千问用一张张「免单卡」，让用户在交互中完成了「找千问」的习惯养成。&lt;/p&gt;&lt;p data-path-to-node="16"&gt;面对这场突如其来的「AI 购物潮」，千问团队在紧急扩容的同时也给出了定心丸：活动将持续至 2 月 28 日，且正在加速接入全国盒马门店和天猫超市。这意味着，免单的范围将从奶茶电影票，一路延伸到年夜饭桌上的澳洲龙虾和车厘子。&lt;/p&gt;&lt;p data-path-to-node="17"&gt;而更关键的是，千问的能力不只局限在「买东西」上。&lt;/p&gt;&lt;p data-path-to-node="18"&gt;比如在春节最令人头疼的出行规划场景中，千问展示了惊人的跨应用调度能力。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_gif/5L8bhP5dIqFmBeMgFVtYyldwceaOTwTJZ2wYAdzbuE6fSq4OiauBOtGyvYKSEGFLVjDxHVjQusJsFrrWSyVgQOVgyw0dcSOhZ87cYm1pwRyo/640?wx_fmt=gif&amp;from=appmsg#imgIndex=4" data-ratio="0.5944444444444444" data-s="300,640" data-type="gif" data-w="1080" type="block" data-imgfileid="503532530" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/12f1ac40-f145-4fd3-8e85-389f5cdcf35a/640.gif" data-order="0" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="19"&gt;&lt;i data-index-in-node="11" data-path-to-node="19"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 调用飞猪、高德示例&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p data-path-to-node="20"&gt;当用户提出复杂的差旅需求时，千问不只是给出文字建议，而是直接调动飞猪的票务网络锁定航班，同时通过高德筛选酒店。这种「所想即所得」的体验，直接省去了用户在多个 APP 之间反复跳转比价的繁琐过程，真正实现了从决策到交易的闭环。&lt;/p&gt;&lt;p data-path-to-node="21"&gt;而在更长尾的家庭消费领域，千问则充当了「全能导购」的角色。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532531" data-ratio="2.0625" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/5L8bhP5dIqFJtGm1iclV7O7kRgMrowHRTic5v5XPWLapBvQcuhj75PgMyY97ovkA4xNF2XgjPVnOXPMjxomBGApHU3atmuyZlibHY2DPia1DJOA/640?wx_fmt=gif&amp;from=appmsg#imgIndex=5" data-type="gif" data-w="400" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/e12bd605-9ec7-48e9-8641-a08674b0cbec/640.gif" data-order="1" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="22"&gt;&lt;i data-index-in-node="11" data-path-to-node="22"&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 调用淘宝示例&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p data-path-to-node="23"&gt;面对海量的商品参数和复杂的促销机制，千问能够迅速调取淘宝天猫的商品库，基于用户真实需求进行多维度筛选。用户无需再做算术题，AI 已经把性价比最高的方案直接呈现在了对话框里。&lt;/p&gt;&lt;p data-path-to-node="24"&gt;&lt;strong&gt;为什么是千问？为什么是现在？&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="25"&gt;今年的春节档，俨然成了互联网大厂的「AI 阅兵场」。各家都在撒钱，各家都在推 AI，但如果仔细观察，你会发现大家的战术动作其实截然不同。&lt;/p&gt;&lt;p data-path-to-node="26"&gt;有的厂商在抢夺聊天框里的时长，有的在争夺生成式内容的流量。而千问这波「免单」操作，通过把 AI 从「聊天框」直接拽进了「购物车」和「取票机」，实际上是在争夺价值更高的东西：&lt;strong&gt;生活消费场景的入口&lt;/strong&gt;。&lt;/p&gt;&lt;p data-path-to-node="27"&gt;这种差异，也反映出中美 AI 发展路径上的不同侧重。&lt;/p&gt;&lt;p data-path-to-node="28"&gt;近期，Anthropic 推出 Claude Cowork 后，Salesforce、Adobe 等一众企业软件公司的股价出现明显波动。市场的担忧很直接：&lt;/p&gt;&lt;p data-path-to-node="29"&gt;如果 AI 已经可以自动整理资料、生成报表、管理邮件、协助决策，那些每年动辄几千美元订阅费的 SaaS 工具，还剩下多少不可替代性？&lt;/p&gt;&lt;p data-path-to-node="30"&gt;这种焦虑，并非空穴来风。&lt;/p&gt;&lt;p data-path-to-node="31"&gt;2 月 6 日凌晨，Anthropic 和 OpenAI 几乎同时更新了自家的基础模型。Claude Opus 4.6 在编码、金融、法律等专业评测中大幅领先前代模型；而 GPT-5.3-Codex 不仅在多项工程基准中刷新纪录，甚至已经开始参与自身的训练、调试和部署过程，加速模型迭代。&lt;/p&gt;&lt;p data-path-to-node="32"&gt;这些能力，指向的是同一个方向：&lt;strong&gt;用 AI 重塑白领的工作方式，直接冲击传统企业软件的价值链。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="33"&gt;这也是为什么，美国 AI 的主要战场，始终集中在高客单价的 B 端市场：企业付费意愿强，SaaS 体系成熟，AI 的每一次性能提升，都可以迅速转化为订阅收入和效率红利。&lt;/p&gt;&lt;p data-path-to-node="34"&gt;而中国的情况，几乎是另一面镜子。&lt;/p&gt;&lt;p data-path-to-node="35"&gt;中国的移动互联网已经高度普及，支付体系高度统一，电商、本地生活、即时配送的数字化程度在全球范围内都极为罕见。对普通用户来说，「下单&amp;mdash;支付&amp;mdash;履约」本身就是一条被反复验证过的顺滑路径。在这样的环境下，AI 如果只停留在聊天或内容生成层面，反而很难释放它真正的价值。&lt;/p&gt;&lt;p data-path-to-node="36"&gt;说白了，中国用户对 AI 的期待一直很简单：&lt;strong&gt;少说点废话，多把事情办完。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="37"&gt;千问之所以能打赢这场仗，核心在于「全栈 AI」的能力门槛。要实现「动动嘴就把事办了」，光有聪明的脑子是不够的，还得有灵活的手脚。&lt;/p&gt;&lt;p data-path-to-node="38"&gt;千问依托庞大的阿里生态，组装出了一副可以在物理世界「横着走」的躯体，形成了一条「AI + 实体」全链路：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="39,0,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,0,0"&gt;大脑&lt;/b&gt;：通义千问大模型（Qwen），负责听懂你那些稀奇古怪的需求。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,1,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,1,0"&gt;双手&lt;/b&gt;：淘宝、天猫、盒马、饿了么，负责在数以亿计的商品池里把东西挑出来，买回来。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,2,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,2,0"&gt;双脚&lt;/b&gt;：飞猪、高德、大麦，负责搞定票务和路线，把你精准地带到目的地。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="39,3,0"&gt;&lt;b data-index-in-node="0" data-path-to-node="39,3,0"&gt;钱包&lt;/b&gt;：支付宝，负责最后的交易闭环，以及这次那个最让心动的：免单。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532532" data-ratio="0.6666666666666666" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqE6y5NMLMicUSYTMBhXrCnx5Nptcn2fOXlqtxCGEIUjMYynvSBwWibjbeoaicNwQUuianq674Fn2JY3KFkuQVWhO9tSicicKARP59wGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/f794ac86-39ce-47a7-8406-e382eca2c340/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="40"&gt;这就是阿里独有的「超级 Agent」路线。它不仅要让 AI「想得到」，更要让 AI「办得到」。&lt;/p&gt;&lt;p data-path-to-node="41"&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="42"&gt;如果说 2023、2024 年是 AI 的「技术元年」，2025 年是 AI 的「应用探索年」，那么这个春节，至少让很多普通用户第一次感受到：&lt;/p&gt;&lt;p data-path-to-node="43"&gt;&lt;strong&gt;AI 开始真的「派得上用场」了。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="44"&gt;去年春节，DeepSeek 用「深度思考」让我们见识了中国 AI 力量；而今年春节，千问正试图用「生活 Agent」打破 AI 的应用边界。&lt;/p&gt;&lt;p data-path-to-node="45"&gt;依托全球最庞大的互联网用户基数、最成熟的移动支付体系、以及最丰富的线上线下融合消费场景，中国 AI 正在走出一条完全不同的路：&lt;strong&gt;将最前沿的技术，无缝嵌入最鲜活的日常生活。&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="46"&gt;从 PC 时代的「搜索框」，到移动时代的「APP 矩阵」，再到如今 AI 时代的「一句话办事」。我们与数字世界的交互方式，正在经历一场不可逆的减法。&lt;/p&gt;&lt;p data-path-to-node="47"&gt;所以，今年过年，与其在各个 APP 里迷路、比价、焦虑，不如试着把这些麻烦事儿都丢给千问。&lt;/p&gt;&lt;p data-path-to-node="48"&gt;毕竟，&lt;strong&gt;让 AI 学会「打工」，让我们腾出时间去真正地享受生活、陪伴家人&lt;/strong&gt;，这才是科技进化的终极奥义，不是吗？&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>CVPR 2026 Workshop征稿｜第六届AdvML@CV：多模态大模型智能体安全</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:55:21 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/36c31d93-d498-4a07-806d-e2aa788dc75d/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="4"&gt;IEEE/CVF 计算机视觉与模式识别会议 CVPR 2026 将于 2026 年 6 月 3 日至 6 月 7 日在美国科罗拉多州丹佛举办。我们将在 CVPR 期间举办第六届对抗机器学习计算机视觉研讨会（6th AdvML@CV），Workshop 预计安排在 6 月 3 日或 6 月 4 日。&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532263" data-ratio="0.19285714285714287" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGIykaRbX7wDeBdFtB8yP11hTHfVRE8hnuWcPjLKYlu9WyPibhaJuINk8pLXzusjWnPEkO0gg07kia8N4wyicSickPwGrXzxgkOljY/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="700" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/14610ac2-ec99-4876-9f6a-0a99d1fb08a8/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="5"&gt;本届主题聚焦：&lt;b data-index-in-node="7" data-path-to-node="5"&gt;Safety of Vision-Language Agents（视觉-语言智能体安全）&lt;/b&gt;。&lt;/p&gt;&lt;p data-path-to-node="6"&gt;&lt;strong&gt;主题聚焦：视觉-语言智能体的安全与鲁棒性&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="7"&gt;多模态基础模型推动了视觉理解、生成与推理能力的跃迁，也让 Vision-Language Agents（视觉-语言智能体）迅速成为「感知&amp;mdash;&amp;mdash;语言推理&amp;mdash;&amp;mdash;行动规划」一体化的新范，在无人驾驶、智能机器人等领域具有广阔应用前景。&lt;/p&gt;&lt;p data-path-to-node="8"&gt;但随着智能体自主性增强，攻击面也从传统像素级扰动扩展到更复杂的安全风险：例如对抗提示（Adversarial Prompts）、指令注入（Instruction Injection）、Jailbreak 操控等，它们可能扰乱推理链条、误导感知决策，甚至诱发危险行为。&lt;/p&gt;&lt;p data-path-to-node="9"&gt;我们希望通过本次 Workshop，汇聚计算机视觉、多模态学习与 AI Safety 社区的研究者与工程实践者，共同推进安全、鲁棒、可信的视觉-语言智能体研究与落地。&lt;/p&gt;&lt;p data-path-to-node="10"&gt;&lt;strong&gt;论文征稿&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="11"&gt;&lt;strong&gt;本次研讨会诚邀与以下主题相关（但不限于）的投稿：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-path-to-node="12,0,0"&gt;Attack and defense on vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,1,0"&gt;Datasets and benchmarks that could evaluate vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,2,0"&gt;Adversarial / Jailbreak attacks on vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,3,0"&gt;Improving the robustness of agents or deep learning systems&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,4,0"&gt;Interpreting and understanding model robustness, especially agentic AI&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,5,0"&gt;Adversarial attacks for social good&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p data-path-to-node="12,6,0"&gt;Alignment of vision-language agents&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="13"&gt;&lt;b data-index-in-node="0" data-path-to-node="13"&gt;投稿类型与格式要求：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Long Paper：正文最多 8 页（不含参考文献）&lt;/li&gt;&lt;li&gt;Extended Abstract：正文最多 4 页（含参考文献）&lt;/li&gt;&lt;li&gt;论文需匿名，并使用 CVPR 2026 Author Kit 模板撰写（LaTeX/Word 均可）、&lt;/li&gt;&lt;li&gt;被录用论文可选择收录至 CVF &amp;amp; IEEE Xplore Proceedings&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="15"&gt;&lt;strong&gt;重要日期&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Abstract Submission Deadline: 2026/03/05&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Paper Submission Deadline: 2026/03/05&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Author Notification: 2026/03/17&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Camera-Ready Deadline: 2026/04/01&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CVPR 2026 Conference: 2026/06/03&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-path-to-node="17"&gt;&lt;strong&gt;演讲嘉宾&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532342" data-ratio="0.6981481481481482" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqFPCDzJTvnMrGVdprbicW3LAuydlVNYyLnQXbgrBUQY52oicibRP9PKT1mnibQ6XAMB7pNFMbZSwsDxJTqCfIK0mJ0lXwVQUpEhgms/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/06b4ec12-eb10-4191-b131-e71d41f8e9d7/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="19"&gt;&lt;strong&gt;组织团队&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532343" data-ratio="0.8712962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHXCmicQhclqvWkCiaEvgDQnUshCx2Ifbicl4jiciatFuuGvFtPMnAYtQgHU7mRm4ic6nXW3bnESX3oS8icQfeCmCKc0vUasUdiauL8fz4/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/8698192d-18fd-436b-8590-f6015983dd56/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;h4 data-path-to-node="21"&gt;&lt;strong&gt;Program Committee&lt;/strong&gt;&lt;/h4&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqG48XjzYvZQXyllNRHia7PlvfFUStqV9IcbVBF1OyjzLFIf4iaxcP9CqtP5KlSjl5ibcX0HAZA5IAjb8FgWXCApozicZDfWjS9NNV4/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.32857142857142857" data-s="300,640" data-type="png" data-w="700" type="block" data-imgfileid="503532273" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/60389a17-b5c4-4989-ad1c-65f43b813986/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;Workshop Sponsor&lt;/strong&gt;&lt;/p&gt;&lt;section&gt;&lt;img data-aistatus="1" data-imgfileid="503532277" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqHzZicW4ZSCnW0nLFFiahe5Xdm3UUxMPVMB44gfriaBUDCDHPN26ibHL6DdHFEUMG0TWEicicsmwfTU9Swkp7kyDRYzTcMYNAvInf9wc/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/c48ae801-e852-4b1d-ac41-ee2dfd51f9a8/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-path-to-node="25"&gt;&lt;strong&gt;投稿入口与会议信息&lt;/strong&gt;&lt;/p&gt;&lt;p data-path-to-node="26"&gt;欢迎转发给有相关研究方向的同学与合作伙伴，我们期待在研讨会现场与大家交流！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Workshop 官网：https://cvpr26-advml.github.io/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenReview 投稿入口&lt;b data-index-in-node="0" data-path-to-node="28"&gt;：&lt;/b&gt;https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/Advml&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>童年的滚球兽「走进」现实？华为天才少年创业，全球首个虚实融合的实时交互视频模型来了</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:50:51 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-8</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜Youli&lt;/section&gt;&lt;p&gt;还记得童年的那个愿望吗？&lt;/p&gt;&lt;p&gt;随着《数码宝贝》进化曲的响起，屏幕前的你我或许都曾幻想过：要是那只从数码蛋中破壳而出的滚球兽，真的可以从电视屏幕那端跳出来，就好了。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqF1mqAjX6FMceLTBOpSAuCNBc4vzp5IsYmT0ic0h1cJz6Uq4Pyv9crYWjSuPUMuEVBibT1QRI5wZoo0DvibkoDOGpsYgAcgJjoiaLg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.5611111111111111" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503532282" data-aistatus="1" data-original-style="width:517px;height:290px;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/3953dfd5-3224-46ab-8c89-97b4b368b140/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;彼时，我们只能将这种天马行空的「美梦」寄希望于「次元裂缝」的开启。再后来，技术增强现实（AR）技术曾一度带来了希望，但几经潮起潮落，结果仍停留在「预先制作的内容叠加」层面，数字角色无法真正感知环境。&lt;/p&gt;&lt;p&gt;而现在已经 2026 年了，生成式 AI、实时渲染、端侧算力、感知模型同时成熟，尤其是 Sora 展现出的前所未有的世界模拟能力，让大家意识到，原来虚拟内容不再需要完全预制，可以被实时生成、驱动，并具有物理合理性。技术的狂奔第一次让曾经的「中二梦」，具备了成为现实的可能：&lt;strong&gt;你真的可以从屏幕中「召唤」出一只滚球兽。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/26363918-a3a4-49cb-b1f7-4fe4326d03a3/1770619538527.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是不是很神奇？手机镜头对准桌面，选取一张滚球兽照片，下一秒，一只滚球兽就「脱屏而出」，出现在桌面上，四处张望。你伸出手，它刚开始会有点警惕，之后就亲昵地蹭你的手心，你轻轻一捏，它会给出Ｑ弹的物理反馈，而当你把手摊开，它甚至可以被你「托」在掌心之中，就好像，这是一只「活」的滚球兽&amp;hellip;&amp;hellip; &lt;strong&gt;通过一个手机摄像头，虚拟角色第一次实现了与现实世界的融合&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这就是由&lt;strong&gt;初创公司 Xmax AI 推出的首个虚实融合的实时交互视频模型 X1&lt;/strong&gt;，没有复杂的 Prompt，不需要漫长的渲染等待，只需要手势进行交互，就可以让虚拟世界与现实相连，在镜头中令「幻想」成真，让用户体验到实时交互的心流体验。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGlsaCFJibAZhMSdk35bOicqZsIwV7qpS3zpdg5lnpbwuqloZDUbNGRKO9pTj2riafM8fCZswMUsyYJdWeVSptgvCv0FtBcJN7qSY/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7287037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503532523" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/3cf72ce8-5bc6-4ebc-ba7e-13d498a08b31/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;目前，Xmax AI 已通过一款技术演示型应用 X-cam（目前开放 testflight 下载），将 X1 的能力开放给部分用户体验，感兴趣的朋友可以通过文末提到的方式获取邀请码，近距离体验一下技术的边界。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「虚实融合 + 实时交互」，视频生成进入「人人可玩」时代&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;过去这一年多，AI 视频生成领域可以说是遍地开花、神仙打架。&lt;/p&gt;&lt;p&gt;数据显示，&lt;strong&gt;2024 年全球 AI 视频生成市场规模已达 6.148 亿美元，预计到 2032 年将飙升至 25.629 亿美元&lt;/strong&gt;。在市场的强需求推动下，从 Sora 到 Runway，各路玩家都在沿着「更强的生成能力」方向极力狂奔：卷画质、卷时长、卷分辨率&amp;hellip;&amp;hellip;&amp;nbsp;&lt;/p&gt;&lt;p&gt;仔细看下来，整个赛道，大多数玩家选择的技术路线依然是文生视频，致力于面向专业领域的创作者 &amp;mdash;&amp;mdash; 影视、广告、内容工业等，打造更强大、更完善的生产力工具。&lt;/p&gt;&lt;p&gt;可不得不承认，在当前的「视频模型军备竞赛」中，普通用户似乎没有参与到狂欢中，感受就是「热闹是他们的，我什么也没有。」&lt;/p&gt;&lt;p&gt;原因很现实，首先是上手难，当然，很多视频生成工具操作起来已经很便捷，可很多时候写出精准的 Prompt 依然像是在编写代码，而且等待时间长，生成时间动辄从数秒到数分钟，再到数十分钟不等，缺乏即时反馈的快感。而漫长的等待后，得到的也不过是一段存在于屏幕里的「只能看、不能碰」，与当下日常生活毫无关系的虚拟视频。&lt;/p&gt;&lt;p&gt;Xmax AI 敏锐地捕捉到了这一点：&lt;strong&gt;AI 视频生成要想真正走入大众，就不能仅停留在「工具」阶段，要容易上手，要让大众有参与感，能够「玩」起来。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d9ef6218-bd9b-4b08-bb97-3c1ba62b3d24/1770619582103.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;可这也就意味着，在基础视频生成能力之外，行业还需要跨越两座「大山」：一是&lt;strong&gt;降低交互门槛&lt;/strong&gt;，改变传统的文生视频工具需要专业想法和 Prompt 撰写能力的方式；二是&lt;strong&gt;要与现实世界有更多结合&lt;/strong&gt;，人是生活在现实中，文生视频模型一定程度上确实满足了完全虚拟化的想象，可人对现实的幻想并没有被满足。&lt;/p&gt;&lt;p&gt;基于此，Xmax AI 走了一条截然不同的路线：推出首个虚实融合的实时交互视频模型 X1，让视频生成告别键盘输入，回归人类最本能的手势与触控，仅需要一个手机摄像头，就能打破虚拟与现实的「壁」。&lt;/p&gt;&lt;p&gt;具体来看，基于 X1 强大的端侧实时生成能力，Xmax AI 将这一技术落地为四大核心玩法：次元互动、世界滤镜、触控动图、表情捕手&amp;hellip;&amp;hellip; 每一台手机似乎都变成了连接虚实的「魔法棒」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;次元互动&lt;/strong&gt;：这就是前面那个视频所展示的能力，手机摄像头拍摄现实场景，任意上传一张角色参考图，就可以将该角色在镜头中「召唤」出来。&lt;/p&gt;&lt;p&gt;比如下面这个小兔子，你可以在镜头前伸出手与它互动，捏一捏、拍一拍，甚至将把它托到手上。视频中可以看到，当抚摸到兔子眼睛旁位置时，它会跟随人的动作转头，甚至可以看到绒毛因为触碰而遮盖眼睛的情况，没有延迟，因为它所有的物理反应都是 X1 模型实时生成的，所以，看起来就好像真的在抚摸一个真实存在的生命体。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/85d74f11-1357-49ad-9f87-e91ad965bdde/1770619611538.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;不仅仅是动漫角色，可以说是任何自己喜欢的纸片人、宠物、毛绒玩具，都可以在镜头中「活」过来。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;世界滤镜&lt;/strong&gt;：任意上传一张风格参考图，就可以将手机摄像头拍摄的画面实时转换，变成指定的风格，例如梵高画风、乐高画风等。可以用于渲染环境，也可以用于渲染人物，甚至可以用于渲染屏幕内容，像是正在玩的游戏画面。&lt;/p&gt;&lt;p&gt;直接来看一个例子，下面视频中的小姐姐通过选取不同风格的参考图，让自己「化身」为图片所示风格的人物，可以是经典动漫中的二次元虚拟形象，也可以是乐高积木风格。而且，当小姐姐做出挥手或是摇头动作时，视频中「变身」后的人物或形象会实时跟着做出相应的动作。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2d1caf9d-83bb-434b-a8e3-c1b7482e1215/1770619628159.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;触控动图&lt;/strong&gt;：让静态照片「活」过来、动起来，不再需要复杂软件。对于任意一张照片，都可以在触摸屏上对照片中的角色进行拖拽控制，让它实时运动起来。&lt;/p&gt;&lt;p&gt;比如下面视频中动漫风格的小兔子，左右拖动它的耳朵，它就开始左右摇头；上下挥动，它就做出被拍脑袋的动作；拖动嘴角，它会露出微笑。「实物」也可以，给自家猫咪狗子拍张照上传，就可以让它挥手、抡拳，跳起舞；眨眼、吐舌、卖起萌。甚至是「恶搞」的，将刘海剪成整齐模样的马，也在镜头下开始摇头晃脑&amp;hellip;&amp;hellip; 就像在操控提线木偶，轻松赋予静止图像以生命力。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/1fb0f76d-2b7e-436e-8827-c652e1c90b57/1770619667166.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;表情捕手&lt;/strong&gt;：将相机镜头对准任意的人或物体，选择一个「大拇指」或「怒气冲冲」的 Emoji，AI 就会实时「捕捉」对方的特征，实时生成一个神态精准、魔性十足的动态表情包。这简直就是「社交神器」，以后聚会也不用担心冷场，随时就可以拿出来玩一下。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/829c893d-f2d7-4e10-b30b-5a0cb074a913/1770619679138.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;强大能力背后的技术挑战与实现&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;是不是很好玩，即便是对技术没什么了解，也可以轻松上手。但在业内人士看来，&lt;strong&gt;这不仅是产品的创新，更是工程能力的「暴力美学」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;「有趣体验背后，是极高的技术挑战。」Xmax AI 向机器之心透露，要实现上述这些效果，必须同时解决当前 AI 行业的三大痛点：&lt;/p&gt;&lt;p&gt;首先是&lt;strong&gt;极致实时&lt;/strong&gt;，从上面的视频中也可以看出来，视频中的人物或是形象的反应随时能够跟着手势变，给用户产生一种「我在和它互动」的感觉，而这就要求延迟必须控制在毫秒级，可当前市面上的大多数所谓「实时」模型响应往往需要数秒，难以满足 Xmax AI 想要在交互场景中呈现的效果需求。&lt;a href="https://mp.weixin.qq.com/s/xnaOGvC5_EVYxsJYxVE_xQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a55fc9f3-417d-433f-959c-ac958afc21ac/1770619702892.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;其次是&lt;strong&gt;意图理解&lt;/strong&gt;，Xmax AI 的想法是希望交互方式多种多样且自然，对普通人来说门槛足够低，这就要求模型做到能够自动理解人的意图，并实时生成精准的反馈结果。可当前大多数模型都是文生视频、图生视频，无法实现这些手势交互效果。比如，对于模型来说，当人做出「捏」这个动作时，要读懂其中的意图，可要比读懂一段文字难得多。&lt;/p&gt;&lt;p&gt;另外，还存在&lt;strong&gt;数据稀缺&lt;/strong&gt;的问题，对于整个 AI 行业来说，数据都足够重要却又极致稀缺，更何况是相对小众的「虚实融合交互数据」，生产成本高，构造难度极大。但现实又是，想要实现好的虚实融合的效果就必须基于大量且专业的高质量训练数据。&lt;/p&gt;&lt;p&gt;这些挑战一度让 Xmax AI 犯了难。&lt;/p&gt;&lt;p&gt;但需要注意的是，&lt;strong&gt;Xmax AI 是一支既懂底层算法，又懂工程化落地，还拥有敏锐产品嗅觉的「特种部队」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;创始人史佳欣，出身于华为「天才少年」计划，是一位典型的技术极客。联合创始人梁宸，现任港科大（广州）助理教授、博导。联合创始人翁跃庭，是一位「六边形战士」型的全栈工程师。而公司核心技术团队则都是来自清华大学 KEG 实验室和 HCI 实验室的人才，是国内大模型领域和人机交互领域的顶尖力量。&lt;/p&gt;&lt;p&gt;不仅如此，团队核心成员也大都在字节、快手、华为、阿里等头部 AI 大厂历练过，有着丰富的技术落地实践经验。&lt;/p&gt;&lt;p&gt;因此，面对上述这些挑战，Xmax AI 交出了一份「硬核」的技术答卷。&lt;/p&gt;&lt;p&gt;针对极致实时性需求，Xmax AI 进行架构创新，提出了&lt;strong&gt;端到端的流式重渲染视频模型架构&lt;/strong&gt;，实现了帧级别的自回归 DiT（Diffusion Transformer），并通过多阶段的蒸馏压缩和对抗训练，百倍提升了每一帧画面的扩散采样速度。不仅将延迟压低至毫秒级，更是通过自研的「循环回归架构」打破了时长的限制，支持无限时长的连续生成。&lt;/p&gt;&lt;p&gt;针对模型对意图理解的高要求，Xmax.AI 则构建了&lt;strong&gt;统一的交互模型架构&lt;/strong&gt;，让模型既能理解摄像头透视下的空间三维关系，也能理解屏幕触控下的平面二维操作，从而对于用户的各类交互行为，模型都能够实现精准的意图识别。&lt;/p&gt;&lt;p&gt;而针对「数据荒漠」难题，Xmax AI 则搭建了&lt;strong&gt;虚实融合数据的合成管线&lt;/strong&gt;，利用半自动化方式，低成本、批量化地生成了高质量的交互训练数据，构建了难以复刻的行业壁垒。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;写在最后&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;体验了这么多玩法，相信大家已经隐约感知到 Xmax AI 想做的事情了。如果说 Sora 代表的是一条极致强化生成能力的路线，让 AI 学会拍电影、构图、运镜、叙事，那么 X1 则是希望 AI 能够陪你玩，随时出现在你周围的生活场景中。&lt;/p&gt;&lt;p&gt;从这个角度来看，对于 Xmax AI 团队而言，&lt;strong&gt;X1 模型仅仅是一个开始&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;其实从前面 X1 的模型能力展现上也可以看出来，Xmax AI 不是想「再造」一个专业的视频创作工具，开发一款 App，更是在&lt;strong&gt;试图搭建下一代内容交互引擎，重新定义用户与 AI 生成内容之间的个性化交互方式&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;在他们的愿景里，这个新时代中，那些曾经只能存在于影视作品和虚拟世界中的角色，不管是数码宝贝，还是银翼杀手式的仿生生命体，都可以走进现实，成为虚实融合的「数字生命体」，进入家庭，成为用户的虚拟陪伴、虚拟宠物等。&lt;/p&gt;&lt;p&gt;与此同时，「万物可交互」也不再只是一个空想，不管是刷短视频、看直播，还是视频通话、线上会议，都可以实时改变视觉形态，一边看一边玩，带来全新的个性化体验；社交互动变得更立体、更有趣，摄像头化身「精灵球」，随时随地「捕捉」一个好友过来，对 TA 进行打扮&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;也就是说，Xmax AI 所做的，是通过 AI 将「幻想」拉得更近，近到可以触碰、互动、分享，真正融入人们的日常生活。&lt;/p&gt;&lt;p&gt;正如 Xmax AI Slogan 所言，&lt;strong&gt;Play the World through AI（用 AI 玩转世界），让世界触手可「玩」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;最后，感兴趣的朋友可以通过 testflight 邀请链接下载 APP，下载后在登录界面点击申请邀请码，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;也可以通过 Xmax AI 官网&lt;/span&gt;来提前体验、感受这一切。这一次，你可以亲自推开那扇通往虚实融合世界的「门」。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;testflight 邀请链接：https://testflight.apple.com/join/8sWgKZeQ&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"text-align: justify; margin-left: 8px; margin-right: 8px; line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;Xmax AI官网链接：&lt;/span&gt;https://xmax.ai/&amp;nbsp;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>英伟达世界模型再进化，一个模型驱动所有机器人！机器人的GPT时刻真正到来</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 09 Feb 2026 14:44:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-02-09-7</link>
      <guid>https://www.jiqizhixin.com/articles/2026-02-09-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;编辑｜冷猫&lt;/section&gt;&lt;p&gt;驱动具身智能进入通用领域最大的问题在哪里？&lt;/p&gt;&lt;p&gt;我们认为，核心问题在于&lt;strong&gt;「跨具身（cross-embodiment）迁移」&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;当然，具身智能执行通用复杂任务的核心是一个完善的世界模型。但是，大多世界模型其实并没有我们想象的那样具备极强的泛化性和迁移能力。&lt;/p&gt;&lt;p&gt;简单来说，这些用在机器人或是智能汽车上的世界模型，基本都是在某个固定的硬件平台上设计训练的，大多不具备很强的泛化能力，跨具身迁移几乎靠运气。&lt;/p&gt;&lt;p&gt;说白了，大多数机器人今天学到的不是 「世界是如何运作的」，而是 「在这台机器该怎么动」。我们需要能学到一个真正理解物理与因果的世界模型 —— 知道世界会怎么变、动作会带来什么后果，才能在不同身体、不同环境中迁移与泛化。&lt;/p&gt;&lt;p&gt;在这个问题上，作为算力的王者，深耕各类世界模型的英伟达再一次发力，构建了一个全新是世界模型，一切都是 Zero-Shot 的。&lt;/p&gt;&lt;p&gt;最近，&lt;strong&gt;英伟达 GEAR 实验室提出 DreamZero，一种基于预训练视频扩散骨干网络构建的世界动作模型（WAM）&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;这是一个拥有 140 亿参数的模型，能够让机器人仅通过简单的文本提示就完成此前从未见过的任务。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/138cced5-668d-450c-ad86-db8653f6d33f/1770619256775.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;实验室负责人 Jim Fan 将其称为机器人领域的&lt;strong&gt;「GPT-2 时刻」&lt;/strong&gt;：研究团队只需输入想法，机器人就能执行相应动作。目前，该模型的代码已在 GitHub 上开源。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGtibHtGULLqkMuKvV1iayuVtYJuMLOD4mcGC3qvOGzc6xgX1hUvky7DibOulicDmick2bjH7RrR7hq3cIUCnibBgVPFPTYxEZYZ8RJM/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.9166666666666666" data-type="png" data-w="1080" data-width="1146" data-height="1050" data-imgfileid="503532131" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/ae63826a-e9ce-4864-97a4-c6d1e91328ba/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_jpg/5L8bhP5dIqHMfVwnl0mYnwzYGJxyLGXIXGjyRjX4uanVbmLL3pEpYAxgXMsLCOxcFqnvmnhuicRY0DNG5yfVnqxtCongS6VFLDjl34ey0YLs/640?wx_fmt=jpeg#imgIndex=2" data-ratio="0.45" data-type="png" data-w="1080" data-width="1296" data-height="584" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/5L8bhP5dIqHGWetuQnqDKk28X3y6UE71yxzbsG9tUqmnJ2L2YfTOH7v3GicxrUkhBkkWrz6bIK2j24SuzhwQHyUSGw6amC1pQ30BkxjxticicU/0?wx_fmt=png&amp;amp;from=appmsg" data-cropx2="1271.3356401384085" data-cropy2="571.7647058823529" data-imgfileid="503532132" data-aistatus="1" data-original-style="width: 567px;height: 255px;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/9cda8a4a-9b7a-4f59-9c50-fd1175d191bd/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;论文标题：World Action Models are Zero-shot Policies&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://dreamzero0.github.io/DreamZero.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Github 链接：https://github.com/dreamzero0/dreamzero&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不同于传统的 VLA 模型，WAM 通过联合预测未来世界状态与动作来学习物理动力学，并以视频作为世界演化的稠密表示。通过对视频与动作的联合建模，DreamZero 能够从异构机器人数据中高效学习多样化技能，而不依赖重复示范。在真实机器人实验中，相比最先进的 VLA，&lt;strong&gt;DreamZero 在新任务与新环境的泛化上实现了超过 2× 的提升&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;至关重要的是，通过模型与系统层面的优化，研究团队让一个 140 亿参数的自回归视频扩散模型实现了 7Hz 的实时闭环控制。此外，研究团队展示了两种跨具身迁移能力：仅使用 10–20 分钟的人类或其他机器人纯视频示范，即可在未见任务上带来 超过 42% 的性能提升。更令人惊讶的是，&lt;strong&gt;DreamZero 只需 30 分钟的 「玩耍数据」，就能适配到全新的机器人，同时仍保持零样本泛化能力&lt;/strong&gt;。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqFUKSqzXgHFEmvc2FqobKv2C0c2V9fazWI1N8wM0O4cYtgVhc0Nck1ibyoYsH4m7RzkKMRLRy4rhAsYAT9bEpmsctic57fqfrGTY/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=3" data-ratio="0.7824074074074074" data-type="png" data-w="1080" data-width="2040" data-height="1596" data-imgfileid="503532134" data-aistatus="1" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/9d654a02-8b0d-4316-bca5-11a4cff8255a/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero 整体概览。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;图中展示了 DreamZero 通过联合预测视频与动作，世界动作模型继承了关于世界物理规律的先验，从而实现了：&lt;/p&gt;&lt;p&gt;1）从多样、非重复的数据中高效学习；&lt;/p&gt;&lt;p&gt;2）在开放世界场景中的强泛化能力；&lt;/p&gt;&lt;p&gt;3）仅依赖纯视频数据即可完成跨具身学习；&lt;/p&gt;&lt;p&gt;4）对新机器人的少样本快速适配。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-aistatus="1" data-height="692" data-imgfileid="503532135" data-ratio="0.3435185185185185" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEibvic8HKKwBzMrL1gziboHMAUK0D5o6YQBhKyYxDE4Ugr0Tm8tEgN1SGP4zjt9XnvS3Fty6IbarIISjiaXKhLjEuLGkkoqZGobvk/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" data-width="2012" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/89189648-87e5-49fe-bb32-627a4e98c440/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero 的模型架构。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;大多的预训练的视频扩散模型凭借来自网页规模数据的丰富时空先验，成为构建机器人策略的理想骨干网络。然而，将这类模型转化为高效的世界动作模型仍面临关键挑战：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1）视频–动作对齐&lt;/strong&gt;：联合预测视频与动作要求对视觉未来与电机指令进行紧密耦合，但如果只是简单地将独立的视频头与动作拼接，往往会导致二者对齐失效；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2）架构设计&lt;/strong&gt;：尚不清楚双向架构还是自回归架构更适合 WAM，这关系到多模态对齐、误差累积以及推理效率等关键问题；&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3）实时推理&lt;/strong&gt;：视频扩散模型需要在高维潜空间中进行多步迭代去噪，使其在闭环控制场景下速度过慢、难以实用。&lt;/p&gt;&lt;p&gt;为此，DreamZero 通过模型设计选择有效应对了上述挑战。&lt;/p&gt;&lt;p&gt;模型接收三类输入：视觉上下文（通过 VAE 编码）、语言指令（通过文本编码器）、以及本体感知状态（通过状态编码器）。这些输入随后被送入一个基于 Flow Matching 的自回归 DiT 主干网络，由其联合预测未来的视频帧与动作，并通过各自独立的解码器输出结果。&lt;/p&gt;&lt;p&gt;在训练阶段，模型以分块（chunk）的方式工作：在给定干净视频上下文作为条件的情况下，对加噪的视频与动作潜变量进行去噪。在推理阶段，模型的预测会以异步方式在真实世界中执行，同时将真实观测结果回灌到 KV cache 中，以防止误差随时间累积。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队在 六种设置下展示了 DreamZero 的能力 —— 其中 五种用于测试泛化，一种用于实时部署。&lt;/p&gt;&lt;p&gt;相关的训练数据以及实验结果的演示可以参考以下链接：&lt;/p&gt;&lt;p&gt;https://dreamzero0.github.io/evals_gallery/&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AgiBot 预训练：已见 &amp;amp; 未见任务&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;研究团队对预训练模型进行开箱即用评测：任务来自预训练分布，但在未见对象的新环境中进行零样本测试。DreamZero（也包含从零训练版本）取得 62.2% 的平均任务进度，相比最佳预训练 VLA 基线（27.4%）提升 超过 2×。从零训练的 VLA 几乎为零；预训练 VLA 有一定进展，但幅度有限。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/5L8bhP5dIqEkNaZyVPMJjvI71BOX6Ihib0TR3zZ0azZ1DXYsrPuiaoPycM3e4Iib8yBHBn54SBvS7ianQbHVvWdb5Ma30ticBqbmfIMmEruvM3xU/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=5" data-ratio="0.6027777777777777" data-type="png" data-w="1080" data-width="1742" data-height="1050" data-imgfileid="503532136" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f931095c-5597-4574-a19a-289947cfb9fb/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;对于训练中完全未出现的任务（如解鞋带、握手），DreamZero 仍达到 39.5% 的任务进度，而 VLA 再次表现吃力。值得注意的是，预训练 VLA 在未见任务上的有限进展，主要源于其无论指令如何都倾向于执行 「抓取 - 放置」 的默认动作，显示其过拟合于主导训练行为，而非真正理解新任务语义。研究团队在 4 台机器人、不同环境与物体上，对每个检查点进行了 80 次 rollouts。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqHskv33PIFqGHlFMbaXMZRxXTciciap07N9U4OfUjn3qkBUREBwMvQwW9NcsDz4rv4XXbmBCicicXOdmiadk63SC5HgZxRibhVS4PqkE/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=6" data-ratio="0.6185185185185185" data-type="png" data-w="1080" data-width="1762" data-height="1090" data-imgfileid="503532137" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/f4d76bc3-d75b-4670-bd94-89652612fae6/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;DROID：已见任务 &amp;amp; 未见动作&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为验证在公开数据上的效果，研究团队在 DROID（最异构的开源机器人数据集之一）上训练 DreamZero，并评测 20 个已见任务与 20 个未见动词任务（DROID 中未出现的动作）。DreamZero 显著优于预训练基线，在未见动词上取得 49% 的任务进度，而最先进的 VLA 仅为 25–32%。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/5L8bhP5dIqGAP5sWm9BDRSlG2xImlQEF5arEsdpqKVCKs9mLLthhLaVJv4ONEibTHFYuqBVPlzAeISuApIzmLhSwPPIEgThM5L046p9P94bo/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=7" data-ratio="0.4638888888888889" data-type="png" data-w="1080" data-width="1676" data-height="778" data-imgfileid="503532138" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/26de7dc1-bf10-4c51-88ec-9dc57ba50d10/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;后训练：分布外泛化&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本部分研究 WAM 在任务特定微调后是否仍保留泛化能力。研究团队在 三项下游任务上进行后训练：叠衬衫、装水果、清理餐桌。DreamZero 在三项任务上均表现更强，表明后训练后仍保持环境泛化能力。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e1f2c384-5b8b-4647-81bd-44353d65067f/1770619337408.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;跨具身迁移&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;仅用 30 分钟的玩耍数据（55 条轨迹），DreamZero 即可适配 YAM 机器人，并对南瓜、泰迪熊、纸袋等新物体实现零样本泛化，同时展现出强大的语言指令遵循能力。来自 AgiBot 预训练的知识可直接迁移，无需大规模重训。这是目前效率最高的具身迁移：以往需要数百小时示范的工作，能够在 30 分钟内完成（未使用任何其他 YAM 数据）。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6cac4cd7-8e43-4f85-8fb4-7cf456a4cd28/1770619347041.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;交互式提示&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;机器人基础模型的 「提示时代」 已经到来。研究团队展示了交互式提示的实战：带着机器人走到不同地方，让人们直接用语言提出新任务。机器人能够完成多种令人惊喜的操作。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/6173f36e-897b-4202-8a54-46c380fffb64/1770619365955.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实时推理&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;通过模型、系统与实现层面的优化，DreamZero 实现了 每个动作块 150ms 的实时推理，支持 7Hz 闭环控制。结合异步推理与动作块平滑，执行过程更加流畅、响应迅速。研究团队对比了 16 / 4 / 1 个扩散步数的效果：步数越少延迟越低，而 DreamZero-Flash 即便在单步推理下也能保持性能。研究团队还展示了动作块平滑与异步推理对执行质量的影响。&lt;a href="https://mp.weixin.qq.com/s/Lxs9FvEkc5dC5MGNsj-8Cg"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/8eb61091-d1b3-4c15-af45-f989d6b3e6eb/1770619375571.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; DreamZero (16 diffusion step) + async &amp;amp; action chunk smoothing&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;零样本泛化能走多远？ 研究团队持续对 DreamZero 进行压力测试：在从未训练过的任务、从未见过的环境中探索能力。从扇汉堡、按电梯按钮，到敲木琴、摇铃鼓，不断涌现出令人惊讶的新能力。&lt;/p&gt;&lt;p&gt;DreamZero 只是开始 —— 它代表了基于视频世界模型的新一代机器人基础模型浪潮。&lt;/p&gt;&lt;p&gt;更多信息，请参阅原论文。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
