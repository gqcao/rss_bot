<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>Manus被收购，智谱也定了8天后上市</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 12:12:45 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30-5</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30-5</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ba352966-44ab-406e-9c07-fd783c62cd08/1767067888960.png" style="width: 700%;" class="fr-fic fr-dib"&gt;AI 大新闻，一桩接一桩。&lt;/p&gt;&lt;p&gt;早上刚传来 &lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651009685&amp;idx=1&amp;sn=bfc44e95b106534d25b4002c3e701ee3&amp;scene=21#wechat_redirect" target="_blank"&gt;Manus&amp;nbsp;&lt;/a&gt;被 Meta 收购的消息，很快，围绕「全球大模型第一股」的竞速，也传来靴子落地的声响。&lt;/p&gt;&lt;p&gt;12 月 30 日，北京智谱华章科技股份有限公司（以下简称「智谱」）正式启动港股招股。招股期将持续至 2026 年 1 月 5 日，并计划于 2026 年 1 月 8 日以股票代码 &amp;ldquo;2513&amp;rdquo; 在香港联交所主板挂牌上市。&lt;/p&gt;&lt;p&gt;根据招股安排，智谱拟进行全球发售 3741.95 万股 H 股，其中香港公开发售 187.1 万股 H 股，国际发售 3554.85 万股 H 股。&lt;/p&gt;&lt;p&gt;IPO 的定价与募资规模也随之揭晓 &amp;mdash;&amp;mdash; 每股发行价定为 116.20 港元。在扣除相关发行费用后，预计本次募资规模约 43 亿港元，对应的 IPO 市值预计将超过 511 亿港元。&lt;/p&gt;&lt;p&gt;公开信息显示，智谱在私募市场的累计融资额已达 83.44 亿元，最新估值攀升至 243.77 亿元。这意味着，在迈向上市的关键一跃中，智谱的市值几乎实现翻倍，如此幅度的「溢价上市」，也是一次难度不低的市场挑战。&lt;/p&gt;&lt;p&gt;基石投资者阵容同样颇为亮眼。公告显示，基石投资者合计拟认购 29.8 亿港元，占本次发行规模近七成（假设超额配股权未获行使）。&lt;/p&gt;&lt;p&gt;参与基石认购的机构包括： JSC International Investment Fund SPC、JinYi Capital Multi-Strategy Fund SPC、Perseveranc Asset Management、上海高毅资产管理、WT Asset Management、泰康人寿、广发基金、3W Fund Management 等 11 家投资机构。&lt;/p&gt;&lt;p&gt;在当前港股科技资产整体承压的背景下，如此高比例的基石认购，也为这场围绕「全球大模型第一股」的竞速，写下了更为明确的市场注脚。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibevQYjFcHqxicibadxy8XEzECe8OGHCZk7UgTMic0PB123j7FhH8MTMZhdA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.6435546875" data-type="png" data-w="1024" data-width="1024" data-height="659" data-imgfileid="503526072" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/b65e1e23-4547-439f-83ba-f1e133693e4d/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;烧钱还在继续，大模型开始走向资本市场&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;放眼行业内部，2024 年曾被热捧的 AI 大模型创业阵营「六小虎」，已经出现明显分化：两家选择主动退出基座模型竞争，转而聚焦垂直应用。&lt;/p&gt;&lt;p&gt;其余四家 &amp;mdash;&amp;mdash; 智谱、MiniMax、月之暗面与阶跃星辰 &amp;mdash;&amp;mdash; 仍试图留在大模型这张牌桌之上。2024 年 12 月中下旬，智谱与 MiniMax 先后披露港股招股书。&lt;/p&gt;&lt;p&gt;与 MiniMax 专注 to C 不同，智谱主要专注企业级方案（to B），已落地金融服务、互联网、智能设备、医疗等行业。&lt;/p&gt;&lt;p&gt;今年上半年智谱收入为 1.91 亿元，期内亏损高达 23.58 亿元，AI 研发成本高达 15.95 亿元。&lt;/p&gt;&lt;p&gt;如果说，2023 年一级市场给予大模型创业公司的高估值，更多是押注宏大的技术叙事。那么进入 2024&amp;mdash;2025 年，市场开始更明确地转向模型能力与商业化兑现路径。&lt;/p&gt;&lt;p&gt;即便是头部公司，也难以绕开对基座模型的持续投入，大模型创业公司也要直面能否持续推进模型迭代、探索应用场景落地的挑战。&lt;/p&gt;&lt;p&gt;而这些，在很大程度上，取决于资本市场是否愿意提供长期、稳定的资金支持。&lt;/p&gt;&lt;p&gt;今年 4 月，智谱曾在证监会北京监管局开启 A 股上市辅导备案。但截至 12 月 12 日，公司并未收到中国证监会关于推进 A 股上市的进一步意见或问询。&lt;/p&gt;&lt;p&gt;在此背景下，智谱选择转向港股，为这场高投入、长周期的大模型竞赛寻找更可持续的燃料。同时，也将直面融资能力与市场信心的双重考验 &amp;mdash;&amp;mdash; 是否有人愿意为 AI 的长期投入买单。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;从 GLM 到 MaaS：智谱的大模型技术底座与商业化路径&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;招股书显示，智谱主要提供从算力、API 接口到 MaaS（模型即服务）的服务，支持本地和云端两种部署模式，已落地多个行业。&lt;/p&gt;&lt;p&gt;作为国内从事通用语言模型研究与产业化的代表性公司之一，智谱技术体系以 GLM 为核心，覆盖文本、多模态与面向应用的模型服务。&lt;/p&gt;&lt;p&gt;GLM 属于基于 Transformer 的大语言模型建模范式，通过将自回归生成与掩码预测相结合，实现对理解类与生成类任务的统一建模。该架构最早由智谱与清华大学相关研究团队提出，并在后续模型中持续迭代。&lt;/p&gt;&lt;p&gt;2021 年，智谱发布中国首个专有预训练大模型框架 GLM，并推出了模型即服务（MaaS）的产品开发与商业化平台，通过该平台向外部提供大模型能力与服务。&lt;/p&gt;&lt;p&gt;2022 年智谱发布并开源 GLM-130B（中英双语千亿参数模型），该模型的推出标志着智谱正式将 GLM 体系运用于预训练大语言模型之上。&lt;/p&gt;&lt;p&gt;2024 年 1 月，GLM 系列迎来重要节点，GLM-4 上线，支持更长的上下文，同时推理速度更快，大大降低推理成本。&lt;/p&gt;&lt;p&gt;2025 年 7 月，智谱进一步开源 GLM-4.5。该模型首发 48 小时内，登顶 Hugging Face（全球最大的开源模型平台）热门榜全球第一。&lt;/p&gt;&lt;p&gt;同年 9 月，智谱发布并开源 GLM-4.6，作为基座模型的进一步升级版本，GLM-4.6 主要强化了编码能力。11 月，GLM-4.6 在 CodeArena 上位列全球第一。&lt;/p&gt;&lt;p&gt;12 月，智谱推出最新旗舰模型 GLM-4.7：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;在核心编码方面，相较前一代 GLM-4.6，GLM-4.7 在多语言智能体编程与基于终端的任务上取得了明显提升，SWE-bench 73.8%（+5.8%）、SWE-bench Multilingual 66.7%（+12.9%）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;氛围编程：GLM-4.7 在 UI 生成质量上实现了重要跃升，能够生成更加简洁、现代化的网页界面，并在演示文稿生成方面提供更准确的布局与尺寸控制，整体视觉效果更佳。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;工具调用：GLM-4.7 的工具使用能力显著提升，在 BrowseComp 所覆盖的网页浏览任务中展现出更强的实际操作能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;复杂推理上：在 HLE（Humanity&amp;rsquo;s Last Exam） 基准测试中取得 42.8% 的成绩，相比 GLM-4.6 提升 12.4 个百分点。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibe3yX1pfVXB3uFWfB1V2EKPIf0ve1Qxf4tIibg9UDs4usooO2f4k6DU6A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7064814814814815" data-type="png" data-w="1080" data-width="4426" data-height="3128" data-imgfileid="503526073" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/afc64f4d-b8e5-4e96-a688-cc378c011788/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;与 GPT-5、GPT-5.1-High、Claude Sonnet 4.5、Gemini 3.0 Pro、DeepSeek-V3.2、Kimi K2 Thinking 相比，GLM-4.7 也表现出色：&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9EricQXXByphb4tN0ha6mibemcdmdiaazffKqKyetctnkUiaYdK87CWSlia5MyHN12dHNoq5dAib3tUiaKA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.8796296296296297" data-type="png" data-w="1080" data-width="1419" data-height="1248" data-imgfileid="503526074" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/93e23209-b692-4f11-96e8-458ff2751799/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;与此同时，智谱还发布了面向不同功能的多模态模型，包括 CogView（图像生成）、GLM-4.5V（视觉理解与推理）、CogVideoX（视频生成）等。&lt;/p&gt;&lt;p&gt;在 AI Agent 方面，智谱基座智能体模型为 AutoGLM。12 月智谱将 AutoGLM 的核心模型全面开源，标志着 AutoGLM 在开放生态中的进一步发展。&lt;/p&gt;&lt;p&gt;截至 2025 年 6 月 30 日，智谱模型已为超过 8000 家机构客户提供支持；截至最后实际可行日期，已为约 8000 万台设备提供支持。&lt;/p&gt;&lt;p&gt;在商业化方面，智谱从 2021 年就开始布局 MaaS 的商业模式。&lt;/p&gt;&lt;p&gt;MaaS 平台主要提供四类模型能力，主要覆盖语言模型、多模态模型、智能体模型和代码模型四类核心模型能力，并同时提供支持模型微调、模型部署及智能体开发的一体化工具链。&lt;/p&gt;&lt;p&gt;从模型能力的扩展、智能体技术的推进，到 MaaS 商业化体系的逐步成型，智谱已经完成了一轮相对完整的技术与产品布局。&lt;/p&gt;&lt;p&gt;但靴子落地，并不意味着终局已定。随着走向公开市场，高强度的研发投入、不断攀升的算力成本，以及通用大模型商业化路径尚未完全跑通的现实，也被一并置于更透明的审视之下。&lt;/p&gt;&lt;p&gt;上市不是终点，而是一场更长周期的公开测试。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>招生 | 港科大（广州）数据科学与分析学域2026-27博士项目申请开放！</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 12:10:15 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30-4</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30-4</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/66c9033c-7b23-4f20-8eb7-0e62a67de700/1767067785948.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvribsZALh2ibzv7ruqjkUUwqHDJTAhhibDJlNEJqSMxQZmQTE6lyvWAoT1g/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-ratio="6.076851851851852" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525944" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/adf85619-e96e-4821-8b9d-eec7588f67ac/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrf0dKtQGpzB8Xvib7F3BicQFDlGQFFfJak4WDW0asaoRwvcCZZxicARdfg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="7.362037037037037" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525945" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/8ccec82d-4f90-4094-80e5-ee0b733c5516/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrbo5kyg4ia5Wusgj6hRicMRLGyw9LVCO4WY2ySia9qX2kczS3PU2ib27KXA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" data-ratio="6.271296296296296" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525946" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/627231d6-2f93-45d5-9227-81b943c4ceb8/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>清华朱军团队Nature Machine Intelligence：多模态扩散模型实现心血管信号实时全面监测</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 12:08:02 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30-3</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30-3</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-imgfileid="503474618" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/86457e54-503e-42a2-8b69-77c0196c2f8c/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;可穿戴健康监测信号由于监测难度高、观测噪声大、易受干扰，高质量的心血管信号仍难以长期便捷获取，这是智能健康监测系统始终面临的现实困境。近日，清华朱军等团队提出了一种统一的多模态生成框架 UniCardio，在单扩散模型中同时实现了心血管信号的去噪、插补与跨模态生成，为真实场景下的人工智能辅助医疗提供了一种新的解决思路。相关工作《Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer》于 &lt;strong&gt;2025 年 12 月 29 日在 Nature Machine Intelligence&lt;/strong&gt; 正式上线。&lt;/p&gt;&lt;p&gt;心血管疾病是人类致死的主要病因之一。对个体而言，光电容积描记信号（photoplethysmography，PPG）、心电信号（electrocardiography，ECG）以及血压信号（blood pressure，BP）能够从不同侧面反映同一底层生理过程：PPG 记录皮下微血管容积变化，便于通过可穿戴设备进行连续采集；ECG 反映心肌电活动，但通常需要更严格的电极放置与专业标定；动脉 BP 则常被视为更接近临床 &amp;ldquo;金标准&amp;rdquo; 的血压信息来源，却往往依赖侵入式或更高负担的采集方式。&lt;/p&gt;&lt;p&gt;现实监测因此呈现出明显的 &amp;ldquo;两难&amp;rdquo;：可穿戴信号获取便捷，却更易受到噪声、运动伪影与信号中断的影响；而高质量或更关键的信号采集，则可能带来不适、风险与成本，难以长期连续部署。&lt;/p&gt;&lt;p&gt;过去的研究往往将这一问题拆解为若干 &amp;ldquo;单点任务&amp;rdquo;：有的方法专注于信号去噪，有的方法聚焦缺失片段补全，即从 &amp;ldquo;&lt;strong&gt;低质量&lt;/strong&gt;&amp;rdquo; 信号重建 &amp;ldquo;&lt;strong&gt;高质量&lt;/strong&gt;&amp;rdquo; 信号。还有的方法研究信号模态转换，即从 &amp;ldquo;&lt;strong&gt;易测量&lt;/strong&gt;&amp;rdquo; 信号预测 &amp;ldquo;&lt;strong&gt;难测量&lt;/strong&gt;&amp;rdquo; 信号。这些方法已在各自任务上取得了进展，但一个显著的局限在于：多数模型仍然是任务特定、模态特定，难以在同一个模型中同时覆盖多任务、多模态、多条件建模，也难以充分利用心血管信号之间天然存在的相关性与互补性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrTletZ3n2ibhS6elc2IGN4iagTGCj3d5K7CnAYAJ5hg4csFGTe8acwQicg/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.48703703703703705" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526028" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/67e21ac8-fe68-4c09-a5f7-01d2404a5837/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="2 2 []"&gt;论文链接：https://www.nature.com/articles/s42256-025-01147-y&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;代码链接：https://github.com/thu-ml/UniCardio&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在此背景下，清华朱军等团队提出了 &lt;strong&gt;UniCardio&lt;/strong&gt;，旨在在一个以&lt;strong&gt;统一生成框架&lt;/strong&gt;同时完成两大类核心能力：其一是信号恢复（signal restoration），包括对低质量信号的去噪以及对间断记录信号的缺失片段插补；其二是模态转换（modality translation），即在给定某些可获得信号的条件下，合成难以获取或未被记录的目标信号，从而为真实场景中的心血管监测与分析提供更完整的信号视角。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr4x3TntQxH74n9oEBu3exFcicxTNbtxWwuA3w7NkJgZOZrg5oDDticYGQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=2" data-ratio="1.2916666666666667" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525958" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/2a467a23-bb7e-4d52-ad26-f537a36ae34e/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;UniCardio 实现心血管生理信号的实时全面监测&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;方法介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;从方法角度，UniCardio 的关键并不在于构建一个简单的 &amp;ldquo;点到点映射器&amp;rdquo;，而在于将多模态心血管信号视为同一生理系统的不同观测，进而学习它们之间的多模态条件分布关系。&lt;/p&gt;&lt;p&gt;UniCardio 采用扩散模型 &amp;ldquo;从噪声到数据&amp;rdquo; 的生成范式：在前向过程中，对不同模态采用统一的噪声化机制；在反向过程中，在条件信息的引导下逐步重建目标信号，从而在同一生成框架中覆盖多种输入 &amp;mdash; 输出配置。在此基础上，UniCardio 使用 Transformer 架构来建模时间维度与模态维度上的依赖关系。&lt;/p&gt;&lt;p&gt;为了同时处理多模态、多任务，UniCardio 为每个模态配置了模态专用的编码器与解码器，以提取和还原具有生理意义的波形特征；另一方面，在 Transformer 的注意力计算中引入任务特定注意力掩码，用于显式约束信息流，只允许与当前任务相关的条件模态到目标模态的交互，从而减少无关模态或无关方向的信息干扰，使不同任务能够在同一网络中被稳定表达与联合学习。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrsmAbL32RQnqMq2281rQtPrB5Xzdx8savVxiaIMmU3n2icDwTeofUFe9w/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=3" data-ratio="0.2574074074074074" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525959" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/b36c6a6a-cddc-4d2f-9431-3cf6e691503d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; UniCardio 的多模态生成模型架构&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;另一个不可回避的实际困难在于：随着模态数量的增加，可用的条件组合会迅速增长。如果在训练阶段简单地将所有任务 &amp;ldquo;混在一起学习&amp;rdquo;，往往会导致样本分配不足与任务权重失衡。&lt;/p&gt;&lt;p&gt;针对这一问题，UniCardio 引入了一种面向生成任务的持续学习（continual learning）范式：以 &amp;ldquo;条件模态数逐步增加&amp;rdquo; 的方式分阶段纳入不同任务，以分配足量的训练样本和平衡不同阶段任务的贡献；并结合学习率调度、训练批次组成以及注意力掩码的结构性约束，以缓解持续学习的灾难性遗忘问题。其目标是让一个统一模型在面对不断扩展的模态组合与任务配置时，依然能够保持稳健而一致的综合能力，而不是学会新任务就忘了旧任务。&lt;/p&gt;&lt;p&gt;这种范式还带来了跨任务 - 模态组合的知识迁移效应：在仅涉及较少模态的生成任务上进行训练，能够有效促进模型在涉及更多模态、条件更复杂的生成任务中的表现。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrPJhZ1JB29AYtFdDjgYSxoquBku8R8F8p6kOL7ltLFlKFNcIyRsk8IA/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-ratio="0.45092592592592595" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525960" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/3ba51c3e-7a87-40fa-8b93-906769f5ea84/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; UniCardio 的持续学习训练范式&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实验及结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在实验结果上，UniCardio 在信号去噪、插补与跨模态转换等多模态、多功能任务中，相较于多种任务特定基线方法展现出稳定而一致的优势，并系统性地体现了多模态互补信息的价值。&lt;/p&gt;&lt;p&gt;论文在多项代表性任务中表明：在仅使用单一条件模态时，UniCardio 已能达到或超越相应的任务特定方法相当。而在进一步引入额外条件模态后，生成误差可显著降低，波形恢复的稳定性也随之提升。例如，在 PPG 与 ECG 插补任务中，引入多模态条件后，生成误差下降至原来的三分之一量级；在 PPG&amp;rarr;ECG 等跨模态生成任务中，UniCardio 在参数规模远小于部分生成基线的情况下，依然取得了更优或更稳健的结果。&lt;/p&gt;&lt;p&gt;这些现象表明，统一建模多模态条件分布本身即可带来跨任务的知识迁移收益，而无需为每一种模态组合单独设计模型。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr5r0E3Z7ggGoFMiaRJxCHARPykw7HCFypBm0iaTKoo4bDdjA4N1RRqghw/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=5" data-ratio="0.5531645569620253" data-s="300,640" data-type="jpeg" data-w="790" type="block" data-imgfileid="503525961" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/db458f99-8b08-46ac-b813-93349a40a52c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; UniCardio 的多模态、多功能生成结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;更进一步，论文将生成信号直接用于下游心血管相关应用进行验证，包括异常状态检测与生命体征估计。&lt;/p&gt;&lt;p&gt;在多个未见域数据集上，基于 UniCardio 生成信号得到的下游任务性能，能够显著优于直接使用噪声或间断信号，并在多数情况下接近使用真实信号的结果。例如，在心电异常检测任务中，由 UniCardio 去噪或插补得到的 ECG 信号能够有效恢复关键诊断信息，使检测准确率与特异性大幅提升，逼近真实 ECG 信号的表现；在心率与血压估计任务中，基于生成信号的预测误差也显著低于仅使用可穿戴信号或简单统计基线的情况。&lt;/p&gt;&lt;p&gt;这些结果表明，UniCardio 生成的信号不仅在数值上 &amp;ldquo;更像&amp;rdquo;，而且在功能层面具备直接支撑下游分析的可用性。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrjIHowibovlFq7ia2ArMvKBcxx44iagLNCDIlicvUuxaHEUUicfKMndtfyicw/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5268518518518519" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525962" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/0f63a359-99da-4aaa-9b88-73d675e38420/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 异常状态检测与生命体征估计结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在可解释性方面，这项工作也给出了相对临床友好的论证路径。生成结果不仅追求误差数值的降低，还尽可能保留可被临床专家识别的诊断特征。论文通过可视化展示多类典型 ECG 异常形态在生成信号中的复现情况，并结合临床评估验证其诊断特征的一致性，表明生成信号在形态层面能够与真实生理信号保持良好对齐。只有当生成结果能够被专家识别、理解并用于判断，模型才有可能被谨慎地纳入真实医疗工作流。&lt;/p&gt;&lt;p&gt;此外，论文还指出，扩散模型逐步去噪的生成过程本身提供了可观察的中间状态，有助于人类专家理解信号的生成演化过程，从而进一步增强模型的可解释性与可信度。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrUxlCpRr82icVC3Q0syoLCmdGhB7LPOFDTaIDc0xicHR6sJmHWph5cBeg/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=7" data-ratio="0.700925925925926" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-imgfileid="503525963" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d01c2636-7de7-4f55-a152-ef7d92c978f3/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 典型 ECG 异常的可视化结果&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;总体而言，UniCardio 将心血管信号生成从以往的单任务、单模态组合，推进到了一个更加统一且具备可扩展性的框架：通过多模态扩散 Transformer 在同一模型中覆盖信号恢复与模态转换，并借助持续学习机制来容纳不断增长的条件组合复杂度。从应用前景看，这类统一的多模态生理信号生成范式不仅有望服务于医疗健康领域中的稳健监测与辅助诊断，也可能进一步拓展到脑科学、心理学与认知科学等同样依赖多源生理信号的研究场景。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;清华大学的朱军教授和王立元助理教授为本论文的共同通讯作者，陈泽华博士、苗雨阳博士和王立元助理教授为本论文的共同第一作者，首都医科大学附属北京安贞医院的范泸韵博士和英国帝国理工学院的 Danilo P. Mandic 教授为本论文的共同作者。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>突发！Meta官宣收购智能体初创公司Manus</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 10:13:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30-2</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30-2</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/322054e3-8207-4129-b818-c1c88c35bc62/1767060780408.png" style="width: 700%;" class="fr-fic fr-dib"&gt;就在刚刚，Meta 完成了一项大收购，将智能体初创公司 Manus 收入麾下。&lt;/section&gt;&lt;p&gt;目前，双方交易的具体细节（包括具体收购金额等）尚未公布。&lt;/p&gt;&lt;p&gt;自今年 3 月推出全球首款通用 Agent 以来，Manus 迅速走红，成为人工智能领域的一大焦点。据公开资料显示，今年 4 月份，Manus 母公司宣布完成 7500 万美元融资，估值接近 5 亿美元，投资方包括知名风投机构基准资本在内的多家投资主体。&lt;/p&gt;&lt;p&gt;此后，Manus 总部及核心研发团队搬到了新加坡。&lt;/p&gt;&lt;p&gt;如今，「靴子终于落地」，Manus 被 Meta 收购，迎来了新的发展机遇。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrLMCbGjAvKJCsNYOnoNKvrV39v8UOdprK70A5Ig0SOGBqMSyDtmNzZg/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.9055555555555556" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526030" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/880ad7f6-d781-4dda-b498-12f3109d8fa4/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;Meta 首席 AI 官 Alexandr Wang 表示，「很高兴 Manus 加入 Meta，帮助我们打造令人惊叹的 AI 产品！Manus 团队在探索当前模型的能力潜力方面处于全球领先地位，致力于构建强大的智能体。」&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503526031" data-ratio="0.48055555555555557" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrxqRH6v5yu8XfZA6nhkp2f58dEkvbNMNjibmxEpTaG3oZMHgrxagOxRA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/bbb1bbf2-3a3b-490b-aa22-03aac1bbb4cf/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;Manus 创始人兼 CEO 肖弘发文称，「今天是一个我将终生难忘的时刻。当我们创办 Manus 时，很少有人相信通用 AI 智能体能够成功。我们被告知时机太早，目标太宏大，挑战太艰难，但我们依然坚持建设。在怀疑、挫折和无数个夜晚的徘徊中，我们曾质疑自己是否在追逐不可能的梦想。但是，&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: justify;margin-bottom: 0px;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;我们没有错。&lt;/span&gt;」&lt;/p&gt;&lt;p&gt;他接着说到，「这不仅仅是一次收购。这是对我们一直在构建的未来的验证，证明它是真实的，而且它的到来比任何人预期的都要快。但是，这不是终点。AI 时代的真正开始，不仅仅是会「说话」，更能「行动」、「创造」并「交付」，才刚刚开始。现在，我们将以自己从未想象过的规模来构建它。感谢那些在我们还没有显现出成果时就相信我们的人。最好的还在后头。」&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503526032" data-ratio="0.6814814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrTaxY30ask5iaHjfYdoNLwAqln8domG6MECmY0N1o8FmtyAwUiaicZDVgw/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a2d2fc6c-b06e-4aa5-bf9b-a6c2b16d2887/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;评论区的网友们也纷纷送来了祝贺。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr4MhVCRcJ7Fy4hSyeBTLvmPl5weTJDELfLSWHiaaWfh8r1tkaUuMEcTQ/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=4" data-ratio="0.687962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503526033" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/ca0a3cd3-02bf-4542-8104-6fbd47aeb050/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;以下为 &lt;strong&gt;Meta 公告内容&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;我们很高兴地宣布，Manus 将加入 Meta，携手将领先的智能体带给全球数十亿人，并为使用我们产品中的企业解锁更多机会。&lt;/p&gt;&lt;p&gt;Manus 已经打造了一个领先的自主通用智能体，可以独立执行复杂任务，如市场调研、编码和数据分析。我们将继续运营和销售 Manus 的服务，并将其整合到我们的产品中。&lt;/p&gt;&lt;p&gt;Manus 已经满足了全球数百万用户和企业的日常需求。今年早些时候，Manus 推出了首个通用 AI 智能体，至今已处理超过 147 万亿个令牌，并创建了超过 8000 万个虚拟计算机。我们计划将这项服务扩展到更多的企业。&lt;/p&gt;&lt;p&gt;Manus 的卓越人才将加入 Meta 团队，为我们的消费品和企业产品提供通用智能体，包括在 Meta AI 中的应用。&lt;/p&gt;&lt;p&gt;我们非常高兴欢迎 Manus 团队的加入，并通过他们的技术帮助改善数十亿人和数百万企业的生活。&lt;/p&gt;&lt;p&gt;以下为&lt;strong&gt;&amp;nbsp;Manus 公告内容&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;我们想要分享一个消息给大家：Manus 即将加入 Meta。&lt;/p&gt;&lt;p&gt;对我们而言，这不只是一条新闻，更是对 Manus 在通用 AI Agent 领域里工作的认可。&lt;/p&gt;&lt;p&gt;自发布以来，Manus 专注于构建通用型 AI Agent，帮助用户高效完成研究、自动化和复杂任务。面对全球越来越多用户的使用需求，团队持续迭代产品，努力使 Manus 在实际使用中更实用、更可靠。根据 12 月初统计的数据，上线至今，Manus 已处理超过 147 万亿个 token，并创建了超过 8000 万台虚拟计算机。&lt;/p&gt;&lt;p&gt;我们深信 AI Agent 的发展前景。此次与 Meta 的携手，将进一步巩固 Manus 在 AI 应用层的战略位置 —— 将先进的人工智能能力转化为可规模化、可靠的系统，在实际使用场景中端到端出色地执行用户交给的任务。&lt;/p&gt;&lt;p&gt;当然，确保这一变化不会影响用户的正常使用，是我们最重视的事情 —— Manus 将继续通过 app 和网站为用户提供产品和订阅服务，同时公司将继续在新加坡运营。&lt;/p&gt;&lt;p&gt;到目前为止，Manus 已经为全球数百万用户提供服务、创造价值。在接下来的时间里，我们希望将 Manus 的服务带给 Meta 平台上的数百万企业和数十亿用户。&lt;/p&gt;&lt;p&gt;肖弘表示：「携手 Meta 使我们能够在不改变 Manus 运作方式和决策机制的前提下，在更强大、更可持续的基础上发展。我们对 Meta 与 Manus 合作的前景充满期待。我们将继续迭代产品，为用户提供超预期的服务 —— 这是 Manus 从上线至今得以存在和发展的根本原因。」&lt;/p&gt;&lt;p&gt;&lt;sup&gt;公告 1 链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.facebook.com/business/news/manus-joins-meta-accelerating-ai-innovation-for-businesses&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;公告 2 原链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://manus.im/zh-cn/blog/manus-joins-meta-for-next-era-of-innovation&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>港大联合字节跳动提出JoVA: 一种基于联合自注意力的视频-音频联合生成模型</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Tue, 30 Dec 2025 10:11:02 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-30</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-30</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBv6ax8e99N0eyLy4Qo7OzKR5sgwWkpGv1vxoygrqI14ssGoXb90ibG6Jw/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-ratio="0.5703703703703704" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503474618" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/9499f88a-6e10-4733-a4da-ed71cbbbb04e/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;作者介绍：本文第一作者黄小虎同学，目前是香港大学的三年级在读博士生，导师是韩锴教授。黄小虎的研究方向是以视频为中心的领域，包括音视频生成、视频理解以及视频识别。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;视频 - 音频联合生成的研究近期在开源与闭源社区都备受关注，其中，如何生成音视频对齐的内容是研究的重点。&lt;/p&gt;&lt;p&gt;近日，来自香港大学和字节跳动的研究团队提出了一种简单有效的框架 &amp;mdash;&amp;mdash;JoVA，它支持视频和音频的 Token 在一个 Transformer 的注意力模块中直接进行跨模态交互。为了解决人物说话时的 &amp;ldquo;口型 - 语音同步&amp;rdquo; 问题，JoVA 引入了一个基于面部关键点检测的嘴部区域特定损失 (Mouth-area specific loss)。&lt;/p&gt;&lt;p&gt;实验表明，JoVA 只采用了约 190 万条训练数据，便在口型同步准确率、语音质量和整体生成保真度上，达到了先进水平。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrNQaakk7OY3Y6ibHEyIbXdpWBIQoWeVE1HQC7G72jaYIwj74ZmbG7e8A/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.40925925925925927" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525803" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/32411780-81df-477c-abd6-04d93560d167/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;项目主页： https://visual-ai.github.io/jova/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文地址：https://arxiv.org/abs/2512.13677&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/ZmID9Gmno3F1N2pypq9mlQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/7c1db443-9754-4db9-b48e-4e31bcc3d5eb/1767060547021.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;strong&gt;一、研究背景与动机&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;目前的开源解决方案通常分为两大类别：一类是 &amp;ldquo;级联式&amp;rdquo;，即先生成视频再配音，或者先生成语音再驱动视频生成，这种方式在一定程度上会导致音频和画面的割裂；另一类是 &amp;ldquo;端到端的联合生成&amp;rdquo;，试图同时输出视频和音频。&lt;/p&gt;&lt;p&gt;如下图 a, 现有的端到端方法（如 OVi 和 Universe 等），为了实现双模态对齐，需要在自注意力层 (self-attention) 之外，额外设计融合模块或跨注意力层 (Cross-attention)。这不仅破坏了 Transformer 架构的简洁性，还可能阻碍进一步的数据和模态扩展。&lt;/p&gt;&lt;p&gt;相比之下，JoVA 采用了更加简洁的设计（如图 b），直接使用联合自注意力层 (joint self-attention) 进行两种模态特征的融合与对齐。它同时承担了单模态内的建模以及跨模态的融合任务，无需引入任何新的模块。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525804" data-ratio="0.6527777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrnBHReZeAibbKypzFrDDHrmzCK1DicVDIY5dvQVdUE1YEQP1ObnG7h32Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/b7296e55-c19f-4a08-9fdf-6612d26f320f/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;二、方法设计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. 架构描述&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;JoVA 采用 Waver 作为基础模型。为了实现音频生成，JoVA 首先通过复制预训练视频主干网络 (Backbone) 的参数来初始化音频扩散模型。在特征提取方面，采用了 MMAudio VAE 将原始音频转换为声谱图潜在表示 (Latent Representation)。&lt;/p&gt;&lt;p&gt;音频分支的训练沿用了与视频分支相同的流匹配 (Flow Matching) 目标函数。在预训练阶段，视频和音频模态是独立训练的；而在后续阶段，两者被统一整合进同一个架构中进行并行处理。此外，对于视频生成，模型支持参考图像 (Reference Image) 作为条件输入。该图像经由视频 VAE 编码后，在通道维度上与噪声视频潜特征进行拼接。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525805" data-ratio="0.7314814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrG3qbpfdeuqbMGOuic5dg8sj4qy54VWUM3ls2tTj6zTSuXZ778NrS4SA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/259a20bc-f2c9-4d6c-ac5f-96ef1eeb2697/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 音频 - 视频 - 文本联合自注意力层&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了实现模态间的融合，JoVA 在 Transformer 块内部采用联合自注意力机制（Joint Self-Attention）。具体而言，视频 Token、音频 Token 以及对应的文本 Token 被拼接在一起，输入到共享的自注意力层中进行处理。这种设计允许不同模态的 Token 在每一层都进行直接的信息交换，既保留了各自的预训练知识，又实现了特征融合。为了确保视频与音频在时间维度上的精确同步，模型采用了源自 MMAudio 的时间对齐旋转位置编码（Temporal-aligned RoPE），在时间维度上同步了两种模态的位置编码。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. 潜空间嘴部区域感知监督（Mouth-Aware Supervision）&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了解决人像生成中的唇形同步问题，JoVA 引入了一种针对嘴部区域的增强监督策略。该过程包含三个步骤：&lt;/p&gt;&lt;p&gt;1. 区域定位：首先在原始视频帧上进行面部关键点检测，计算出覆盖嘴部区域的像素级边界框。&lt;/p&gt;&lt;p&gt;2. 潜空间映射：将像素空间的边界框映射到 VAE 的潜空间。这包括空间上的缩放（除以空间下采样因子 s）和时间上的滑动窗口聚合（根据时间下采样因子 t 合并窗口内的边界框），以精确定位潜特征中的嘴部区域。&lt;/p&gt;&lt;p&gt;3. 加权损失：在训练目标函数中引入了专门的嘴部损失项。该损失仅对视频潜特征中的嘴部掩码区域计算流匹配损失，并通过权重系数进行调节。最终的总损失函数由视频损失、音频损失和嘴部区域损失共同构成，从而在不增加推理阶段架构复杂度的前提下，强制模型学习细粒度的唇形 - 语音对齐。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrSl0bibBicLfKiaUzpfibIicTndoQ4ID24oibUhWywiaQEg0W8cwzZGusHyvXQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.09537037037037037" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525794" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/5d3f83b7-ec01-4662-9286-3e9bc4e7cb35/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;如下图，我们可以发现，这种映射方式可以很好地在潜空间定位到嘴部区域：&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525795" data-ratio="0.7472222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrdwEJ03tYo9uddaqWu0dCBNI6IibOnKkbUdJ0a5SLEKpITcEj30cE4Hw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/db3d332a-a395-4e38-b3b5-c265f99d079c/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;三、训练数据集构建&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;作者构建了包含三个部分的训练数据集：Text2Audio（环境音）、Text2Video-Audio（自然场景视听对）以及 Text2Avatar-Speech（数字人 / 说话人视频），总共约 1.9M 的训练样本。数据标注采用了一套自动化流水线：使用 Tarsier2 生成视频描述，Audio-flamingo3 生成音频描述，并利用 Whisper 进行自动语音识别（ASR）以获取语音文本。&lt;/p&gt;&lt;p&gt;在实施细节上，采用两阶段训练策略：先进行语音单模态独立训练（80K 步），再进行联合视听训练（50K 步），并在推理时使用了分类器无关引导（Classifier-Free Guidance）以提升生成质量。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrSuJW78fs7BsZOFzLyrYZ8fbvk9g6dFicgl87zC7l2OEDzZ84Plp0DSA/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.29814814814814816" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525806" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/c9c9f040-ab0d-4590-8637-4a03da696757/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;四、实验结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. SOTA 方法对比&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在 UniAvatar-Bench（作者精选的 100 个样本）和 Verse-Bench（600 个多样化样本）两个基准上进行了评估。对比对象包括两类：一是使用真实音频驱动的视频生成模型（如 Wan-S2V, Fantasy-Talking），二是联合视听生成模型（如 Universe-1, OVI）。&lt;/p&gt;&lt;p&gt;UniAvatar-Bench 表现：JoVA 在整体性能上表现最佳。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;唇形同步（LSE-C）：得分为 6.64，不仅优于联合生成模型 OVI (6.41) 和 Universe-1 (1.62)，甚至超过了使用真实音频驱动的 Wan-S2V (6.43)，证明了嘴部监督策略的有效性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;语音与音频质量：在文本转语音准确性上，JoVA 取得了最低的词错误率（WER 0.18）；在音频生成指标（FD, KL, CE, CU, PQ）上均取得最佳分数。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;视频质量：在动态程度（MS 0.98）和美学评分（AS 0.47）上均领先。虽然身份一致性（ID 0.78）低于音频驱动模型，但在联合生成任务中处于合理范围。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525807" data-ratio="0.3416666666666667" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr3ic5I8RVwicqZtKUOZRiadiaotzdXsicmvhknrWlicNxMYzXNPMyNk9DFVJQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/86e5124b-c9f9-4f26-8b0a-5d7ee0eb4e87/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;Verse-Bench 表现：JoVA 展现了在多样化场景下的鲁棒性。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;语音准确性：WER 低至 0.11，验证了其稳健的语音合成能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;视听对齐：LSE-C 得分为 6.51，略低于 OVI (6.61) 但远高于 Universe (1.62)。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;综合质量：在保持最高视频动态（MS 0.80）和美学质量（AS 0.48）的同时，音频生成的一致性（CS, CE）也达到了最优水平。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525808" data-ratio="0.37777777777777777" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrwuuWaXt3dibeDbZof1H34Key0WeEVaF6w5HkYpiauibg3octWV7u5iapmw/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/7e0e883d-37e6-46ff-8913-5b024a16c5f3/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;模型扩展性与效率分析&lt;/p&gt;&lt;p&gt;研究进一步对比了基于 Waver-1.6B（总参数量 3.2B）和 Waver-12B（总参数量 24B）主干网络的 JoVA 模型性能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;小模型的高效性：仅使用 3.2B 参数和 1.9M 训练数据的 JoVA 模型，其 LSE-C 得分达到 6.20，显著优于参数量更大（7.1B）且训练数据更多（6.4M）的 Universe-1 模型（LSE-C 1.62），并与 10.9B 参数的 OVI 模型具备竞争力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;大模型的性能上限：随着参数量增加至 24B，JoVA 在各项指标上均达到最佳水平（LSE-C 提升至 6.64，WER 降至 0.18）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525810" data-ratio="0.18888888888888888" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrrsvQ5VvicJMdrNOTMgAN800k33sribx3JpVmVTVxgbR3X3Ob6HepicllQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/d6dcfeed-ea2b-4477-86f8-9148240bfe32/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;2. 融合实验对比&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了验证各模块的有效性，作者进行了多项消融实验：&lt;/p&gt;&lt;p&gt;嘴部感知损失（Mouth-Aware Loss）的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;当权重为 0.0 时，模型无法学习细粒度的唇形对齐（LSE-C 仅为 1.39）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;增加权重至 5.0 时，LSE-C 显著提升至 6.64，且未损害其他音频或视频质量指标。这表明针对嘴部区域的显式监督对于实现精确同步至关重要。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525813" data-ratio="0.3444976076555024" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr5U7yPWwKWTpr8h5MpTxapMoF7wt4Qe0pdeqV4t5mLRyzb3jysia9EAg/640?wx_fmt=png&amp;from=appmsg#imgIndex=10" data-type="png" data-w="836" type="block" data-original-style="null" data-index="12" src="https://image.jiqizhixin.com/uploads/editor/c6b9aa36-a250-4d65-af04-689658ae97d8/640.png" alt="图片" data-report-img-idx="10" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;时间对齐 RoPE 的影响：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;采用时间对齐的 RoPE（视频和音频共享时间维度的位置编码）相比未对齐版本，LSE-C 从 6.58 提升至 6.64。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;尽管在音频分布相似度（FD）上存在轻微折损（0.58 vs 0.69），但该设计显著增强了帧级的时间对应关系，更利于人像视频生成。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525814" data-ratio="0.27340823970037453" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrl3oOJPBZfne89uia3Fp9dowNe3ZnjeSoqcs2dxzl88hQricav15cfVGA/640?wx_fmt=png&amp;from=appmsg#imgIndex=11" data-type="png" data-w="801" type="block" data-original-style="null" data-index="13" src="https://image.jiqizhixin.com/uploads/editor/e1fac8de-039b-40a4-b10b-0c3464fb776b/640.png" alt="图片" data-report-img-idx="11" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;联合自注意力 vs. 交叉注意力：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;对比结果显示，联合自注意力（Joint Self-Attention） 机制在唇形同步（LSE-C 6.64）和语音准确性（WER 0.18）上均优于交叉注意力变体。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;特别是带线性适配层的交叉注意力方案表现最差（LSE-C 1.63）。这证实了在统一的注意力空间内直接处理多模态 Token，比通过独立的交叉注意力模块更能促进特征的有效对齐。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525815" data-ratio="0.1925925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrbTyh8QiaGprYO7zTAcTooWYuXt08Viby9l6USpD95xaeztIIh8GyK98w/640?wx_fmt=png&amp;from=appmsg#imgIndex=12" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="14" src="https://image.jiqizhixin.com/uploads/editor/c533d1b8-016c-4547-94ee-ead2fca7a6e1/640.png" alt="图片" data-report-img-idx="12" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>AI引爆内存荒：手机电脑不仅要涨价，还要减配</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 16:55:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-11</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-11</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/86a70f67-943a-44d5-a5ee-1cbc486d771a/1766998276892.png" style="width: 700%;" class="fr-fic fr-dib"&gt;「我一直在告诉大家，&lt;strong&gt;如果你想买（电子）设备，现在就买&lt;/strong&gt;。我自己要买的 iPhone 17 就已经下手了。」这是咨询公司 TrendForce 高级研究副总裁 Avril Wu 在最近接受采访时说的一句话。&lt;/p&gt;&lt;p&gt;她之所以给出这个建议，是因为他们有一个核心判断：AI 发展带来的内存短缺问题，已经波及到消费电子领域，导致电子设备价格上涨。而且这一问题短期内难以缓解。&lt;/p&gt;&lt;p&gt;上周，我们报道了一个&lt;a data-itemshowtype="0" data-linktype="2" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2651009149&amp;idx=1&amp;sn=2261b03f3e9ad5831fe0a5922c5fa142&amp;scene=21#wechat_redirect" target="_blank"&gt;消息&lt;/a&gt;，电脑内存（RAM）—— 这个长期以来在配置里不占大头的组件，现在的价格已经涨到了令人乍舌的程度，一根 256GB 的内存条比一块 RTX5090 显卡要贵 1000 多美元。今年 2 月份还可以用 899 元买到的两根 32GB 内存条，现在已经涨到了 3499 元。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvrE7ibeI3OtwfMuUtEujmtmhdUIcQyyYRTfZyMDnm6RBjlvvXNa6UjVvA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=1" data-ratio="0.7335600907029478" data-s="300,640" data-type="png" data-w="882" type="block" data-imgfileid="503525984" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/be7494b2-3710-40a3-8ce4-bda9b3fb2b3b/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvr8baasw0EGvKucrJePeHIZUeCeiby0D6hNztWjmR5vOOEA6ib0JicjWjzg/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=2" data-ratio="0.2175925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525985" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/83760d34-8551-4454-9ba1-0f69709a1659/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;涨价的根本原因在于：&lt;strong&gt;产能都被 AI 截胡了，目前的内存市场正处于一场由 AI 算力需求引发的「结构性紧缺」中。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;TrendForce 的数据表明，当前，RAM 芯片的需求比供应高出 10%—— 而且增长速度如此之快，以至于制造商每个月购买芯片的成本都大幅增加。&lt;/p&gt;&lt;p&gt;仅本季度，他们为 DRAM（最常见的一种内存）支付的费用就比上一季度高出 50%。而且，如果生产商想更快拿到这些芯片，他们要支付的费用会是原来的两到三倍。&lt;/p&gt;&lt;p&gt;Wu 预计，DRAM 价格将在接下来的季度再上涨 40%，并且她不认为 2026 年价格会下降。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8L9eIe4cvYXicnfDf06cNvryBA2rMSXSIlNGaeGSWQIiaZRrYueLWZkg9yibIUvibswOGwDaBXjyqOLA/640?wx_fmt=png&amp;amp;from=appmsg#imgIndex=3" data-ratio="1.3185185185185184" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525986" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/e4ce4705-6d4b-40d3-bb9c-ace05d7b7e2c/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 网友在 12 月 17 日购买的内存条，8 天后上涨 34%。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI 可能吞噬全球 DRAM 用量近 2 成&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;要理解这场涨价潮的深层逻辑，需要先了解 AI 对内存的依赖程度。&lt;/p&gt;&lt;p&gt;科技咨询公司 Greyhound Research 的 CEO Sanchit Vir Gogia 解释道：「AI 工作负载是围绕内存构建的。」训练和推理系统需要大容量、持久性的内存占用，极高的带宽，以及与计算单元的紧密协同。「你没办法在不损害性能的前提下缩减这些配置。」&lt;/p&gt;&lt;p&gt;以「100 万 token」的长上下文情境为例，即便采用 FP8 等较省容量的格式，推理过程仍可能需要约 60GB 高速 DRAM 保存中间状态；若采用 FP16，需求可能翻倍到 100GB 以上。相较目前常见的 8K token 约 1GB，等同出现约 60 倍的跳增，形成 AI 扩张的隐形成本。&lt;/p&gt;&lt;p&gt;与此同时，AI 公司正在全球范围内快速建设数据中心，投入数十亿美元。这就是为什么 Gogia 认为&lt;strong&gt;这不是周期性的市场波动，而是一种结构性的转变&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;有媒体引用行业专家的预测，到 2026 年，仅云端高速内存的消耗量就可能达到 3 艾字节（EB）。这个数字由三部分构成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;核心推理工作负载：Google 的 Gemini、AWS 的 Bedrock、OpenAI 的 ChatGPT 等主要平台，实时内存需求约为 750PB，考虑到实际部署所需的冗余和安全余量，这一数字将翻倍至约 1.5EB。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;私有云基础设施：Meta 和苹果的私有云，加上中国国内市场，贡献约 800PB。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;下一代模型训练：包括检查点存储和参数保存在内的训练需求，再增加 500PB。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但真正令人担忧的不是这 3EB 本身，而是它对整个内存产业的「挤出效应」。&lt;/p&gt;&lt;p&gt;AI 竞赛的焦点正在从单纯的算力比拼，转向内存容量和推理成本的较量。随着推理过程中需要存储的中间状态数据激增，每个用户、每个 AI Agent 所需的内存都在成倍增长，对 HBM（高带宽内存）和 GDDR7 等高速内存的需求随之飙升。&lt;/p&gt;&lt;p&gt;问题在于，高速内存的制造远比普通内存更「吃」产能：生产 1GB 的 HBM 所消耗的晶圆产能，相当于 4GB 的标准 DRAM；GDDR7 则是 1.7 倍。这意味着，AI 对制造产能的实际占用，远远超过其内存出货量所显示的比例。&lt;/p&gt;&lt;p&gt;有报告指出，2026 年全球 DRAM 总产能预计为 40EB，而 AI 的「等效消耗」将占到总产量的近 20%。考虑到 DRAM 年产能增长仅有 10% 至 15%，这种需求激增将不可避免地挤压 PC、智能手机和服务器 DDR5 等标准 DRAM 产品的供应，加剧短缺风险和价格上涨压力。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;芯片巨头的「二选一」困境&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对 AI 带来的高利润订单，内存芯片制造商纷纷调整产能分配。总部位于爱达荷州的美光科技（Micron Technology）是全球最大的 RAM 制造商之一，上周公布的季度财报超出预期，正是得益于内存芯片价格的上涨。&lt;/p&gt;&lt;p&gt;美光 CEO Sanjay Mehrotra 在财报电话会议上表示：「我们认为，在可预见的未来，整个行业的供应量将大幅低于需求。」&lt;/p&gt;&lt;p&gt;但问题在于：&lt;strong&gt;当芯片厂商将更多产能倾斜给 AI 领域的高端内存时，留给个人电脑、智能手机、游戏设备和电视等消费电子产品的芯片就相应减少了&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;戴尔科技首席运营官 Jeff Clarke 在 11 月 25 日的财报电话会议上直言不讳地谈到了成本压力：「我看不出这些成本如何不转嫁到消费者身上。」&lt;/p&gt;&lt;p&gt;分析人士认为，这一困局短期内没有解决方案。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;内存的短缺，还很有可能连带着让 GPU 的供应也变得紧张。&lt;/strong&gt;根据目前的行业动态和供应链报告，英伟达在 2026 年缩减消费级 GPU（GeForce RTX 系列）产量的消息确实在业界流传，且具有较高的可信度。&lt;/p&gt;&lt;p&gt;根据供应链媒体 Benchlife 和 Board Channels 的爆料，英伟达计划在 2026 年上半年对其最新的 RTX 50 系列（Blackwell 架构） 进行大规模产量缩减。预计相比 2025 年同期，供应量将会减少 30-40%，波及的机型当然首先是显存比较大的高端款，特别是 RTX 5070 Ti 和 RTX 5060 Ti 16GB。&lt;/p&gt;&lt;p&gt;在 2026 年，英伟达预计不会发布全新的 GeForce 架构 GPU（如 RTX 60 系列），而是以 RTX 50 系列的生命周期维护为主，市场的重心将转移到 AI 芯片和「AI 工厂」上。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;不光涨价，可能还要减配&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wu 指出，内存芯片行业正面临严重的产能瓶颈。到 2026 年底，芯片制造商将在现有工厂设施中达到产能扩张的极限。&lt;/p&gt;&lt;p&gt;而下一座预计投产的新工厂 —— 美光正在爱达荷州建设的项目 —— 要到 2027 年才能正式运营。&lt;/p&gt;&lt;p&gt;这意味着，在未来至少一年半到两年的时间里，消费者将不得不面对持续上涨的电子设备价格。正如 Wu 所言：「预计供应商在可预见的未来将继续提价。」&lt;/p&gt;&lt;p&gt;对于普通消费者而言，如果近期有购买电脑、手机或其他电子设备的计划，现在或许真的是出手的时机 —— 因为越等，可能越贵。&lt;/p&gt;&lt;p&gt;而且，「更贵」还不一定更好，因为品牌方正在被迫「缩水」产品配置。&lt;/p&gt;&lt;p&gt;根据 TrendForce 最新发布的报告，2026 年第一季度内存价格的涨幅已超出此前预期，终端产品的物料成本（BOM）压力正逼近临界点。面对这种局面，电子设备制造商不得不采取一系列应对措施，包括将原本计划的降价促销叫停、缩减产品规格（比如原本标配 16GB 内存的手机，可能改为 12GB 甚至 8GB），DRAM 规格被迫向下分级等。&lt;/p&gt;&lt;p&gt;报告还指出，不同价位的产品将面临不同的命运：高端产品会通过调整定价和促销策略来转嫁成本；而中低端产品的处境更为艰难 —— 要么被迫涨价，要么直接加速停产退市（EOL）。&lt;/p&gt;&lt;p&gt;你近期有购买电子设备的打算吗？打算何时出手？&lt;/p&gt;&lt;p&gt;&lt;sup&gt;参考链接：&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.trendforce.com/research/download/RP251208NA&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;https://www.trendforce.com/news/2025/12/26/news-ai-reportedly-to-consume-20-of-global-dram-wafer-capacity-in-2026-hbm-gddr7-lead-demand/&lt;/sup&gt;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>全景视觉的Depth Anything来了！Insta360推出DAP，200万数据打造全场景360°空间智能新高度</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 16:49:04 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-10</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-10</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-imgfileid="503474619" data-ratio="0.5703703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWic1GuW68DykycvknmG9tyBvLRsVGY4rRKCGuKKSkOqnGrvGwXxqqDxHlia88ZCbqyicswl2HC89BcZA/640?wx_fmt=png&amp;from=appmsg#imgIndex=0" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/00bbb55f-0d2e-4de9-b0cf-e45b5c261705/640.png" alt="图片" data-report-img-idx="0" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;在空间智能（Spatial Intelligence）飞速发展的今天，全景视角因其 360&amp;deg; 的环绕覆盖能力，成为了机器人导航、自动驾驶及虚拟现实的核心基石。然而，全景深度估计长期面临 &amp;ldquo;数据荒&amp;rdquo; 与 &amp;ldquo;模型泛化差&amp;rdquo; 的瓶颈。&lt;/p&gt;&lt;p&gt;近日，&lt;strong&gt;来自 Insta360 研究团队、加州大学圣地亚哥分校 (UCSD)、武汉大学以及加州大学默塞德分校的研究者&lt;/strong&gt;共同推出了 &lt;strong&gt;Depth Any Panoramas (DAP)&lt;/strong&gt;。这是首个在大规模多样化数据集上训练的全景度量深度（Metric Depth）基础模型，不仅统一了室内外场景，更通过 200 万量级的数据引擎与创新的几何一致性设计，刷新了多项 benchmark 纪录，在多种 open-world 场景下保持优异的效果。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525534" data-ratio="0.22685185185185186" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6Isv79ZnDDC7dsJCnfRAxMjUoLRed9QHFXickr8ib0uq0icicMQ2xXRelZQdkA/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/f178dbcf-5d2f-4bd9-b6e5-57be800ca2d8/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="0 0 []"&gt;论文标题：Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;项目主页：https://insta360-research-team.github.io/DAP_website/&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;论文链接：https://insta360-research-team.github.io/DAP_website/assets/paper.pdf&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Demo：https://huggingface.co/spaces/Insta360-Research/DAP&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;模型对由 Gemini 或 DiT-360 等合成的全景图同样展现出了极佳的预测效果，生成的深度图边缘锐利、逻辑自洽，是空间 AIGC 链路中理想的几何基石。 除了静态图像，DAP 在处理全景视频流时同样展现出了极佳的预测效果，具备优秀的帧间一致性与稳定性 。&lt;a href="https://mp.weixin.qq.com/s/1JYzIvqOi-YzhBCahI8MMQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/d0f51f3d-c54a-412e-b078-88ccf4dcb1d8/1766997843047.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvvCKUqXmzdut8Ah63vD9fMWekXuKvkSQqtCbc3YulgB9Um5VJ7e4qUg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.7592592592592593" data-s="300,640" data-type="png" data-w="1080" type="block" data-imgfileid="503525525" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/0240ae7a-97c7-4867-8147-a68ab888bd34/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;破局：从「贫矿」到 200 万量级的「数据海洋」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在深度学习时代，数据的规模决定了模型的上限。然而，获取带高精度深度标注的全景数据成本极高，导致学术界长期依赖于几万张规模的小型数据集，如 Stanford2D3D 或 Matterport3D。&lt;/p&gt;&lt;p&gt;为了打破这一僵局，DAP 团队构建了一个规模空前的全景数据引擎，将数据量直接推向了&lt;strong&gt; 200 万&lt;/strong&gt;（2M）级别，除了现有的 Structured3D：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;1.7M 互联网真实全景图&lt;/strong&gt;：从海量网页中收集并精细过滤，覆盖了极为丰富的真实世界场景。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;UE5 模拟器精准补全&lt;/strong&gt;：利用基于虚幻引擎 5 的 AirSim360 模拟器，生成了 90K 张高质量、带像素级深度标签的室外航拍数据，解决了户外训练数据稀缺的痛点。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;AIGC 技术协同&lt;/strong&gt;：引入 DiT360 模型生成了 200K 张室内全景图，进一步增强了模型对多样化室内环境的理解力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvmIGwVibuemico9ibNoiaicaPLdDykQ9XYeiaeLw58niaz31Hftdia6qf2IZSwg/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="0.34296724470134876" data-type="png" data-w="1038" data-width="1038" data-height="356" data-imgfileid="503525527" data-original-style="null" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/a6cc431d-7d42-4968-be4f-056ded24353d/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;三阶段伪标签管线：让「无监督」变「强监督」&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;面对 1.9M 没有任何标签的原始全景图，如何挖掘它们的价值？&lt;/p&gt;&lt;p&gt;DAP 巧妙地设计了一个&lt;strong&gt;三阶段伪标签精炼管线&lt;/strong&gt;，像漏斗一样层层筛选，最终淬炼出高质量的监督信号：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Stage 1：场景不变标注器&lt;/strong&gt;。先用小规模但精准的合成数据（Structured3D + DAP-2M-Labeled）练出一个基本功扎实的标注器，确立物理意义上的深度基准。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Stage 2：写实性不变标注器&lt;/strong&gt;。引入专门的深度质量判别器（Discriminator），从 1.9M 预测结果中筛选出最靠谱的 600K 样本（300K 室内 + 300K 户外），再次训练标注器，消除合成数据与真实场景之间的纹理鸿沟。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3. Stage 3：全量 DAP 训练&lt;/strong&gt;。在汇集了精炼伪标签和原始强监督标签的 2M 数据集上，正式炼成 DAP 基础模型。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525528" data-ratio="0.29259259259259257" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvxnQgxkmk3LB96VXqV4yztAVkFtGf4H3xOWGR6G0N3ibbWBSTGRtAS0w/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/76ee2fce-fa38-46a9-a027-bdb65795b740/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;模型架构细节：DINOv3 骨干+动态距离掩码&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了海量数据，DAP 在模型架构上也进行了设计：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;强大的 &amp;ldquo;大脑&amp;rdquo;&lt;/strong&gt;：采用最新的&amp;nbsp;&lt;strong&gt;DINOv3-Large&lt;/strong&gt;&amp;nbsp;作为特征提取骨干，赋予了模型极强的视觉先验和零样本泛化能力。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;距离自适应（Range Mask Head）&lt;/strong&gt;：模型内置了即插即用的距离阈值分支，允许用户根据应用场景（如室内扫地机器人 vs 户外无人机）切换深度感知范围，有效解决了全景图中远景区域深度分布不均、预测不稳的问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;多维几何优化&lt;/strong&gt;：引入了包括&lt;strong&gt; SILog 损失、锋利度损失（LDF/Lgrad）、表面法线损失&lt;/strong&gt;以及&lt;strong&gt;点云一致性损失&lt;/strong&gt;在内的联合优化。这些损失函数专门针对全景图的等距柱状投影（ERP）进行了畸变补偿，确保预测出的深度图不仅数值准，而且边缘锐利、几何结构不崩塌。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-imgfileid="503525529" data-ratio="0.26296296296296295" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsveoI18ic7iaOj8ZWogDMYgicFZXto6oLNnqO3th5kt31HD3icSeqQeicCeYw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/3b35fef5-74d6-4b6b-95a5-322347506cf0/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;效果：三大主流榜单&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在多项严苛的零样本（Zero-shot）测试中，DAP 展现了优异的效果：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;室内场景（Stanford2D3D / Matterport3D）&lt;/strong&gt;：DAP 的绝对相对误差（AbsRel）大幅下降，在没有针对目标数据集进行任何微调的情况下，依然保持了极高的预测一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;户外场景（Deep360 / DAP-Test）&lt;/strong&gt;：在极具挑战性的户外测试集中，DAP 显著超越了此前的 DAC 和 Unik3D。它预测出的建筑物边缘清晰，天空区域深度稳定，不再出现传统模型的 &amp;ldquo;深度空洞&amp;rdquo; 或 &amp;ldquo;结构扭曲&amp;rdquo;。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvfjCVx8HE9wa4HTpC2sRojvvJA4U5FdPhysJyliattnIZTvUJ4niaJHKQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.34814814814814815" data-type="png" data-w="1080" data-width="2254" data-height="784" data-imgfileid="503525530" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/9513cd28-88d6-440d-8011-3eae39095929/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-height="554" data-imgfileid="503525531" data-ratio="0.5055555555555555" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6Isv2UlxwpDoLqKkyLSriaFQqaBCWlq2Zicic5CaFVyrW3yjqfaicicLwcsRmeA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-type="png" data-w="1080" data-width="1096" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/abdfbdd1-ba6a-4aab-b4a5-597cc3e0ab14/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 50%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;图示对比&lt;/strong&gt;：图中的实测对比中可以看到，对比 baseline 出现的远景模糊和天空深度误判，DAP 无论是复杂的家具纹理还是远处的山脉轮廓，都清晰可见。&lt;/p&gt;&lt;section&gt;&lt;img data-imgfileid="503525532" data-ratio="0.7314814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvhsBEwY5xK7Wy8k0Q16Et0P4B6yzvDNpnLTJEUMZSZPBIiahS6cjP5vA/640?wx_fmt=png&amp;from=appmsg#imgIndex=8" data-type="png" data-w="1080" type="block" data-original-style="null" data-index="10" src="https://image.jiqizhixin.com/uploads/editor/098c241e-084a-496a-be96-df823f25e578/640.png" alt="图片" data-report-img-idx="8" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;全空间智能的新里程碑&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DAP 的出现，标志着全景深度估计正式进入了 open-world 时代。&lt;/p&gt;&lt;p&gt;它不仅能为自动驾驶、机器人避障提供更广阔的 &amp;ldquo;全知视角&amp;rdquo;，也为 3D 场景重建、VR/AR 内容创作提供了极低成本的深度获取手段。正如论文总结所言，DAP 通过大规模数据扩展和统一的三阶段管线，成功构建了一个能跨越室内外、统一米制深度的全景视觉基座。&lt;/p&gt;&lt;p&gt;目前，DAP 的项目页面已经正式上线，相关的代码与模型也已开源。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&amp;ldquo;数据是在全景领域实现 AGI 感知的关键。&lt;/strong&gt;&amp;rdquo;&amp;nbsp;DAP 不仅为机器人全向避障提供了更精准的 &amp;ldquo;眼睛&amp;rdquo;，也为 VR/AR 场景的大规模 3D 重建和场景生成奠定了坚实的技术底座。如果你对全景视觉、空间计算或深度估计感兴趣，DAP 绝对是不容错过的年度之作！&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-height="828" data-imgfileid="503525533" data-ratio="0.5583333333333333" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8kicb6DJDhpMpHyKLrd6IsvXxMOiaC8OBWqFX1iaYzwbEB2UbjBoqgem2icZ0sXudBwkSGgYbrIToEOg/640?wx_fmt=png&amp;from=appmsg#imgIndex=9" data-type="png" data-w="1080" data-width="1482" data-original-style="null" data-index="11" src="https://image.jiqizhixin.com/uploads/editor/02dfef6e-ad1d-483a-9679-7a9022cbd5d0/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>九章云极完成新一轮战略融资，持续巩固全球智算云第一梯队地位</title>
      <description>&lt;![CDATA[工智能基础设施与智算云提供商九章云极宣布完成新一轮融资。]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 16:42:59 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-9</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;工智能基础设施与智算云提供商九章云极宣布完成新一轮融资。本轮由北京信息产业发展投资基金与北京市人工智能产业投资基金联合领投，老股东启辰星跟投。此次&amp;ldquo;国家队&amp;rdquo;资本的战略入股，体现了国有资本对建设先进人工智能设施建设的高度重视，并为九章云极夯实技术领先优势和拓展普惠算力生态注入强劲动力。&lt;/p&gt;&lt;p&gt;在资金规划与战略布局上，本次融资将重点聚焦两大核心方向：其一，持续加码AI加速计算优化技术研发，进一步巩固并扩大公司在AI训练、智能体开发、强化学习等关键领域的技术壁垒与领先优势；其二，加大普惠智算云平台建设投入，致力于打造国内领先的普惠智算生态体系，推动企业级AI应用的规模化落地与商业化普及。&lt;/p&gt;&lt;p&gt;北京AI产投基金自成立以来，始终围绕北京市在人工智能领域的总体布局开展直接投资，其重点方向涵盖人工智能芯片、大模型算法创新、具身智能等关键领域。此前已投资智谱AI等大模型企业，此次战略入股九章云极，显示出其对AI基础设施这一核心细分领域的重点布局，对九章云极的战略价值显著。&lt;/p&gt;&lt;p&gt;2025年，随着普惠算力需求爆发，九章云极旗下九章智算云（Alaya NeW Cloud）发展迅猛。6月，公司宣布全面升级Alaya NeW Cloud 2.0，并发布全球首个强化学习平台&amp;ldquo;AgentiCTRL&amp;rdquo;及智算生态基金。市场数据显示，九章云极的普惠算力战略成效卓越：截至目前，其智能算力纳管规模已突破万PFLOP，以13.1%的市场份额稳居华南地区第三方普惠智算云市场首位；算力服务版图同步扩张，覆盖国内多数&amp;ldquo;双一流&amp;rdquo;高校AI研究团队及QS前100高校，并以68%的占比成为中小企业首选；在已部署智算云的百人规模企业中，其占比与AWS、Lambda、Azure等国际巨头形成有力竞争。凭借&amp;ldquo;普惠AI&amp;rdquo;战略，九章云极已跻身亚太最具潜力创新企业，领跑全球AI基础设施云赛道。&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着模型训练与推理需求持续攀升，智算云正逐步显现为算力供给的终局形态，其在弹性调度、跨区域部署和算力利用效率上的优势愈发凸显：Forrester数据显示，62%的企业决策者计划2026年布局AI原生智算云以提升训练灵活性，76%的管理者认定AI训练工作负载云端部署&amp;ldquo;至关重要&amp;rdquo;，智算云已成为企业在&amp;nbsp;AI&amp;nbsp;时代构建核心竞争力的关键基础设施。围绕本土模型训练需求，九章云极构建起以&amp;ldquo;普惠训练&amp;rdquo;为核心的智算云生态。九章智算云不仅提供算力资源，更通过训练流程优化、资源调度与实践经验输出，引导用户&amp;ldquo;会训练、训得好&amp;ldquo;，推动更多主体真正参与到模型训练这一创造智能的事业之中。&lt;/p&gt;&lt;p&gt;北京信息产业发展投资基金与北京市人工智能产业投资基金表示：我们看好九章云极在AI基础设施领域的长期价值。其高度前瞻性的发展路径，不仅具备商业潜力，更对构建健康、自主的AI产业生态具有重要战略意义。&lt;/p&gt;&lt;p&gt;据了解，九章云极已锚定未来三年建成10万PFLOP普惠智算储备的目标，将围绕&amp;ldquo;技术创新+普惠落地&amp;rdquo;的发展理念，不断强化其在全球智算云市场的核心竞争力，向引领AI基础设施发展的领军企业持续迈进。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>药物靶标不再停留在器官层级，首次在全身尺度解析药物结合的单细胞图谱</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>ScienceAI</author>
      <pubDate>Mon, 29 Dec 2025 14:33:20 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-8</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-8</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfD9ONDOGU5HYjFWictDQqiapWwoeNxSB8gPvKibMyRibsz6ndXVqkMWjrxw/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.55" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="318" data-imgfileid="100027009" data-original-style="width: 100%;" data-index="1" src="https://image.jiqizhixin.com/uploads/editor/c370dd4c-0891-4522-9b46-10c7c3e50791/640.png" data-sec-load-status="2" data-report-img-idx="0" alt="图片" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p data-pm-slice="0 0 []"&gt;编辑丨&amp;amp;&lt;/p&gt;&lt;p&gt;随着科研人员对生物系统的理解达到单细胞和高空间分辨率，药理学方法也必须跟上，以匹配这种精确度，理解药物作用。&lt;/p&gt;&lt;p&gt;目前，常用的临床检测只能显示药物在某个器官里大致浓度，但无法看清药物真正结合在哪些细胞上。打个比方来说，药物在体内的去向常像被雾霾遮住的景象&amp;mdash;&amp;mdash;一切都处在模糊的感知中，想要精确定位并不简单。&lt;/p&gt;&lt;p&gt;这种处在黑箱之中的现状被终结了。以美国斯克里普斯研究中心（Scripps Research）与霍华德&amp;middot;休斯医学研究所（Howard Hughes Medical Institute）为主的研究团队开发了一种突破性的成像技术，能够照亮整个小鼠体内药物结合的单个细胞。&lt;/p&gt;&lt;p&gt;相关研究内容以「Mapping cellular targets of covalent cancer drugs in the entire mammalian body」为题，于 2025 年 12 月 22 日发布在《Cell》。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfKwS8FZ37LYWZC8HLJzDXXBAbO05Zk7zia5bFT4j5RNRb8PpTu5iaXQ1A/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.22685185185185186" data-type="png" data-w="1080" data-width="1105" data-height="251" data-imgfileid="100027004" data-original-style="null" data-index="2" src="https://image.jiqizhixin.com/uploads/editor/b4335471-54e9-43f0-8eaa-2d1b5c0d743a/640.png" alt="图片" data-before-load-time="1766989966264" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;论文链接：https://www.cell.com/cell/fulltext/S0092-8674(25)01365-0&lt;/p&gt;&lt;p&gt;&lt;strong&gt;vCATCH&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;体积清除辅助组织点击化学（volumetric clearing-assisted tissue click chemistry），后称为 vCATCH，是该团队所开发的能够在体内精确定位药物、细胞结合情况的成像技术。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfoQ1NmYMgo3pElzz31T3swcIRkckT0qwCtBWIPE9XGg6yO8ZSibYgJcA/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-ratio="1.1722745625841184" data-type="png" data-w="743" data-width="743" data-height="871" data-backw="546" data-backh="640" data-imgfileid="100027007" data-original-style="width: 100%;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/8c416b1d-86c5-410c-8e43-91f1ba9b79b5/640.png" alt="图片" data-before-load-time="1766989966277" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：建立深层组织点击反应的原理。&lt;/p&gt;&lt;p&gt;vCATCH 是对 2022 年提出的 CATCH 方法的升级方向，主要采用以下几种关键技术的组合：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内部位标记药物分子&lt;/strong&gt;：将包含可与目标蛋白发生共价作用的过量的铜进行适当的化学改造，以深入染色器官。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内给药与固定&lt;/strong&gt;：药物按常规方式给药后，在体内与目标结合。随后对整个动物组织进行取样与预处理。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;&lt;strong&gt;Click Chemistry&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;反应标记结合位点&lt;/strong&gt;：使用高选择性的化学反应给结合了药物的细胞分子进行荧光标记。该反应极具专一性并能穿透深层组织。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;体内组织清晰化 + 三维成像&lt;/strong&gt;：通过组织清晰化方法让整个组织透明，并使用光片荧光显微成像系统获全身三维数据。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;AI 辅助图像分析&lt;/strong&gt;：成像产生的数据通常以&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;「&lt;/span&gt;TB 级&lt;span data-pm-slice='1 1 ["para",{"tagName":"p","attributes":{"style":"margin: 16px 16px 0px;line-height: 1.75em;"},"namespaceURI":"http://www.w3.org/1999/xhtml"}]'&gt;」&lt;/span&gt;规模呈现，使用先进的 AI 数据管线自动识别并定位每一个结合细胞。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;测试与结果&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了测试这一新方法，叶的实验室绘制了两种靶向癌症药物的结合：用于治疗血癌的&amp;nbsp;Ibrutinib（Imbruvica）和用于非小细胞肺癌处方的 Afatinib（Gilotrif）。&lt;/p&gt;&lt;p&gt;对于前者，vCATCH 分辨出的结合定位显示&lt;strong&gt;不仅在血液细胞中&lt;/strong&gt;，还在心脏、血管壁甚至肝脏的免疫细胞中出现明确结合信号，这一发现&lt;strong&gt;可能解释其临床上观察到的心血管副作用&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfmHnJ5LUIjfBlRevyoJSDdyI4iatyMUY67XPA9BBlSETqM8WLO2gLiaVQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="1.248995983935743" data-type="png" data-w="747" data-width="747" data-height="933" data-backw="546" data-backh="682" data-imgfileid="100027005" data-original-style="width: 100%;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/70916bf6-d575-4f9c-9f89-2b47d68942be/640.png" alt="图片" data-before-load-time="1766989966358" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：vCATCH 显示了全体 TKI 分布，具有高空间分辨率。&lt;/p&gt;&lt;p&gt;而对于后者，表现出了如预期结果中的一致性。&lt;/p&gt;&lt;p&gt;除此之外，团队还发现：尽管这两种药物都与肾脏结合，但它们在器官内表现出不同的模式。更高分辨率的 vCATCH 显示了它们在器官内分布上的差异：Afatinib 在肝组织中表现出冠状信号，而 Ibrutinib 则与稀疏散布的单个细胞相关，这一点由光片和共焦成像均有体现。&lt;/p&gt;&lt;section&gt;&lt;img data-src="https://mmbiz.qpic.cn/mmbiz_png/XLCp9HBkwLmCH2dPknwg0292LfTMBtRfjLQIhdFGFlGCS8lXV3QPhJuOQ1RfwpOwnphWEqdI1bzXIk0zgBnUBQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="1.4318529862174578" data-type="png" data-w="653" data-width="653" data-height="935" data-backw="546" data-backh="782" data-imgfileid="100027006" data-original-style="width: 100%;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/60876c13-157f-435d-905b-1fe39f67d70c/640.png" alt="图片" data-before-load-time="1766989966581" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;图示：药物富集组织中 TKI 参与的细胞类型特征。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;药物开发的全新视角&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;传统药效评估局限于整体组织层级，而 vCATCH 提供了&amp;nbsp;&lt;strong&gt;逐细胞的药物结合情况&lt;/strong&gt;，有助于识别药物靶标之外的非预期结合位点，这正是许多未知副作用的根源。&lt;/p&gt;&lt;p&gt;在药物开发的早期阶段，研发团队可以凭此快速观察到药物与细胞的交互，也可以从 vCATCH 处获得有关副作用的细胞级别解释。结合 AI 工具分析全身结合数据后，甚至可以对不同小鼠个体乃至未来人体样本执行比对分析，在个性化医学中具备潜在应用。&lt;/p&gt;&lt;p&gt;相关报道：https://www.eurekalert.org/news-releases/1110881&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>全球首个具身 VLTA 多模态数据集开源，它石加速具身智能真实世界落地</title>
      <description>&lt;![CDATA[在具身智能领域，数据是智能的最重要燃料，它石智航正成为“为众人抱薪者”]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 14:20:00 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-12</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-12</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;12&amp;nbsp;月&amp;nbsp;26&amp;nbsp;日，它石智航重磅开源了全球首个大规模真实世界具身&amp;nbsp;VLTA（Vision-Language-Tactile-Action）多模态数据集&amp;nbsp;——World In Your Hands（简称&amp;nbsp;“WIYH&amp;nbsp;数据集”）：https://wiyh.tars-ai.com/。该数据集首次亮相于今年&amp;nbsp;10&amp;nbsp;月，并在刚刚结束的它石智航技术首秀发布上，作为「超级算法」中的核心成果之一得到行业广泛认可。首秀现场展示的世界首台可人工刺绣机器人等成果，其丝滑动作背后的核心正是基于&amp;nbsp;WIYH&amp;nbsp;数据集训练的AWE2.0模型。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/074387bd-db38-4aa8-8259-c447a41c9943/1766999899190.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;WIYH&amp;nbsp;数据集通过首创&amp;nbsp;“Human-centric”（以人为中心）的数据采集新范式，破解了遥操作数据采集规模化成本高、仿真数据在&amp;nbsp;sim2real&amp;nbsp;时存在&amp;nbsp;GAP，难以丝滑迁移到现实世界的痛点，填补了具身智能所需的高质量、可泛化、大规模真实世界数据的空白，为具身基座模型实现&amp;nbsp;Scaling Law&amp;nbsp;提供了关键语料。要知道，当前具身智能所需数据量与现有储备之间至少相差两个数量级，WIYH的开源正为填补这一鸿沟提供了核心解法，加速了具身智能落地真实世界的进程。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/74d4fba5-5fa2-48a4-bef5-5dbdc1758952/1766999923177.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;Human-centric&amp;nbsp;采集范式下形成的&amp;nbsp;WIYH&amp;nbsp;数据集，具备了真实可靠、丰富多元、全面多模态、规模化等特征，并拥有海量数据：包含超过&lt;strong&gt;10&lt;/strong&gt;&lt;strong&gt;万&lt;/strong&gt;条以上的真实人类操作视频、&lt;strong&gt;40&lt;/strong&gt;&lt;strong&gt;余种&lt;/strong&gt;任务类型、&lt;strong&gt;100&lt;/strong&gt;多种人类技能，覆盖了含&lt;strong&gt;520&lt;/strong&gt; 余种真实物品，真实还原商超、酒店、餐饮、工业、办公、家居等多行业的&lt;strong&gt;10&lt;/strong&gt;种核心场景全链路任务，数据将分批次释放。WIYH是业内最大的Human-centric数据集，且仍在不断扩展和丰富中。&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/44dbdc0e-4e3c-4fee-9a91-5db715df4a2e/1766999942861.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;场景和任务分布&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/be9d8352-f15e-4f71-917b-3607698b2845/1766999963371.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;各场景技能分布&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/f9aa131e-7769-4b84-a4ea-c5983d85730b/1766999976292.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;物体和技能词云&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;采集了丰富的数据之后，行业还面临数据迁移这另一核心难题，为此它石构建了 &lt;strong&gt;TARS Datacore&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;具身数据引擎&lt;/strong&gt;。作为云端大模型，它实现了全流程的自动化标注，将原始视频转化为机器人能理解的“教科书”。这套标注体系不仅涵盖了标定、深度、动作、指令、思维链&amp;nbsp;(COT)、掩码（Mask）和触觉（Tactile）等基础信息，更通过以下三类精准标注，形成了从感知到动作的全链路闭环：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;原子任务标注：理解“做什么”。将复杂动作拆解为“抓取、移动、放置”等不可再细分的逻辑单元，并配以自然语言指令，让机器人明白长流程任务的操作顺序与步骤内涵；&lt;/li&gt;&lt;li&gt;图像感知标注：看清“在哪里”。通过云端语义模型给每个物体打上清晰的边界标签（掩码），并利用&amp;nbsp;3D&amp;nbsp;视觉技术计算画面中物与人的深度距离，赋予机器人精准的空间感，使其能理解物体的边界与远近；&lt;/li&gt;&lt;li&gt;视觉语言标注：思考“为什么”。这是最关键的“大脑训练”，它通过空间明确指代操作对象，防止机器人在杂乱环境中错认目标。同时，让模型预测下一步任务，并设置逻辑陷阱来校验机器人的判断力。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;它石首席科学家丁文超博士表示，&lt;strong&gt;“Human-centric&lt;/strong&gt;&lt;strong&gt;数据采集范式配合&lt;/strong&gt;&lt;strong&gt;TARS Datacore&lt;/strong&gt;&lt;strong&gt;数据引擎，可以记录和生产最高质量、最丰富的具身智能数据，真正使得&lt;/strong&gt;&lt;strong&gt;scaling law&lt;/strong&gt;&lt;strong&gt;成为可能。&lt;/strong&gt;&lt;strong&gt;”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;强大的数据采集与迁移能力，共同打磨出&amp;nbsp;WIYH&amp;nbsp;数据集在空间推理、世界模型、跨本体迁移等方面的独特优势。目前，多项基准测试结果已印证了&amp;nbsp;WIYH&amp;nbsp;数据集的核心价值：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在视言大模型（VLM）的空间推理评测中，通过对&amp;nbsp;GPT-4o、Qwen-VL-Plus&amp;nbsp;等主流模型在空间推理（SR）和空间感知（SP）等维度的对比发现，虽然各模型在通用视觉任务上表现出色，但在处理复杂的以人为中心的空间逻辑时仍存在显著差异。这一基准测试直观地揭示了当前大模型在感知操作空间时的局限，也凸显了&amp;nbsp;WIYH&amp;nbsp;数据集在训练更高阶空间感知能力方面的独特价值。&lt;/li&gt;&lt;li&gt;在世界模型（World Model）的物理一致性验证上，WIYH&amp;nbsp;数据集展现了强大的“物理引擎”属性。评测结果显示，在加入&amp;nbsp;WIYH&amp;nbsp;数据后，COGVIDEO&amp;nbsp;和&amp;nbsp;DYNAMICRAFTER&amp;nbsp;等视频生成模型在一致性、流畅度、动态性和质量等四大关键指标上均实现了全面跨越，其中动态性（Dynamic）指标得分提升了&amp;nbsp;15.6&amp;nbsp;分。配合&amp;nbsp;4D&amp;nbsp;重建技术，通过对“倒酒”、“叠衣服”等任务进行精确的几何重建，为模型理解真实世界的物理动态提供了高真值的监督信号，确保生成的动作既流畅又符合物理常识。&lt;/li&gt;&lt;li&gt;在机器人跨本体迁移实验（Cross-embodiment Experiments）层面，WIYH&amp;nbsp;数据集真正实现了“从人到机器”的能力迁移。通过将人类演示视频与机器人操作数据进行协同训练（Co-training），机器人在复杂场景下的泛化能力得到了质的提升。实验数据表明，在极其杂乱的场景中，仅机器人操作数据只能达到&amp;nbsp;8%&amp;nbsp;成功率的任务，在引入&amp;nbsp;WIYH&amp;nbsp;人类视频辅助后，成功率暴涨至&amp;nbsp;60%。这一显著的性能增益证明了&amp;nbsp;WIYH&amp;nbsp;数据集不仅是视觉语料，更是提升机器人实战能力、解决真实世界非结构化环境操作难题的核心“养料”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ee67720b-5efa-4dfb-97d5-52e786e301a0/1766999994578.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;它石创始人兼 CEO 陈亦伦博士认为，&lt;strong&gt;“&lt;/strong&gt;&lt;strong&gt;在&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;领域，最极致的开放不是开源模型，而是开源数据集，因为所有的&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;模型本质上都可以通过数据和合适的训练方法&lt;/strong&gt;&lt;strong&gt;‘&lt;/strong&gt;&lt;strong&gt;生长&lt;/strong&gt;&lt;strong&gt;’&lt;/strong&gt;&lt;strong&gt;出来。&lt;/strong&gt;&lt;strong&gt;”&amp;nbsp;&lt;/strong&gt;它石WIYH的此番开源，正是以开放的态度，为行业通用具身基座模型训练提供关键语料和基础设施，助力具身智能迈向通用智能的新高峰。&lt;br&gt;&amp;nbsp;它石WIYH数据集于今日起（12.26）可正式访问。该网站构建了从认知到实践的全方位支撑：用户既能通过结构化的数据集全景展示、基准测试结果与典型用例，快速完成对数据集性能与场景的初步了解，又能配合快速入门指南与自动化 SDK，直接进入深度的落地实践与开发部署。网站公布了标准化的 off-the-shelf (OTS) 开源可复现方案，也放出了 TARS商用级Human-centric数据解决方案TARS SenseHub的相关信息。 TARS SenseHub 是由它石自研的超轻量数据采集套件，包含 “眼睛” TARS-Vision 与 “双手” TARS-Glove 两大关键组件。其核心理念是让人类和机器共享同一套感知体系，即机器人通过数据“能看人之所看，感人之所感”。&lt;/p&gt;&lt;p&gt;未来，它石智航将继续坚持以人为中心的具身数据与模型新范式，持续挖掘真实世界的数据价值，让面向万物、赋能万业的具身智能真正从实验室走进千家万户。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>《连线》杂志：2026年将是阿里千问之年</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>新闻资讯</author>
      <pubDate>Mon, 29 Dec 2025 14:12:27 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-7</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-7</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;12月29日，美国著名科技媒体《连线》（WIRED）发表头条文章《再见，GPT-5。你好，千问》，文章指出，GPT-5未能激起市场热情，而开源开放的阿里千问，性能优异，适于灵活部署应用，2026年将属于千问。&amp;ldquo;衡量任何AI模型价值的关键标准，不应仅限于其&amp;lsquo;聪明&amp;rsquo;程度，更应看它被用于构建其他应用的广度。若以此为尺度，千问等中国开源大模型无疑正处于上升势头。&amp;rdquo;&lt;img src="https://image.jiqizhixin.com/uploads/editor/a88f2b1c-0615-4e76-8b38-957d8c956ddb/%E5%9B%BE%E7%89%8700.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;《连线》（WIRED）是全球顶级科技媒体之一，《失控》作者凯文&amp;middot;凯利是该杂志创始主编，《连线》对科技及行业趋势的精准洞察，在硅谷和华尔街享有&amp;ldquo;数字时代圣经&amp;rdquo;的美誉。每年年末，《连线》会刊出对全球科技预测的前瞻文章，《再见，GPT-5。你好，千问》即是年终重磅文章系列之一。&lt;/p&gt;&lt;p&gt;《连线》杂志观察指出，尽管美国OpenAI的GPT-5、谷歌的Gemini 3以及Anthropic的Claude通常得分更高，但阿里千问、DeepSeek等中国模型性能也稳居第一梯队，并且变得越来越受欢迎，&amp;ldquo;原因在于它们既性能优异，又易于开发者灵活调整和使用&amp;rdquo;。&lt;/p&gt;&lt;p&gt;根据全球最大的AI开源社区HuggingFace数据，中国开源模型的下载量在2025年7月已经超过美国，其中阿里千问位居第一；海外模型API调用第三方平台OpenRouter数据也显示，千问在今年异军突起，调用量一度冲至所有开闭源模型的全球第四。&lt;img src="https://image.jiqizhixin.com/uploads/editor/d4b3235f-d733-49e7-91c9-eb3d42eba2be/%E5%9B%BE%E7%89%87000.png" style="width: 70%;" class="fr-fic fr-dib"&gt;&lt;/p&gt;&lt;p&gt;《连线》杂志采访了数家企业和研究机构，发现千问能够胜任大多数先进AI模型所能完成的任务，已经成为企业落地AI大模型的首选：比如中国&amp;ldquo;电动汽车之王&amp;rdquo;比亚迪，将千问集成到其新款车载仪表盘助手中；智能眼镜Rokid，基于千问微调出适配的大模型，实现多端部署多个千问模型；硅谷公司爱彼迎（Airbnb）、Perplexity和英伟达均已将千问纳入技术栈；甚至曾是开源模型先驱的Meta，如今也被传正在借助千问协助开发新一代模型。&lt;/p&gt;&lt;p&gt;《连线》观察到，相较于不常开源的OpenAI，阿里在模型构建与持续更新方面投入更多精力，且千问技术细节通常公开透明，这让千问成为AI创新的第一策源地。在今年顶级人工智能会议NeurIPS上，阿里千问夺得最佳论文，会上收录的数百篇学术论文均采用了千问Qwen。倡导美国开源模型发展的非营利组织Laude Institute联合创始人安迪&amp;middot;康温斯基（Andy Konwinski）表示：&amp;ldquo;许多科研人员都在使用Qwen，因为它是目前最好的开源大模型。&amp;rdquo;&lt;/p&gt;]]&gt;</content:encoded>
    </item>
    <item>
      <title>上线不到一年，收徒百万，首个真人级AI导师技术底牌首次曝光</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Mon, 29 Dec 2025 13:56:37 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2025-12-29-6</link>
      <guid>https://www.jiqizhixin.com/articles/2025-12-29-6</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;section&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/e30ea275-9195-415d-ba58-eefc8813433f/1766987374648.png" style="width: 700%;" class="fr-fic fr-dib"&gt;第一次见到「爱学」前，王佳佳（化名）害怕和老师互动。&lt;/section&gt;&lt;section&gt;&lt;br&gt;&lt;/section&gt;&lt;section&gt;这个来自安徽阜阳的初三女生，性格内向，在课堂上几乎从不举手。题不会，不敢问，宁愿空着；一被老师点名，就紧张到大脑一片空白。久而久之，数学和英语成了她最不愿面对的两门课。&lt;/section&gt;&lt;p&gt;直到有一天，她开始反复和一个「不会不耐烦」的对象对话。&lt;span data-pm-slice="0 0 []"&gt;一句没听懂，就一直追问，直到彻底弄清楚&lt;/span&gt;。对方有表情，会根据她的反应实时调整讲解节奏，也会在她犹豫、走神时主动追问，把她拉回来。&lt;/p&gt;&lt;p&gt;慢慢地，王佳佳敢开口了，学习也变得主动。最近一次数学随堂考试，她考了 103 分，比上一次整整提高了 40 分。&lt;/p&gt;&lt;p data-pm-slice="2 3 []"&gt;「爱学」所承载的并不是一位真人老师，而是一个真人级 AI 导师。&lt;/p&gt;&lt;p data-pm-slice="3 3 []"&gt;2025 年初，成立不到两年的首批 AI 原生应用企业与爱为舞率先落地了&lt;strong&gt;国内首个真人级 AI 一对一导师&lt;/strong&gt;产品「爱学」。App 上线不到一年，已经被超过百万名学员真实使用。&lt;/p&gt;&lt;p&gt;单次课可能持续 1&amp;mdash;2 小时，没有任何真人介入，完课率却高达 92.4%。单个学员的最长学习时长已达到 9000 分钟。&lt;/p&gt;&lt;p&gt;在&amp;nbsp;AI&amp;nbsp;课堂中，单次课的答题正确率&lt;span data-pm-slice="0 0 []"&gt;也从&amp;nbsp;&lt;/span&gt;59.1%&amp;nbsp;提升至&amp;nbsp;83.2%。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/ecb35147-e303-41b3-bbde-c03d52f65d0c/1766987407036.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 学员小苹果学英语，进步明显。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI&amp;nbsp;&lt;/strong&gt;&lt;strong&gt;导师，拒走&lt;/strong&gt;&lt;strong&gt;「&lt;/strong&gt;&lt;strong&gt;答题」老路&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;放眼当前的 AI 教育市场，大模型似乎成了基础设施，但真正意义上的突破，却迟迟没有出现。大量产品仍停留在「舒适区」&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;把 LLM 「套进」学习机或 App ，当作一个更聪明的聊天机器人来用。它擅长预测下一个 token，却没能走进教学本身。&lt;/p&gt;&lt;p&gt;最常见的使用方式是被大量用于搜题、批改。用户卡题了，点一下 AI，答案瞬间生成。所谓「讲解」，只是堆叠冗长而晦涩的推理链，对低龄用户来说如同天书。好一点的情况，会调用提前录制好的人类视频，但交互为零。&lt;/p&gt;&lt;p&gt;&lt;em&gt;例如，一个常见的语法题：My sister plans _____ （meet）her teacher after school tomorrow. 这里的 meet 应该填什么形式？&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;很多 AI 的回答只是就着答案做解释（尤其像 AI 拍搜）：&lt;/em&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;1.答案：to meet&lt;/p&gt;&lt;p&gt;a.核心原因：固定搭配+非谓语动词用法&lt;/p&gt;&lt;p&gt;b.动词plan的固定搭配规则：plan + to do sth.，即计划做某事&lt;/p&gt;&lt;p&gt;2.语法逻辑解析：&lt;/p&gt;&lt;p&gt;a.plan作为谓语动词，后面需要接&amp;ldquo;宾语&amp;rdquo;来表示&amp;ldquo;计划的内容&amp;rdquo;&lt;/p&gt;&lt;p&gt;b.英语中，动词原形（meet）不能直接做宾语，需要通过不定式（to+动词原形）构成不定式短语（to meet her teacher），才能充当plan的宾语，表示&amp;ldquo;计划的具体动作&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;更常见的是打着「AI 课程」旗号的产品，本质仍是录播，AI 交互浅到几乎可以忽略。这些产品解决了「有教」的一部分，却仍难以触及「会教」，更别提「因材施教」。&lt;/p&gt;&lt;p&gt;要将 AI 真正引入教育，必须在「教学」这件事上下功夫。与爱为舞认为，新一代 AI 教育范式应该&lt;strong&gt;因材施教、授人以渔。&lt;/strong&gt;这套判断，来自一支典型的「互联网大厂 &amp;times; 头部教育」跨界团队。公司创始人兼 CEO 张怀亭，曾担任百度商业化系统「凤巢」核心负责人、高途联合创始人。创始人兼 COO 刘威，曾任高途集团副总裁、高途课堂总经理。&lt;/p&gt;&lt;p&gt;就拿前面那道英语题来说，为什么是 &lt;strong&gt;&lt;em&gt;to meet&lt;/em&gt;&lt;/strong&gt;，而不是其他形式？&lt;/p&gt;&lt;p&gt;「授人以渔」的 AI ，不会一上来就告诉你答案是 &lt;em&gt;to meet&lt;/em&gt;。它会先判断学员卡在哪一层，是不是没搞懂非谓语动词？&lt;/p&gt;&lt;p&gt;随后，通过追问与对比引导学员自己发现：&lt;em&gt;plan&amp;nbsp;&lt;/em&gt;表达的是尚未发生的计划动作，英语里通常用 &lt;em&gt;&lt;strong&gt;to do&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;来承载这种未来意图。&lt;/p&gt;&lt;p&gt;再通过举一反三让学员理解共性规律。当遇到 &lt;em&gt;plan / decide / hope / want&amp;nbsp;&lt;/em&gt;这类动词时，不必死背搭配，也能判断该如何表达。&lt;/p&gt;&lt;p&gt;近两年，AI Agent 概念大热，市场上也出现了一对一 AI 导师的雏形，但仍局限在英语、数学等单一学科 。相比之下，「爱学」已实现&lt;strong&gt;全年龄段&lt;/strong&gt;覆盖、&lt;strong&gt;「多学科 + 长课时」&lt;/strong&gt;深度陪伴。&lt;/p&gt;&lt;p&gt;而支撑这一跨代产品的，是自研的&lt;strong&gt;国内首个 AI 原生教育框架&lt;/strong&gt;&amp;mdash;&amp;mdash;一个打通了&lt;strong&gt;「数字人 + 语音 + 大模型 + 工程」的全栈技术体系&lt;/strong&gt; 。与爱为舞也因此成为国内首个在教育场景下打通全栈技术能力的公司。&lt;/p&gt;&lt;p&gt;那么，一个好的 AI 导师是如何炼出来的？&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="1.8537037037037036" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="562" data-backh="1042" data-imgfileid="503525698" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGB5swibwqIVI8Ug8VeuibnXag8lDXoiapQuelicbXvpKpyzR7MydoVbI3cQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=1" data-original-style="width: 100%;height: auto !important;" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/6267e2f1-7c85-4106-8706-6373f4a3f3c3/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 70%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;「爱学」背后的全栈技术能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;「筑魂」：&lt;/strong&gt;&lt;strong&gt;从 ChatBot 到 MDP 决策&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;为了打造一个真正「会教」的 AI 导师，与爱为舞自研了&lt;strong&gt;三大基础模型体系&lt;/strong&gt;&amp;mdash;&amp;mdash;从感知的「皮囊」，到负责决策的「灵魂」&amp;mdash;&amp;mdash;完整覆盖 AI 教学的核心能力闭环。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.562962962962963" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="325" data-imgfileid="503525699" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGW4L6jI1RSPdyeuNeUwnSMaREvLkiaa5rpLouiaLJGJ8XHyMOz38MKxFA/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-original-style="width: 100%;height: auto !important;" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/4f92bc5f-ef0f-4a16-a7ef-81e56bf27a26/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 与爱为舞认为，一个优秀的AI导师需要具备上面四个方面的核心能力。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;其中，&lt;strong&gt;爱学教育大模型，第一次让 AI 拥有了真正的「教学之魂」&lt;/strong&gt;，更是与竞对拉开身位的关键赛点。&lt;/p&gt;&lt;p&gt;与市面上仍停留在预测下一个 token 的对话系统不同，「爱学」从一开始就被设计成一套实时教学决策系统&amp;mdash;&amp;mdash;&lt;/p&gt;&lt;p&gt;「一对一教学」不再只是问答交互，而被抽象为一&lt;strong&gt;个持续演化的&lt;/strong&gt;&lt;strong&gt;马尔科夫决策过程（MDP）&lt;/strong&gt;，一次教学就是一场目标明确的博弈。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;学员的状态&amp;mdash;&amp;mdash;包括理解程度、情绪变化、长时间犹豫等&amp;mdash;&amp;mdash;被视作环境；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;每一次提问、追问、提示、鼓励与纠偏，都是 AI 导师可选择的教学策略；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;系统的奖励，不再是「题对了+1 分」，而是学员是否真的学会了、学得更快、学得更开心。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，AI&amp;nbsp;导师每一次讲解、每一次追问、每一次纠错，都不是「接一句话」，而是在&lt;strong&gt;当前学员状态下做出的最优教学决策&lt;/strong&gt;。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/2eae5893-2ab2-40bf-b01d-52ec570f42db/1766987535186.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 数学课上，学员和AI导师一问一答，互动学习。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;为了让模型真正具备「师魂」，与爱为舞设计了两个进化阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SFT（启发式注入）：不只是喂数据，而是通过思维链（CoT）将大量名师的隐性经验系统化。它不只学「说什么」，更在学「为什么要这么教」。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RL（强化学习进化）：引入定制化 GRPO 算法。围绕教学规划的质量与灵活性构建 Reward 函数，让 AI 在数亿次的模拟试错中，打磨出类似真人的「教学直觉」，做到因材施教。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;img data-ratio="0.4425925925925926" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="256" data-imgfileid="503525677" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGwObTwfvkICr1k2Kjre83tDnia2qWyty4ib6f8BrfLRoQmUfFk90GMdPQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=3" data-original-style="width: 100%;height: auto !important;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/e2a9f614-5c00-4275-b69c-e7858c0e8b5f/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;AI 导师是如何炼成的？这是在真实课堂 + 仿真课堂中持续进化的完整数据闭环与训练体系。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;b align="" alt="" border="" data-aiimageid="" data-aiimagesource="" data-asynid="" data-backh="" data-backw="" data-before-oversubscription-url="" data-cacheurl="" data-cardimg="" data-copyright="" data-croporisrc="" data-cropselx1="" data-cropselx2="" data-cropsely1="" data-cropsely2="" data-cropx1="" data-cropx2="" data-cropy1="" data-cropy2="" data-fileid="" data-fromlib="" data-galleryid="" data-gallerysupplier="" data-height="" data-imgfileid="503525676" data-imgid="" data-imgqrcoded="" data-oversubscription-url="" data-positionback="" data-ratio="" data-remoteid="" data-retry="" data-s="300,640" data-src="" data-type="png" data-upload="1" data-w="" data-width="" height="" ismap="" sizes="" src="" title="" type="block" usemap="" width=""&gt;如果说模型架构决定了「导师素质」的上限，那么数据工程则构成了能力的下限。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;与爱为舞没有简单堆砌题库或对话语料，而是先搭建了一套&lt;strong&gt;可运行的&amp;nbsp;AI&amp;nbsp;教学环境&lt;/strong&gt;，让数据在真实教学逻辑中自然生长。数据被系统性地拆解为三层核心要素。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;学科本体。通过构建覆盖各学科的核心知识图谱、关键考点与解题方法，将教材与考纲转化为&amp;nbsp;AI&amp;nbsp;可理解、可调用的教学结构；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;教学方法。通过名师参与课程设计，沉淀「为什么这么教、先讲什么、后练什么」的课程逻辑；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;真实课堂中的学员交互数据。这也是最关键、最稀缺的一层&lt;/strong&gt;。学员的回答方式、犹豫与卡顿、追问与反馈，都被完整记录下来，形成高价值的实时互动样本。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;三者共同构成一套「可运行的数字课堂」，也为后续模型训练提供了区别于传统题库与对话数据的核心养料：不仅知道教什么、怎么教，更知道该如何根据学员状态去教。&lt;/p&gt;&lt;p&gt;为补齐真实数据的稀缺与长尾问题，研发团队进一步引入了类似 AlphaGo 的自博弈机制：让「学员模拟器」与「AI 导师」在虚拟课堂中反复对弈，自生成千万级训练样本。&lt;/p&gt;&lt;p&gt;真实课堂每周数万小时的数据持续回流，驱动 SFT 与 RL 的高速迭代，形成一套稳定运转的数据飞轮。&lt;/p&gt;&lt;p&gt;最终，具备教学判断力的&amp;nbsp;AI&amp;nbsp;导师，得以在&lt;strong&gt;教学目标、教学路径、课堂交互与作业巩固&lt;/strong&gt;上，实现真正意义上的个性化学习。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.44074074074074077" data-s="300,640" data-type="jpeg" data-w="1080" type="block" data-backw="578" data-backh="255" data-imgfileid="503525701" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGw3Jmjz2FDhqageZ3ZXEHibW4ia0J6Xynlqyszvp1x56MBhawaLX7xJpQ/640?wx_fmt=jpeg&amp;from=appmsg#imgIndex=4" data-original-style="width: 100%;height: auto !important;" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/859ffbdf-1af6-43fa-aef5-e6152228d5a4/640.png" alt="图片" data-report-img-idx="4" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; AI 导师从四个方面实现「个性化」施教：教学目标、路径、交互与作业巩固。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;与爱为舞依靠真实的教学互动数据，通过数据飞轮，推动算法以周为单位迭代。目前已更新 20 多个版本，建立了业界首个在线一对一 AI 教学 Agent，全方位提升了教学效果。&lt;/p&gt;&lt;p&gt;1、持续优化互动频次，每节课 AI 导师都能与学员进行几十次的一对一互动，牢牢抓住学员的注意力。&lt;/p&gt;&lt;p&gt;2、持续优化互动质量，学员在一对一互动中的有效回答率提升到 95% 以上，说明学员的注意力得到显著提升。&lt;/p&gt;&lt;p&gt;3、持续优化个性化教学质量，通过个性化教学目标和个性化教学路径，将学员做题的准确率从不足 60% 提升到 83% ，部分课程正确率超过 95%，说明学员在集中精力学习之后，确实掌握了相关知识点。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重做「听说」：&lt;/strong&gt;&lt;strong&gt;上下文 ASR +流式 TTS + 全双工语音交互&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在打磨「师魂」的同时，与爱为舞也在&lt;strong&gt;感知层&lt;/strong&gt;完成了一次彻底重构。&lt;/p&gt;&lt;p&gt;原因并不复杂：教学本质上是一种高频互动行为，语音是所有交互的第一道门槛。老师听得准、反应快，学员才愿意继续说下去。&lt;/p&gt;&lt;p&gt;传统 ASR 只会「听写」，至于这句话是在教几何还是教英语，一概不知。于是，因环境嘈杂或学员口音，「four」被听成「for」，「D答案」被听成「第一答案」，「有理数」被听成「有礼数」也就不足为奇。&lt;/p&gt;&lt;p&gt;与爱为舞不再把 ASR 当作「听写」工具，而是把它升级为「课堂参与者」。&lt;strong&gt;自研多模态语音理解模型，在解码最底层引入教学语境约束，&lt;/strong&gt;让「听」从一开始就带着教学目的。约束来自三方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;教学任务（Task）：如当前正在攻克哪一个知识点？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;教学进度（Step）：目前处于引入、练习还是总结阶段？&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;学员画像（Persona）：学员此前的错误分布和表达习惯。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由于上下文信息直接参与解码路径计算，模型在第一时间就能排除那些在纯语音层面「听起来合理」，但在教学逻辑上完全荒谬的候选结果，&lt;strong&gt;ASR 准确率从行业最好开放能力约&lt;/strong&gt; &lt;strong&gt;80%&amp;nbsp;左右提升至&amp;nbsp;95%+&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;除了「听懂」，AI 导师说话得有人味儿。因此，TTS 也被重做了一番。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;自研流式 TTS 大模型&lt;/strong&gt;将首字延迟压到 &lt;strong&gt;300ms&amp;nbsp;以内&lt;/strong&gt;，通过语义/声学双 Speech Token + 强化学习联合优化，让语音能随语境动态调整节奏、重音与情绪&amp;mdash;&amp;mdash;讲诗词会留白，讲推导会干净利落，甚至能用少样本快速对齐「名师腔」。&lt;/p&gt;&lt;p&gt;让我们一起听听下面这段音频，猜猜是真人还是 AI。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/aa98279a-ed23-41c2-9ce0-c5dc77dd446d/1766987649696.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;听出来了吗？下面揭晓答案：短短 18 秒的音频，混合了 3 段真人音频和 3 段 AI ，是不是完全听不出来？&lt;/p&gt;&lt;p&gt;更关键的是，AI 导师还能被随时打断。通过全双工语音交互，结合流式语义 VAD 与打断拒识模型，实现真正意义上的边说边听，说话过程中即可识别学员插话，&lt;strong&gt;打断识别准确率&lt;/strong&gt; &lt;strong&gt;90%+&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.8969210174029452" data-s="300,640" data-type="png" data-w="747" type="block" data-backw="578" data-backh="518" data-imgfileid="503525693" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGFI8J0TibgdKlEiahxOz6MVkzM6nsoqkNBz0co3G0jDY8kShGfCNpLCPw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-original-style="width: 100%;height: auto !important;" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f1bc6eda-13d6-4d4b-9236-cde3a03a116e/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;「肉身」进化：&lt;/strong&gt;&lt;strong&gt;数字人百 FPS 实时，不出戏&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在课堂里，声音与「人」必须严格同步。多数数字人 Demo，十几秒足够惊艳，但一旦拉长到 40 多分钟课堂，就会迅速滑向「恐怖片」：&lt;/p&gt;&lt;p&gt;穿模、口型错位、动作僵硬、抖动、表情漂移、声音输出和其唇部动作之间存在明显延迟&amp;hellip;&amp;hellip;&lt;/p&gt;&lt;p&gt;能稳稳&amp;nbsp;hold&amp;nbsp;住一整堂课的&amp;nbsp;AI&amp;nbsp;导师，对数字人技术提出了近乎苛刻的要求&amp;mdash;&amp;mdash;&lt;strong&gt;极致的实时互动能力，以及长期一致性&lt;/strong&gt;。围绕这些目标，与爱为舞的数字人系统开启了一次从 1.0 到 6.0 的疯狂进化。&lt;a href="https://mp.weixin.qq.com/s/eNVVbsrzHfgscRRhq57yPQ"&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/51f6f98c-fa42-46cf-adaf-95fb6a1e4546/1766987674879.png" style="width: 700%;" class="fr-fic fr-dib"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;一旦 AI 导师出现卡顿，学习心流就会瞬间崩溃。为了把实时性做到极致，他们先在架构上做了彻底解耦&amp;mdash;&amp;mdash;引入 NeRF 与 3D Gaussian Splatting 建模，构建实时驱动框架，将口型、表情、身体动作分离建模；音频不再只是驱动嘴巴，而是在毫秒级联动微表情与姿态变化，让反应真正贴合课堂节奏。&lt;/p&gt;&lt;p&gt;再把性能推到百 FPS 级：系统不再「生成完再播放」，而是云端实时「存活」并持续输出；并实现无需训练的秒级生成，仅凭音频输入即可秒级生成高质量视频，内容生产从「提前制作」走向「实时发生」。&lt;/p&gt;&lt;p&gt;在「一致性」上，核心策略只有一个：锁定人格稳定性。&lt;/p&gt;&lt;p&gt;通过构建跨 ID 动作驱动体系，名师动作可稳定迁移；高精度骨骼提取保证复杂姿态下也不穿模、不崩坏，内容生产效率提升 5 倍。&lt;/p&gt;&lt;section data-pm-slice="1 3 []"&gt;&lt;img data-ratio="0.25" data-s="300,640" data-type="gif" data-w="800" type="block" data-backw="562" data-backh="140" data-imgfileid="503525671" data-src="https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gWibf48GXRwD3ZuKQKiaf1ia7BGwQH62NOLXTjIteUn0mcjjFq0T1Nnd1FLCcZLh4dqbJiaWtZ6OtRxr4w/640?wx_fmt=gif&amp;from=appmsg#imgIndex=6" data-original-style="width: 100%;height: auto !important;" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/8f8774e8-371b-4102-9105-9891d7f56f5c/640.gif" data-order="0" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;基于真人动作捕捉与骨架识别，将手势与姿态精准映射到虚拟导师，实现自然同步的数字人教学演示。&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;最新 6.0 架构进一步把语音、文本、动作、情绪与人物 ID 融合进统一多模态模型调度，即便 45 分钟情绪持续变化，外观一致性与动作分布依然自然如初，音素级口型同步也终于告别「永远对不上的嘴型」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;钢铁骨架：&lt;/strong&gt;&lt;strong&gt;万人并发，1&amp;ndash;1.6s 即时响应&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果说，「师魂」决定教学逻辑，「皮囊」负责感知，那么，能规模化交付的底线只有两个字：不崩。这意味着&lt;strong&gt;高并发、低延迟&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;现实很残酷。传统方案一旦冲到万人并发，端到端延迟往往直接飙到 3 秒以上，交互体验断崖式下滑。而与爱为舞从零搭建的这套「 AI 课堂操作系统」，硬是把 ASR、教学决策、内容生成、TTS、数字人驱动到音视频推流的整条长链路，压缩到了 &lt;strong&gt;1.0&amp;ndash;1.6&amp;nbsp;秒&lt;/strong&gt;&amp;mdash;&amp;mdash;&lt;strong&gt;万人同时在线，依然做到即问即答&lt;/strong&gt;。&lt;/p&gt;&lt;section&gt;&lt;img data-ratio="0.7472222222222222" data-s="300,640" data-type="png" data-w="1080" type="block" data-backw="578" data-backh="432" data-imgfileid="503525738" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9XdQ7EuVQP1r9OHHvXQCLuATVfjnhtLmeUSQaLg4RRj2Yt2Vj0ujKBRMRo2tNt0GDdpnicnSV0iaicQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-original-style="width: 100%;height: auto !important;" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/d9712cf4-bb29-4a1c-9400-54f3a45a2951/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;「爱学」如何真正跑起来？背后一整套工程系统，万人并发下还能实时上课的端到端 AI 教学架构图。&lt;/sup&gt;&lt;/p&gt;&lt;p data-pm-slice="2 2 []"&gt;为了把时间一分一秒抠出来，他们做了几项工程改造。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第一刀，砍在 ASR上&lt;/strong&gt;。通过ASR 预判 + 并行执行，语音识别链路延迟被压到 100ms 级。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第二刀，打断意图秒识&lt;/strong&gt;。基于历史先验的意图识别，判断学员是不是要插话、追问、纠错。整条打断链路，1.6 秒内闭环，不会让 AI 导师「慢半拍」。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;第三刀，落在缓存体系&lt;/strong&gt;，把「每问必算」改成「能不算就不算」。用 Prefill Cache 消灭重复计算，用语义 Cache 复用专家答案，真正把响应时间压到人感知不到的区间。&lt;/p&gt;&lt;p&gt;如果说延迟是体验底线，那成本就是商业底线。&lt;/p&gt;&lt;p&gt;数字人渲染，是典型的 GPU 吞噬型任务。如果一张 GPU 只能服务一两名学员，万人并发意味着服务器成本直接失控。&lt;/p&gt;&lt;p&gt;为此，他们一方面通过GPU 显存全共享，榨干单张 GPU 承载极限。另一方面，通过统一调度「大脑」而非模型堆叠，在万人并发下，对不同形象、不同语音素材进行毫秒级自动分配。&lt;/p&gt;&lt;p&gt;真正难的，其实在运营阶段。当系统跑起来，一万个学员就有一万个进度。学习路径高度碎片化，请求分布不可预测&amp;mdash;&amp;mdash;这是任何 AI 教育系统的「噩梦」。&lt;/p&gt;&lt;p&gt;与爱为舞决定拆解「复杂性」&amp;mdash;&amp;mdash;把教学拆成乐高积木一样的零件，原本「随心所欲」的交互，被重构为「按剧本执行」的自动化指令，复杂调度变得可预测、可收敛。&lt;/p&gt;&lt;p&gt;为对抗「意外中断」，系统还引入可重入（Re-entrant）机制，保证 AI 导师不会「断片儿」，随时恢复状态，陪你学。&lt;/p&gt;&lt;p&gt;在过去不到一年的时间里，像王佳佳这样的变化，并不只发生在安徽阜阳的一间书房。它出现在佳木斯的清晨，也出现在三沙的落日里。有人甚至死磕同一节课12 次、交互 585 次，从「不知道」走到「全部知道」。真人级 AI 导师正在为天南地北的孩子，持续规划各自独一无二的学习路径。&lt;/p&gt;&lt;p&gt;这不只是一次产品能力的展示，更是一种新范式的显形：AI 以导师的身份，进入真实、复杂、对结果高度敏感的学习现场，并稳定发挥作用。它也为中国AI Agent 的规模化落地，定义了一套清晰的范式，甚至走在了世界前列。&lt;/p&gt;&lt;p&gt;当知识的获取不再是刷题、排名与淘汰的赛跑，而是一段被理解、被引导、被尊重的旅程&amp;mdash;&amp;mdash;学习这件事，永远值得投入，也永远值得期待。&lt;/p&gt;]]&gt;</content:encoded>
    </item>
  </channel>
</rss>
