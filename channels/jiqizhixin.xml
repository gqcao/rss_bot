<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:wp="http://wordpress.org/export/1.0/">
  <channel>
    <title>机器之心</title>
    <link>https://www.jiqizhixin.com/</link>
    <description>机器之心</description>
    <language>zh-cn</language>
    <image>
      <url>https://cdn.jiqizhixin.com/assets/logo-324f67bf5f492bd3893d9ad58908e81cb12f7f7f507af266fbfb6e7691ad68e7.png</url>
      <title>机器之心</title>
      <link>https://www.jiqizhixin.com/rss</link>
    </image>
    <item>
      <title>Sebastian Raschka万字年终复盘：2025，属于「推理模型」的一年</title>
      <description>&lt;![CDATA[]]&gt;</description>
      <author>机器之心</author>
      <pubDate>Sat, 03 Jan 2026 00:58:51 +0800</pubDate>
      <link>https://www.jiqizhixin.com/articles/2026-01-03-9</link>
      <guid>https://www.jiqizhixin.com/articles/2026-01-03-9</guid>
      <source>机器之心</source>
      <content:encoded>&lt;![CDATA[&lt;p&gt;&lt;img src="https://image.jiqizhixin.com/uploads/editor/fb81ab4c-43c5-47ad-a6bc-1be0d559462c/1767372891034.png" style="width: 700%;" class="fr-fic fr-dib"&gt;随着2025年的日历翻过最后一页，AI 领域再次证明了预测未来的难度。&lt;/p&gt;&lt;p&gt;在这一年，Scaling Law 并没有失效，但它的战场已经转移：从单纯的参数堆叠转向了推理侧的强化。DeepSeek R1 的横空出世，不仅打破了专有模型的神话，更让 RLVR 和 GRPO 算法成为了年度技术风向标。与此同时，我们在架构上看到了 MoE 与高效注意力机制的收敛，也在行业中目睹了「极限刷榜」带来的评估困境。&lt;/p&gt;&lt;p&gt;著名 AI 教育家与研究员 Sebastian Raschka 在他今年的年度总结中，以其一贯的「硬核工程视角」对 2025 年进行了全面复盘。从 DeepSeek 的成本经济学到推理模型的算法细节，从工具使用的演进到 AI 辅助编程的真实体验，Raschka 不仅梳理了技术脉络，还反思了人与 AI 的协作边界。&lt;/p&gt;&lt;p&gt;以下是 Sebastian Raschka 的博客原文：&lt;/p&gt;&lt;p&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LGfm3xFmTvoajA6OCRy19ia47jib2vmJGLicqiaRRGr1IRgqaBkOwWpMeIQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=1" data-ratio="0.4064814814814815" data-type="png" data-w="1080" data-width="1371" data-height="557" data-imgfileid="503526344" data-aistatus="1" data-original-style="null" data-index="3" src="https://image.jiqizhixin.com/uploads/editor/e0dbd922-153f-4099-b22b-f3d7425d3c25/640.png" alt="图片" data-report-img-idx="1" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p data-pm-slice="0 0 []"&gt;https://magazine.sebastianraschka.com/p/state-of-llms-2025&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;随着 2025 年接近尾声，我想回顾一下大语言模型（LLM）在本年度的一些最重要进展，反思现存的局限性和未解难题，并分享一些关于未来的想法。&lt;/p&gt;&lt;p&gt;正如我每年常说的那样，2025 年对于 LLM 和 AI 来说又是充满变数的一年，而且今年没有迹象表明这种进步正在饱和或放缓。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1、推理之年：RLVR 与 GRPO&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我想探讨的有趣话题很多，让我们按时间顺序从 2025 年 1 月开始说起。&lt;/p&gt;&lt;p&gt;Scaling 仍然有效，但它并没有真正改变 LLM 在实际应用中的表现或感觉（唯一的例外是 OpenAI 刚发布的 o1，它增加了推理轨迹）。因此，当 DeepSeek 在 2025 年 1 月发布 R1 论文，展示了类似推理的行为可以通过强化学习开发出来时，这意义非凡。（在 LLM 的语境下，推理意味着模型会解释其答案，而这种解释本身通常会带来答案准确性的提升。）&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lp4e3meiaLExm6gBeViavozaCaMdyRtxbvhKlAnvPY8aWia1lVwoe0gDZg/640?wx_fmt=png&amp;from=appmsg#imgIndex=2" data-ratio="0.9787037037037037" data-type="png" data-w="1080" data-width="1496" data-height="1464" data-imgfileid="503526345" data-aistatus="1" data-original-style="null" data-index="4" src="https://image.jiqizhixin.com/uploads/editor/c7a7b942-af3e-440c-8ff2-db7d116b57ff/640.png" alt="图片" data-report-img-idx="7" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图 1：一个简短的回答和一个包含中间步骤的更长的回答，后者通常是推理模型生成的。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.1 DeepSeek 时刻&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;DeepSeek R1 因各种原因备受关注：&lt;/p&gt;&lt;p&gt;首先，DeepSeek R1 是作为开放权重模型发布的，其表现非常出色，足以媲美当时最好的专有模型（如 ChatGPT, Gemini 等）。&lt;/p&gt;&lt;p&gt;其次，DeepSeek R1 的论文促使许多人（尤其是投资者和记者）重新审视 2024 年 12 月发布的 DeepSeek V3 论文。这导致了一个修正后的结论：虽然训练最先进的模型仍然昂贵，但其成本可能比之前假设的低一个数量级，估计更接近 500 万美元，而不是 5000 万或 5 亿美元。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LhwBls2CmcZBdRcl9SRGiarXvicNvHicuEwnxyzJHwrXZniafNqsS4s6ibLA/640?wx_fmt=jpeg#imgIndex=3" data-ratio="0.18888888888888888" data-type="png" data-w="1080" data-width="1456" data-height="351" data-croporisrc="https://mmbiz.qlogo.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L5I76ibsxkINcEUXNtPYWYcXkfVsWMAHu2xS3NHiaqianfPFpUDIHjBCuA/0?wx_fmt=png&amp;from=appmsg" data-cropx2="1456" data-cropy1="42.82352941176471" data-cropy2="317.3979238754326" data-imgfileid="503526346" data-aistatus="1" data-original-style="width: 578px;height: 109px;" data-index="5" src="https://image.jiqizhixin.com/uploads/editor/35c9c5f4-a0ee-4ae7-971a-925272beec72/640.png" alt="图片" data-report-img-idx="2" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 2：来自 DeepSeek V3 论文 的表格，估计训练 6710 亿参数 DeepSeek V3 模型的成本。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;DeepSeek R1 的补充材料估计，在 DeepSeek V3 基础上训练 R1 模型的成本仅需额外的 29.4 万美元，这再次远低于所有人的预期。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LypJzibIqWMIibn7jhS6gXUEqMC3mHKJCPlBoN9AK9zJCQBhfGaDdCCFQ/640?wx_fmt=png&amp;from=appmsg#imgIndex=4" data-ratio="0.21203703703703702" data-type="png" data-w="1080" data-width="1358" data-height="288" data-imgfileid="503526348" data-aistatus="1" data-original-style="null" data-index="6" src="https://image.jiqizhixin.com/uploads/editor/d993acdd-ee64-4cce-ad31-a8b16fac9840/640.png" alt="图片" data-report-img-idx="9" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 3：来自 DeepSeek R1 论文补充材料的表格，估计在 DeepSeek V3 基础上训练 R1 模型的成本。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;当然，关于 500 万美元的估算有许多注意事项。例如，它仅涵盖了最终模型运行的算力信用成本，并未计入研究人员的薪水以及与超参数调整和实验相关的其他开发成本。&lt;/p&gt;&lt;p&gt;第三，也是最有趣的一点，该论文提出了带有可验证奖励的强化学习 (RLVR) 配合 GRPO 算法，作为一种新的（或至少是改进的）算法方法，用于开发所谓的推理模型并在后训练阶段改进 LLM。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4LRTPlWqqflKSwPibDODO19NNYk5dmDOxSicQRzVnofHvZhc3C42lQbmcw/640?wx_fmt=png&amp;from=appmsg#imgIndex=5" data-ratio="0.875" data-type="png" data-w="1080" data-width="1456" data-height="1274" data-imgfileid="503526349" data-aistatus="1" data-original-style="null" data-index="7" src="https://image.jiqizhixin.com/uploads/editor/f4f8531a-ed73-4938-8c52-a7ea832864cf/640.png" alt="图片" data-report-img-idx="5" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 4：强化学习应用的广泛概述及其时机。在这一概述中，我跳过了许多细节，但有兴趣的读者可以在我的《LLMs 推理的强化学习现状》一文中阅读更多内容。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;在此之前，像监督指令微调 (SFT) 和基于人类反馈的强化学习 (RLHF) 这样的后训练方法（它们仍然是训练流程的重要组成部分）一直受限于昂贵的书面回复或偏好标签。（当然，人们也可以用其他 LLM 合成生成这些数据，但这有点像「先有鸡还是先有蛋」的问题。）&lt;/p&gt;&lt;p&gt;DeepSeek R1 和 RLVR 的重要性在于，它们允许我们在大量数据上对 LLM 进行后训练，这使它们成为通过在后训练期间扩展算力来改进和解锁能力的绝佳候选者（假设有可用的算力预算）。&lt;/p&gt;&lt;p&gt;RLVR 中的 V 代表「可验证」，意味着我们可以使用确定性方法来分配正确性标签，而这些标签足以让 LLM 学习复杂的问题解决能力。（典型的类别是数学和代码，但也有可能将此想法扩展到其他领域。）&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4L8qmOyFvp2nZibke3IPXUx629WVtUMFHEbTtFzJNDlcYImjx79XVWA2Q/640?wx_fmt=png&amp;from=appmsg#imgIndex=6" data-ratio="0.5361111111111111" data-type="png" data-w="1080" data-width="1082" data-height="580" data-imgfileid="503526350" data-aistatus="1" data-original-style="null" data-index="8" src="https://image.jiqizhixin.com/uploads/editor/e6db5b0d-853f-4f20-96a2-30fc906a7fa1/640.png" alt="图片" data-report-img-idx="6" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 图5：可验证奖励的一个简单示例。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;我不想在这里过于纠结技术细节，因为我想在这篇年度回顾文章中涵盖其他方面。关于推理 LLM 和 RLVR，完全可以写整篇文章或整本书。例如，如果您有兴趣了解更多，可以查看我之前的文章。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;综上所述，结论是：今年的 LLM 发展本质上是由使用 RLVR 和 GRPO 的推理模型主导的。 基本上，继 DeepSeek R1 之后，每一个主要的开放权重或专有 LLM 开发商都发布了其模型的推理（通常称为「思考/Thinking」）变体。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.2 LLM 关注重点&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果我要简洁地总结每一年 LLM 开发的关注重点（除了单纯扩展架构和预训练算力之外），我的列表会是这样的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;2022: RLHF + PPO&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;2023: LoRA SFT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;2024: 中期训练 (Mid-Training)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;2025: RLVR + GRPO&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;预训练仍然是一切的必要基础。除此之外，RLHF（通过 PPO 算法）当然是早在 2022 年带来最初 ChatGPT 模型的功臣。&lt;/p&gt;&lt;p&gt;在 2023 年，重点大量集中在 LoRA 和类 LoRA 的参数高效微调技术上，用于训练小型自定义 LLM。&lt;/p&gt;&lt;section data-pm-slice="0 0 []"&gt;&lt;img data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9fn3Ku1BLjJMJA5E8HHQ4Lb3le591ibg3TBNr3SsxooOTJu42nl8Lc9nulFDXquv80XvRzVskk4rA/640?wx_fmt=png&amp;from=appmsg#imgIndex=7" data-ratio="0.36018518518518516" data-type="png" data-w="1080" data-width="1456" data-height="524" data-imgfileid="503526351" data-aistatus="1" data-original-style="null" data-index="9" src="https://image.jiqizhixin.com/uploads/editor/9488821f-174a-4165-a48f-4b6b461d8c6e/640.png" alt="图片" data-report-img-idx="3" data-fail="0" class="fr-fic fr-dib" style="width: 700%;"&gt;&lt;/section&gt;&lt;p&gt;&lt;sup&gt;图 6：近年来专有和开源权重 LLM 开发的一些关注领域。请注意，这是累积性的，意味着例如 RLHF + PPO 仍然相关且被使用。然而，它已不再是讨论的热点话题。&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;接着，在 2024 年，所有主要实验室开始通过关注合成数据、优化数据混合、使用特定领域数据以及增加专门的长上下文训练阶段，使其（预）训练流程更加复杂。我在当时的 2024 年文章中总结了这些不同的方法（当时我将这些技术归类为预训练，因为「中期训练」这个术语当时还没被创造出来）：&lt;/p&gt;&lt;p&gt;当时，我认为这些是预训练技术，因为它们使用相同的预训练算法和目标。今天，这些紧随常规通用数据预训练之后的、稍微更专业化的预训练阶段，通常被称为「中期训练」（作为常规预训练和包括 SFT、RLHF 以及现在的 RLVR 在内的后训练之间的桥梁）。&lt;/p&gt;&lt;p&gt;那么，你可能会问，接下来是什么？&lt;/p&gt;&lt;p&gt;我认为明年我们会看到对 RLVR 的（更多）关注。目前，RLVR 主要应用于数学和代码领域。 下一个合乎逻辑的步骤是，不仅使用最终答案的正确性作为奖励信号，还要在 RLVR 训练期间评判 LLM 的解释。这在过去多年里一直以「过程奖励模型」的研究标签存在。然而，它尚未取得超级成功。例如，引用 DeepSeek R1 论文：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;4.2. 不成功的尝试 [...] 总之，虽然 PRM 展示了良好的能力来对模型生成的前 N 个响应进行重新排序或辅助引导搜索 (Snell et al., 2024)，但在我们的实验中，与其在大规模强化学习过程中引入的额外计算开销相比，其优势是有限的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;然而，看看上个月发布的最新 DeepSeekMath-V2 论文（我在之前的文章《从 DeepSeek V3 到 V3.2：架构、稀疏注意力和 RL 更新》中讨论过），我认为未来我们会看到更多将「解释评分」作为训练信号的做法。&l